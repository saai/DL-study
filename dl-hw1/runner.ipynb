{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver import Solver\n",
    "from visualize import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.5991\t Accuracy 0.0300\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 1.8743\t Accuracy 0.6500\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 1.8826\t Accuracy 0.6600\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 1.8876\t Accuracy 0.7100\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 1.8474\t Accuracy 0.7900\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 1.8802\t Accuracy 0.7400\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 1.8343\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 1.8876\t Accuracy 0.7400\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 1.8522\t Accuracy 0.8300\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 1.8636\t Accuracy 0.7800\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 1.8493\t Accuracy 0.7900\n",
      "\n",
      "Epoch [0]\t Average training loss 1.8781\t Average training accuracy 0.7288\n",
      "Epoch [0]\t Average validation loss 1.8172\t Average validation accuracy 0.8468\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 1.8801\t Accuracy 0.7600\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 1.8293\t Accuracy 0.8200\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 1.8130\t Accuracy 0.8400\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 1.8364\t Accuracy 0.7800\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 1.8588\t Accuracy 0.8100\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 1.8759\t Accuracy 0.8300\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 1.8611\t Accuracy 0.8100\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 1.8163\t Accuracy 0.8300\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 1.8709\t Accuracy 0.7800\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 1.8888\t Accuracy 0.7800\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 1.8319\t Accuracy 0.8200\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8371\t Average training accuracy 0.8216\n",
      "Epoch [1]\t Average validation loss 1.8064\t Average validation accuracy 0.8602\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 1.8371\t Accuracy 0.8500\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 1.8253\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 1.8553\t Accuracy 0.8500\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 1.8292\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 1.7594\t Accuracy 0.8300\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 1.8321\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 1.8519\t Accuracy 0.8100\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 1.8198\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 1.8493\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 1.8064\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 1.8389\t Accuracy 0.8500\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8307\t Average training accuracy 0.8347\n",
      "Epoch [2]\t Average validation loss 1.8069\t Average validation accuracy 0.8774\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 1.8444\t Accuracy 0.8200\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 1.8456\t Accuracy 0.8500\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 1.7978\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 1.7934\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 1.8288\t Accuracy 0.8700\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 1.7881\t Accuracy 0.8500\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 1.8406\t Accuracy 0.8100\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 1.8088\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 1.8186\t Accuracy 0.8500\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 1.8748\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 1.7984\t Accuracy 0.8500\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8275\t Average training accuracy 0.8408\n",
      "Epoch [3]\t Average validation loss 1.8041\t Average validation accuracy 0.8732\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 1.8316\t Accuracy 0.8200\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 1.8231\t Accuracy 0.8600\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 1.8134\t Accuracy 0.8400\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 1.8119\t Accuracy 0.8700\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 1.7950\t Accuracy 0.8400\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 1.8429\t Accuracy 0.8700\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 1.8216\t Accuracy 0.8700\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 1.8595\t Accuracy 0.8300\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 1.8388\t Accuracy 0.8300\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 1.8299\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 1.7879\t Accuracy 0.8500\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8255\t Average training accuracy 0.8436\n",
      "Epoch [4]\t Average validation loss 1.7998\t Average validation accuracy 0.8740\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 1.8439\t Accuracy 0.8300\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 1.8045\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 1.8234\t Accuracy 0.8100\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 1.8112\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 1.7749\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 1.8128\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 1.8133\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 1.8480\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 1.8626\t Accuracy 0.7700\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 1.8157\t Accuracy 0.8300\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 1.8456\t Accuracy 0.8300\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8244\t Average training accuracy 0.8441\n",
      "Epoch [5]\t Average validation loss 1.7996\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 1.8621\t Accuracy 0.7600\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 1.8498\t Accuracy 0.8100\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 1.7979\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 1.8449\t Accuracy 0.8100\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 1.8510\t Accuracy 0.8300\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 1.7723\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 1.8212\t Accuracy 0.8300\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 1.8149\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 1.8343\t Accuracy 0.8100\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 1.8239\t Accuracy 0.8600\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 1.8207\t Accuracy 0.8600\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8233\t Average training accuracy 0.8465\n",
      "Epoch [6]\t Average validation loss 1.7972\t Average validation accuracy 0.8806\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 1.8146\t Accuracy 0.8500\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 1.8316\t Accuracy 0.8400\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 1.8199\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 1.7928\t Accuracy 0.8800\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 1.8402\t Accuracy 0.8600\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 1.8384\t Accuracy 0.8600\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 1.8305\t Accuracy 0.8200\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 1.8222\t Accuracy 0.8400\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 1.8328\t Accuracy 0.8400\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 1.7937\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 1.8506\t Accuracy 0.8500\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8224\t Average training accuracy 0.8460\n",
      "Epoch [7]\t Average validation loss 1.7999\t Average validation accuracy 0.8770\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 1.8087\t Accuracy 0.8600\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 1.8541\t Accuracy 0.8300\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 1.8478\t Accuracy 0.7900\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 1.8202\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 1.8104\t Accuracy 0.8300\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 1.8062\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 1.8063\t Accuracy 0.8300\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 1.8265\t Accuracy 0.7800\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 1.8175\t Accuracy 0.8700\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 1.7840\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 1.7822\t Accuracy 0.8600\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8220\t Average training accuracy 0.8467\n",
      "Epoch [8]\t Average validation loss 1.7985\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 1.8319\t Accuracy 0.8300\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 1.7919\t Accuracy 0.8700\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 1.7923\t Accuracy 0.8700\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 1.8313\t Accuracy 0.8300\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 1.8037\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 1.8533\t Accuracy 0.8100\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 1.7987\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 1.8058\t Accuracy 0.8700\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 1.8163\t Accuracy 0.8500\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 1.8336\t Accuracy 0.8600\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 1.8110\t Accuracy 0.8600\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8215\t Average training accuracy 0.8481\n",
      "Epoch [9]\t Average validation loss 1.7968\t Average validation accuracy 0.8774\n",
      "\n",
      "Epoch [0][10]\t Batch [0][275]\t Training Loss 2.4782\t Accuracy 0.0950\n",
      "Epoch [0][10]\t Batch [50][275]\t Training Loss 1.8999\t Accuracy 0.6250\n",
      "Epoch [0][10]\t Batch [100][275]\t Training Loss 1.8848\t Accuracy 0.7050\n",
      "Epoch [0][10]\t Batch [150][275]\t Training Loss 1.8701\t Accuracy 0.7400\n",
      "Epoch [0][10]\t Batch [200][275]\t Training Loss 1.8421\t Accuracy 0.8250\n",
      "Epoch [0][10]\t Batch [250][275]\t Training Loss 1.8323\t Accuracy 0.7900\n",
      "\n",
      "Epoch [0]\t Average training loss 1.9003\t Average training accuracy 0.6860\n",
      "Epoch [0]\t Average validation loss 1.8263\t Average validation accuracy 0.8276\n",
      "\n",
      "Epoch [1][10]\t Batch [0][275]\t Training Loss 1.8182\t Accuracy 0.8300\n",
      "Epoch [1][10]\t Batch [50][275]\t Training Loss 1.8457\t Accuracy 0.8350\n",
      "Epoch [1][10]\t Batch [100][275]\t Training Loss 1.8544\t Accuracy 0.8100\n",
      "Epoch [1][10]\t Batch [150][275]\t Training Loss 1.8193\t Accuracy 0.8300\n",
      "Epoch [1][10]\t Batch [200][275]\t Training Loss 1.8354\t Accuracy 0.8250\n",
      "Epoch [1][10]\t Batch [250][275]\t Training Loss 1.8416\t Accuracy 0.7900\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8461\t Average training accuracy 0.8040\n",
      "Epoch [1]\t Average validation loss 1.8158\t Average validation accuracy 0.8514\n",
      "\n",
      "Epoch [2][10]\t Batch [0][275]\t Training Loss 1.8547\t Accuracy 0.7950\n",
      "Epoch [2][10]\t Batch [50][275]\t Training Loss 1.8383\t Accuracy 0.7950\n",
      "Epoch [2][10]\t Batch [100][275]\t Training Loss 1.8290\t Accuracy 0.8450\n",
      "Epoch [2][10]\t Batch [150][275]\t Training Loss 1.8298\t Accuracy 0.8450\n",
      "Epoch [2][10]\t Batch [200][275]\t Training Loss 1.8366\t Accuracy 0.8150\n",
      "Epoch [2][10]\t Batch [250][275]\t Training Loss 1.8380\t Accuracy 0.8100\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8377\t Average training accuracy 0.8216\n",
      "Epoch [2]\t Average validation loss 1.8145\t Average validation accuracy 0.8622\n",
      "\n",
      "Epoch [3][10]\t Batch [0][275]\t Training Loss 1.8451\t Accuracy 0.8550\n",
      "Epoch [3][10]\t Batch [50][275]\t Training Loss 1.8637\t Accuracy 0.7800\n",
      "Epoch [3][10]\t Batch [100][275]\t Training Loss 1.8223\t Accuracy 0.8300\n",
      "Epoch [3][10]\t Batch [150][275]\t Training Loss 1.8406\t Accuracy 0.8350\n",
      "Epoch [3][10]\t Batch [200][275]\t Training Loss 1.8425\t Accuracy 0.8450\n",
      "Epoch [3][10]\t Batch [250][275]\t Training Loss 1.8633\t Accuracy 0.7900\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8337\t Average training accuracy 0.8293\n",
      "Epoch [3]\t Average validation loss 1.8068\t Average validation accuracy 0.8712\n",
      "\n",
      "Epoch [4][10]\t Batch [0][275]\t Training Loss 1.8211\t Accuracy 0.8750\n",
      "Epoch [4][10]\t Batch [50][275]\t Training Loss 1.8586\t Accuracy 0.7900\n",
      "Epoch [4][10]\t Batch [100][275]\t Training Loss 1.8355\t Accuracy 0.8200\n",
      "Epoch [4][10]\t Batch [150][275]\t Training Loss 1.8292\t Accuracy 0.8250\n",
      "Epoch [4][10]\t Batch [200][275]\t Training Loss 1.8053\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [250][275]\t Training Loss 1.8486\t Accuracy 0.7900\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8311\t Average training accuracy 0.8335\n",
      "Epoch [4]\t Average validation loss 1.8082\t Average validation accuracy 0.8708\n",
      "\n",
      "Epoch [5][10]\t Batch [0][275]\t Training Loss 1.8480\t Accuracy 0.8300\n",
      "Epoch [5][10]\t Batch [50][275]\t Training Loss 1.8234\t Accuracy 0.8150\n",
      "Epoch [5][10]\t Batch [100][275]\t Training Loss 1.8252\t Accuracy 0.8550\n",
      "Epoch [5][10]\t Batch [150][275]\t Training Loss 1.8535\t Accuracy 0.7500\n",
      "Epoch [5][10]\t Batch [200][275]\t Training Loss 1.8279\t Accuracy 0.8600\n",
      "Epoch [5][10]\t Batch [250][275]\t Training Loss 1.7905\t Accuracy 0.8700\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8287\t Average training accuracy 0.8381\n",
      "Epoch [5]\t Average validation loss 1.8090\t Average validation accuracy 0.8764\n",
      "\n",
      "Epoch [6][10]\t Batch [0][275]\t Training Loss 1.8393\t Accuracy 0.8550\n",
      "Epoch [6][10]\t Batch [50][275]\t Training Loss 1.8062\t Accuracy 0.9050\n",
      "Epoch [6][10]\t Batch [100][275]\t Training Loss 1.8047\t Accuracy 0.8600\n",
      "Epoch [6][10]\t Batch [150][275]\t Training Loss 1.8194\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [200][275]\t Training Loss 1.7994\t Accuracy 0.8500\n",
      "Epoch [6][10]\t Batch [250][275]\t Training Loss 1.8367\t Accuracy 0.8200\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8276\t Average training accuracy 0.8391\n",
      "Epoch [6]\t Average validation loss 1.8055\t Average validation accuracy 0.8760\n",
      "\n",
      "Epoch [7][10]\t Batch [0][275]\t Training Loss 1.8469\t Accuracy 0.8150\n",
      "Epoch [7][10]\t Batch [50][275]\t Training Loss 1.8370\t Accuracy 0.7900\n",
      "Epoch [7][10]\t Batch [100][275]\t Training Loss 1.8089\t Accuracy 0.8600\n",
      "Epoch [7][10]\t Batch [150][275]\t Training Loss 1.8374\t Accuracy 0.8600\n",
      "Epoch [7][10]\t Batch [200][275]\t Training Loss 1.8162\t Accuracy 0.8400\n",
      "Epoch [7][10]\t Batch [250][275]\t Training Loss 1.8086\t Accuracy 0.8600\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8264\t Average training accuracy 0.8426\n",
      "Epoch [7]\t Average validation loss 1.8049\t Average validation accuracy 0.8800\n",
      "\n",
      "Epoch [8][10]\t Batch [0][275]\t Training Loss 1.7949\t Accuracy 0.8950\n",
      "Epoch [8][10]\t Batch [50][275]\t Training Loss 1.8247\t Accuracy 0.8400\n",
      "Epoch [8][10]\t Batch [100][275]\t Training Loss 1.8536\t Accuracy 0.8450\n",
      "Epoch [8][10]\t Batch [150][275]\t Training Loss 1.8255\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [200][275]\t Training Loss 1.8082\t Accuracy 0.8550\n",
      "Epoch [8][10]\t Batch [250][275]\t Training Loss 1.8019\t Accuracy 0.8700\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8255\t Average training accuracy 0.8436\n",
      "Epoch [8]\t Average validation loss 1.7967\t Average validation accuracy 0.8800\n",
      "\n",
      "Epoch [9][10]\t Batch [0][275]\t Training Loss 1.8384\t Accuracy 0.8150\n",
      "Epoch [9][10]\t Batch [50][275]\t Training Loss 1.8448\t Accuracy 0.8250\n",
      "Epoch [9][10]\t Batch [100][275]\t Training Loss 1.8011\t Accuracy 0.8600\n",
      "Epoch [9][10]\t Batch [150][275]\t Training Loss 1.8341\t Accuracy 0.8150\n",
      "Epoch [9][10]\t Batch [200][275]\t Training Loss 1.8287\t Accuracy 0.8550\n",
      "Epoch [9][10]\t Batch [250][275]\t Training Loss 1.8193\t Accuracy 0.8800\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8247\t Average training accuracy 0.8450\n",
      "Epoch [9]\t Average validation loss 1.8002\t Average validation accuracy 0.8780\n",
      "\n",
      "Epoch [0][10]\t Batch [0][110]\t Training Loss 2.4717\t Accuracy 0.1320\n",
      "Epoch [0][10]\t Batch [50][110]\t Training Loss 1.9157\t Accuracy 0.6100\n",
      "Epoch [0][10]\t Batch [100][110]\t Training Loss 1.8703\t Accuracy 0.7220\n",
      "\n",
      "Epoch [0]\t Average training loss 1.9574\t Average training accuracy 0.5557\n",
      "Epoch [0]\t Average validation loss 1.8540\t Average validation accuracy 0.7504\n",
      "\n",
      "Epoch [1][10]\t Batch [0][110]\t Training Loss 1.8789\t Accuracy 0.7340\n",
      "Epoch [1][10]\t Batch [50][110]\t Training Loss 1.8848\t Accuracy 0.7580\n",
      "Epoch [1][10]\t Batch [100][110]\t Training Loss 1.8561\t Accuracy 0.7640\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8689\t Average training accuracy 0.7419\n",
      "Epoch [1]\t Average validation loss 1.8329\t Average validation accuracy 0.8070\n",
      "\n",
      "Epoch [2][10]\t Batch [0][110]\t Training Loss 1.8542\t Accuracy 0.7660\n",
      "Epoch [2][10]\t Batch [50][110]\t Training Loss 1.8506\t Accuracy 0.7780\n",
      "Epoch [2][10]\t Batch [100][110]\t Training Loss 1.8700\t Accuracy 0.7740\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8552\t Average training accuracy 0.7778\n",
      "Epoch [2]\t Average validation loss 1.8215\t Average validation accuracy 0.8306\n",
      "\n",
      "Epoch [3][10]\t Batch [0][110]\t Training Loss 1.8523\t Accuracy 0.7820\n",
      "Epoch [3][10]\t Batch [50][110]\t Training Loss 1.8494\t Accuracy 0.8020\n",
      "Epoch [3][10]\t Batch [100][110]\t Training Loss 1.8486\t Accuracy 0.8020\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8476\t Average training accuracy 0.7969\n",
      "Epoch [3]\t Average validation loss 1.8192\t Average validation accuracy 0.8480\n",
      "\n",
      "Epoch [4][10]\t Batch [0][110]\t Training Loss 1.8413\t Accuracy 0.8360\n",
      "Epoch [4][10]\t Batch [50][110]\t Training Loss 1.8498\t Accuracy 0.8020\n",
      "Epoch [4][10]\t Batch [100][110]\t Training Loss 1.8409\t Accuracy 0.7940\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8429\t Average training accuracy 0.8069\n",
      "Epoch [4]\t Average validation loss 1.8160\t Average validation accuracy 0.8574\n",
      "\n",
      "Epoch [5][10]\t Batch [0][110]\t Training Loss 1.8297\t Accuracy 0.8280\n",
      "Epoch [5][10]\t Batch [50][110]\t Training Loss 1.8507\t Accuracy 0.7940\n",
      "Epoch [5][10]\t Batch [100][110]\t Training Loss 1.8227\t Accuracy 0.8440\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8393\t Average training accuracy 0.8154\n",
      "Epoch [5]\t Average validation loss 1.8141\t Average validation accuracy 0.8604\n",
      "\n",
      "Epoch [6][10]\t Batch [0][110]\t Training Loss 1.8428\t Accuracy 0.8060\n",
      "Epoch [6][10]\t Batch [50][110]\t Training Loss 1.8262\t Accuracy 0.8160\n",
      "Epoch [6][10]\t Batch [100][110]\t Training Loss 1.8208\t Accuracy 0.8260\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8373\t Average training accuracy 0.8207\n",
      "Epoch [6]\t Average validation loss 1.8099\t Average validation accuracy 0.8640\n",
      "\n",
      "Epoch [7][10]\t Batch [0][110]\t Training Loss 1.8297\t Accuracy 0.8580\n",
      "Epoch [7][10]\t Batch [50][110]\t Training Loss 1.8413\t Accuracy 0.8100\n",
      "Epoch [7][10]\t Batch [100][110]\t Training Loss 1.8108\t Accuracy 0.8580\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8352\t Average training accuracy 0.8256\n",
      "Epoch [7]\t Average validation loss 1.8067\t Average validation accuracy 0.8686\n",
      "\n",
      "Epoch [8][10]\t Batch [0][110]\t Training Loss 1.8358\t Accuracy 0.8200\n",
      "Epoch [8][10]\t Batch [50][110]\t Training Loss 1.8381\t Accuracy 0.8200\n",
      "Epoch [8][10]\t Batch [100][110]\t Training Loss 1.8458\t Accuracy 0.8160\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8334\t Average training accuracy 0.8281\n",
      "Epoch [8]\t Average validation loss 1.8059\t Average validation accuracy 0.8680\n",
      "\n",
      "Epoch [9][10]\t Batch [0][110]\t Training Loss 1.8230\t Accuracy 0.8420\n",
      "Epoch [9][10]\t Batch [50][110]\t Training Loss 1.8418\t Accuracy 0.8160\n",
      "Epoch [9][10]\t Batch [100][110]\t Training Loss 1.8448\t Accuracy 0.8180\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8317\t Average training accuracy 0.8311\n",
      "Epoch [9]\t Average validation loss 1.8062\t Average validation accuracy 0.8718\n",
      "\n",
      "Epoch [0][10]\t Batch [0][55]\t Training Loss 2.4725\t Accuracy 0.1080\n",
      "Epoch [0][10]\t Batch [50][55]\t Training Loss 1.9154\t Accuracy 0.6190\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0318\t Average training accuracy 0.4373\n",
      "Epoch [0]\t Average validation loss 1.8870\t Average validation accuracy 0.6766\n",
      "\n",
      "Epoch [1][10]\t Batch [0][55]\t Training Loss 1.9160\t Accuracy 0.6430\n",
      "Epoch [1][10]\t Batch [50][55]\t Training Loss 1.8773\t Accuracy 0.7250\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8876\t Average training accuracy 0.6832\n",
      "Epoch [1]\t Average validation loss 1.8506\t Average validation accuracy 0.7638\n",
      "\n",
      "Epoch [2][10]\t Batch [0][55]\t Training Loss 1.8734\t Accuracy 0.7330\n",
      "Epoch [2][10]\t Batch [50][55]\t Training Loss 1.8579\t Accuracy 0.7380\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8706\t Average training accuracy 0.7355\n",
      "Epoch [2]\t Average validation loss 1.8374\t Average validation accuracy 0.7982\n",
      "\n",
      "Epoch [3][10]\t Batch [0][55]\t Training Loss 1.8647\t Accuracy 0.7450\n",
      "Epoch [3][10]\t Batch [50][55]\t Training Loss 1.8655\t Accuracy 0.7670\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8610\t Average training accuracy 0.7627\n",
      "Epoch [3]\t Average validation loss 1.8307\t Average validation accuracy 0.8144\n",
      "\n",
      "Epoch [4][10]\t Batch [0][55]\t Training Loss 1.8552\t Accuracy 0.7640\n",
      "Epoch [4][10]\t Batch [50][55]\t Training Loss 1.8457\t Accuracy 0.7730\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8553\t Average training accuracy 0.7791\n",
      "Epoch [4]\t Average validation loss 1.8262\t Average validation accuracy 0.8268\n",
      "\n",
      "Epoch [5][10]\t Batch [0][55]\t Training Loss 1.8547\t Accuracy 0.7830\n",
      "Epoch [5][10]\t Batch [50][55]\t Training Loss 1.8637\t Accuracy 0.7770\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8517\t Average training accuracy 0.7893\n",
      "Epoch [5]\t Average validation loss 1.8218\t Average validation accuracy 0.8366\n",
      "\n",
      "Epoch [6][10]\t Batch [0][55]\t Training Loss 1.8628\t Accuracy 0.7700\n",
      "Epoch [6][10]\t Batch [50][55]\t Training Loss 1.8325\t Accuracy 0.8190\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8480\t Average training accuracy 0.7982\n",
      "Epoch [6]\t Average validation loss 1.8192\t Average validation accuracy 0.8416\n",
      "\n",
      "Epoch [7][10]\t Batch [0][55]\t Training Loss 1.8408\t Accuracy 0.8020\n",
      "Epoch [7][10]\t Batch [50][55]\t Training Loss 1.8527\t Accuracy 0.7900\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8455\t Average training accuracy 0.8054\n",
      "Epoch [7]\t Average validation loss 1.8161\t Average validation accuracy 0.8462\n",
      "\n",
      "Epoch [8][10]\t Batch [0][55]\t Training Loss 1.8399\t Accuracy 0.8010\n",
      "Epoch [8][10]\t Batch [50][55]\t Training Loss 1.8455\t Accuracy 0.8050\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8430\t Average training accuracy 0.8099\n",
      "Epoch [8]\t Average validation loss 1.8152\t Average validation accuracy 0.8502\n",
      "\n",
      "Epoch [9][10]\t Batch [0][55]\t Training Loss 1.8386\t Accuracy 0.8090\n",
      "Epoch [9][10]\t Batch [50][55]\t Training Loss 1.8398\t Accuracy 0.8070\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8415\t Average training accuracy 0.8143\n",
      "Epoch [9]\t Average validation loss 1.8139\t Average validation accuracy 0.8534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train without momentum\n",
    "cfg1 = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.005,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "runner1 = Solver(cfg1)\n",
    "loss1, acc1 = runner1.train()\n",
    "cfg2 = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 200,\n",
    "    'learning_rate': 0.005,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "runner2 = Solver(cfg2)\n",
    "loss2, acc2 = runner2.train()\n",
    "\n",
    "cfg3 = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 500,\n",
    "    'learning_rate': 0.005,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "runner3 = Solver(cfg3)\n",
    "loss3, acc3 = runner3.train()\n",
    "\n",
    "cfg4 = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 1000,\n",
    "    'learning_rate': 0.005,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner4 = Solver(cfg4)\n",
    "loss4, acc4 = runner4.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][275]\t Training Loss 2.4552\t Accuracy 0.1200\n",
      "Epoch [0][10]\t Batch [50][275]\t Training Loss 1.8679\t Accuracy 0.6900\n",
      "Epoch [0][10]\t Batch [100][275]\t Training Loss 1.8610\t Accuracy 0.7250\n",
      "Epoch [0][10]\t Batch [150][275]\t Training Loss 1.8227\t Accuracy 0.7950\n",
      "Epoch [0][10]\t Batch [200][275]\t Training Loss 1.8846\t Accuracy 0.7650\n",
      "Epoch [0][10]\t Batch [250][275]\t Training Loss 1.8731\t Accuracy 0.7600\n",
      "\n",
      "Epoch [0]\t Average training loss 1.9007\t Average training accuracy 0.6800\n",
      "Epoch [0]\t Average validation loss 1.8273\t Average validation accuracy 0.8280\n",
      "\n",
      "Epoch [1][10]\t Batch [0][275]\t Training Loss 1.8714\t Accuracy 0.7650\n",
      "Epoch [1][10]\t Batch [50][275]\t Training Loss 1.8616\t Accuracy 0.8300\n",
      "Epoch [1][10]\t Batch [100][275]\t Training Loss 1.8536\t Accuracy 0.7900\n",
      "Epoch [1][10]\t Batch [150][275]\t Training Loss 1.8036\t Accuracy 0.8750\n",
      "Epoch [1][10]\t Batch [200][275]\t Training Loss 1.8505\t Accuracy 0.7900\n",
      "Epoch [1][10]\t Batch [250][275]\t Training Loss 1.8266\t Accuracy 0.8700\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8462\t Average training accuracy 0.8052\n",
      "Epoch [1]\t Average validation loss 1.8166\t Average validation accuracy 0.8570\n",
      "\n",
      "Epoch [2][10]\t Batch [0][275]\t Training Loss 1.8374\t Accuracy 0.7950\n",
      "Epoch [2][10]\t Batch [50][275]\t Training Loss 1.8262\t Accuracy 0.8250\n",
      "Epoch [2][10]\t Batch [100][275]\t Training Loss 1.8478\t Accuracy 0.8100\n",
      "Epoch [2][10]\t Batch [150][275]\t Training Loss 1.8471\t Accuracy 0.8400\n",
      "Epoch [2][10]\t Batch [200][275]\t Training Loss 1.8601\t Accuracy 0.7800\n",
      "Epoch [2][10]\t Batch [250][275]\t Training Loss 1.8326\t Accuracy 0.8750\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8383\t Average training accuracy 0.8241\n",
      "Epoch [2]\t Average validation loss 1.8075\t Average validation accuracy 0.8642\n",
      "\n",
      "Epoch [3][10]\t Batch [0][275]\t Training Loss 1.8108\t Accuracy 0.8750\n",
      "Epoch [3][10]\t Batch [50][275]\t Training Loss 1.8237\t Accuracy 0.8300\n",
      "Epoch [3][10]\t Batch [100][275]\t Training Loss 1.8445\t Accuracy 0.7650\n",
      "Epoch [3][10]\t Batch [150][275]\t Training Loss 1.8108\t Accuracy 0.8600\n",
      "Epoch [3][10]\t Batch [200][275]\t Training Loss 1.8126\t Accuracy 0.8650\n",
      "Epoch [3][10]\t Batch [250][275]\t Training Loss 1.8294\t Accuracy 0.8300\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8332\t Average training accuracy 0.8319\n",
      "Epoch [3]\t Average validation loss 1.8148\t Average validation accuracy 0.8702\n",
      "\n",
      "Epoch [4][10]\t Batch [0][275]\t Training Loss 1.8433\t Accuracy 0.8050\n",
      "Epoch [4][10]\t Batch [50][275]\t Training Loss 1.8541\t Accuracy 0.8300\n",
      "Epoch [4][10]\t Batch [100][275]\t Training Loss 1.8129\t Accuracy 0.8600\n",
      "Epoch [4][10]\t Batch [150][275]\t Training Loss 1.8214\t Accuracy 0.8500\n",
      "Epoch [4][10]\t Batch [200][275]\t Training Loss 1.8093\t Accuracy 0.8450\n",
      "Epoch [4][10]\t Batch [250][275]\t Training Loss 1.8537\t Accuracy 0.8450\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8313\t Average training accuracy 0.8374\n",
      "Epoch [4]\t Average validation loss 1.8094\t Average validation accuracy 0.8726\n",
      "\n",
      "Epoch [5][10]\t Batch [0][275]\t Training Loss 1.8457\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [50][275]\t Training Loss 1.8537\t Accuracy 0.8250\n",
      "Epoch [5][10]\t Batch [100][275]\t Training Loss 1.8327\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [150][275]\t Training Loss 1.8484\t Accuracy 0.8300\n",
      "Epoch [5][10]\t Batch [200][275]\t Training Loss 1.8457\t Accuracy 0.8300\n",
      "Epoch [5][10]\t Batch [250][275]\t Training Loss 1.8430\t Accuracy 0.8550\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8291\t Average training accuracy 0.8407\n",
      "Epoch [5]\t Average validation loss 1.8067\t Average validation accuracy 0.8772\n",
      "\n",
      "Epoch [6][10]\t Batch [0][275]\t Training Loss 1.8512\t Accuracy 0.7950\n",
      "Epoch [6][10]\t Batch [50][275]\t Training Loss 1.8206\t Accuracy 0.8650\n",
      "Epoch [6][10]\t Batch [100][275]\t Training Loss 1.7995\t Accuracy 0.8500\n",
      "Epoch [6][10]\t Batch [150][275]\t Training Loss 1.7899\t Accuracy 0.8850\n",
      "Epoch [6][10]\t Batch [200][275]\t Training Loss 1.8319\t Accuracy 0.8250\n",
      "Epoch [6][10]\t Batch [250][275]\t Training Loss 1.8493\t Accuracy 0.8150\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8277\t Average training accuracy 0.8429\n",
      "Epoch [6]\t Average validation loss 1.8029\t Average validation accuracy 0.8776\n",
      "\n",
      "Epoch [7][10]\t Batch [0][275]\t Training Loss 1.8206\t Accuracy 0.8400\n",
      "Epoch [7][10]\t Batch [50][275]\t Training Loss 1.8327\t Accuracy 0.8450\n",
      "Epoch [7][10]\t Batch [100][275]\t Training Loss 1.8190\t Accuracy 0.8850\n",
      "Epoch [7][10]\t Batch [150][275]\t Training Loss 1.8084\t Accuracy 0.8650\n",
      "Epoch [7][10]\t Batch [200][275]\t Training Loss 1.8206\t Accuracy 0.8500\n",
      "Epoch [7][10]\t Batch [250][275]\t Training Loss 1.8335\t Accuracy 0.8550\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8264\t Average training accuracy 0.8440\n",
      "Epoch [7]\t Average validation loss 1.8045\t Average validation accuracy 0.8764\n",
      "\n",
      "Epoch [8][10]\t Batch [0][275]\t Training Loss 1.8216\t Accuracy 0.8450\n",
      "Epoch [8][10]\t Batch [50][275]\t Training Loss 1.8404\t Accuracy 0.8250\n",
      "Epoch [8][10]\t Batch [100][275]\t Training Loss 1.8270\t Accuracy 0.8450\n",
      "Epoch [8][10]\t Batch [150][275]\t Training Loss 1.8203\t Accuracy 0.8400\n",
      "Epoch [8][10]\t Batch [200][275]\t Training Loss 1.7982\t Accuracy 0.8400\n",
      "Epoch [8][10]\t Batch [250][275]\t Training Loss 1.8291\t Accuracy 0.8600\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8252\t Average training accuracy 0.8458\n",
      "Epoch [8]\t Average validation loss 1.8042\t Average validation accuracy 0.8772\n",
      "\n",
      "Epoch [9][10]\t Batch [0][275]\t Training Loss 1.8256\t Accuracy 0.8500\n",
      "Epoch [9][10]\t Batch [50][275]\t Training Loss 1.8150\t Accuracy 0.8700\n",
      "Epoch [9][10]\t Batch [100][275]\t Training Loss 1.8149\t Accuracy 0.8650\n",
      "Epoch [9][10]\t Batch [150][275]\t Training Loss 1.8480\t Accuracy 0.8350\n",
      "Epoch [9][10]\t Batch [200][275]\t Training Loss 1.8399\t Accuracy 0.8250\n",
      "Epoch [9][10]\t Batch [250][275]\t Training Loss 1.8342\t Accuracy 0.8550\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8250\t Average training accuracy 0.8462\n",
      "Epoch [9]\t Average validation loss 1.8020\t Average validation accuracy 0.8826\n",
      "\n",
      "Epoch [0][10]\t Batch [0][275]\t Training Loss 2.5693\t Accuracy 0.0450\n",
      "Epoch [0][10]\t Batch [50][275]\t Training Loss 1.8854\t Accuracy 0.6900\n",
      "Epoch [0][10]\t Batch [100][275]\t Training Loss 1.8489\t Accuracy 0.7650\n",
      "Epoch [0][10]\t Batch [150][275]\t Training Loss 1.8669\t Accuracy 0.7550\n",
      "Epoch [0][10]\t Batch [200][275]\t Training Loss 1.8639\t Accuracy 0.7600\n",
      "Epoch [0][10]\t Batch [250][275]\t Training Loss 1.8450\t Accuracy 0.8150\n",
      "\n",
      "Epoch [0]\t Average training loss 1.8958\t Average training accuracy 0.6938\n",
      "Epoch [0]\t Average validation loss 1.8289\t Average validation accuracy 0.8352\n",
      "\n",
      "Epoch [1][10]\t Batch [0][275]\t Training Loss 1.8362\t Accuracy 0.8050\n",
      "Epoch [1][10]\t Batch [50][275]\t Training Loss 1.8473\t Accuracy 0.7900\n",
      "Epoch [1][10]\t Batch [100][275]\t Training Loss 1.8704\t Accuracy 0.7900\n",
      "Epoch [1][10]\t Batch [150][275]\t Training Loss 1.8226\t Accuracy 0.8300\n",
      "Epoch [1][10]\t Batch [200][275]\t Training Loss 1.8610\t Accuracy 0.7950\n",
      "Epoch [1][10]\t Batch [250][275]\t Training Loss 1.8304\t Accuracy 0.8400\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8446\t Average training accuracy 0.8075\n",
      "Epoch [1]\t Average validation loss 1.8125\t Average validation accuracy 0.8596\n",
      "\n",
      "Epoch [2][10]\t Batch [0][275]\t Training Loss 1.8488\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [50][275]\t Training Loss 1.8360\t Accuracy 0.8350\n",
      "Epoch [2][10]\t Batch [100][275]\t Training Loss 1.8400\t Accuracy 0.8500\n",
      "Epoch [2][10]\t Batch [150][275]\t Training Loss 1.8654\t Accuracy 0.7750\n",
      "Epoch [2][10]\t Batch [200][275]\t Training Loss 1.8545\t Accuracy 0.8100\n",
      "Epoch [2][10]\t Batch [250][275]\t Training Loss 1.8220\t Accuracy 0.8500\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8365\t Average training accuracy 0.8260\n",
      "Epoch [2]\t Average validation loss 1.8065\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [3][10]\t Batch [0][275]\t Training Loss 1.8324\t Accuracy 0.8200\n",
      "Epoch [3][10]\t Batch [50][275]\t Training Loss 1.8657\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [100][275]\t Training Loss 1.8229\t Accuracy 0.8300\n",
      "Epoch [3][10]\t Batch [150][275]\t Training Loss 1.8418\t Accuracy 0.8350\n",
      "Epoch [3][10]\t Batch [200][275]\t Training Loss 1.8347\t Accuracy 0.8200\n",
      "Epoch [3][10]\t Batch [250][275]\t Training Loss 1.7744\t Accuracy 0.8600\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8323\t Average training accuracy 0.8335\n",
      "Epoch [3]\t Average validation loss 1.8066\t Average validation accuracy 0.8732\n",
      "\n",
      "Epoch [4][10]\t Batch [0][275]\t Training Loss 1.8402\t Accuracy 0.8400\n",
      "Epoch [4][10]\t Batch [50][275]\t Training Loss 1.8129\t Accuracy 0.8550\n",
      "Epoch [4][10]\t Batch [100][275]\t Training Loss 1.8153\t Accuracy 0.8400\n",
      "Epoch [4][10]\t Batch [150][275]\t Training Loss 1.8320\t Accuracy 0.8050\n",
      "Epoch [4][10]\t Batch [200][275]\t Training Loss 1.8151\t Accuracy 0.8450\n",
      "Epoch [4][10]\t Batch [250][275]\t Training Loss 1.8531\t Accuracy 0.8550\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8299\t Average training accuracy 0.8387\n",
      "Epoch [4]\t Average validation loss 1.7989\t Average validation accuracy 0.8764\n",
      "\n",
      "Epoch [5][10]\t Batch [0][275]\t Training Loss 1.8281\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [50][275]\t Training Loss 1.8352\t Accuracy 0.8450\n",
      "Epoch [5][10]\t Batch [100][275]\t Training Loss 1.7942\t Accuracy 0.8900\n",
      "Epoch [5][10]\t Batch [150][275]\t Training Loss 1.8150\t Accuracy 0.8550\n",
      "Epoch [5][10]\t Batch [200][275]\t Training Loss 1.8432\t Accuracy 0.8250\n",
      "Epoch [5][10]\t Batch [250][275]\t Training Loss 1.8202\t Accuracy 0.8550\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8276\t Average training accuracy 0.8418\n",
      "Epoch [5]\t Average validation loss 1.8030\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [6][10]\t Batch [0][275]\t Training Loss 1.8525\t Accuracy 0.8300\n",
      "Epoch [6][10]\t Batch [50][275]\t Training Loss 1.8225\t Accuracy 0.8500\n",
      "Epoch [6][10]\t Batch [100][275]\t Training Loss 1.8435\t Accuracy 0.8450\n",
      "Epoch [6][10]\t Batch [150][275]\t Training Loss 1.8603\t Accuracy 0.8400\n",
      "Epoch [6][10]\t Batch [200][275]\t Training Loss 1.8340\t Accuracy 0.8500\n",
      "Epoch [6][10]\t Batch [250][275]\t Training Loss 1.8152\t Accuracy 0.8800\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8265\t Average training accuracy 0.8439\n",
      "Epoch [6]\t Average validation loss 1.7990\t Average validation accuracy 0.8760\n",
      "\n",
      "Epoch [7][10]\t Batch [0][275]\t Training Loss 1.8568\t Accuracy 0.7750\n",
      "Epoch [7][10]\t Batch [50][275]\t Training Loss 1.8262\t Accuracy 0.8350\n",
      "Epoch [7][10]\t Batch [100][275]\t Training Loss 1.8153\t Accuracy 0.8500\n",
      "Epoch [7][10]\t Batch [150][275]\t Training Loss 1.8118\t Accuracy 0.8350\n",
      "Epoch [7][10]\t Batch [200][275]\t Training Loss 1.8252\t Accuracy 0.8450\n",
      "Epoch [7][10]\t Batch [250][275]\t Training Loss 1.8270\t Accuracy 0.8250\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8252\t Average training accuracy 0.8449\n",
      "Epoch [7]\t Average validation loss 1.8005\t Average validation accuracy 0.8766\n",
      "\n",
      "Epoch [8][10]\t Batch [0][275]\t Training Loss 1.8104\t Accuracy 0.8300\n",
      "Epoch [8][10]\t Batch [50][275]\t Training Loss 1.8277\t Accuracy 0.8300\n",
      "Epoch [8][10]\t Batch [100][275]\t Training Loss 1.8350\t Accuracy 0.8350\n",
      "Epoch [8][10]\t Batch [150][275]\t Training Loss 1.8241\t Accuracy 0.8450\n",
      "Epoch [8][10]\t Batch [200][275]\t Training Loss 1.8149\t Accuracy 0.8200\n",
      "Epoch [8][10]\t Batch [250][275]\t Training Loss 1.7989\t Accuracy 0.8500\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8247\t Average training accuracy 0.8459\n",
      "Epoch [8]\t Average validation loss 1.8021\t Average validation accuracy 0.8788\n",
      "\n",
      "Epoch [9][10]\t Batch [0][275]\t Training Loss 1.8227\t Accuracy 0.8650\n",
      "Epoch [9][10]\t Batch [50][275]\t Training Loss 1.8132\t Accuracy 0.8500\n",
      "Epoch [9][10]\t Batch [100][275]\t Training Loss 1.8159\t Accuracy 0.8600\n",
      "Epoch [9][10]\t Batch [150][275]\t Training Loss 1.8322\t Accuracy 0.8750\n",
      "Epoch [9][10]\t Batch [200][275]\t Training Loss 1.8250\t Accuracy 0.8350\n",
      "Epoch [9][10]\t Batch [250][275]\t Training Loss 1.8488\t Accuracy 0.8350\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8239\t Average training accuracy 0.8465\n",
      "Epoch [9]\t Average validation loss 1.8010\t Average validation accuracy 0.8762\n",
      "\n",
      "Epoch [0][10]\t Batch [0][275]\t Training Loss 2.5175\t Accuracy 0.1200\n",
      "Epoch [0][10]\t Batch [50][275]\t Training Loss 1.9155\t Accuracy 0.6050\n",
      "Epoch [0][10]\t Batch [100][275]\t Training Loss 1.8811\t Accuracy 0.7450\n",
      "Epoch [0][10]\t Batch [150][275]\t Training Loss 1.8821\t Accuracy 0.7450\n",
      "Epoch [0][10]\t Batch [200][275]\t Training Loss 1.8645\t Accuracy 0.7900\n",
      "Epoch [0][10]\t Batch [250][275]\t Training Loss 1.8628\t Accuracy 0.8050\n",
      "\n",
      "Epoch [0]\t Average training loss 1.8911\t Average training accuracy 0.7001\n",
      "Epoch [0]\t Average validation loss 1.8264\t Average validation accuracy 0.8396\n",
      "\n",
      "Epoch [1][10]\t Batch [0][275]\t Training Loss 1.8612\t Accuracy 0.8250\n",
      "Epoch [1][10]\t Batch [50][275]\t Training Loss 1.8434\t Accuracy 0.8200\n",
      "Epoch [1][10]\t Batch [100][275]\t Training Loss 1.8352\t Accuracy 0.8350\n",
      "Epoch [1][10]\t Batch [150][275]\t Training Loss 1.8393\t Accuracy 0.8100\n",
      "Epoch [1][10]\t Batch [200][275]\t Training Loss 1.8410\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [250][275]\t Training Loss 1.8280\t Accuracy 0.8450\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8427\t Average training accuracy 0.8129\n",
      "Epoch [1]\t Average validation loss 1.8103\t Average validation accuracy 0.8642\n",
      "\n",
      "Epoch [2][10]\t Batch [0][275]\t Training Loss 1.8431\t Accuracy 0.8150\n",
      "Epoch [2][10]\t Batch [50][275]\t Training Loss 1.8432\t Accuracy 0.8300\n",
      "Epoch [2][10]\t Batch [100][275]\t Training Loss 1.8535\t Accuracy 0.7850\n",
      "Epoch [2][10]\t Batch [150][275]\t Training Loss 1.8547\t Accuracy 0.8400\n",
      "Epoch [2][10]\t Batch [200][275]\t Training Loss 1.8458\t Accuracy 0.8100\n",
      "Epoch [2][10]\t Batch [250][275]\t Training Loss 1.8137\t Accuracy 0.8600\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8342\t Average training accuracy 0.8296\n",
      "Epoch [2]\t Average validation loss 1.8104\t Average validation accuracy 0.8708\n",
      "\n",
      "Epoch [3][10]\t Batch [0][275]\t Training Loss 1.8297\t Accuracy 0.8650\n",
      "Epoch [3][10]\t Batch [50][275]\t Training Loss 1.8232\t Accuracy 0.8750\n",
      "Epoch [3][10]\t Batch [100][275]\t Training Loss 1.8099\t Accuracy 0.8650\n",
      "Epoch [3][10]\t Batch [150][275]\t Training Loss 1.8178\t Accuracy 0.8600\n",
      "Epoch [3][10]\t Batch [200][275]\t Training Loss 1.8152\t Accuracy 0.8150\n",
      "Epoch [3][10]\t Batch [250][275]\t Training Loss 1.8537\t Accuracy 0.8400\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8307\t Average training accuracy 0.8373\n",
      "Epoch [3]\t Average validation loss 1.8035\t Average validation accuracy 0.8774\n",
      "\n",
      "Epoch [4][10]\t Batch [0][275]\t Training Loss 1.8476\t Accuracy 0.8250\n",
      "Epoch [4][10]\t Batch [50][275]\t Training Loss 1.8354\t Accuracy 0.8500\n",
      "Epoch [4][10]\t Batch [100][275]\t Training Loss 1.8408\t Accuracy 0.8050\n",
      "Epoch [4][10]\t Batch [150][275]\t Training Loss 1.8218\t Accuracy 0.8650\n",
      "Epoch [4][10]\t Batch [200][275]\t Training Loss 1.8500\t Accuracy 0.8250\n",
      "Epoch [4][10]\t Batch [250][275]\t Training Loss 1.8312\t Accuracy 0.8450\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8282\t Average training accuracy 0.8397\n",
      "Epoch [4]\t Average validation loss 1.8040\t Average validation accuracy 0.8756\n",
      "\n",
      "Epoch [5][10]\t Batch [0][275]\t Training Loss 1.8241\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [50][275]\t Training Loss 1.8593\t Accuracy 0.8050\n",
      "Epoch [5][10]\t Batch [100][275]\t Training Loss 1.8066\t Accuracy 0.8550\n",
      "Epoch [5][10]\t Batch [150][275]\t Training Loss 1.8402\t Accuracy 0.8300\n",
      "Epoch [5][10]\t Batch [200][275]\t Training Loss 1.8109\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [250][275]\t Training Loss 1.8484\t Accuracy 0.8000\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8265\t Average training accuracy 0.8433\n",
      "Epoch [5]\t Average validation loss 1.8029\t Average validation accuracy 0.8822\n",
      "\n",
      "Epoch [6][10]\t Batch [0][275]\t Training Loss 1.8071\t Accuracy 0.8850\n",
      "Epoch [6][10]\t Batch [50][275]\t Training Loss 1.8231\t Accuracy 0.8500\n",
      "Epoch [6][10]\t Batch [100][275]\t Training Loss 1.8204\t Accuracy 0.8300\n",
      "Epoch [6][10]\t Batch [150][275]\t Training Loss 1.7687\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [200][275]\t Training Loss 1.8220\t Accuracy 0.8100\n",
      "Epoch [6][10]\t Batch [250][275]\t Training Loss 1.8255\t Accuracy 0.8450\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8252\t Average training accuracy 0.8445\n",
      "Epoch [6]\t Average validation loss 1.8013\t Average validation accuracy 0.8776\n",
      "\n",
      "Epoch [7][10]\t Batch [0][275]\t Training Loss 1.8251\t Accuracy 0.8600\n",
      "Epoch [7][10]\t Batch [50][275]\t Training Loss 1.7968\t Accuracy 0.8700\n",
      "Epoch [7][10]\t Batch [100][275]\t Training Loss 1.7992\t Accuracy 0.8750\n",
      "Epoch [7][10]\t Batch [150][275]\t Training Loss 1.8030\t Accuracy 0.8700\n",
      "Epoch [7][10]\t Batch [200][275]\t Training Loss 1.8381\t Accuracy 0.8250\n",
      "Epoch [7][10]\t Batch [250][275]\t Training Loss 1.8137\t Accuracy 0.8450\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8243\t Average training accuracy 0.8449\n",
      "Epoch [7]\t Average validation loss 1.8026\t Average validation accuracy 0.8776\n",
      "\n",
      "Epoch [8][10]\t Batch [0][275]\t Training Loss 1.8314\t Accuracy 0.8500\n",
      "Epoch [8][10]\t Batch [50][275]\t Training Loss 1.8551\t Accuracy 0.8250\n",
      "Epoch [8][10]\t Batch [100][275]\t Training Loss 1.8377\t Accuracy 0.8250\n",
      "Epoch [8][10]\t Batch [150][275]\t Training Loss 1.8219\t Accuracy 0.8200\n",
      "Epoch [8][10]\t Batch [200][275]\t Training Loss 1.8225\t Accuracy 0.8650\n",
      "Epoch [8][10]\t Batch [250][275]\t Training Loss 1.8037\t Accuracy 0.8750\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8237\t Average training accuracy 0.8461\n",
      "Epoch [8]\t Average validation loss 1.8002\t Average validation accuracy 0.8812\n",
      "\n",
      "Epoch [9][10]\t Batch [0][275]\t Training Loss 1.8149\t Accuracy 0.8550\n",
      "Epoch [9][10]\t Batch [50][275]\t Training Loss 1.8374\t Accuracy 0.8200\n",
      "Epoch [9][10]\t Batch [100][275]\t Training Loss 1.8099\t Accuracy 0.8750\n",
      "Epoch [9][10]\t Batch [150][275]\t Training Loss 1.8093\t Accuracy 0.8700\n",
      "Epoch [9][10]\t Batch [200][275]\t Training Loss 1.8247\t Accuracy 0.8500\n",
      "Epoch [9][10]\t Batch [250][275]\t Training Loss 1.7942\t Accuracy 0.8750\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8229\t Average training accuracy 0.8466\n",
      "Epoch [9]\t Average validation loss 1.7960\t Average validation accuracy 0.8798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg5 = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 200,\n",
    "    'learning_rate': 0.005,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "runner5 = Solver(cfg5)\n",
    "loss5, acc5 = runner5.train()\n",
    "cfg6 = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 200,\n",
    "    'learning_rate': 0.006,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "runner6 = Solver(cfg6)\n",
    "loss6, acc6 = runner6.train()\n",
    "\n",
    "cfg7 = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 200,\n",
    "    'learning_rate': 0.007,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "runner7 = Solver(cfg7)\n",
    "loss7, acc7 = runner7.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_loss_and_acc({\n",
    "#     \"momentum=0\": [loss1, acc1],\n",
    "#     \"momentum=0.2\": [loss2, acc2],\n",
    "#     \"momentum=0.4\": [loss3, acc3],\n",
    "#     \"momentum=0.6\": [loss4, acc4],\n",
    "#     \"momentum=0.9\": [loss5, acc5],\n",
    "#     \"momentum=0.95\": [loss6, acc6],\n",
    "#     \"momentum=0.99\": [loss7, acc7]\n",
    "# })\n",
    "\n",
    "#learning rate: 0.001, 0.003, 0.005, 0.01, 0.02, 0.05, 0.1\n",
    "# plot_loss_and_acc({\n",
    "#     \"learning rate=0.001\": [loss1, acc1],\n",
    "#     \"learning rate=0.003\": [loss2, acc2],\n",
    "#     \"learning rate=0.005\": [loss3, acc3],\n",
    "#     \"learning rate=0.01\": [loss4, acc4],\n",
    "#     \"learning rate=0.02\": [loss5, acc5],\n",
    "#     \"learning rate=0.05\": [loss6, acc6],\n",
    "#     \"learning rate=0.1\": [loss7, acc7]\n",
    "# })\n",
    "\n",
    "\n",
    "plot_loss_and_acc({\n",
    "    \"batch size=100\": [loss1, acc1],\n",
    "    \"batch size=200\": [loss2, acc2],\n",
    "    \"batch size=500\": [loss3, acc3],\n",
    "    \"batch size=1000\": [loss4, acc4]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss5: 1.824968156508823 acc5: 0.8461636363636363\n",
      "loss6: 1.8238737851753921 acc6: 0.8464909090909091\n",
      "loss7: 1.8229002118533226 acc7: 0.8465636363636364\n"
     ]
    }
   ],
   "source": [
    "# print(\"loss1:\", loss1[9], \"acc1:\", acc1[9])\n",
    "# print(\"loss2:\", loss2[9], \"acc2:\", acc2[9])\n",
    "# print(\"loss3:\", loss3[9], \"acc3:\", acc3[9])\n",
    "# print(\"loss4:\", loss4[9], \"acc4:\", acc4[9])\n",
    "print(\"loss5:\", loss5[9], \"acc5:\", acc5[9])\n",
    "print(\"loss6:\", loss6[9], \"acc6:\", acc6[9])\n",
    "print(\"loss7:\", loss7[9], \"acc7:\", acc7[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.8545\n",
      "\n",
      "Final test accuracy 0.8581\n",
      "\n",
      "Final test accuracy 0.8598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss5, test_acc5 = runner5.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc5))\n",
    "\n",
    "test_loss6, test_acc6 = runner6.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc6))\n",
    "\n",
    "test_loss7, test_acc7 = runner7.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.658960163641729 acc: 0.08\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.6590\t Accuracy 0.0800\n",
      "loss: 2.459243293817953 acc: 0.1\n",
      "loss: 2.449989370427463 acc: 0.06\n",
      "loss: 2.4417985771001174 acc: 0.03\n",
      "loss: 2.3836437365910585 acc: 0.13\n",
      "loss: 2.361188286867592 acc: 0.11\n",
      "loss: 2.306270215698415 acc: 0.14\n",
      "loss: 2.2114060224411043 acc: 0.14\n",
      "loss: 2.158143143460408 acc: 0.2\n",
      "loss: 2.1073584179571565 acc: 0.3\n",
      "loss: 2.0716445873558427 acc: 0.33\n",
      "loss: 2.024712529903712 acc: 0.29\n",
      "loss: 1.9783474872958353 acc: 0.41\n",
      "loss: 1.9527578218330246 acc: 0.4\n",
      "loss: 1.859041179061892 acc: 0.51\n",
      "loss: 1.8544080800329166 acc: 0.5\n",
      "loss: 1.86048923462536 acc: 0.54\n",
      "loss: 1.9216850340586147 acc: 0.44\n",
      "loss: 1.8876936273692848 acc: 0.47\n",
      "loss: 1.9009675624865365 acc: 0.51\n",
      "loss: 1.8565811781286408 acc: 0.58\n",
      "loss: 1.8632673881170774 acc: 0.57\n",
      "loss: 1.9213123718913065 acc: 0.52\n",
      "loss: 1.9049213529149884 acc: 0.59\n",
      "loss: 1.8131022470419855 acc: 0.61\n",
      "loss: 1.9030453016172266 acc: 0.62\n",
      "loss: 1.8702407912900103 acc: 0.66\n",
      "loss: 1.92725023944229 acc: 0.6\n",
      "loss: 1.933784599232041 acc: 0.62\n",
      "loss: 1.9457218127699876 acc: 0.55\n",
      "loss: 1.995202317520856 acc: 0.54\n",
      "loss: 1.9489221518036184 acc: 0.56\n",
      "loss: 1.9278960749745744 acc: 0.6\n",
      "loss: 1.9414392654545436 acc: 0.61\n",
      "loss: 1.954322034829619 acc: 0.54\n",
      "loss: 1.9603192576997237 acc: 0.52\n",
      "loss: 1.9670255617900059 acc: 0.62\n",
      "loss: 1.9328910709828049 acc: 0.59\n",
      "loss: 1.9406125773272396 acc: 0.65\n",
      "loss: 1.8754713395390812 acc: 0.72\n",
      "loss: 1.919321929705845 acc: 0.57\n",
      "loss: 1.911843765786154 acc: 0.64\n",
      "loss: 1.8688541919361177 acc: 0.72\n",
      "loss: 1.882168798519526 acc: 0.64\n",
      "loss: 1.8730520992482729 acc: 0.66\n",
      "loss: 1.868776749502179 acc: 0.7\n",
      "loss: 1.8556873939417171 acc: 0.7\n",
      "loss: 1.8570770858863943 acc: 0.68\n",
      "loss: 1.8714566328244728 acc: 0.71\n",
      "loss: 1.9154909864405962 acc: 0.67\n",
      "loss: 1.8618046593472173 acc: 0.7\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 1.8618\t Accuracy 0.7000\n",
      "loss: 1.9026845310680616 acc: 0.68\n",
      "loss: 1.8681800531918378 acc: 0.7\n",
      "loss: 1.85532670873167 acc: 0.74\n",
      "loss: 1.8576789298554228 acc: 0.76\n",
      "loss: 1.9136064038934069 acc: 0.71\n",
      "loss: 1.8279521456984076 acc: 0.78\n",
      "loss: 1.8834507716066533 acc: 0.66\n",
      "loss: 1.8410884347844665 acc: 0.74\n",
      "loss: 1.8648746580127975 acc: 0.71\n",
      "loss: 1.8747112927994356 acc: 0.68\n",
      "loss: 1.8763515077747865 acc: 0.72\n",
      "loss: 1.8846940139221948 acc: 0.72\n",
      "loss: 1.8834495004550242 acc: 0.69\n",
      "loss: 1.8950082694367365 acc: 0.73\n",
      "loss: 1.884334414251388 acc: 0.72\n",
      "loss: 1.8637311649031545 acc: 0.76\n",
      "loss: 1.8913800018424753 acc: 0.68\n",
      "loss: 1.8801462616222364 acc: 0.67\n",
      "loss: 1.8403962272638268 acc: 0.78\n",
      "loss: 1.8861555741576612 acc: 0.72\n",
      "loss: 1.8079513966392695 acc: 0.89\n",
      "loss: 1.8729826730572474 acc: 0.68\n",
      "loss: 1.8879333844915698 acc: 0.68\n",
      "loss: 1.8729110190830938 acc: 0.73\n",
      "loss: 1.8528357562339366 acc: 0.77\n",
      "loss: 1.858161838183072 acc: 0.76\n",
      "loss: 1.8863969456662861 acc: 0.69\n",
      "loss: 1.8972982300975128 acc: 0.65\n",
      "loss: 1.8413136853735017 acc: 0.78\n",
      "loss: 1.874223932992977 acc: 0.71\n",
      "loss: 1.8239696532922505 acc: 0.71\n",
      "loss: 1.884731880808906 acc: 0.74\n",
      "loss: 1.8668152050723767 acc: 0.7\n",
      "loss: 1.8867624508765755 acc: 0.71\n",
      "loss: 1.8392799381654268 acc: 0.74\n",
      "loss: 1.8757927873720548 acc: 0.68\n",
      "loss: 1.875814186535117 acc: 0.7\n",
      "loss: 1.8591393703025225 acc: 0.74\n",
      "loss: 1.8412363851936937 acc: 0.79\n",
      "loss: 1.8815668207239806 acc: 0.78\n",
      "loss: 1.8551322595037656 acc: 0.76\n",
      "loss: 1.8676924072287568 acc: 0.73\n",
      "loss: 1.8682803374497619 acc: 0.7\n",
      "loss: 1.8990325911689028 acc: 0.69\n",
      "loss: 1.8469471153773498 acc: 0.78\n",
      "loss: 1.899260267255754 acc: 0.7\n",
      "loss: 1.863828813308666 acc: 0.74\n",
      "loss: 1.904487599919901 acc: 0.66\n",
      "loss: 1.9230014946604197 acc: 0.71\n",
      "loss: 1.903830922498925 acc: 0.73\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 1.9038\t Accuracy 0.7300\n",
      "loss: 1.8617119586081403 acc: 0.79\n",
      "loss: 1.8225407332472678 acc: 0.83\n",
      "loss: 1.8185575583313887 acc: 0.78\n",
      "loss: 1.890718580777474 acc: 0.74\n",
      "loss: 1.8323753878511226 acc: 0.75\n",
      "loss: 1.86676928833633 acc: 0.7\n",
      "loss: 1.8513700102199417 acc: 0.74\n",
      "loss: 1.851035447703748 acc: 0.74\n",
      "loss: 1.832006304435425 acc: 0.79\n",
      "loss: 1.8221276630224577 acc: 0.79\n",
      "loss: 1.863878241341789 acc: 0.68\n",
      "loss: 1.8351140357374771 acc: 0.83\n",
      "loss: 1.8598195154546482 acc: 0.74\n",
      "loss: 1.8464509298089338 acc: 0.72\n",
      "loss: 1.8922120620837994 acc: 0.73\n",
      "loss: 1.818916545630267 acc: 0.84\n",
      "loss: 1.855226380625064 acc: 0.76\n",
      "loss: 1.8679569201365605 acc: 0.75\n",
      "loss: 1.8726418164885612 acc: 0.76\n",
      "loss: 1.8935751923237762 acc: 0.72\n",
      "loss: 1.8578893702181571 acc: 0.74\n",
      "loss: 1.8923307775249092 acc: 0.78\n",
      "loss: 1.8229216128919874 acc: 0.8\n",
      "loss: 1.83150580994758 acc: 0.82\n",
      "loss: 1.8621509493535076 acc: 0.76\n",
      "loss: 1.8660353772064853 acc: 0.74\n",
      "loss: 1.8964928637578553 acc: 0.73\n",
      "loss: 1.8894771732740483 acc: 0.79\n",
      "loss: 1.8853003426951098 acc: 0.73\n",
      "loss: 1.8475877201998296 acc: 0.78\n",
      "loss: 1.8873930024248708 acc: 0.67\n",
      "loss: 1.8546328723907528 acc: 0.81\n",
      "loss: 1.8354241244313716 acc: 0.79\n",
      "loss: 1.9350722058691028 acc: 0.63\n",
      "loss: 1.8754164236394475 acc: 0.74\n",
      "loss: 1.8411874829186834 acc: 0.8\n",
      "loss: 1.890164967814945 acc: 0.75\n",
      "loss: 1.8576017535941596 acc: 0.79\n",
      "loss: 1.8541417128725213 acc: 0.79\n",
      "loss: 1.8513740905854588 acc: 0.74\n",
      "loss: 1.858217316097201 acc: 0.75\n",
      "loss: 1.8830380265337336 acc: 0.68\n",
      "loss: 1.8673354929458474 acc: 0.77\n",
      "loss: 1.8557479450536931 acc: 0.79\n",
      "loss: 1.8711432587007573 acc: 0.7\n",
      "loss: 1.861458807083437 acc: 0.79\n",
      "loss: 1.8654404806225708 acc: 0.76\n",
      "loss: 1.8310756057167001 acc: 0.81\n",
      "loss: 1.8955909451960633 acc: 0.74\n",
      "loss: 1.8224679231419783 acc: 0.81\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 1.8225\t Accuracy 0.8100\n",
      "loss: 1.8552132521391453 acc: 0.78\n",
      "loss: 1.8379499639827923 acc: 0.8\n",
      "loss: 1.889859572139312 acc: 0.75\n",
      "loss: 1.856351073412538 acc: 0.75\n",
      "loss: 1.887762656913647 acc: 0.77\n",
      "loss: 1.853877153204906 acc: 0.8\n",
      "loss: 1.821869768938741 acc: 0.82\n",
      "loss: 1.8492083256209284 acc: 0.79\n",
      "loss: 1.8528169811884967 acc: 0.77\n",
      "loss: 1.8133814279148082 acc: 0.81\n",
      "loss: 1.8655079205759681 acc: 0.77\n",
      "loss: 1.849126359983368 acc: 0.79\n",
      "loss: 1.849317402061308 acc: 0.8\n",
      "loss: 1.8503597788744437 acc: 0.72\n",
      "loss: 1.8502843627349288 acc: 0.77\n",
      "loss: 1.8223011847753456 acc: 0.74\n",
      "loss: 1.8532087218729865 acc: 0.76\n",
      "loss: 1.821867585523034 acc: 0.82\n",
      "loss: 1.8084026664436754 acc: 0.81\n",
      "loss: 1.8325333158763528 acc: 0.79\n",
      "loss: 1.825167213851725 acc: 0.82\n",
      "loss: 1.8173131348077154 acc: 0.77\n",
      "loss: 1.819309272476251 acc: 0.79\n",
      "loss: 1.8499554269095397 acc: 0.8\n",
      "loss: 1.822658900787227 acc: 0.82\n",
      "loss: 1.8712776273160763 acc: 0.74\n",
      "loss: 1.8772712674802674 acc: 0.81\n",
      "loss: 1.8547246759891618 acc: 0.79\n",
      "loss: 1.8465157001283166 acc: 0.73\n",
      "loss: 1.9244111141977618 acc: 0.71\n",
      "loss: 1.8899580799621847 acc: 0.75\n",
      "loss: 1.823216657029503 acc: 0.81\n",
      "loss: 1.8872864035784698 acc: 0.77\n",
      "loss: 1.8798233541157503 acc: 0.78\n",
      "loss: 1.856869577135027 acc: 0.81\n",
      "loss: 1.8388269680471003 acc: 0.79\n",
      "loss: 1.8378042489584032 acc: 0.79\n",
      "loss: 1.8642824222767962 acc: 0.75\n",
      "loss: 1.8792893536925401 acc: 0.77\n",
      "loss: 1.8141140676706042 acc: 0.85\n",
      "loss: 1.845293400823837 acc: 0.78\n",
      "loss: 1.8429179071538966 acc: 0.8\n",
      "loss: 1.8495247191761757 acc: 0.76\n",
      "loss: 1.8392797756355082 acc: 0.83\n",
      "loss: 1.8532448008344613 acc: 0.76\n",
      "loss: 1.8487256556697818 acc: 0.74\n",
      "loss: 1.8350974901068973 acc: 0.81\n",
      "loss: 1.8199913256031501 acc: 0.83\n",
      "loss: 1.79530738862736 acc: 0.91\n",
      "loss: 1.806844432825713 acc: 0.82\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 1.8068\t Accuracy 0.8200\n",
      "loss: 1.8805496645920103 acc: 0.75\n",
      "loss: 1.820313627044663 acc: 0.8\n",
      "loss: 1.8363465419963734 acc: 0.81\n",
      "loss: 1.8813754838476229 acc: 0.77\n",
      "loss: 1.8561032604626857 acc: 0.75\n",
      "loss: 1.8972399035438199 acc: 0.78\n",
      "loss: 1.8302658530771356 acc: 0.82\n",
      "loss: 1.8492911869080877 acc: 0.81\n",
      "loss: 1.8672863934445258 acc: 0.79\n",
      "loss: 1.8560096785300848 acc: 0.84\n",
      "loss: 1.876414292511556 acc: 0.8\n",
      "loss: 1.8021706283893908 acc: 0.84\n",
      "loss: 1.8547639391725579 acc: 0.81\n",
      "loss: 1.8189703199380611 acc: 0.84\n",
      "loss: 1.842541018505229 acc: 0.78\n",
      "loss: 1.8535757394574575 acc: 0.79\n",
      "loss: 1.864070221546225 acc: 0.8\n",
      "loss: 1.8703306990039998 acc: 0.75\n",
      "loss: 1.8348984731406321 acc: 0.8\n",
      "loss: 1.8383010953528909 acc: 0.83\n",
      "loss: 1.8338026552718194 acc: 0.82\n",
      "loss: 1.836841895664702 acc: 0.82\n",
      "loss: 1.804832898633202 acc: 0.85\n",
      "loss: 1.8377093899761854 acc: 0.8\n",
      "loss: 1.8747456401828166 acc: 0.72\n",
      "loss: 1.8431967807410776 acc: 0.77\n",
      "loss: 1.8107019376965305 acc: 0.84\n",
      "loss: 1.8496089621022147 acc: 0.77\n",
      "loss: 1.8671163983696746 acc: 0.74\n",
      "loss: 1.8167441914945486 acc: 0.81\n",
      "loss: 1.8641104793206582 acc: 0.7\n",
      "loss: 1.7797691098527944 acc: 0.82\n",
      "loss: 1.8577946188138963 acc: 0.79\n",
      "loss: 1.8508577237156063 acc: 0.77\n",
      "loss: 1.8643233876543512 acc: 0.77\n",
      "loss: 1.849524061652158 acc: 0.82\n",
      "loss: 1.8309166384673012 acc: 0.77\n",
      "loss: 1.847402206148196 acc: 0.82\n",
      "loss: 1.8226979063649724 acc: 0.88\n",
      "loss: 1.7932852427700194 acc: 0.82\n",
      "loss: 1.8649727106013392 acc: 0.72\n",
      "loss: 1.8383123214822996 acc: 0.83\n",
      "loss: 1.8778817764053952 acc: 0.79\n",
      "loss: 1.8378112929390877 acc: 0.8\n",
      "loss: 1.7991166965416474 acc: 0.83\n",
      "loss: 1.8577205441421785 acc: 0.79\n",
      "loss: 1.8164285475115072 acc: 0.78\n",
      "loss: 1.8698730732932842 acc: 0.83\n",
      "loss: 1.8396906680291067 acc: 0.82\n",
      "loss: 1.8370108701446868 acc: 0.77\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 1.8370\t Accuracy 0.7700\n",
      "loss: 1.8345996257788635 acc: 0.8\n",
      "loss: 1.824772922550149 acc: 0.76\n",
      "loss: 1.8628525949835677 acc: 0.8\n",
      "loss: 1.8512661527716108 acc: 0.76\n",
      "loss: 1.8185266742130672 acc: 0.87\n",
      "loss: 1.7923308597631427 acc: 0.86\n",
      "loss: 1.8734240128139013 acc: 0.83\n",
      "loss: 1.8522638399814315 acc: 0.79\n",
      "loss: 1.864589981034244 acc: 0.81\n",
      "loss: 1.805091935836113 acc: 0.84\n",
      "loss: 1.82885081197895 acc: 0.84\n",
      "loss: 1.8636257524566096 acc: 0.77\n",
      "loss: 1.8744221542623978 acc: 0.8\n",
      "loss: 1.8080987548309593 acc: 0.79\n",
      "loss: 1.8849892492132614 acc: 0.76\n",
      "loss: 1.8521154228517887 acc: 0.79\n",
      "loss: 1.852709447060488 acc: 0.83\n",
      "loss: 1.8741057614474281 acc: 0.8\n",
      "loss: 1.8791523655673104 acc: 0.76\n",
      "loss: 1.8948370147807092 acc: 0.72\n",
      "loss: 1.8716576130467635 acc: 0.77\n",
      "loss: 1.853211558407064 acc: 0.82\n",
      "loss: 1.8682849607158938 acc: 0.75\n",
      "loss: 1.8093088418593894 acc: 0.88\n",
      "loss: 1.8324059816720975 acc: 0.85\n",
      "loss: 1.7987720725368288 acc: 0.86\n",
      "loss: 1.8096690749120803 acc: 0.89\n",
      "loss: 1.8289124557605805 acc: 0.83\n",
      "loss: 1.8613605380866185 acc: 0.75\n",
      "loss: 1.8673633570406059 acc: 0.69\n",
      "loss: 1.8000698170426248 acc: 0.84\n",
      "loss: 1.800401296752307 acc: 0.82\n",
      "loss: 1.8133463359409534 acc: 0.85\n",
      "loss: 1.8494578255170995 acc: 0.81\n",
      "loss: 1.8118524286814996 acc: 0.85\n",
      "loss: 1.8436882413789981 acc: 0.82\n",
      "loss: 1.8358510094192124 acc: 0.8\n",
      "loss: 1.870642756812702 acc: 0.78\n",
      "loss: 1.8303515941141548 acc: 0.81\n",
      "loss: 1.8395731553089067 acc: 0.77\n",
      "loss: 1.8361792123293659 acc: 0.86\n",
      "loss: 1.8576243374389672 acc: 0.78\n",
      "loss: 1.8499790080767196 acc: 0.77\n",
      "loss: 1.9116243520632998 acc: 0.77\n",
      "loss: 1.8088596924755933 acc: 0.82\n",
      "loss: 1.8171679516379975 acc: 0.86\n",
      "loss: 1.8423603505459512 acc: 0.83\n",
      "loss: 1.8184858355272808 acc: 0.85\n",
      "loss: 1.8349304566317546 acc: 0.79\n",
      "loss: 1.835499471247585 acc: 0.85\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 1.8355\t Accuracy 0.8500\n",
      "loss: 1.8699978253934106 acc: 0.74\n",
      "loss: 1.7991177046390414 acc: 0.8\n",
      "loss: 1.8179039067864862 acc: 0.8\n",
      "loss: 1.83075770438577 acc: 0.77\n",
      "loss: 1.862140282835209 acc: 0.76\n",
      "loss: 1.8527640751075183 acc: 0.82\n",
      "loss: 1.826126192611398 acc: 0.79\n",
      "loss: 1.8467739881180136 acc: 0.77\n",
      "loss: 1.8562489436489757 acc: 0.78\n",
      "loss: 1.8701509260709563 acc: 0.81\n",
      "loss: 1.8514103350523454 acc: 0.85\n",
      "loss: 1.7933289317012993 acc: 0.82\n",
      "loss: 1.8560118708584854 acc: 0.79\n",
      "loss: 1.8656318609050075 acc: 0.78\n",
      "loss: 1.7777511684402751 acc: 0.89\n",
      "loss: 1.849070793465863 acc: 0.78\n",
      "loss: 1.840056672374582 acc: 0.87\n",
      "loss: 1.8732176617524428 acc: 0.79\n",
      "loss: 1.859958961602533 acc: 0.81\n",
      "loss: 1.8608227470354899 acc: 0.76\n",
      "loss: 1.850555473569965 acc: 0.83\n",
      "loss: 1.816626356453833 acc: 0.79\n",
      "loss: 1.828664176384501 acc: 0.84\n",
      "loss: 1.8440792707669396 acc: 0.82\n",
      "loss: 1.8430364808893653 acc: 0.81\n",
      "loss: 1.8430829764330106 acc: 0.85\n",
      "loss: 1.861987841037866 acc: 0.76\n",
      "loss: 1.8556823037050665 acc: 0.83\n",
      "loss: 1.8541941085167175 acc: 0.82\n",
      "loss: 1.867183031865248 acc: 0.8\n",
      "loss: 1.873107944003197 acc: 0.82\n",
      "loss: 1.8477048917320955 acc: 0.77\n",
      "loss: 1.844027714799562 acc: 0.8\n",
      "loss: 1.8470120410266326 acc: 0.83\n",
      "loss: 1.8332502066053227 acc: 0.8\n",
      "loss: 1.801348590986936 acc: 0.89\n",
      "loss: 1.8441958416124522 acc: 0.85\n",
      "loss: 1.8165382988271455 acc: 0.88\n",
      "loss: 1.8246331188657225 acc: 0.82\n",
      "loss: 1.8661587509598443 acc: 0.81\n",
      "loss: 1.829752375039883 acc: 0.81\n",
      "loss: 1.8279067089069057 acc: 0.83\n",
      "loss: 1.8528282169304058 acc: 0.78\n",
      "loss: 1.8592647122579506 acc: 0.84\n",
      "loss: 1.8164965587113655 acc: 0.79\n",
      "loss: 1.8301426981191815 acc: 0.77\n",
      "loss: 1.8225826408450216 acc: 0.81\n",
      "loss: 1.8424115878419003 acc: 0.83\n",
      "loss: 1.8098951766759228 acc: 0.85\n",
      "loss: 1.8330875133575113 acc: 0.77\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 1.8331\t Accuracy 0.7700\n",
      "loss: 1.7843125397882544 acc: 0.89\n",
      "loss: 1.829610618354047 acc: 0.79\n",
      "loss: 1.8538292058004262 acc: 0.77\n",
      "loss: 1.8511383262257883 acc: 0.78\n",
      "loss: 1.8124528390152828 acc: 0.87\n",
      "loss: 1.8358963692384245 acc: 0.79\n",
      "loss: 1.8447607836418687 acc: 0.82\n",
      "loss: 1.7965595627688293 acc: 0.87\n",
      "loss: 1.8169567803518742 acc: 0.82\n",
      "loss: 1.8079820382894471 acc: 0.86\n",
      "loss: 1.8623861400536825 acc: 0.75\n",
      "loss: 1.8015886512023287 acc: 0.84\n",
      "loss: 1.8627681826327906 acc: 0.8\n",
      "loss: 1.8236119535401765 acc: 0.91\n",
      "loss: 1.8200477323825461 acc: 0.8\n",
      "loss: 1.806057673509535 acc: 0.86\n",
      "loss: 1.837737153117922 acc: 0.77\n",
      "loss: 1.8733050486894283 acc: 0.77\n",
      "loss: 1.8378748761837582 acc: 0.82\n",
      "loss: 1.8028052201260345 acc: 0.82\n",
      "loss: 1.8437930823500204 acc: 0.81\n",
      "loss: 1.8724630233483972 acc: 0.76\n",
      "loss: 1.7978567126049032 acc: 0.84\n",
      "loss: 1.8429002988744372 acc: 0.86\n",
      "loss: 1.9029472917588464 acc: 0.71\n",
      "loss: 1.8785652496119596 acc: 0.79\n",
      "loss: 1.852178236376234 acc: 0.81\n",
      "loss: 1.8299806890737014 acc: 0.8\n",
      "loss: 1.7924079353814955 acc: 0.86\n",
      "loss: 1.8470442567424823 acc: 0.8\n",
      "loss: 1.8431813106709387 acc: 0.82\n",
      "loss: 1.836355031722314 acc: 0.82\n",
      "loss: 1.8769142828134202 acc: 0.74\n",
      "loss: 1.8177435029932167 acc: 0.83\n",
      "loss: 1.8207814419267172 acc: 0.86\n",
      "loss: 1.8412106334660925 acc: 0.81\n",
      "loss: 1.824673866056252 acc: 0.83\n",
      "loss: 1.8752495095067823 acc: 0.77\n",
      "loss: 1.8594684270925317 acc: 0.8\n",
      "loss: 1.8299611331842556 acc: 0.87\n",
      "loss: 1.83421185171795 acc: 0.77\n",
      "loss: 1.8307993187827467 acc: 0.76\n",
      "loss: 1.8355277622875814 acc: 0.82\n",
      "loss: 1.8397774947132393 acc: 0.82\n",
      "loss: 1.8840314307671935 acc: 0.77\n",
      "loss: 1.8296621760463168 acc: 0.79\n",
      "loss: 1.8188161569381094 acc: 0.84\n",
      "loss: 1.8418849326504647 acc: 0.82\n",
      "loss: 1.8802743009511491 acc: 0.73\n",
      "loss: 1.8518015670127606 acc: 0.78\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 1.8518\t Accuracy 0.7800\n",
      "loss: 1.8472920483221784 acc: 0.82\n",
      "loss: 1.849141134829979 acc: 0.84\n",
      "loss: 1.8442289181401248 acc: 0.79\n",
      "loss: 1.8612370412660062 acc: 0.84\n",
      "loss: 1.8611602263193747 acc: 0.82\n",
      "loss: 1.8541064963822922 acc: 0.81\n",
      "loss: 1.853774415020528 acc: 0.74\n",
      "loss: 1.8650175712363457 acc: 0.79\n",
      "loss: 1.791938868426849 acc: 0.83\n",
      "loss: 1.8529455406237716 acc: 0.83\n",
      "loss: 1.8620395343534468 acc: 0.76\n",
      "loss: 1.8194692503860788 acc: 0.83\n",
      "loss: 1.7864149456819114 acc: 0.86\n",
      "loss: 1.8224200127147083 acc: 0.86\n",
      "loss: 1.8366749203175652 acc: 0.77\n",
      "loss: 1.8521591790001828 acc: 0.79\n",
      "loss: 1.856626222022756 acc: 0.75\n",
      "loss: 1.8734823382460295 acc: 0.79\n",
      "loss: 1.851459258548436 acc: 0.77\n",
      "loss: 1.8590643430202822 acc: 0.79\n",
      "loss: 1.8234283623497185 acc: 0.81\n",
      "loss: 1.8167034101304664 acc: 0.86\n",
      "loss: 1.8266865894162774 acc: 0.83\n",
      "loss: 1.8435295052376788 acc: 0.84\n",
      "loss: 1.828909790637884 acc: 0.82\n",
      "loss: 1.8497678939793403 acc: 0.77\n",
      "loss: 1.8276841606975784 acc: 0.82\n",
      "loss: 1.812315998249196 acc: 0.84\n",
      "loss: 1.8498273359345316 acc: 0.85\n",
      "loss: 1.8324054056351071 acc: 0.81\n",
      "loss: 1.8343181748438355 acc: 0.77\n",
      "loss: 1.8584221041370341 acc: 0.77\n",
      "loss: 1.8231705069321347 acc: 0.82\n",
      "loss: 1.8122325572814517 acc: 0.88\n",
      "loss: 1.8393785680778192 acc: 0.84\n",
      "loss: 1.8897188693298446 acc: 0.81\n",
      "loss: 1.8497793091800352 acc: 0.81\n",
      "loss: 1.8815413446950542 acc: 0.8\n",
      "loss: 1.791489741335614 acc: 0.88\n",
      "loss: 1.8348992735186287 acc: 0.82\n",
      "loss: 1.837671267465201 acc: 0.82\n",
      "loss: 1.8486160974272077 acc: 0.82\n",
      "loss: 1.843743764085574 acc: 0.86\n",
      "loss: 1.8682894372425067 acc: 0.84\n",
      "loss: 1.7716912589842764 acc: 0.91\n",
      "loss: 1.8108787895889449 acc: 0.86\n",
      "loss: 1.8262546175639756 acc: 0.8\n",
      "loss: 1.8475290122255865 acc: 0.76\n",
      "loss: 1.8264620233615125 acc: 0.87\n",
      "loss: 1.863159659276667 acc: 0.76\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 1.8632\t Accuracy 0.7600\n",
      "loss: 1.8461880822656263 acc: 0.85\n",
      "loss: 1.8219955761057005 acc: 0.84\n",
      "loss: 1.8540597758827473 acc: 0.82\n",
      "loss: 1.8136031654170666 acc: 0.81\n",
      "loss: 1.8454661084262631 acc: 0.78\n",
      "loss: 1.8424791183295801 acc: 0.77\n",
      "loss: 1.7962456570220977 acc: 0.88\n",
      "loss: 1.8244694252021785 acc: 0.88\n",
      "loss: 1.8523862732151324 acc: 0.81\n",
      "loss: 1.8140478590103652 acc: 0.79\n",
      "loss: 1.7998002046469181 acc: 0.86\n",
      "loss: 1.85039123027731 acc: 0.79\n",
      "loss: 1.8165163975919967 acc: 0.82\n",
      "loss: 1.8683127963891084 acc: 0.8\n",
      "loss: 1.794250170643078 acc: 0.91\n",
      "loss: 1.8124029493068874 acc: 0.81\n",
      "loss: 1.846327600240992 acc: 0.87\n",
      "loss: 1.7929841039212535 acc: 0.87\n",
      "loss: 1.8686322266779882 acc: 0.76\n",
      "loss: 1.8375615964194723 acc: 0.85\n",
      "loss: 1.8173258607475193 acc: 0.83\n",
      "loss: 1.8140010683933396 acc: 0.85\n",
      "loss: 1.7990924483187154 acc: 0.84\n",
      "loss: 1.820323482205263 acc: 0.82\n",
      "loss: 1.854120908774954 acc: 0.83\n",
      "loss: 1.841826900201506 acc: 0.77\n",
      "loss: 1.8496736974817345 acc: 0.86\n",
      "loss: 1.841780046167579 acc: 0.81\n",
      "loss: 1.8310711329528488 acc: 0.9\n",
      "loss: 1.8376814639585215 acc: 0.84\n",
      "loss: 1.8879424455185048 acc: 0.75\n",
      "loss: 1.8630728408733 acc: 0.76\n",
      "loss: 1.8725653690092532 acc: 0.8\n",
      "loss: 1.851245632594363 acc: 0.82\n",
      "loss: 1.8415811370191781 acc: 0.77\n",
      "loss: 1.852476610382547 acc: 0.83\n",
      "loss: 1.8106657122366232 acc: 0.85\n",
      "loss: 1.8010288712359903 acc: 0.86\n",
      "loss: 1.838078398496366 acc: 0.86\n",
      "loss: 1.8645387296061213 acc: 0.76\n",
      "loss: 1.816933357347256 acc: 0.85\n",
      "loss: 1.812767879748848 acc: 0.88\n",
      "loss: 1.8281949331162597 acc: 0.86\n",
      "loss: 1.8362153273406767 acc: 0.82\n",
      "loss: 1.8245060264732482 acc: 0.84\n",
      "loss: 1.8288431666201383 acc: 0.83\n",
      "loss: 1.8240529070590439 acc: 0.86\n",
      "loss: 1.8712056643644441 acc: 0.79\n",
      "loss: 1.8733860184418858 acc: 0.73\n",
      "loss: 1.8244778380484126 acc: 0.84\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 1.8245\t Accuracy 0.8400\n",
      "loss: 1.8090053509162407 acc: 0.86\n",
      "loss: 1.8195434735225593 acc: 0.81\n",
      "loss: 1.780614288990684 acc: 0.89\n",
      "loss: 1.808020892139941 acc: 0.88\n",
      "loss: 1.802259372323051 acc: 0.84\n",
      "loss: 1.8452383102523149 acc: 0.88\n",
      "loss: 1.8430594289409223 acc: 0.82\n",
      "loss: 1.83231369862003 acc: 0.8\n",
      "loss: 1.84864451955725 acc: 0.82\n",
      "loss: 1.841852742062131 acc: 0.81\n",
      "loss: 1.8695375808244319 acc: 0.82\n",
      "loss: 1.8703205412958992 acc: 0.8\n",
      "loss: 1.8452920657454626 acc: 0.79\n",
      "loss: 1.8618266032241026 acc: 0.86\n",
      "loss: 1.8190873104072802 acc: 0.82\n",
      "loss: 1.8025319907316697 acc: 0.89\n",
      "loss: 1.8257806856906436 acc: 0.83\n",
      "loss: 1.8443234950447869 acc: 0.84\n",
      "loss: 1.8404630618390556 acc: 0.88\n",
      "loss: 1.8453753169199814 acc: 0.87\n",
      "loss: 1.8384087289297213 acc: 0.8\n",
      "loss: 1.8577208582426505 acc: 0.8\n",
      "loss: 1.8246266494177001 acc: 0.83\n",
      "loss: 1.8513646666750359 acc: 0.79\n",
      "loss: 1.8117189319594695 acc: 0.82\n",
      "loss: 1.836537192608157 acc: 0.85\n",
      "loss: 1.8053823426422724 acc: 0.82\n",
      "loss: 1.8597659463409348 acc: 0.8\n",
      "loss: 1.8072252565162148 acc: 0.82\n",
      "loss: 1.7996148380303847 acc: 0.85\n",
      "loss: 1.8195611368750684 acc: 0.78\n",
      "loss: 1.821024397195121 acc: 0.82\n",
      "loss: 1.8239297436194497 acc: 0.83\n",
      "loss: 1.8160930194290532 acc: 0.79\n",
      "loss: 1.8551522337197823 acc: 0.77\n",
      "loss: 1.850746469112388 acc: 0.8\n",
      "loss: 1.8182701684091669 acc: 0.86\n",
      "loss: 1.8456227185157978 acc: 0.82\n",
      "loss: 1.8253987424882139 acc: 0.79\n",
      "loss: 1.8730069492027104 acc: 0.77\n",
      "loss: 1.824083686803737 acc: 0.78\n",
      "loss: 1.8878731747678423 acc: 0.75\n",
      "loss: 1.8384207855516823 acc: 0.8\n",
      "loss: 1.8458471508472079 acc: 0.81\n",
      "loss: 1.8616093092957473 acc: 0.82\n",
      "loss: 1.8279755774629325 acc: 0.84\n",
      "loss: 1.8608887694696654 acc: 0.76\n",
      "loss: 1.8631793032586503 acc: 0.81\n",
      "loss: 1.8070765097177162 acc: 0.86\n",
      "loss: 1.798472802115113 acc: 0.85\n",
      "loss: 1.8010141937798365 acc: 0.87\n",
      "loss: 1.8224971233238458 acc: 0.88\n",
      "loss: 1.8295253421373312 acc: 0.87\n",
      "loss: 1.8343973433751553 acc: 0.89\n",
      "loss: 1.8158766183036632 acc: 0.87\n",
      "loss: 1.7689241871064554 acc: 0.84\n",
      "loss: 1.7736582835504877 acc: 0.84\n",
      "loss: 1.8283231144924992 acc: 0.9\n",
      "loss: 1.7524485309535804 acc: 0.91\n",
      "loss: 1.8057050823554104 acc: 0.89\n",
      "loss: 1.804707162683902 acc: 0.86\n",
      "loss: 1.861222989072262 acc: 0.82\n",
      "loss: 1.87091072601679 acc: 0.84\n",
      "loss: 1.881015463455068 acc: 0.87\n",
      "loss: 1.7966670000245903 acc: 0.94\n",
      "loss: 1.8200891339320497 acc: 0.82\n",
      "loss: 1.7716111511428636 acc: 0.9\n",
      "loss: 1.8001475565821417 acc: 0.86\n",
      "loss: 1.8037600564391183 acc: 0.88\n",
      "loss: 1.8951604091906598 acc: 0.85\n",
      "loss: 1.8225253855549886 acc: 0.86\n",
      "loss: 1.8412302309133886 acc: 0.75\n",
      "loss: 1.8735187540932647 acc: 0.83\n",
      "loss: 1.838527092800546 acc: 0.87\n",
      "loss: 1.8490527604295115 acc: 0.83\n",
      "loss: 1.8654416802850624 acc: 0.75\n",
      "loss: 1.9001068048788263 acc: 0.75\n",
      "loss: 1.8262637269198936 acc: 0.9\n",
      "loss: 1.7841532441662462 acc: 0.9\n",
      "loss: 1.8197634985135351 acc: 0.83\n",
      "loss: 1.7888049502021783 acc: 0.9\n",
      "loss: 1.7559180849611897 acc: 0.92\n",
      "loss: 1.8064633724361305 acc: 0.88\n",
      "loss: 1.7816504449640238 acc: 0.85\n",
      "loss: 1.8165902910912504 acc: 0.89\n",
      "loss: 1.8686026683254848 acc: 0.88\n",
      "loss: 1.8489072043842663 acc: 0.87\n",
      "loss: 1.7506302642280516 acc: 0.9\n",
      "loss: 1.717244822762802 acc: 0.96\n",
      "loss: 1.7460801696300112 acc: 0.93\n",
      "loss: 1.7423868496253692 acc: 0.97\n",
      "loss: 1.8018143108225613 acc: 0.86\n",
      "loss: 1.7889038085048226 acc: 0.83\n",
      "loss: 1.7404041510954198 acc: 0.82\n",
      "loss: 1.8054008100171044 acc: 0.9\n",
      "loss: 1.8120466913385147 acc: 0.92\n",
      "loss: 1.902385978662817 acc: 0.78\n",
      "loss: 1.7047255977150104 acc: 0.94\n",
      "loss: 1.8917799123695032 acc: 0.79\n",
      "\n",
      "Epoch [0]\t Average training loss 1.8595\t Average training accuracy 0.7672\n",
      "Epoch [0]\t Average validation loss 1.8125\t Average validation accuracy 0.8662\n",
      "\n",
      "loss: 1.8542883299567614 acc: 0.85\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 1.8543\t Accuracy 0.8500\n",
      "loss: 1.8467902917993069 acc: 0.84\n",
      "loss: 1.8397452422983809 acc: 0.83\n",
      "loss: 1.8375273809451653 acc: 0.8\n",
      "loss: 1.8087199765653168 acc: 0.88\n",
      "loss: 1.853463341670427 acc: 0.85\n",
      "loss: 1.7846403936124406 acc: 0.88\n",
      "loss: 1.8305800109821682 acc: 0.85\n",
      "loss: 1.8388151234116454 acc: 0.81\n",
      "loss: 1.8456971398527142 acc: 0.77\n",
      "loss: 1.8638166785603396 acc: 0.75\n",
      "loss: 1.8293815662563275 acc: 0.81\n",
      "loss: 1.8024880897469924 acc: 0.92\n",
      "loss: 1.8313418670082486 acc: 0.82\n",
      "loss: 1.7965455439313418 acc: 0.87\n",
      "loss: 1.8228360527652898 acc: 0.81\n",
      "loss: 1.829990022463091 acc: 0.84\n",
      "loss: 1.8634917058370066 acc: 0.8\n",
      "loss: 1.8524275241090786 acc: 0.81\n",
      "loss: 1.8546550217717377 acc: 0.82\n",
      "loss: 1.8158439518778875 acc: 0.87\n",
      "loss: 1.7954385418744685 acc: 0.9\n",
      "loss: 1.864556317153903 acc: 0.79\n",
      "loss: 1.84116182574153 acc: 0.84\n",
      "loss: 1.8730008365813022 acc: 0.77\n",
      "loss: 1.812557953665594 acc: 0.81\n",
      "loss: 1.835673301458442 acc: 0.82\n",
      "loss: 1.8634575939190743 acc: 0.82\n",
      "loss: 1.819091632450826 acc: 0.86\n",
      "loss: 1.832266099689579 acc: 0.83\n",
      "loss: 1.8440039265052233 acc: 0.81\n",
      "loss: 1.8175386167568934 acc: 0.86\n",
      "loss: 1.8733355487546681 acc: 0.82\n",
      "loss: 1.8080292749286797 acc: 0.86\n",
      "loss: 1.791983165182265 acc: 0.85\n",
      "loss: 1.846285739366537 acc: 0.77\n",
      "loss: 1.8004829185712015 acc: 0.86\n",
      "loss: 1.8245323147405847 acc: 0.87\n",
      "loss: 1.7859850217961237 acc: 0.89\n",
      "loss: 1.8428271470525395 acc: 0.83\n",
      "loss: 1.8183692810312748 acc: 0.84\n",
      "loss: 1.7940030979686155 acc: 0.83\n",
      "loss: 1.8001988540662381 acc: 0.79\n",
      "loss: 1.8500959455610937 acc: 0.81\n",
      "loss: 1.8212872213098503 acc: 0.84\n",
      "loss: 1.842690749376471 acc: 0.83\n",
      "loss: 1.8756702566692125 acc: 0.75\n",
      "loss: 1.8360858155319502 acc: 0.78\n",
      "loss: 1.8373993036992087 acc: 0.81\n",
      "loss: 1.8831269342652575 acc: 0.82\n",
      "loss: 1.8416178024214667 acc: 0.83\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 1.8416\t Accuracy 0.8300\n",
      "loss: 1.8236126059310143 acc: 0.82\n",
      "loss: 1.8553081457441665 acc: 0.76\n",
      "loss: 1.8203749987278461 acc: 0.87\n",
      "loss: 1.8292795972700637 acc: 0.86\n",
      "loss: 1.806277075455248 acc: 0.83\n",
      "loss: 1.8175380955876406 acc: 0.8\n",
      "loss: 1.8239445337994855 acc: 0.83\n",
      "loss: 1.8154555059300455 acc: 0.82\n",
      "loss: 1.871942890767655 acc: 0.75\n",
      "loss: 1.8259023328389572 acc: 0.84\n",
      "loss: 1.84088309628313 acc: 0.79\n",
      "loss: 1.8548114329000938 acc: 0.82\n",
      "loss: 1.841126092683638 acc: 0.82\n",
      "loss: 1.8254255847315721 acc: 0.87\n",
      "loss: 1.8475892613197704 acc: 0.81\n",
      "loss: 1.8312577178217992 acc: 0.88\n",
      "loss: 1.8122710302197722 acc: 0.85\n",
      "loss: 1.8274709246057501 acc: 0.83\n",
      "loss: 1.82857484384093 acc: 0.8\n",
      "loss: 1.842873677482447 acc: 0.83\n",
      "loss: 1.8690604156476893 acc: 0.83\n",
      "loss: 1.857790807898612 acc: 0.77\n",
      "loss: 1.8117978932110028 acc: 0.84\n",
      "loss: 1.8287620090600714 acc: 0.87\n",
      "loss: 1.8445990827031722 acc: 0.79\n",
      "loss: 1.8090776132865247 acc: 0.88\n",
      "loss: 1.8263751843346876 acc: 0.84\n",
      "loss: 1.817139747565342 acc: 0.88\n",
      "loss: 1.7913920443920113 acc: 0.92\n",
      "loss: 1.8301961185454183 acc: 0.84\n",
      "loss: 1.833891647967088 acc: 0.84\n",
      "loss: 1.8281544401132666 acc: 0.86\n",
      "loss: 1.8230710381366304 acc: 0.81\n",
      "loss: 1.8019469568004773 acc: 0.83\n",
      "loss: 1.8397411110372113 acc: 0.81\n",
      "loss: 1.8009102140920437 acc: 0.89\n",
      "loss: 1.811321252150242 acc: 0.86\n",
      "loss: 1.8369286882284164 acc: 0.82\n",
      "loss: 1.796507721979852 acc: 0.85\n",
      "loss: 1.8240970766354743 acc: 0.85\n",
      "loss: 1.8427937827175098 acc: 0.8\n",
      "loss: 1.8565869066506158 acc: 0.76\n",
      "loss: 1.833703519963325 acc: 0.84\n",
      "loss: 1.8402303231652757 acc: 0.85\n",
      "loss: 1.810993790631006 acc: 0.87\n",
      "loss: 1.8351739174221515 acc: 0.81\n",
      "loss: 1.8105311426935433 acc: 0.84\n",
      "loss: 1.8278827932009067 acc: 0.79\n",
      "loss: 1.8284116207328316 acc: 0.75\n",
      "loss: 1.8187668264900663 acc: 0.81\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 1.8188\t Accuracy 0.8100\n",
      "loss: 1.7967592192021917 acc: 0.88\n",
      "loss: 1.8016062030171291 acc: 0.86\n",
      "loss: 1.7814733066380297 acc: 0.86\n",
      "loss: 1.9022401992900548 acc: 0.74\n",
      "loss: 1.8499069688882777 acc: 0.84\n",
      "loss: 1.8684338040314026 acc: 0.8\n",
      "loss: 1.8233371844810224 acc: 0.9\n",
      "loss: 1.8353245249725765 acc: 0.85\n",
      "loss: 1.8545926179118744 acc: 0.87\n",
      "loss: 1.8094128051983807 acc: 0.87\n",
      "loss: 1.853122255649269 acc: 0.86\n",
      "loss: 1.813684264807776 acc: 0.85\n",
      "loss: 1.847845796928097 acc: 0.83\n",
      "loss: 1.8366484964135927 acc: 0.82\n",
      "loss: 1.826040564439426 acc: 0.8\n",
      "loss: 1.839067159389201 acc: 0.82\n",
      "loss: 1.8402884432089115 acc: 0.84\n",
      "loss: 1.822115362275659 acc: 0.83\n",
      "loss: 1.821793571655716 acc: 0.83\n",
      "loss: 1.8154611584731741 acc: 0.89\n",
      "loss: 1.8742546460564637 acc: 0.78\n",
      "loss: 1.7993344200559163 acc: 0.85\n",
      "loss: 1.8205887328766182 acc: 0.85\n",
      "loss: 1.8176871404446475 acc: 0.82\n",
      "loss: 1.7797142483022892 acc: 0.86\n",
      "loss: 1.8108177289101015 acc: 0.83\n",
      "loss: 1.8668908944404907 acc: 0.84\n",
      "loss: 1.8556929415526948 acc: 0.76\n",
      "loss: 1.8741959221592905 acc: 0.73\n",
      "loss: 1.8506769649859507 acc: 0.86\n",
      "loss: 1.8642337314541733 acc: 0.82\n",
      "loss: 1.8257192712556165 acc: 0.84\n",
      "loss: 1.8526450828958343 acc: 0.78\n",
      "loss: 1.8389449309060169 acc: 0.78\n",
      "loss: 1.8142534057603614 acc: 0.83\n",
      "loss: 1.843948012296961 acc: 0.8\n",
      "loss: 1.8536792218547422 acc: 0.8\n",
      "loss: 1.852906356594193 acc: 0.78\n",
      "loss: 1.8120057237827887 acc: 0.84\n",
      "loss: 1.8419939618325645 acc: 0.83\n",
      "loss: 1.8123564021025829 acc: 0.82\n",
      "loss: 1.8088390064618087 acc: 0.87\n",
      "loss: 1.7752282856598898 acc: 0.9\n",
      "loss: 1.818643740014556 acc: 0.85\n",
      "loss: 1.846905150786134 acc: 0.85\n",
      "loss: 1.832476824354437 acc: 0.82\n",
      "loss: 1.7834362230054603 acc: 0.91\n",
      "loss: 1.8037572713199614 acc: 0.85\n",
      "loss: 1.8071166992956051 acc: 0.87\n",
      "loss: 1.7880567899897692 acc: 0.87\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 1.7881\t Accuracy 0.8700\n",
      "loss: 1.8083762917624173 acc: 0.84\n",
      "loss: 1.8397789980916084 acc: 0.81\n",
      "loss: 1.8425034566388705 acc: 0.79\n",
      "loss: 1.8378242413409402 acc: 0.87\n",
      "loss: 1.789971333130706 acc: 0.88\n",
      "loss: 1.79494704187241 acc: 0.85\n",
      "loss: 1.826321480736841 acc: 0.83\n",
      "loss: 1.8313976450558218 acc: 0.81\n",
      "loss: 1.8655217739928673 acc: 0.8\n",
      "loss: 1.8221897270644702 acc: 0.83\n",
      "loss: 1.808771909599766 acc: 0.81\n",
      "loss: 1.8225777844477573 acc: 0.85\n",
      "loss: 1.7986857019324827 acc: 0.82\n",
      "loss: 1.876082500448474 acc: 0.75\n",
      "loss: 1.8535420952018151 acc: 0.84\n",
      "loss: 1.8353809773168508 acc: 0.79\n",
      "loss: 1.808120341189457 acc: 0.82\n",
      "loss: 1.8005381461963759 acc: 0.84\n",
      "loss: 1.7847280089043907 acc: 0.89\n",
      "loss: 1.7962185805453863 acc: 0.86\n",
      "loss: 1.8452895541443124 acc: 0.82\n",
      "loss: 1.8499091112533594 acc: 0.78\n",
      "loss: 1.7948006682142335 acc: 0.83\n",
      "loss: 1.8310523417927904 acc: 0.81\n",
      "loss: 1.8255497894811754 acc: 0.82\n",
      "loss: 1.8495612595220448 acc: 0.84\n",
      "loss: 1.854509364062447 acc: 0.82\n",
      "loss: 1.822355285325916 acc: 0.85\n",
      "loss: 1.7737514758281103 acc: 0.9\n",
      "loss: 1.8065565006761473 acc: 0.91\n",
      "loss: 1.846210472882304 acc: 0.77\n",
      "loss: 1.825823325076664 acc: 0.87\n",
      "loss: 1.8169477521808626 acc: 0.83\n",
      "loss: 1.8038475230906215 acc: 0.82\n",
      "loss: 1.8322982805109786 acc: 0.82\n",
      "loss: 1.830313202572561 acc: 0.82\n",
      "loss: 1.8172386803942877 acc: 0.86\n",
      "loss: 1.8182637241757675 acc: 0.85\n",
      "loss: 1.822617885073225 acc: 0.88\n",
      "loss: 1.8087861960026075 acc: 0.84\n",
      "loss: 1.8136075374794935 acc: 0.86\n",
      "loss: 1.851337326697471 acc: 0.76\n",
      "loss: 1.835370168471932 acc: 0.81\n",
      "loss: 1.8279693331598974 acc: 0.86\n",
      "loss: 1.80041234271122 acc: 0.82\n",
      "loss: 1.884810430399388 acc: 0.78\n",
      "loss: 1.832388152412194 acc: 0.85\n",
      "loss: 1.8296193955824223 acc: 0.86\n",
      "loss: 1.8367023599925714 acc: 0.8\n",
      "loss: 1.8669610564889978 acc: 0.81\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 1.8670\t Accuracy 0.8100\n",
      "loss: 1.827463716475699 acc: 0.84\n",
      "loss: 1.8015731809291569 acc: 0.88\n",
      "loss: 1.8163557922275706 acc: 0.87\n",
      "loss: 1.815708168822411 acc: 0.87\n",
      "loss: 1.8262080158579082 acc: 0.82\n",
      "loss: 1.8369496172240485 acc: 0.85\n",
      "loss: 1.8377789087903487 acc: 0.81\n",
      "loss: 1.8295747146836938 acc: 0.87\n",
      "loss: 1.8457396539186532 acc: 0.8\n",
      "loss: 1.8658911087785544 acc: 0.77\n",
      "loss: 1.8693969248722682 acc: 0.82\n",
      "loss: 1.8850089670796695 acc: 0.74\n",
      "loss: 1.7979403376211378 acc: 0.91\n",
      "loss: 1.8436614881757847 acc: 0.84\n",
      "loss: 1.836760196706632 acc: 0.82\n",
      "loss: 1.7916783293544276 acc: 0.84\n",
      "loss: 1.8350755292676117 acc: 0.83\n",
      "loss: 1.8259304246405903 acc: 0.81\n",
      "loss: 1.8248043532887928 acc: 0.8\n",
      "loss: 1.809641764332101 acc: 0.9\n",
      "loss: 1.858261378605027 acc: 0.81\n",
      "loss: 1.8557119024629174 acc: 0.82\n",
      "loss: 1.8173392668572346 acc: 0.82\n",
      "loss: 1.8212089530093132 acc: 0.82\n",
      "loss: 1.8068577272917599 acc: 0.85\n",
      "loss: 1.8220001601698195 acc: 0.87\n",
      "loss: 1.87913048439284 acc: 0.73\n",
      "loss: 1.8932459106223232 acc: 0.76\n",
      "loss: 1.841646461425667 acc: 0.84\n",
      "loss: 1.8448638022705128 acc: 0.82\n",
      "loss: 1.7928611254932778 acc: 0.86\n",
      "loss: 1.8307810422719113 acc: 0.82\n",
      "loss: 1.8455500666829792 acc: 0.82\n",
      "loss: 1.8489618213170198 acc: 0.83\n",
      "loss: 1.8395493062449884 acc: 0.86\n",
      "loss: 1.8343335432773804 acc: 0.83\n",
      "loss: 1.8321430501151317 acc: 0.86\n",
      "loss: 1.9182820971739025 acc: 0.74\n",
      "loss: 1.8320669862233012 acc: 0.9\n",
      "loss: 1.8063971877582425 acc: 0.86\n",
      "loss: 1.8703321892414133 acc: 0.83\n",
      "loss: 1.848985142893081 acc: 0.84\n",
      "loss: 1.8576247834016113 acc: 0.82\n",
      "loss: 1.8223891477883405 acc: 0.9\n",
      "loss: 1.8676434598796998 acc: 0.77\n",
      "loss: 1.7859913226503565 acc: 0.9\n",
      "loss: 1.8008542946443316 acc: 0.9\n",
      "loss: 1.8750136655612186 acc: 0.77\n",
      "loss: 1.8569415016508888 acc: 0.83\n",
      "loss: 1.812824054295979 acc: 0.88\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 1.8128\t Accuracy 0.8800\n",
      "loss: 1.8356276398523659 acc: 0.79\n",
      "loss: 1.788715553985904 acc: 0.86\n",
      "loss: 1.82058111983948 acc: 0.9\n",
      "loss: 1.8005031528093403 acc: 0.82\n",
      "loss: 1.8439796957006755 acc: 0.82\n",
      "loss: 1.8179594458317567 acc: 0.88\n",
      "loss: 1.8483532380345813 acc: 0.87\n",
      "loss: 1.7941375832252893 acc: 0.87\n",
      "loss: 1.8436164396735257 acc: 0.79\n",
      "loss: 1.824869617108003 acc: 0.85\n",
      "loss: 1.8226270481754359 acc: 0.83\n",
      "loss: 1.8164498670889986 acc: 0.85\n",
      "loss: 1.8239656183108455 acc: 0.8\n",
      "loss: 1.7935990030616553 acc: 0.87\n",
      "loss: 1.8184509075540867 acc: 0.86\n",
      "loss: 1.8492989516668021 acc: 0.79\n",
      "loss: 1.7751797973841656 acc: 0.89\n",
      "loss: 1.85453896875579 acc: 0.82\n",
      "loss: 1.8497170419940234 acc: 0.8\n",
      "loss: 1.8962464994812995 acc: 0.72\n",
      "loss: 1.8654997864458647 acc: 0.79\n",
      "loss: 1.8531185435855722 acc: 0.77\n",
      "loss: 1.8268169456272398 acc: 0.84\n",
      "loss: 1.7907531521756763 acc: 0.89\n",
      "loss: 1.8341108302036957 acc: 0.83\n",
      "loss: 1.8266736588942631 acc: 0.84\n",
      "loss: 1.7805583196132466 acc: 0.87\n",
      "loss: 1.8272672541828143 acc: 0.91\n",
      "loss: 1.8194034585265524 acc: 0.88\n",
      "loss: 1.859766968181693 acc: 0.86\n",
      "loss: 1.8268168557317737 acc: 0.81\n",
      "loss: 1.7950358418263945 acc: 0.9\n",
      "loss: 1.8650751811724138 acc: 0.81\n",
      "loss: 1.8735253462944699 acc: 0.74\n",
      "loss: 1.8281622338636327 acc: 0.81\n",
      "loss: 1.8227718273573326 acc: 0.85\n",
      "loss: 1.848615495131011 acc: 0.79\n",
      "loss: 1.8568066581141693 acc: 0.74\n",
      "loss: 1.8506808758624635 acc: 0.78\n",
      "loss: 1.788454895046692 acc: 0.86\n",
      "loss: 1.8552061653896805 acc: 0.79\n",
      "loss: 1.8269911269185426 acc: 0.83\n",
      "loss: 1.819564676413307 acc: 0.83\n",
      "loss: 1.8231944084818008 acc: 0.85\n",
      "loss: 1.8459744404560319 acc: 0.84\n",
      "loss: 1.8596238484957215 acc: 0.77\n",
      "loss: 1.8300535303498615 acc: 0.87\n",
      "loss: 1.8180145709976967 acc: 0.84\n",
      "loss: 1.847193692039804 acc: 0.81\n",
      "loss: 1.8270639315507997 acc: 0.83\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 1.8271\t Accuracy 0.8300\n",
      "loss: 1.8155448811543966 acc: 0.86\n",
      "loss: 1.7974781148168615 acc: 0.86\n",
      "loss: 1.8431948357211987 acc: 0.79\n",
      "loss: 1.8310686905628901 acc: 0.86\n",
      "loss: 1.7983516517547184 acc: 0.87\n",
      "loss: 1.8587999346525361 acc: 0.81\n",
      "loss: 1.8267618217264459 acc: 0.9\n",
      "loss: 1.8502276074480295 acc: 0.81\n",
      "loss: 1.7934657980220694 acc: 0.89\n",
      "loss: 1.8364696884254128 acc: 0.8\n",
      "loss: 1.8478926247982086 acc: 0.88\n",
      "loss: 1.801583610210177 acc: 0.85\n",
      "loss: 1.8240186049836695 acc: 0.81\n",
      "loss: 1.7942830250411683 acc: 0.84\n",
      "loss: 1.8499211782217493 acc: 0.81\n",
      "loss: 1.8612259881738902 acc: 0.84\n",
      "loss: 1.8596806826559285 acc: 0.87\n",
      "loss: 1.862828191151289 acc: 0.77\n",
      "loss: 1.8657627952095481 acc: 0.76\n",
      "loss: 1.8135668729793315 acc: 0.83\n",
      "loss: 1.8007054991490852 acc: 0.8\n",
      "loss: 1.8216954355089487 acc: 0.87\n",
      "loss: 1.797079692238366 acc: 0.83\n",
      "loss: 1.8215734495635068 acc: 0.83\n",
      "loss: 1.8430993978311028 acc: 0.84\n",
      "loss: 1.8868419836823218 acc: 0.75\n",
      "loss: 1.8202680416542143 acc: 0.87\n",
      "loss: 1.8634431939339704 acc: 0.8\n",
      "loss: 1.8465525392830366 acc: 0.8\n",
      "loss: 1.7945277752810256 acc: 0.78\n",
      "loss: 1.821636261711117 acc: 0.84\n",
      "loss: 1.7991505180388787 acc: 0.83\n",
      "loss: 1.8280734633937066 acc: 0.86\n",
      "loss: 1.8448942042804866 acc: 0.8\n",
      "loss: 1.8922538886003302 acc: 0.78\n",
      "loss: 1.8571493008744655 acc: 0.8\n",
      "loss: 1.8265659961650784 acc: 0.82\n",
      "loss: 1.7740126239642044 acc: 0.88\n",
      "loss: 1.7895359759409621 acc: 0.88\n",
      "loss: 1.876673068049995 acc: 0.75\n",
      "loss: 1.8370104904816438 acc: 0.81\n",
      "loss: 1.8346886464012773 acc: 0.85\n",
      "loss: 1.8201552344734073 acc: 0.88\n",
      "loss: 1.8678925929128394 acc: 0.77\n",
      "loss: 1.8137388298798742 acc: 0.84\n",
      "loss: 1.83516833048963 acc: 0.85\n",
      "loss: 1.8615132358743522 acc: 0.81\n",
      "loss: 1.8708613156525329 acc: 0.8\n",
      "loss: 1.8724876513744615 acc: 0.78\n",
      "loss: 1.7947420454441696 acc: 0.92\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 1.7947\t Accuracy 0.9200\n",
      "loss: 1.8778030832321722 acc: 0.76\n",
      "loss: 1.8145305056457948 acc: 0.85\n",
      "loss: 1.8116456663946239 acc: 0.84\n",
      "loss: 1.8031051976390315 acc: 0.86\n",
      "loss: 1.8069858246180823 acc: 0.88\n",
      "loss: 1.7922889160602649 acc: 0.84\n",
      "loss: 1.874374322115096 acc: 0.81\n",
      "loss: 1.8376954061819206 acc: 0.86\n",
      "loss: 1.8431700758367284 acc: 0.84\n",
      "loss: 1.8828184244982313 acc: 0.78\n",
      "loss: 1.853753985119483 acc: 0.76\n",
      "loss: 1.8194446691343513 acc: 0.89\n",
      "loss: 1.8820186790219304 acc: 0.77\n",
      "loss: 1.8354732360479087 acc: 0.82\n",
      "loss: 1.8397369694827697 acc: 0.83\n",
      "loss: 1.8171061326972688 acc: 0.83\n",
      "loss: 1.8362242861357623 acc: 0.84\n",
      "loss: 1.8013691244653476 acc: 0.85\n",
      "loss: 1.8685399444152757 acc: 0.81\n",
      "loss: 1.8261913093460798 acc: 0.85\n",
      "loss: 1.8817104704598617 acc: 0.75\n",
      "loss: 1.8347361413121905 acc: 0.82\n",
      "loss: 1.8568656353610242 acc: 0.82\n",
      "loss: 1.8080142940659867 acc: 0.83\n",
      "loss: 1.8038956517656235 acc: 0.92\n",
      "loss: 1.8589067444752978 acc: 0.79\n",
      "loss: 1.8438108739428294 acc: 0.83\n",
      "loss: 1.8101477616177422 acc: 0.88\n",
      "loss: 1.844565831426186 acc: 0.84\n",
      "loss: 1.8381918723016688 acc: 0.82\n",
      "loss: 1.8500075243094187 acc: 0.78\n",
      "loss: 1.7763103013372932 acc: 0.85\n",
      "loss: 1.7908573693883658 acc: 0.87\n",
      "loss: 1.8420788122336085 acc: 0.81\n",
      "loss: 1.7999120685519017 acc: 0.84\n",
      "loss: 1.7916768329651203 acc: 0.86\n",
      "loss: 1.8203011149563955 acc: 0.85\n",
      "loss: 1.8267311452649415 acc: 0.82\n",
      "loss: 1.848321577891164 acc: 0.76\n",
      "loss: 1.8534249162663103 acc: 0.8\n",
      "loss: 1.8642634814111587 acc: 0.8\n",
      "loss: 1.7999248569233972 acc: 0.84\n",
      "loss: 1.8309838782399255 acc: 0.83\n",
      "loss: 1.8251913609388577 acc: 0.82\n",
      "loss: 1.806430925748285 acc: 0.81\n",
      "loss: 1.8524584584004986 acc: 0.83\n",
      "loss: 1.8584616064105381 acc: 0.88\n",
      "loss: 1.8229085661351165 acc: 0.81\n",
      "loss: 1.8436781075020687 acc: 0.84\n",
      "loss: 1.848777150009918 acc: 0.82\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 1.8488\t Accuracy 0.8200\n",
      "loss: 1.8603514400437362 acc: 0.82\n",
      "loss: 1.8267989616763496 acc: 0.76\n",
      "loss: 1.8080036875168037 acc: 0.89\n",
      "loss: 1.8371702702955925 acc: 0.85\n",
      "loss: 1.8509051171443571 acc: 0.82\n",
      "loss: 1.8231698114277979 acc: 0.84\n",
      "loss: 1.8806290563932806 acc: 0.78\n",
      "loss: 1.8324969846314665 acc: 0.84\n",
      "loss: 1.8204462468870366 acc: 0.79\n",
      "loss: 1.811436404925436 acc: 0.86\n",
      "loss: 1.8818933563506435 acc: 0.78\n",
      "loss: 1.8353944367636308 acc: 0.84\n",
      "loss: 1.8424063169241187 acc: 0.8\n",
      "loss: 1.7954923490960342 acc: 0.86\n",
      "loss: 1.8132481004742274 acc: 0.83\n",
      "loss: 1.8610338528448935 acc: 0.78\n",
      "loss: 1.8056520119800217 acc: 0.86\n",
      "loss: 1.8581627715053686 acc: 0.84\n",
      "loss: 1.8392681363040546 acc: 0.89\n",
      "loss: 1.819422814457318 acc: 0.81\n",
      "loss: 1.87094974748306 acc: 0.8\n",
      "loss: 1.8656518349678364 acc: 0.8\n",
      "loss: 1.7820290930511118 acc: 0.91\n",
      "loss: 1.8318373633592204 acc: 0.83\n",
      "loss: 1.8135833433482924 acc: 0.79\n",
      "loss: 1.8938111541459124 acc: 0.8\n",
      "loss: 1.8112405058279775 acc: 0.89\n",
      "loss: 1.8267754004495445 acc: 0.84\n",
      "loss: 1.7953430676187327 acc: 0.89\n",
      "loss: 1.8497237379789688 acc: 0.79\n",
      "loss: 1.904692469261298 acc: 0.65\n",
      "loss: 1.8144127584938392 acc: 0.82\n",
      "loss: 1.8220286929464438 acc: 0.81\n",
      "loss: 1.8442843497198067 acc: 0.79\n",
      "loss: 1.8492435630984363 acc: 0.8\n",
      "loss: 1.8585723437995614 acc: 0.85\n",
      "loss: 1.82195966532063 acc: 0.83\n",
      "loss: 1.8160520679076575 acc: 0.87\n",
      "loss: 1.8332442271406535 acc: 0.88\n",
      "loss: 1.8161365796227285 acc: 0.9\n",
      "loss: 1.8358444925109916 acc: 0.84\n",
      "loss: 1.8340531210631372 acc: 0.8\n",
      "loss: 1.8433878093202125 acc: 0.85\n",
      "loss: 1.8571950811429738 acc: 0.81\n",
      "loss: 1.833195044286792 acc: 0.8\n",
      "loss: 1.812654379979805 acc: 0.86\n",
      "loss: 1.8072928494375382 acc: 0.9\n",
      "loss: 1.812100703823594 acc: 0.85\n",
      "loss: 1.8076142108025763 acc: 0.88\n",
      "loss: 1.8798963466341307 acc: 0.79\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 1.8799\t Accuracy 0.7900\n",
      "loss: 1.8527625069292482 acc: 0.8\n",
      "loss: 1.8181988266447504 acc: 0.87\n",
      "loss: 1.832723182525864 acc: 0.82\n",
      "loss: 1.820416222444978 acc: 0.81\n",
      "loss: 1.813978979157251 acc: 0.81\n",
      "loss: 1.8281509490283867 acc: 0.79\n",
      "loss: 1.7880300214659706 acc: 0.92\n",
      "loss: 1.800794787731607 acc: 0.9\n",
      "loss: 1.792240394426518 acc: 0.91\n",
      "loss: 1.8326709679203368 acc: 0.82\n",
      "loss: 1.8103573181238093 acc: 0.89\n",
      "loss: 1.7825198105964548 acc: 0.92\n",
      "loss: 1.843146374638537 acc: 0.86\n",
      "loss: 1.8447710568349676 acc: 0.78\n",
      "loss: 1.8110518966497835 acc: 0.9\n",
      "loss: 1.8283378674235096 acc: 0.81\n",
      "loss: 1.8175195690542847 acc: 0.83\n",
      "loss: 1.8204529704113654 acc: 0.82\n",
      "loss: 1.8078623241164973 acc: 0.88\n",
      "loss: 1.814431340456607 acc: 0.87\n",
      "loss: 1.7855428015648371 acc: 0.87\n",
      "loss: 1.802703758081646 acc: 0.89\n",
      "loss: 1.8655470361325037 acc: 0.79\n",
      "loss: 1.8024127886064278 acc: 0.86\n",
      "loss: 1.799088256827101 acc: 0.85\n",
      "loss: 1.7781177444817615 acc: 0.89\n",
      "loss: 1.842652128447618 acc: 0.86\n",
      "loss: 1.8294451797175728 acc: 0.84\n",
      "loss: 1.8426854996642579 acc: 0.75\n",
      "loss: 1.8437223813353927 acc: 0.84\n",
      "loss: 1.834347693637345 acc: 0.76\n",
      "loss: 1.7974929723402153 acc: 0.86\n",
      "loss: 1.8372947115172011 acc: 0.85\n",
      "loss: 1.7982559077869977 acc: 0.87\n",
      "loss: 1.8094533372563135 acc: 0.9\n",
      "loss: 1.8192459805594239 acc: 0.87\n",
      "loss: 1.8479153824466987 acc: 0.78\n",
      "loss: 1.838190511468826 acc: 0.77\n",
      "loss: 1.854465663126473 acc: 0.79\n",
      "loss: 1.8167646295113635 acc: 0.85\n",
      "loss: 1.8510418826837005 acc: 0.81\n",
      "loss: 1.7912514331469458 acc: 0.87\n",
      "loss: 1.7861768194596421 acc: 0.9\n",
      "loss: 1.8251317564568843 acc: 0.84\n",
      "loss: 1.807582552321128 acc: 0.9\n",
      "loss: 1.842339549265744 acc: 0.84\n",
      "loss: 1.858176066466959 acc: 0.8\n",
      "loss: 1.8208913438626972 acc: 0.85\n",
      "loss: 1.8416923801700162 acc: 0.79\n",
      "loss: 1.8676440306191473 acc: 0.78\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 1.8676\t Accuracy 0.7800\n",
      "loss: 1.8314080339412755 acc: 0.85\n",
      "loss: 1.835961585730448 acc: 0.86\n",
      "loss: 1.816516676120709 acc: 0.92\n",
      "loss: 1.8571112701212034 acc: 0.89\n",
      "loss: 1.8010202926480487 acc: 0.84\n",
      "loss: 1.8039177449953294 acc: 0.88\n",
      "loss: 1.86692717282694 acc: 0.77\n",
      "loss: 1.8380903579271635 acc: 0.86\n",
      "loss: 1.814194062997894 acc: 0.82\n",
      "loss: 1.8061129836391516 acc: 0.84\n",
      "loss: 1.8017054411691706 acc: 0.87\n",
      "loss: 1.8189553599707102 acc: 0.86\n",
      "loss: 1.7971741640765289 acc: 0.88\n",
      "loss: 1.8853010692921517 acc: 0.77\n",
      "loss: 1.8618030046986038 acc: 0.81\n",
      "loss: 1.8492288192011563 acc: 0.8\n",
      "loss: 1.8277730054719563 acc: 0.82\n",
      "loss: 1.8285575190819627 acc: 0.84\n",
      "loss: 1.791189383176014 acc: 0.89\n",
      "loss: 1.8404276176809014 acc: 0.87\n",
      "loss: 1.8350693465763672 acc: 0.86\n",
      "loss: 1.8277006923975088 acc: 0.86\n",
      "loss: 1.835177293006675 acc: 0.82\n",
      "loss: 1.81939657373079 acc: 0.87\n",
      "loss: 1.8570227342457628 acc: 0.82\n",
      "loss: 1.8258490787874433 acc: 0.89\n",
      "loss: 1.8436328209436126 acc: 0.85\n",
      "loss: 1.8096607782116478 acc: 0.84\n",
      "loss: 1.8355774464109313 acc: 0.89\n",
      "loss: 1.842037690477192 acc: 0.81\n",
      "loss: 1.7469469024827038 acc: 0.91\n",
      "loss: 1.7782139209602479 acc: 0.91\n",
      "loss: 1.8424299874450054 acc: 0.76\n",
      "loss: 1.8327019950152483 acc: 0.8\n",
      "loss: 1.8210273521101104 acc: 0.83\n",
      "loss: 1.8169108180938642 acc: 0.85\n",
      "loss: 1.8229667824339737 acc: 0.84\n",
      "loss: 1.7887389724881817 acc: 0.89\n",
      "loss: 1.8106764611880166 acc: 0.83\n",
      "loss: 1.810839952060607 acc: 0.87\n",
      "loss: 1.8656201600455242 acc: 0.83\n",
      "loss: 1.8030526810679854 acc: 0.91\n",
      "loss: 1.7792397171517396 acc: 0.92\n",
      "loss: 1.794660189041816 acc: 0.88\n",
      "loss: 1.8441658800537726 acc: 0.82\n",
      "loss: 1.8141805654161476 acc: 0.84\n",
      "loss: 1.8086163651510534 acc: 0.86\n",
      "loss: 1.819804966286697 acc: 0.8\n",
      "loss: 1.8434607306471793 acc: 0.83\n",
      "loss: 1.7932204307611617 acc: 0.86\n",
      "loss: 1.7991641555606324 acc: 0.87\n",
      "loss: 1.8087921666455282 acc: 0.88\n",
      "loss: 1.8347844633792303 acc: 0.87\n",
      "loss: 1.8307259865590768 acc: 0.89\n",
      "loss: 1.80325849460969 acc: 0.85\n",
      "loss: 1.760892201795062 acc: 0.86\n",
      "loss: 1.7631575387536076 acc: 0.84\n",
      "loss: 1.8268498113384748 acc: 0.85\n",
      "loss: 1.7566205392764085 acc: 0.9\n",
      "loss: 1.8007333337215903 acc: 0.87\n",
      "loss: 1.7989917118990126 acc: 0.83\n",
      "loss: 1.8507498511505276 acc: 0.82\n",
      "loss: 1.864450661470517 acc: 0.83\n",
      "loss: 1.8719940352729194 acc: 0.86\n",
      "loss: 1.7938037644245604 acc: 0.91\n",
      "loss: 1.810868975749553 acc: 0.83\n",
      "loss: 1.763183116321912 acc: 0.91\n",
      "loss: 1.784215443933583 acc: 0.88\n",
      "loss: 1.7992816896287764 acc: 0.88\n",
      "loss: 1.877760997997497 acc: 0.85\n",
      "loss: 1.8317145768430114 acc: 0.88\n",
      "loss: 1.8200340144501395 acc: 0.75\n",
      "loss: 1.8634992520616924 acc: 0.86\n",
      "loss: 1.8314527481530205 acc: 0.88\n",
      "loss: 1.8521097911407511 acc: 0.79\n",
      "loss: 1.8689113584442472 acc: 0.77\n",
      "loss: 1.8829733748790993 acc: 0.78\n",
      "loss: 1.8293638986404988 acc: 0.9\n",
      "loss: 1.7987162973137807 acc: 0.88\n",
      "loss: 1.8221055515725961 acc: 0.84\n",
      "loss: 1.781211082989894 acc: 0.92\n",
      "loss: 1.7527752929355436 acc: 0.9\n",
      "loss: 1.7894509548561988 acc: 0.88\n",
      "loss: 1.7891052553824847 acc: 0.85\n",
      "loss: 1.8205888222095998 acc: 0.89\n",
      "loss: 1.8581617538896509 acc: 0.9\n",
      "loss: 1.846853341200335 acc: 0.84\n",
      "loss: 1.7317621612764384 acc: 0.89\n",
      "loss: 1.7152855649505054 acc: 0.97\n",
      "loss: 1.7468785833774394 acc: 0.96\n",
      "loss: 1.7510033223546824 acc: 0.96\n",
      "loss: 1.8022285195319363 acc: 0.89\n",
      "loss: 1.7905253574188733 acc: 0.84\n",
      "loss: 1.732581179716395 acc: 0.83\n",
      "loss: 1.8041769849811744 acc: 0.93\n",
      "loss: 1.8067596041709808 acc: 0.92\n",
      "loss: 1.9007564775871313 acc: 0.81\n",
      "loss: 1.709985675684828 acc: 0.95\n",
      "loss: 1.861717936144076 acc: 0.85\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8300\t Average training accuracy 0.8331\n",
      "Epoch [1]\t Average validation loss 1.8077\t Average validation accuracy 0.8690\n",
      "\n",
      "loss: 1.8862406409749475 acc: 0.75\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 1.8862\t Accuracy 0.7500\n",
      "loss: 1.802260623414609 acc: 0.89\n",
      "loss: 1.8079921290971155 acc: 0.84\n",
      "loss: 1.8047169253867943 acc: 0.91\n",
      "loss: 1.8408095197832761 acc: 0.83\n",
      "loss: 1.8213633460610579 acc: 0.82\n",
      "loss: 1.8439094342874023 acc: 0.84\n",
      "loss: 1.8283675948639675 acc: 0.83\n",
      "loss: 1.8080013984346903 acc: 0.86\n",
      "loss: 1.8884368730952317 acc: 0.77\n",
      "loss: 1.8107288419692187 acc: 0.84\n",
      "loss: 1.794448378822799 acc: 0.85\n",
      "loss: 1.8359635249273787 acc: 0.85\n",
      "loss: 1.8053169561185056 acc: 0.91\n",
      "loss: 1.7995355427277375 acc: 0.89\n",
      "loss: 1.808958004220749 acc: 0.85\n",
      "loss: 1.8206927758087925 acc: 0.87\n",
      "loss: 1.8430948619977932 acc: 0.83\n",
      "loss: 1.8218379116452184 acc: 0.85\n",
      "loss: 1.8241851659760062 acc: 0.83\n",
      "loss: 1.8062444412018726 acc: 0.91\n",
      "loss: 1.829068492833008 acc: 0.86\n",
      "loss: 1.8202509244607052 acc: 0.86\n",
      "loss: 1.8420082525050179 acc: 0.81\n",
      "loss: 1.835252905513926 acc: 0.81\n",
      "loss: 1.8253312227282406 acc: 0.86\n",
      "loss: 1.7969146715794353 acc: 0.87\n",
      "loss: 1.8002122452661267 acc: 0.82\n",
      "loss: 1.8029450369909634 acc: 0.89\n",
      "loss: 1.8322492887734478 acc: 0.79\n",
      "loss: 1.8413314133434044 acc: 0.88\n",
      "loss: 1.8407807110187604 acc: 0.82\n",
      "loss: 1.836289915146132 acc: 0.85\n",
      "loss: 1.8263335125168418 acc: 0.86\n",
      "loss: 1.8568632503994416 acc: 0.83\n",
      "loss: 1.8010047560693507 acc: 0.88\n",
      "loss: 1.8696355487811496 acc: 0.84\n",
      "loss: 1.842630789450788 acc: 0.8\n",
      "loss: 1.7744598572810226 acc: 0.85\n",
      "loss: 1.8482873652954583 acc: 0.83\n",
      "loss: 1.8220266713097037 acc: 0.82\n",
      "loss: 1.8293582967862205 acc: 0.87\n",
      "loss: 1.8070938765192597 acc: 0.9\n",
      "loss: 1.777591275796297 acc: 0.92\n",
      "loss: 1.8351702689449363 acc: 0.84\n",
      "loss: 1.8442455347055604 acc: 0.82\n",
      "loss: 1.8131268118019568 acc: 0.86\n",
      "loss: 1.8467203852562961 acc: 0.79\n",
      "loss: 1.8579456091863933 acc: 0.81\n",
      "loss: 1.8196214653537799 acc: 0.86\n",
      "loss: 1.8390729818750182 acc: 0.81\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 1.8391\t Accuracy 0.8100\n",
      "loss: 1.817862568277432 acc: 0.9\n",
      "loss: 1.7991242768169697 acc: 0.82\n",
      "loss: 1.8087713346295766 acc: 0.83\n",
      "loss: 1.7875884163464377 acc: 0.86\n",
      "loss: 1.8374934499622009 acc: 0.77\n",
      "loss: 1.831692452123233 acc: 0.83\n",
      "loss: 1.84436068267883 acc: 0.85\n",
      "loss: 1.8139258912845246 acc: 0.84\n",
      "loss: 1.8683161210617119 acc: 0.84\n",
      "loss: 1.8412084520601513 acc: 0.86\n",
      "loss: 1.8053198981149785 acc: 0.86\n",
      "loss: 1.8189091388561716 acc: 0.86\n",
      "loss: 1.8514821055588198 acc: 0.79\n",
      "loss: 1.8078402357189283 acc: 0.88\n",
      "loss: 1.7854035403828896 acc: 0.87\n",
      "loss: 1.8065757079728884 acc: 0.86\n",
      "loss: 1.8508849162598353 acc: 0.81\n",
      "loss: 1.8139463976050774 acc: 0.82\n",
      "loss: 1.813169695021895 acc: 0.83\n",
      "loss: 1.8569625246301382 acc: 0.8\n",
      "loss: 1.8249254700071917 acc: 0.83\n",
      "loss: 1.8231339775972217 acc: 0.88\n",
      "loss: 1.848127470499165 acc: 0.87\n",
      "loss: 1.7768584480846088 acc: 0.89\n",
      "loss: 1.8295181595935133 acc: 0.84\n",
      "loss: 1.8202755573087408 acc: 0.82\n",
      "loss: 1.8121023774493004 acc: 0.88\n",
      "loss: 1.7924299797842158 acc: 0.84\n",
      "loss: 1.8392673108953759 acc: 0.81\n",
      "loss: 1.7510752462869932 acc: 0.9\n",
      "loss: 1.7916846226636816 acc: 0.89\n",
      "loss: 1.8127807076493283 acc: 0.81\n",
      "loss: 1.8153536393195915 acc: 0.84\n",
      "loss: 1.8066678531971605 acc: 0.89\n",
      "loss: 1.81940148715584 acc: 0.81\n",
      "loss: 1.8523672800022672 acc: 0.81\n",
      "loss: 1.8456063002263958 acc: 0.82\n",
      "loss: 1.8151048516627855 acc: 0.9\n",
      "loss: 1.8391505118834914 acc: 0.82\n",
      "loss: 1.8373068542267057 acc: 0.81\n",
      "loss: 1.788622121218026 acc: 0.84\n",
      "loss: 1.8516915017561768 acc: 0.81\n",
      "loss: 1.8003684266561413 acc: 0.85\n",
      "loss: 1.799212474640006 acc: 0.86\n",
      "loss: 1.8122327407815524 acc: 0.85\n",
      "loss: 1.8473667261118607 acc: 0.8\n",
      "loss: 1.853239611508257 acc: 0.82\n",
      "loss: 1.8428402888749187 acc: 0.81\n",
      "loss: 1.8350865588238332 acc: 0.81\n",
      "loss: 1.8175536757421649 acc: 0.83\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 1.8176\t Accuracy 0.8300\n",
      "loss: 1.8196551967914478 acc: 0.84\n",
      "loss: 1.8660128735149966 acc: 0.77\n",
      "loss: 1.8257076098974392 acc: 0.84\n",
      "loss: 1.8185467209992263 acc: 0.82\n",
      "loss: 1.8485452348290474 acc: 0.81\n",
      "loss: 1.824094270262629 acc: 0.81\n",
      "loss: 1.8100742289678726 acc: 0.84\n",
      "loss: 1.7858752085497307 acc: 0.91\n",
      "loss: 1.80251934318639 acc: 0.87\n",
      "loss: 1.8804730806960481 acc: 0.78\n",
      "loss: 1.8345679820337528 acc: 0.85\n",
      "loss: 1.8318627545279031 acc: 0.85\n",
      "loss: 1.8026337484771673 acc: 0.83\n",
      "loss: 1.8148853108058205 acc: 0.86\n",
      "loss: 1.8419103086220767 acc: 0.88\n",
      "loss: 1.827859153957139 acc: 0.81\n",
      "loss: 1.794258452720755 acc: 0.85\n",
      "loss: 1.8372797811730046 acc: 0.81\n",
      "loss: 1.8357698225705312 acc: 0.85\n",
      "loss: 1.8045996601385932 acc: 0.92\n",
      "loss: 1.790249038528184 acc: 0.85\n",
      "loss: 1.837157079264902 acc: 0.83\n",
      "loss: 1.8368862413701894 acc: 0.82\n",
      "loss: 1.8537642018774272 acc: 0.79\n",
      "loss: 1.8220248232968117 acc: 0.84\n",
      "loss: 1.8125644872171716 acc: 0.84\n",
      "loss: 1.8178345606736008 acc: 0.88\n",
      "loss: 1.8184477104542964 acc: 0.82\n",
      "loss: 1.8175555094697455 acc: 0.82\n",
      "loss: 1.830476889648861 acc: 0.87\n",
      "loss: 1.8157796370830428 acc: 0.91\n",
      "loss: 1.8540783241111194 acc: 0.8\n",
      "loss: 1.826849067615517 acc: 0.84\n",
      "loss: 1.8229181403680759 acc: 0.85\n",
      "loss: 1.849581898347102 acc: 0.83\n",
      "loss: 1.834292807391516 acc: 0.77\n",
      "loss: 1.8465909400952443 acc: 0.86\n",
      "loss: 1.824813782864711 acc: 0.86\n",
      "loss: 1.801971089941569 acc: 0.88\n",
      "loss: 1.8431145574899332 acc: 0.84\n",
      "loss: 1.7880165050695231 acc: 0.87\n",
      "loss: 1.7732113139610408 acc: 0.88\n",
      "loss: 1.8146933889160142 acc: 0.8\n",
      "loss: 1.825710760615963 acc: 0.84\n",
      "loss: 1.8838094643448966 acc: 0.75\n",
      "loss: 1.834000434838037 acc: 0.86\n",
      "loss: 1.8260883603181102 acc: 0.85\n",
      "loss: 1.8378660692858468 acc: 0.79\n",
      "loss: 1.8158919071946764 acc: 0.79\n",
      "loss: 1.8213372775510772 acc: 0.8\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 1.8213\t Accuracy 0.8000\n",
      "loss: 1.8210084567537006 acc: 0.84\n",
      "loss: 1.844219675898143 acc: 0.83\n",
      "loss: 1.7831981695178447 acc: 0.88\n",
      "loss: 1.824471088833335 acc: 0.85\n",
      "loss: 1.8182997506398282 acc: 0.88\n",
      "loss: 1.8195878029698271 acc: 0.87\n",
      "loss: 1.8017679074683999 acc: 0.81\n",
      "loss: 1.815482842191791 acc: 0.86\n",
      "loss: 1.8314552684009973 acc: 0.85\n",
      "loss: 1.8236858779974368 acc: 0.85\n",
      "loss: 1.80827706130799 acc: 0.87\n",
      "loss: 1.8335021658805362 acc: 0.83\n",
      "loss: 1.85242337669158 acc: 0.77\n",
      "loss: 1.805690729821957 acc: 0.84\n",
      "loss: 1.8429898414182782 acc: 0.8\n",
      "loss: 1.849701015318939 acc: 0.75\n",
      "loss: 1.8302853815174691 acc: 0.83\n",
      "loss: 1.807525445127156 acc: 0.87\n",
      "loss: 1.8123611259718546 acc: 0.92\n",
      "loss: 1.791047606716115 acc: 0.88\n",
      "loss: 1.7972555066866815 acc: 0.89\n",
      "loss: 1.8417334979801447 acc: 0.81\n",
      "loss: 1.8464440204219956 acc: 0.77\n",
      "loss: 1.8179089501182386 acc: 0.9\n",
      "loss: 1.8153626871999915 acc: 0.89\n",
      "loss: 1.8168633540678534 acc: 0.88\n",
      "loss: 1.8305804632253921 acc: 0.78\n",
      "loss: 1.821522989925663 acc: 0.78\n",
      "loss: 1.760262577466162 acc: 0.87\n",
      "loss: 1.821139139731786 acc: 0.8\n",
      "loss: 1.7992258847038967 acc: 0.83\n",
      "loss: 1.797013371627036 acc: 0.81\n",
      "loss: 1.784615342767301 acc: 0.87\n",
      "loss: 1.8537703905319034 acc: 0.81\n",
      "loss: 1.8258492673519942 acc: 0.81\n",
      "loss: 1.8428022866488782 acc: 0.81\n",
      "loss: 1.7923190374990086 acc: 0.89\n",
      "loss: 1.8700457331050035 acc: 0.76\n",
      "loss: 1.866012227611632 acc: 0.84\n",
      "loss: 1.8406708592095082 acc: 0.8\n",
      "loss: 1.8381071608619686 acc: 0.83\n",
      "loss: 1.7816862009782313 acc: 0.87\n",
      "loss: 1.841584054440261 acc: 0.85\n",
      "loss: 1.841045439877454 acc: 0.83\n",
      "loss: 1.8648782956931402 acc: 0.77\n",
      "loss: 1.7858060530844477 acc: 0.87\n",
      "loss: 1.8453571539119915 acc: 0.85\n",
      "loss: 1.824107697176037 acc: 0.87\n",
      "loss: 1.802362354466155 acc: 0.89\n",
      "loss: 1.795513301727636 acc: 0.84\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 1.7955\t Accuracy 0.8400\n",
      "loss: 1.8043956506616243 acc: 0.88\n",
      "loss: 1.8200635614801133 acc: 0.88\n",
      "loss: 1.8735172704516896 acc: 0.76\n",
      "loss: 1.8332092000165148 acc: 0.85\n",
      "loss: 1.832392476194991 acc: 0.84\n",
      "loss: 1.8211190727786672 acc: 0.89\n",
      "loss: 1.7836167576635629 acc: 0.92\n",
      "loss: 1.8265061586890912 acc: 0.87\n",
      "loss: 1.798688599013262 acc: 0.91\n",
      "loss: 1.7873980831283771 acc: 0.87\n",
      "loss: 1.822134048954959 acc: 0.86\n",
      "loss: 1.837593187658215 acc: 0.87\n",
      "loss: 1.8236980666135634 acc: 0.82\n",
      "loss: 1.8318061032558048 acc: 0.84\n",
      "loss: 1.8030942110014263 acc: 0.84\n",
      "loss: 1.8187130714063005 acc: 0.88\n",
      "loss: 1.8038122237284608 acc: 0.85\n",
      "loss: 1.8487058664177178 acc: 0.83\n",
      "loss: 1.8228437183973296 acc: 0.82\n",
      "loss: 1.844153872883654 acc: 0.75\n",
      "loss: 1.7921907039886766 acc: 0.9\n",
      "loss: 1.8463982596283812 acc: 0.77\n",
      "loss: 1.7805714917490156 acc: 0.89\n",
      "loss: 1.8516512566842345 acc: 0.82\n",
      "loss: 1.841983409945237 acc: 0.84\n",
      "loss: 1.834825596116501 acc: 0.85\n",
      "loss: 1.8120039619675004 acc: 0.91\n",
      "loss: 1.828999872061673 acc: 0.86\n",
      "loss: 1.8445766037842943 acc: 0.84\n",
      "loss: 1.8765249900618632 acc: 0.76\n",
      "loss: 1.8330218025917346 acc: 0.81\n",
      "loss: 1.8271786593640151 acc: 0.86\n",
      "loss: 1.8535379217979355 acc: 0.8\n",
      "loss: 1.8222500196440052 acc: 0.84\n",
      "loss: 1.849539946362865 acc: 0.83\n",
      "loss: 1.8364629836410273 acc: 0.79\n",
      "loss: 1.8364051859075838 acc: 0.84\n",
      "loss: 1.8228469531593299 acc: 0.81\n",
      "loss: 1.8093214214674398 acc: 0.86\n",
      "loss: 1.8519304467588382 acc: 0.83\n",
      "loss: 1.8167950611508956 acc: 0.86\n",
      "loss: 1.8263017036628606 acc: 0.84\n",
      "loss: 1.835865083065243 acc: 0.89\n",
      "loss: 1.8375584161386018 acc: 0.81\n",
      "loss: 1.8332764385728084 acc: 0.82\n",
      "loss: 1.8146040266758552 acc: 0.83\n",
      "loss: 1.8056267204479648 acc: 0.92\n",
      "loss: 1.8668235367036272 acc: 0.81\n",
      "loss: 1.8482166859062823 acc: 0.76\n",
      "loss: 1.8710982565444598 acc: 0.81\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 1.8711\t Accuracy 0.8100\n",
      "loss: 1.792632839126859 acc: 0.94\n",
      "loss: 1.8252232559707273 acc: 0.87\n",
      "loss: 1.818062637729628 acc: 0.81\n",
      "loss: 1.8408472770543434 acc: 0.83\n",
      "loss: 1.8400106041624846 acc: 0.86\n",
      "loss: 1.842074603980202 acc: 0.86\n",
      "loss: 1.8618654902887115 acc: 0.82\n",
      "loss: 1.8092644453070603 acc: 0.81\n",
      "loss: 1.8646754607812408 acc: 0.85\n",
      "loss: 1.7797023337440177 acc: 0.91\n",
      "loss: 1.799406222246838 acc: 0.88\n",
      "loss: 1.838212306956462 acc: 0.87\n",
      "loss: 1.8029715296427709 acc: 0.89\n",
      "loss: 1.839061900640469 acc: 0.88\n",
      "loss: 1.7907272528807814 acc: 0.87\n",
      "loss: 1.8241160027662642 acc: 0.85\n",
      "loss: 1.7982956781487218 acc: 0.83\n",
      "loss: 1.822277847477434 acc: 0.8\n",
      "loss: 1.7789613964506745 acc: 0.87\n",
      "loss: 1.806450941483998 acc: 0.83\n",
      "loss: 1.8245599042983165 acc: 0.8\n",
      "loss: 1.8048486290587646 acc: 0.83\n",
      "loss: 1.8068830971234817 acc: 0.86\n",
      "loss: 1.8247674337983362 acc: 0.84\n",
      "loss: 1.877348470575358 acc: 0.84\n",
      "loss: 1.848086113716032 acc: 0.77\n",
      "loss: 1.7891299818918969 acc: 0.87\n",
      "loss: 1.8107970301776275 acc: 0.88\n",
      "loss: 1.817671764311026 acc: 0.84\n",
      "loss: 1.8108297311805743 acc: 0.87\n",
      "loss: 1.831565806482312 acc: 0.81\n",
      "loss: 1.8158700113291542 acc: 0.9\n",
      "loss: 1.824580640431233 acc: 0.85\n",
      "loss: 1.8120872061022106 acc: 0.84\n",
      "loss: 1.845036005199657 acc: 0.76\n",
      "loss: 1.8212588551202853 acc: 0.81\n",
      "loss: 1.8457875956364516 acc: 0.79\n",
      "loss: 1.8197532309701119 acc: 0.88\n",
      "loss: 1.7937118680457405 acc: 0.9\n",
      "loss: 1.8488725445973384 acc: 0.84\n",
      "loss: 1.8308699249352034 acc: 0.83\n",
      "loss: 1.7961835996691058 acc: 0.87\n",
      "loss: 1.8105264660180165 acc: 0.86\n",
      "loss: 1.8768297888104895 acc: 0.81\n",
      "loss: 1.8254498983241183 acc: 0.84\n",
      "loss: 1.8470120369109808 acc: 0.81\n",
      "loss: 1.7982940767412803 acc: 0.84\n",
      "loss: 1.8210821023230142 acc: 0.85\n",
      "loss: 1.8411643130063788 acc: 0.83\n",
      "loss: 1.7871789294063458 acc: 0.86\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 1.7872\t Accuracy 0.8600\n",
      "loss: 1.803797401696105 acc: 0.83\n",
      "loss: 1.871793780436166 acc: 0.81\n",
      "loss: 1.818703240371883 acc: 0.87\n",
      "loss: 1.8450398963782648 acc: 0.81\n",
      "loss: 1.8411700965714923 acc: 0.84\n",
      "loss: 1.8217189913700462 acc: 0.82\n",
      "loss: 1.8557679989931848 acc: 0.75\n",
      "loss: 1.871450542718708 acc: 0.75\n",
      "loss: 1.8519495385601659 acc: 0.78\n",
      "loss: 1.8111444085020887 acc: 0.89\n",
      "loss: 1.801323598731662 acc: 0.85\n",
      "loss: 1.8301335010955617 acc: 0.85\n",
      "loss: 1.793251450545215 acc: 0.8\n",
      "loss: 1.8340229320865473 acc: 0.83\n",
      "loss: 1.8096580495030268 acc: 0.84\n",
      "loss: 1.8704363626388496 acc: 0.79\n",
      "loss: 1.8028933799745692 acc: 0.88\n",
      "loss: 1.8260512477132713 acc: 0.88\n",
      "loss: 1.8534923287719807 acc: 0.81\n",
      "loss: 1.8042792000672945 acc: 0.87\n",
      "loss: 1.8079550785440708 acc: 0.83\n",
      "loss: 1.8352494985024512 acc: 0.83\n",
      "loss: 1.8127749212200401 acc: 0.89\n",
      "loss: 1.8263429932244692 acc: 0.79\n",
      "loss: 1.8265541100874574 acc: 0.82\n",
      "loss: 1.8368950342794188 acc: 0.78\n",
      "loss: 1.8732422702961136 acc: 0.8\n",
      "loss: 1.851550496066475 acc: 0.81\n",
      "loss: 1.8129074041045368 acc: 0.85\n",
      "loss: 1.8087194338257382 acc: 0.89\n",
      "loss: 1.8364450484279797 acc: 0.83\n",
      "loss: 1.7658801909165118 acc: 0.89\n",
      "loss: 1.846292826339225 acc: 0.83\n",
      "loss: 1.8330450599329127 acc: 0.82\n",
      "loss: 1.8418041281703013 acc: 0.82\n",
      "loss: 1.8124969617216862 acc: 0.83\n",
      "loss: 1.7959116150327694 acc: 0.89\n",
      "loss: 1.815860833120733 acc: 0.84\n",
      "loss: 1.809290094120841 acc: 0.87\n",
      "loss: 1.8335071897661044 acc: 0.81\n",
      "loss: 1.8469236799612019 acc: 0.82\n",
      "loss: 1.8095476280904246 acc: 0.86\n",
      "loss: 1.8535665742409975 acc: 0.81\n",
      "loss: 1.853422636718125 acc: 0.8\n",
      "loss: 1.850063130351379 acc: 0.85\n",
      "loss: 1.7820793232894616 acc: 0.87\n",
      "loss: 1.847509849606046 acc: 0.79\n",
      "loss: 1.834139215605934 acc: 0.82\n",
      "loss: 1.829042238790677 acc: 0.82\n",
      "loss: 1.8189019235211985 acc: 0.87\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 1.8189\t Accuracy 0.8700\n",
      "loss: 1.819948865228567 acc: 0.85\n",
      "loss: 1.8403055538289845 acc: 0.81\n",
      "loss: 1.8653102273990705 acc: 0.8\n",
      "loss: 1.824425769789456 acc: 0.8\n",
      "loss: 1.8287257405550748 acc: 0.84\n",
      "loss: 1.831306927746567 acc: 0.81\n",
      "loss: 1.8112544412174034 acc: 0.86\n",
      "loss: 1.8475666852377477 acc: 0.81\n",
      "loss: 1.8257085622581066 acc: 0.84\n",
      "loss: 1.8411088872088888 acc: 0.82\n",
      "loss: 1.789422760869164 acc: 0.88\n",
      "loss: 1.8106610497093873 acc: 0.84\n",
      "loss: 1.8113517413061988 acc: 0.88\n",
      "loss: 1.8318607379754765 acc: 0.83\n",
      "loss: 1.81196185166125 acc: 0.85\n",
      "loss: 1.8790346843791113 acc: 0.82\n",
      "loss: 1.8148763225070974 acc: 0.91\n",
      "loss: 1.8321937282337373 acc: 0.85\n",
      "loss: 1.825333973190373 acc: 0.89\n",
      "loss: 1.8532317049614493 acc: 0.78\n",
      "loss: 1.787693421891238 acc: 0.87\n",
      "loss: 1.8495249990977987 acc: 0.81\n",
      "loss: 1.8682222024479131 acc: 0.82\n",
      "loss: 1.8085982834120446 acc: 0.89\n",
      "loss: 1.8525333341299048 acc: 0.82\n",
      "loss: 1.8276690163113052 acc: 0.86\n",
      "loss: 1.7986448001597557 acc: 0.86\n",
      "loss: 1.8282366695742436 acc: 0.85\n",
      "loss: 1.8111711059753661 acc: 0.84\n",
      "loss: 1.805631501135433 acc: 0.84\n",
      "loss: 1.8090071606859968 acc: 0.84\n",
      "loss: 1.8390045526156678 acc: 0.81\n",
      "loss: 1.816276682425301 acc: 0.81\n",
      "loss: 1.8131348102236224 acc: 0.87\n",
      "loss: 1.8148010060207636 acc: 0.81\n",
      "loss: 1.8392475540760431 acc: 0.82\n",
      "loss: 1.793532949361737 acc: 0.85\n",
      "loss: 1.8397982787071272 acc: 0.87\n",
      "loss: 1.8116973359801138 acc: 0.9\n",
      "loss: 1.838144119825288 acc: 0.81\n",
      "loss: 1.861548660539695 acc: 0.8\n",
      "loss: 1.812140568365443 acc: 0.85\n",
      "loss: 1.795400117030402 acc: 0.85\n",
      "loss: 1.835744701545288 acc: 0.85\n",
      "loss: 1.8398733826427818 acc: 0.88\n",
      "loss: 1.7668667052093736 acc: 0.87\n",
      "loss: 1.777726627109019 acc: 0.86\n",
      "loss: 1.796045709573836 acc: 0.88\n",
      "loss: 1.7797489776272035 acc: 0.9\n",
      "loss: 1.8058659456452084 acc: 0.83\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 1.8059\t Accuracy 0.8300\n",
      "loss: 1.8620730455575336 acc: 0.77\n",
      "loss: 1.782757207922122 acc: 0.91\n",
      "loss: 1.8418350821069231 acc: 0.84\n",
      "loss: 1.8363910214953554 acc: 0.87\n",
      "loss: 1.7904083589133801 acc: 0.81\n",
      "loss: 1.8153055277499015 acc: 0.76\n",
      "loss: 1.8244888611920391 acc: 0.86\n",
      "loss: 1.8699125592144918 acc: 0.79\n",
      "loss: 1.8200981974939328 acc: 0.87\n",
      "loss: 1.842290960609094 acc: 0.82\n",
      "loss: 1.7943188338460276 acc: 0.88\n",
      "loss: 1.8142095026144096 acc: 0.81\n",
      "loss: 1.8224266171143182 acc: 0.83\n",
      "loss: 1.814712748669976 acc: 0.84\n",
      "loss: 1.834244008726109 acc: 0.87\n",
      "loss: 1.7864902809640437 acc: 0.88\n",
      "loss: 1.8388906227134214 acc: 0.81\n",
      "loss: 1.8541397145213199 acc: 0.79\n",
      "loss: 1.7822006287838996 acc: 0.88\n",
      "loss: 1.8037934842657364 acc: 0.87\n",
      "loss: 1.8796680734988 acc: 0.75\n",
      "loss: 1.8093591894370777 acc: 0.8\n",
      "loss: 1.828056768506429 acc: 0.85\n",
      "loss: 1.8408883765704247 acc: 0.8\n",
      "loss: 1.8327396215442593 acc: 0.8\n",
      "loss: 1.8413372101012673 acc: 0.85\n",
      "loss: 1.824612558505171 acc: 0.82\n",
      "loss: 1.809040459177202 acc: 0.85\n",
      "loss: 1.8666496077903005 acc: 0.79\n",
      "loss: 1.809298909787945 acc: 0.87\n",
      "loss: 1.7907966009318594 acc: 0.92\n",
      "loss: 1.851613557529356 acc: 0.86\n",
      "loss: 1.8820948072540395 acc: 0.79\n",
      "loss: 1.8055267078160973 acc: 0.84\n",
      "loss: 1.8274063674846197 acc: 0.84\n",
      "loss: 1.8181968088052716 acc: 0.89\n",
      "loss: 1.7882262571428509 acc: 0.88\n",
      "loss: 1.833421609699136 acc: 0.78\n",
      "loss: 1.8475901078031252 acc: 0.88\n",
      "loss: 1.801496607027405 acc: 0.83\n",
      "loss: 1.8220196283327283 acc: 0.86\n",
      "loss: 1.83535956440999 acc: 0.87\n",
      "loss: 1.80492223643008 acc: 0.86\n",
      "loss: 1.8068108797304723 acc: 0.87\n",
      "loss: 1.817364853630181 acc: 0.87\n",
      "loss: 1.8290252018445081 acc: 0.84\n",
      "loss: 1.8337163941942751 acc: 0.86\n",
      "loss: 1.8380225034431552 acc: 0.81\n",
      "loss: 1.8107973154937493 acc: 0.84\n",
      "loss: 1.8198009942144455 acc: 0.8\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 1.8198\t Accuracy 0.8000\n",
      "loss: 1.8226430908437643 acc: 0.84\n",
      "loss: 1.8687270210497433 acc: 0.78\n",
      "loss: 1.8390325006754973 acc: 0.82\n",
      "loss: 1.8603754076350938 acc: 0.82\n",
      "loss: 1.8494548325930094 acc: 0.82\n",
      "loss: 1.7896083717814832 acc: 0.82\n",
      "loss: 1.8884024809920417 acc: 0.78\n",
      "loss: 1.8577968875835251 acc: 0.78\n",
      "loss: 1.851486268936799 acc: 0.79\n",
      "loss: 1.8113680821688505 acc: 0.86\n",
      "loss: 1.8473775972419049 acc: 0.8\n",
      "loss: 1.853876880659091 acc: 0.76\n",
      "loss: 1.8565114841904011 acc: 0.79\n",
      "loss: 1.8179476581623362 acc: 0.9\n",
      "loss: 1.858951846105635 acc: 0.86\n",
      "loss: 1.8239365961522234 acc: 0.83\n",
      "loss: 1.8315155942658645 acc: 0.87\n",
      "loss: 1.8572852848905919 acc: 0.83\n",
      "loss: 1.7911174581168516 acc: 0.87\n",
      "loss: 1.823410136428418 acc: 0.83\n",
      "loss: 1.8437976954420932 acc: 0.83\n",
      "loss: 1.8569508817152696 acc: 0.74\n",
      "loss: 1.8167025324204005 acc: 0.83\n",
      "loss: 1.8191158552170663 acc: 0.86\n",
      "loss: 1.793644765573253 acc: 0.85\n",
      "loss: 1.8368835574313758 acc: 0.79\n",
      "loss: 1.8374928639945878 acc: 0.82\n",
      "loss: 1.8165532453296425 acc: 0.85\n",
      "loss: 1.813748907944256 acc: 0.84\n",
      "loss: 1.8179882233440035 acc: 0.89\n",
      "loss: 1.7756644407000926 acc: 0.82\n",
      "loss: 1.840747870043705 acc: 0.77\n",
      "loss: 1.8414544293604314 acc: 0.77\n",
      "loss: 1.835058723933348 acc: 0.75\n",
      "loss: 1.816408581726481 acc: 0.81\n",
      "loss: 1.8006327047392634 acc: 0.83\n",
      "loss: 1.8256351969799887 acc: 0.78\n",
      "loss: 1.819236786413616 acc: 0.86\n",
      "loss: 1.8170186311917127 acc: 0.85\n",
      "loss: 1.7901598497075815 acc: 0.86\n",
      "loss: 1.8373103413175127 acc: 0.89\n",
      "loss: 1.7998120412339689 acc: 0.84\n",
      "loss: 1.839253101630695 acc: 0.83\n",
      "loss: 1.8328943844480088 acc: 0.84\n",
      "loss: 1.795450947884984 acc: 0.88\n",
      "loss: 1.8958014173322948 acc: 0.77\n",
      "loss: 1.7716139367442936 acc: 0.91\n",
      "loss: 1.8388352802002061 acc: 0.87\n",
      "loss: 1.8316435699626132 acc: 0.84\n",
      "loss: 1.8323711362516384 acc: 0.85\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 1.8324\t Accuracy 0.8500\n",
      "loss: 1.8394884009052117 acc: 0.79\n",
      "loss: 1.882759054718825 acc: 0.81\n",
      "loss: 1.8327577012091114 acc: 0.78\n",
      "loss: 1.8673438939282057 acc: 0.79\n",
      "loss: 1.7985267914105574 acc: 0.87\n",
      "loss: 1.8586364745833581 acc: 0.8\n",
      "loss: 1.8007087763174523 acc: 0.88\n",
      "loss: 1.8210655081448746 acc: 0.83\n",
      "loss: 1.8350268014661193 acc: 0.85\n",
      "loss: 1.8092294305450691 acc: 0.85\n",
      "loss: 1.7872678776004454 acc: 0.87\n",
      "loss: 1.8664851655773316 acc: 0.71\n",
      "loss: 1.8558474297371672 acc: 0.8\n",
      "loss: 1.8296513759525057 acc: 0.84\n",
      "loss: 1.7796729567336675 acc: 0.86\n",
      "loss: 1.8164332188773005 acc: 0.85\n",
      "loss: 1.802595440532819 acc: 0.85\n",
      "loss: 1.8249872510760017 acc: 0.82\n",
      "loss: 1.8292982881373065 acc: 0.89\n",
      "loss: 1.8400549936748496 acc: 0.84\n",
      "loss: 1.8401770572709546 acc: 0.86\n",
      "loss: 1.8292179762965164 acc: 0.79\n",
      "loss: 1.7947261455209342 acc: 0.82\n",
      "loss: 1.7798808549189917 acc: 0.87\n",
      "loss: 1.7927670610253004 acc: 0.89\n",
      "loss: 1.7596824675974923 acc: 0.94\n",
      "loss: 1.8085194598657832 acc: 0.85\n",
      "loss: 1.8411594811242693 acc: 0.81\n",
      "loss: 1.8430848925504353 acc: 0.82\n",
      "loss: 1.8153646852063536 acc: 0.83\n",
      "loss: 1.8252595437220813 acc: 0.84\n",
      "loss: 1.866076877978071 acc: 0.77\n",
      "loss: 1.814121624906318 acc: 0.85\n",
      "loss: 1.8146618001758483 acc: 0.91\n",
      "loss: 1.856996542153401 acc: 0.85\n",
      "loss: 1.790066101766452 acc: 0.89\n",
      "loss: 1.8259578283590399 acc: 0.89\n",
      "loss: 1.799891594309502 acc: 0.84\n",
      "loss: 1.8431046844467676 acc: 0.8\n",
      "loss: 1.8467573071806647 acc: 0.81\n",
      "loss: 1.8108630342149474 acc: 0.88\n",
      "loss: 1.837654010997291 acc: 0.8\n",
      "loss: 1.824458768328795 acc: 0.81\n",
      "loss: 1.832571574798632 acc: 0.8\n",
      "loss: 1.8591859103606305 acc: 0.83\n",
      "loss: 1.831144076707117 acc: 0.84\n",
      "loss: 1.8582938002259473 acc: 0.81\n",
      "loss: 1.831110196098557 acc: 0.86\n",
      "loss: 1.844169496535288 acc: 0.85\n",
      "loss: 1.7935747352130955 acc: 0.86\n",
      "loss: 1.792974447994451 acc: 0.88\n",
      "loss: 1.8021805533992878 acc: 0.89\n",
      "loss: 1.8278058452849968 acc: 0.87\n",
      "loss: 1.81341974900898 acc: 0.91\n",
      "loss: 1.803455707531633 acc: 0.86\n",
      "loss: 1.7601396483785618 acc: 0.83\n",
      "loss: 1.7547015829967092 acc: 0.87\n",
      "loss: 1.817671150284739 acc: 0.86\n",
      "loss: 1.7488277431520618 acc: 0.93\n",
      "loss: 1.787944841444056 acc: 0.88\n",
      "loss: 1.793225746661989 acc: 0.85\n",
      "loss: 1.841024797961483 acc: 0.82\n",
      "loss: 1.8571831935775918 acc: 0.84\n",
      "loss: 1.8635414849782144 acc: 0.84\n",
      "loss: 1.7900284565331475 acc: 0.92\n",
      "loss: 1.8063884409831323 acc: 0.86\n",
      "loss: 1.7548582368822583 acc: 0.92\n",
      "loss: 1.784414154545912 acc: 0.88\n",
      "loss: 1.7963296672700795 acc: 0.89\n",
      "loss: 1.8718546412005654 acc: 0.85\n",
      "loss: 1.8245894593282543 acc: 0.89\n",
      "loss: 1.820229562825521 acc: 0.76\n",
      "loss: 1.85977053614744 acc: 0.86\n",
      "loss: 1.8297623692032048 acc: 0.84\n",
      "loss: 1.8460152367462073 acc: 0.82\n",
      "loss: 1.8609346211421949 acc: 0.78\n",
      "loss: 1.8827904237573014 acc: 0.8\n",
      "loss: 1.8248769951017918 acc: 0.92\n",
      "loss: 1.8001566148647365 acc: 0.89\n",
      "loss: 1.8162602690494132 acc: 0.84\n",
      "loss: 1.7714834955952035 acc: 0.92\n",
      "loss: 1.7452944929193306 acc: 0.94\n",
      "loss: 1.7849737845021085 acc: 0.89\n",
      "loss: 1.7789121372295318 acc: 0.86\n",
      "loss: 1.8117702480588078 acc: 0.88\n",
      "loss: 1.84733912339802 acc: 0.89\n",
      "loss: 1.8357636643836528 acc: 0.87\n",
      "loss: 1.7420521298352174 acc: 0.9\n",
      "loss: 1.709516600089725 acc: 0.96\n",
      "loss: 1.7364481193200634 acc: 0.96\n",
      "loss: 1.7373136800363238 acc: 0.98\n",
      "loss: 1.7884554536484156 acc: 0.91\n",
      "loss: 1.7713471998330723 acc: 0.87\n",
      "loss: 1.7279100913496968 acc: 0.86\n",
      "loss: 1.7992314406264638 acc: 0.93\n",
      "loss: 1.7923175852703728 acc: 0.93\n",
      "loss: 1.8834293282538834 acc: 0.83\n",
      "loss: 1.700951352374326 acc: 0.97\n",
      "loss: 1.8578686489197502 acc: 0.84\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8252\t Average training accuracy 0.8389\n",
      "Epoch [2]\t Average validation loss 1.8010\t Average validation accuracy 0.8780\n",
      "\n",
      "loss: 1.8215285054481325 acc: 0.85\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 1.8215\t Accuracy 0.8500\n",
      "loss: 1.8386961494228677 acc: 0.84\n",
      "loss: 1.853988909938814 acc: 0.83\n",
      "loss: 1.7889762160122504 acc: 0.93\n",
      "loss: 1.8031211517513959 acc: 0.87\n",
      "loss: 1.777852407196844 acc: 0.88\n",
      "loss: 1.7891773433048215 acc: 0.86\n",
      "loss: 1.8216327815463742 acc: 0.9\n",
      "loss: 1.7831556994852233 acc: 0.88\n",
      "loss: 1.795626269332807 acc: 0.87\n",
      "loss: 1.8307564706823576 acc: 0.79\n",
      "loss: 1.8278563301485276 acc: 0.82\n",
      "loss: 1.877012497230053 acc: 0.78\n",
      "loss: 1.8030838305322667 acc: 0.82\n",
      "loss: 1.8294512407380465 acc: 0.8\n",
      "loss: 1.7948103687760817 acc: 0.85\n",
      "loss: 1.8536519860812672 acc: 0.79\n",
      "loss: 1.845306234479101 acc: 0.77\n",
      "loss: 1.8014551272376946 acc: 0.88\n",
      "loss: 1.8289548890856133 acc: 0.81\n",
      "loss: 1.8057252979864236 acc: 0.82\n",
      "loss: 1.8213604075099277 acc: 0.82\n",
      "loss: 1.8305367458941382 acc: 0.82\n",
      "loss: 1.7974224748311107 acc: 0.86\n",
      "loss: 1.8000179294992529 acc: 0.85\n",
      "loss: 1.8147069626501817 acc: 0.78\n",
      "loss: 1.822452882814284 acc: 0.82\n",
      "loss: 1.8305787054762859 acc: 0.84\n",
      "loss: 1.8317391608205111 acc: 0.83\n",
      "loss: 1.839585297191913 acc: 0.82\n",
      "loss: 1.8525995838011347 acc: 0.79\n",
      "loss: 1.813457066080456 acc: 0.85\n",
      "loss: 1.8336750507273445 acc: 0.83\n",
      "loss: 1.7814097784969436 acc: 0.87\n",
      "loss: 1.8112247040492264 acc: 0.87\n",
      "loss: 1.838046511318148 acc: 0.79\n",
      "loss: 1.8171612022827026 acc: 0.88\n",
      "loss: 1.8126594451699045 acc: 0.88\n",
      "loss: 1.8247130026926734 acc: 0.8\n",
      "loss: 1.8248313689581652 acc: 0.84\n",
      "loss: 1.7941713073450034 acc: 0.85\n",
      "loss: 1.8001735973475466 acc: 0.87\n",
      "loss: 1.8216382636007546 acc: 0.82\n",
      "loss: 1.8643636877385754 acc: 0.75\n",
      "loss: 1.8133878033017523 acc: 0.83\n",
      "loss: 1.861909799222665 acc: 0.8\n",
      "loss: 1.7740869561125772 acc: 0.9\n",
      "loss: 1.8523647733928648 acc: 0.84\n",
      "loss: 1.820698065986223 acc: 0.86\n",
      "loss: 1.837443625557258 acc: 0.85\n",
      "loss: 1.7963860281796755 acc: 0.86\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 1.7964\t Accuracy 0.8600\n",
      "loss: 1.7988837517476182 acc: 0.83\n",
      "loss: 1.785113630357938 acc: 0.89\n",
      "loss: 1.8009848007472307 acc: 0.85\n",
      "loss: 1.8237275834711866 acc: 0.84\n",
      "loss: 1.8639548689330043 acc: 0.79\n",
      "loss: 1.813008527532254 acc: 0.91\n",
      "loss: 1.8497729955312285 acc: 0.84\n",
      "loss: 1.818326955293568 acc: 0.87\n",
      "loss: 1.8194309211409223 acc: 0.87\n",
      "loss: 1.8164729797137342 acc: 0.87\n",
      "loss: 1.816136809582172 acc: 0.87\n",
      "loss: 1.796300822304189 acc: 0.88\n",
      "loss: 1.8171346038149414 acc: 0.82\n",
      "loss: 1.8336741208696483 acc: 0.82\n",
      "loss: 1.8243922068808647 acc: 0.83\n",
      "loss: 1.858091687010574 acc: 0.79\n",
      "loss: 1.8169685283803236 acc: 0.89\n",
      "loss: 1.813874589014157 acc: 0.85\n",
      "loss: 1.7918103577723128 acc: 0.88\n",
      "loss: 1.801182112854025 acc: 0.9\n",
      "loss: 1.8226276353148116 acc: 0.88\n",
      "loss: 1.8869690275242639 acc: 0.74\n",
      "loss: 1.8231579375715554 acc: 0.83\n",
      "loss: 1.8146738830156306 acc: 0.88\n",
      "loss: 1.8088835087941249 acc: 0.81\n",
      "loss: 1.8142860773688656 acc: 0.9\n",
      "loss: 1.8509647198020316 acc: 0.81\n",
      "loss: 1.7912384372059342 acc: 0.88\n",
      "loss: 1.804864619173002 acc: 0.8\n",
      "loss: 1.8066536248145268 acc: 0.82\n",
      "loss: 1.7903119461091979 acc: 0.85\n",
      "loss: 1.802423864290775 acc: 0.88\n",
      "loss: 1.8390578841093028 acc: 0.84\n",
      "loss: 1.803759223924546 acc: 0.86\n",
      "loss: 1.819530245402172 acc: 0.85\n",
      "loss: 1.8355734324947277 acc: 0.84\n",
      "loss: 1.8038133519146486 acc: 0.83\n",
      "loss: 1.8036852771102894 acc: 0.92\n",
      "loss: 1.8235988505680012 acc: 0.87\n",
      "loss: 1.8313644963265583 acc: 0.83\n",
      "loss: 1.8427648813474857 acc: 0.8\n",
      "loss: 1.8193426510848283 acc: 0.85\n",
      "loss: 1.8482152605306237 acc: 0.83\n",
      "loss: 1.7930009661628707 acc: 0.92\n",
      "loss: 1.7799527426624844 acc: 0.85\n",
      "loss: 1.7979798797651245 acc: 0.84\n",
      "loss: 1.8290092282761008 acc: 0.87\n",
      "loss: 1.8567539868886842 acc: 0.77\n",
      "loss: 1.8531037225108207 acc: 0.86\n",
      "loss: 1.8558342651255828 acc: 0.82\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 1.8558\t Accuracy 0.8200\n",
      "loss: 1.8054650680739044 acc: 0.84\n",
      "loss: 1.8321710915583214 acc: 0.82\n",
      "loss: 1.851924195758148 acc: 0.86\n",
      "loss: 1.787021503122897 acc: 0.84\n",
      "loss: 1.8210826997439984 acc: 0.88\n",
      "loss: 1.8032054150981753 acc: 0.88\n",
      "loss: 1.860140311060733 acc: 0.81\n",
      "loss: 1.863734660350973 acc: 0.8\n",
      "loss: 1.8621824624374925 acc: 0.83\n",
      "loss: 1.7980446401174484 acc: 0.84\n",
      "loss: 1.811963800825788 acc: 0.84\n",
      "loss: 1.7964898302540757 acc: 0.86\n",
      "loss: 1.7800533484410916 acc: 0.88\n",
      "loss: 1.830307659596252 acc: 0.81\n",
      "loss: 1.850670512503446 acc: 0.83\n",
      "loss: 1.8326187252726742 acc: 0.81\n",
      "loss: 1.8355421502167835 acc: 0.8\n",
      "loss: 1.831137886610774 acc: 0.83\n",
      "loss: 1.7825312922739744 acc: 0.84\n",
      "loss: 1.8258298734964071 acc: 0.85\n",
      "loss: 1.8022760650282001 acc: 0.85\n",
      "loss: 1.8217140602360162 acc: 0.82\n",
      "loss: 1.826987088433408 acc: 0.83\n",
      "loss: 1.8188378733421744 acc: 0.86\n",
      "loss: 1.830662939130272 acc: 0.84\n",
      "loss: 1.8090028968826335 acc: 0.87\n",
      "loss: 1.8239972833920048 acc: 0.86\n",
      "loss: 1.8550365781597438 acc: 0.84\n",
      "loss: 1.8697742886775324 acc: 0.8\n",
      "loss: 1.8462712875750154 acc: 0.83\n",
      "loss: 1.8576650286813767 acc: 0.83\n",
      "loss: 1.8499309791553313 acc: 0.82\n",
      "loss: 1.8374803288377146 acc: 0.83\n",
      "loss: 1.8308956952869047 acc: 0.85\n",
      "loss: 1.8408082462393367 acc: 0.82\n",
      "loss: 1.8171337445244566 acc: 0.8\n",
      "loss: 1.8048652673494772 acc: 0.89\n",
      "loss: 1.8599763872115778 acc: 0.81\n",
      "loss: 1.840148439300147 acc: 0.86\n",
      "loss: 1.852758101207031 acc: 0.8\n",
      "loss: 1.8489571701575553 acc: 0.84\n",
      "loss: 1.8229924359578762 acc: 0.79\n",
      "loss: 1.8097555083843941 acc: 0.86\n",
      "loss: 1.8555820913295915 acc: 0.79\n",
      "loss: 1.8176192764459618 acc: 0.86\n",
      "loss: 1.8662801654165173 acc: 0.81\n",
      "loss: 1.879559290195107 acc: 0.78\n",
      "loss: 1.7984535995355933 acc: 0.89\n",
      "loss: 1.8495139212947884 acc: 0.79\n",
      "loss: 1.7759370482423582 acc: 0.9\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 1.7759\t Accuracy 0.9000\n",
      "loss: 1.8200692599754345 acc: 0.82\n",
      "loss: 1.8523465714421394 acc: 0.83\n",
      "loss: 1.85722757001017 acc: 0.84\n",
      "loss: 1.805432804698937 acc: 0.83\n",
      "loss: 1.8475611226706738 acc: 0.83\n",
      "loss: 1.7997377174051652 acc: 0.88\n",
      "loss: 1.818429107039678 acc: 0.87\n",
      "loss: 1.8419105608122703 acc: 0.79\n",
      "loss: 1.8219545974139661 acc: 0.83\n",
      "loss: 1.8289761450665534 acc: 0.87\n",
      "loss: 1.8660260571560527 acc: 0.75\n",
      "loss: 1.8407163433647384 acc: 0.81\n",
      "loss: 1.7625161524491082 acc: 0.9\n",
      "loss: 1.823376213978735 acc: 0.83\n",
      "loss: 1.8589611451748163 acc: 0.81\n",
      "loss: 1.832217070146596 acc: 0.84\n",
      "loss: 1.7706842229625377 acc: 0.9\n",
      "loss: 1.8365462880891155 acc: 0.85\n",
      "loss: 1.8226267911720644 acc: 0.83\n",
      "loss: 1.8002510548088586 acc: 0.83\n",
      "loss: 1.8403840389954107 acc: 0.9\n",
      "loss: 1.802521744555808 acc: 0.87\n",
      "loss: 1.8194612279808136 acc: 0.83\n",
      "loss: 1.8243712073454459 acc: 0.79\n",
      "loss: 1.7847272127684624 acc: 0.88\n",
      "loss: 1.8818193549905287 acc: 0.78\n",
      "loss: 1.8318992660601203 acc: 0.76\n",
      "loss: 1.8104251610251547 acc: 0.83\n",
      "loss: 1.8237298085312332 acc: 0.85\n",
      "loss: 1.8150596641310544 acc: 0.82\n",
      "loss: 1.7898360327249088 acc: 0.85\n",
      "loss: 1.810535279205282 acc: 0.83\n",
      "loss: 1.8200142128036356 acc: 0.84\n",
      "loss: 1.7861496256667346 acc: 0.89\n",
      "loss: 1.831725274139592 acc: 0.88\n",
      "loss: 1.8318682115257192 acc: 0.88\n",
      "loss: 1.827183594349075 acc: 0.85\n",
      "loss: 1.8546417590105728 acc: 0.78\n",
      "loss: 1.8262543201097692 acc: 0.84\n",
      "loss: 1.8007985072609296 acc: 0.83\n",
      "loss: 1.8646282464006692 acc: 0.77\n",
      "loss: 1.8491295023977918 acc: 0.84\n",
      "loss: 1.8324489017743166 acc: 0.87\n",
      "loss: 1.7778149863110608 acc: 0.96\n",
      "loss: 1.8718888299445984 acc: 0.79\n",
      "loss: 1.830312418828303 acc: 0.85\n",
      "loss: 1.793065699912316 acc: 0.9\n",
      "loss: 1.8236738454437904 acc: 0.86\n",
      "loss: 1.8509490253731462 acc: 0.82\n",
      "loss: 1.837576107017124 acc: 0.84\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 1.8376\t Accuracy 0.8400\n",
      "loss: 1.8515532498353517 acc: 0.84\n",
      "loss: 1.8336873979293902 acc: 0.83\n",
      "loss: 1.8174288698408012 acc: 0.81\n",
      "loss: 1.8055483563691772 acc: 0.9\n",
      "loss: 1.8170386764882323 acc: 0.85\n",
      "loss: 1.8035987936494198 acc: 0.89\n",
      "loss: 1.8253067365801738 acc: 0.82\n",
      "loss: 1.8045096002059273 acc: 0.85\n",
      "loss: 1.8296795214844936 acc: 0.83\n",
      "loss: 1.8665802243732015 acc: 0.77\n",
      "loss: 1.8681058257625118 acc: 0.84\n",
      "loss: 1.8221204687820964 acc: 0.88\n",
      "loss: 1.8305941828256609 acc: 0.84\n",
      "loss: 1.8043522551346212 acc: 0.85\n",
      "loss: 1.815901323838587 acc: 0.87\n",
      "loss: 1.8445391455108402 acc: 0.85\n",
      "loss: 1.832256931717064 acc: 0.8\n",
      "loss: 1.8024396566151462 acc: 0.83\n",
      "loss: 1.806339871684206 acc: 0.89\n",
      "loss: 1.8412661325399169 acc: 0.82\n",
      "loss: 1.8255941849701316 acc: 0.87\n",
      "loss: 1.8022612349851643 acc: 0.84\n",
      "loss: 1.8228413149127118 acc: 0.83\n",
      "loss: 1.836039871964558 acc: 0.77\n",
      "loss: 1.8308145431227074 acc: 0.81\n",
      "loss: 1.8029763501757745 acc: 0.83\n",
      "loss: 1.8835888398170477 acc: 0.78\n",
      "loss: 1.8573994795954047 acc: 0.82\n",
      "loss: 1.8249085097270774 acc: 0.83\n",
      "loss: 1.8284617726711552 acc: 0.84\n",
      "loss: 1.8679442835239422 acc: 0.8\n",
      "loss: 1.8689246410841966 acc: 0.77\n",
      "loss: 1.8419110419498295 acc: 0.85\n",
      "loss: 1.8129395290524406 acc: 0.84\n",
      "loss: 1.884735636609073 acc: 0.76\n",
      "loss: 1.8501764789297244 acc: 0.78\n",
      "loss: 1.8453189548020887 acc: 0.81\n",
      "loss: 1.816164947111277 acc: 0.88\n",
      "loss: 1.8252807591682887 acc: 0.81\n",
      "loss: 1.8525110816331605 acc: 0.86\n",
      "loss: 1.7985303162699047 acc: 0.88\n",
      "loss: 1.8145291645308728 acc: 0.83\n",
      "loss: 1.7954951635524157 acc: 0.89\n",
      "loss: 1.833916860979998 acc: 0.86\n",
      "loss: 1.8589792540945667 acc: 0.82\n",
      "loss: 1.844723179813619 acc: 0.85\n",
      "loss: 1.7990510157933395 acc: 0.85\n",
      "loss: 1.8437557167196093 acc: 0.8\n",
      "loss: 1.788666723103676 acc: 0.85\n",
      "loss: 1.8220116648750686 acc: 0.81\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 1.8220\t Accuracy 0.8100\n",
      "loss: 1.8489294613337082 acc: 0.78\n",
      "loss: 1.8114425386025834 acc: 0.9\n",
      "loss: 1.8058102037721975 acc: 0.88\n",
      "loss: 1.8262784172026267 acc: 0.86\n",
      "loss: 1.820851196201533 acc: 0.84\n",
      "loss: 1.8276672723092553 acc: 0.88\n",
      "loss: 1.8231042404043467 acc: 0.84\n",
      "loss: 1.8292023795539718 acc: 0.82\n",
      "loss: 1.8243342798868698 acc: 0.87\n",
      "loss: 1.812411812393232 acc: 0.85\n",
      "loss: 1.8089767999042559 acc: 0.89\n",
      "loss: 1.814223677618408 acc: 0.85\n",
      "loss: 1.8118737252278359 acc: 0.83\n",
      "loss: 1.7867388362592098 acc: 0.88\n",
      "loss: 1.8140761443545832 acc: 0.83\n",
      "loss: 1.8302893099284199 acc: 0.82\n",
      "loss: 1.8174019797857595 acc: 0.87\n",
      "loss: 1.8584561761230864 acc: 0.82\n",
      "loss: 1.7878049912608294 acc: 0.88\n",
      "loss: 1.8176235004169827 acc: 0.9\n",
      "loss: 1.8598171064664757 acc: 0.82\n",
      "loss: 1.830021358832473 acc: 0.83\n",
      "loss: 1.8073163718357796 acc: 0.83\n",
      "loss: 1.8192239013817257 acc: 0.89\n",
      "loss: 1.8544344482733928 acc: 0.76\n",
      "loss: 1.779744449992759 acc: 0.91\n",
      "loss: 1.762053987051728 acc: 0.89\n",
      "loss: 1.8249431572667536 acc: 0.83\n",
      "loss: 1.7951641090790913 acc: 0.86\n",
      "loss: 1.8240145736045066 acc: 0.83\n",
      "loss: 1.7959497965290296 acc: 0.89\n",
      "loss: 1.8371286934734812 acc: 0.84\n",
      "loss: 1.7638454216405606 acc: 0.88\n",
      "loss: 1.8564635178608964 acc: 0.83\n",
      "loss: 1.8728425910735402 acc: 0.78\n",
      "loss: 1.857631223264967 acc: 0.8\n",
      "loss: 1.8600420985668316 acc: 0.8\n",
      "loss: 1.8521284079648876 acc: 0.8\n",
      "loss: 1.8279679319466333 acc: 0.86\n",
      "loss: 1.852513652698972 acc: 0.85\n",
      "loss: 1.8570523267232624 acc: 0.8\n",
      "loss: 1.8270050596765282 acc: 0.86\n",
      "loss: 1.8695529518347258 acc: 0.81\n",
      "loss: 1.8184272496370812 acc: 0.83\n",
      "loss: 1.8278698094266213 acc: 0.83\n",
      "loss: 1.83980840300276 acc: 0.83\n",
      "loss: 1.8242582838183425 acc: 0.89\n",
      "loss: 1.8398840370646312 acc: 0.81\n",
      "loss: 1.8564099282282647 acc: 0.77\n",
      "loss: 1.7913175013679776 acc: 0.86\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 1.7913\t Accuracy 0.8600\n",
      "loss: 1.820979243166004 acc: 0.8\n",
      "loss: 1.789571399072358 acc: 0.84\n",
      "loss: 1.7991611512876837 acc: 0.9\n",
      "loss: 1.8203154582846606 acc: 0.84\n",
      "loss: 1.805161567182697 acc: 0.85\n",
      "loss: 1.81138835907174 acc: 0.78\n",
      "loss: 1.8118444241647156 acc: 0.9\n",
      "loss: 1.818639122857188 acc: 0.82\n",
      "loss: 1.788000170406622 acc: 0.92\n",
      "loss: 1.7752842068490673 acc: 0.84\n",
      "loss: 1.7870791300398274 acc: 0.86\n",
      "loss: 1.8163020310323577 acc: 0.84\n",
      "loss: 1.8260231796780744 acc: 0.84\n",
      "loss: 1.8324039784290336 acc: 0.86\n",
      "loss: 1.8397750237284343 acc: 0.79\n",
      "loss: 1.814189264634507 acc: 0.88\n",
      "loss: 1.7899518348441168 acc: 0.87\n",
      "loss: 1.8107058627447408 acc: 0.86\n",
      "loss: 1.8260650389470252 acc: 0.85\n",
      "loss: 1.8083841982704774 acc: 0.9\n",
      "loss: 1.8403005392079677 acc: 0.8\n",
      "loss: 1.8010040112646875 acc: 0.88\n",
      "loss: 1.79998949288595 acc: 0.85\n",
      "loss: 1.87021098424394 acc: 0.84\n",
      "loss: 1.855318811341374 acc: 0.81\n",
      "loss: 1.8420069084252657 acc: 0.79\n",
      "loss: 1.8344302726515134 acc: 0.81\n",
      "loss: 1.822547647279786 acc: 0.89\n",
      "loss: 1.8608814519431105 acc: 0.82\n",
      "loss: 1.8269914866209407 acc: 0.84\n",
      "loss: 1.8343845882488463 acc: 0.86\n",
      "loss: 1.8259153490690412 acc: 0.82\n",
      "loss: 1.7518669205758288 acc: 0.9\n",
      "loss: 1.8198955568814839 acc: 0.83\n",
      "loss: 1.8144974240442706 acc: 0.87\n",
      "loss: 1.8239988098321882 acc: 0.78\n",
      "loss: 1.8173335879006176 acc: 0.85\n",
      "loss: 1.842687695787746 acc: 0.82\n",
      "loss: 1.816994363141103 acc: 0.88\n",
      "loss: 1.8280498639066456 acc: 0.84\n",
      "loss: 1.8226370949031445 acc: 0.86\n",
      "loss: 1.885488212523624 acc: 0.78\n",
      "loss: 1.792989162910965 acc: 0.82\n",
      "loss: 1.8034524920197448 acc: 0.85\n",
      "loss: 1.8540016552149186 acc: 0.84\n",
      "loss: 1.8095223398537363 acc: 0.88\n",
      "loss: 1.8455512109084875 acc: 0.76\n",
      "loss: 1.8110298146178463 acc: 0.86\n",
      "loss: 1.8339452243642018 acc: 0.8\n",
      "loss: 1.80987359602632 acc: 0.86\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 1.8099\t Accuracy 0.8600\n",
      "loss: 1.8550541574915733 acc: 0.77\n",
      "loss: 1.8572709924724087 acc: 0.84\n",
      "loss: 1.8087697041126223 acc: 0.86\n",
      "loss: 1.7991577419769331 acc: 0.89\n",
      "loss: 1.8589600194018105 acc: 0.86\n",
      "loss: 1.8473573618900232 acc: 0.8\n",
      "loss: 1.8022445716983966 acc: 0.84\n",
      "loss: 1.795564301013688 acc: 0.88\n",
      "loss: 1.8093224971743738 acc: 0.89\n",
      "loss: 1.8309077450580469 acc: 0.83\n",
      "loss: 1.8224043091517164 acc: 0.82\n",
      "loss: 1.8177979969243965 acc: 0.83\n",
      "loss: 1.7858019694500666 acc: 0.87\n",
      "loss: 1.827179409798473 acc: 0.84\n",
      "loss: 1.843733937353673 acc: 0.86\n",
      "loss: 1.8130233891428957 acc: 0.81\n",
      "loss: 1.842014037536769 acc: 0.8\n",
      "loss: 1.8242486656963295 acc: 0.83\n",
      "loss: 1.8341204891470853 acc: 0.84\n",
      "loss: 1.8060590571193984 acc: 0.85\n",
      "loss: 1.8411324700453406 acc: 0.8\n",
      "loss: 1.8136301696412807 acc: 0.84\n",
      "loss: 1.8295764813300273 acc: 0.86\n",
      "loss: 1.8017427921070341 acc: 0.89\n",
      "loss: 1.8289362626906926 acc: 0.8\n",
      "loss: 1.8442236249358885 acc: 0.88\n",
      "loss: 1.823799640702512 acc: 0.84\n",
      "loss: 1.8316193384285782 acc: 0.8\n",
      "loss: 1.8362492142786937 acc: 0.9\n",
      "loss: 1.8377219159047327 acc: 0.87\n",
      "loss: 1.7743552464760575 acc: 0.9\n",
      "loss: 1.8399970970985529 acc: 0.85\n",
      "loss: 1.836014682111045 acc: 0.81\n",
      "loss: 1.7936095457199672 acc: 0.84\n",
      "loss: 1.797401021765635 acc: 0.83\n",
      "loss: 1.801151959092655 acc: 0.89\n",
      "loss: 1.8106991854013073 acc: 0.83\n",
      "loss: 1.8239027962683116 acc: 0.83\n",
      "loss: 1.8141253279787128 acc: 0.82\n",
      "loss: 1.8483495862310126 acc: 0.76\n",
      "loss: 1.8262576997500752 acc: 0.87\n",
      "loss: 1.8644095239697211 acc: 0.78\n",
      "loss: 1.8139433110797694 acc: 0.88\n",
      "loss: 1.8412954053244703 acc: 0.85\n",
      "loss: 1.787345997542279 acc: 0.89\n",
      "loss: 1.832112552616264 acc: 0.84\n",
      "loss: 1.8414373560350332 acc: 0.8\n",
      "loss: 1.8203937748555068 acc: 0.85\n",
      "loss: 1.8383252246687887 acc: 0.84\n",
      "loss: 1.826257876697706 acc: 0.8\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 1.8263\t Accuracy 0.8000\n",
      "loss: 1.8264060039853263 acc: 0.84\n",
      "loss: 1.838977543635235 acc: 0.82\n",
      "loss: 1.8295983263341449 acc: 0.79\n",
      "loss: 1.8381184602451521 acc: 0.79\n",
      "loss: 1.813250175874021 acc: 0.85\n",
      "loss: 1.805601623168058 acc: 0.88\n",
      "loss: 1.8256680914102876 acc: 0.82\n",
      "loss: 1.8443889041861996 acc: 0.8\n",
      "loss: 1.8470690704070538 acc: 0.81\n",
      "loss: 1.7943066641772054 acc: 0.88\n",
      "loss: 1.7358639755836986 acc: 0.89\n",
      "loss: 1.8416539889899752 acc: 0.84\n",
      "loss: 1.871337492746893 acc: 0.78\n",
      "loss: 1.7837986001810082 acc: 0.9\n",
      "loss: 1.8663773367577523 acc: 0.8\n",
      "loss: 1.8240527803086772 acc: 0.8\n",
      "loss: 1.8340528804997072 acc: 0.83\n",
      "loss: 1.8460215785335934 acc: 0.82\n",
      "loss: 1.8243888145219316 acc: 0.88\n",
      "loss: 1.7510593581656757 acc: 0.92\n",
      "loss: 1.820060176032004 acc: 0.84\n",
      "loss: 1.8245719030352012 acc: 0.83\n",
      "loss: 1.8343869618954929 acc: 0.85\n",
      "loss: 1.8518085834537599 acc: 0.82\n",
      "loss: 1.8160754700994357 acc: 0.87\n",
      "loss: 1.7826369108174995 acc: 0.86\n",
      "loss: 1.7930243039760212 acc: 0.9\n",
      "loss: 1.82170715397236 acc: 0.86\n",
      "loss: 1.8279007356374735 acc: 0.87\n",
      "loss: 1.8254519795755515 acc: 0.81\n",
      "loss: 1.8128846662728728 acc: 0.88\n",
      "loss: 1.8308951819648291 acc: 0.77\n",
      "loss: 1.7837214681790703 acc: 0.86\n",
      "loss: 1.822905209428882 acc: 0.85\n",
      "loss: 1.8139755807031503 acc: 0.89\n",
      "loss: 1.8047600464794653 acc: 0.79\n",
      "loss: 1.8439335587097225 acc: 0.77\n",
      "loss: 1.7856261362881032 acc: 0.89\n",
      "loss: 1.8662417244983178 acc: 0.83\n",
      "loss: 1.820578537109476 acc: 0.83\n",
      "loss: 1.808893389400057 acc: 0.86\n",
      "loss: 1.839331547306748 acc: 0.86\n",
      "loss: 1.8057243503938962 acc: 0.88\n",
      "loss: 1.855099886474361 acc: 0.85\n",
      "loss: 1.8437176521260468 acc: 0.8\n",
      "loss: 1.8359053403059542 acc: 0.81\n",
      "loss: 1.8340364890679988 acc: 0.9\n",
      "loss: 1.8107724946600183 acc: 0.85\n",
      "loss: 1.7960616360462414 acc: 0.82\n",
      "loss: 1.8108041364175589 acc: 0.86\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 1.8108\t Accuracy 0.8600\n",
      "loss: 1.8378512567547816 acc: 0.84\n",
      "loss: 1.8320143726889069 acc: 0.86\n",
      "loss: 1.8117229980634013 acc: 0.81\n",
      "loss: 1.7928975570338455 acc: 0.86\n",
      "loss: 1.8262503982227682 acc: 0.83\n",
      "loss: 1.807987421555419 acc: 0.86\n",
      "loss: 1.8031691694789194 acc: 0.84\n",
      "loss: 1.8336012501152734 acc: 0.84\n",
      "loss: 1.852312619063912 acc: 0.81\n",
      "loss: 1.8255531758720716 acc: 0.83\n",
      "loss: 1.804175175803025 acc: 0.84\n",
      "loss: 1.8415573632753452 acc: 0.81\n",
      "loss: 1.8435660268315999 acc: 0.83\n",
      "loss: 1.7717132970707528 acc: 0.87\n",
      "loss: 1.8111009371805864 acc: 0.87\n",
      "loss: 1.831565907546363 acc: 0.86\n",
      "loss: 1.7916997846146772 acc: 0.88\n",
      "loss: 1.8175239197877067 acc: 0.82\n",
      "loss: 1.815244740925204 acc: 0.83\n",
      "loss: 1.7973450556889239 acc: 0.87\n",
      "loss: 1.8310309488479808 acc: 0.86\n",
      "loss: 1.8360782899228647 acc: 0.81\n",
      "loss: 1.7973376160951668 acc: 0.86\n",
      "loss: 1.8709762476150018 acc: 0.78\n",
      "loss: 1.8258274260275837 acc: 0.83\n",
      "loss: 1.840347544258546 acc: 0.8\n",
      "loss: 1.8469432315504262 acc: 0.84\n",
      "loss: 1.8301029753084803 acc: 0.84\n",
      "loss: 1.8468489374893104 acc: 0.82\n",
      "loss: 1.813375864615926 acc: 0.83\n",
      "loss: 1.8269817624849312 acc: 0.87\n",
      "loss: 1.862728673223379 acc: 0.81\n",
      "loss: 1.7910070056411218 acc: 0.89\n",
      "loss: 1.793892624304383 acc: 0.85\n",
      "loss: 1.7982214991564982 acc: 0.86\n",
      "loss: 1.799431836260431 acc: 0.87\n",
      "loss: 1.8098989736242175 acc: 0.85\n",
      "loss: 1.8697803110791984 acc: 0.76\n",
      "loss: 1.81368675525818 acc: 0.82\n",
      "loss: 1.8278275751734447 acc: 0.83\n",
      "loss: 1.805521873415861 acc: 0.9\n",
      "loss: 1.825599112020262 acc: 0.84\n",
      "loss: 1.8410871578905315 acc: 0.84\n",
      "loss: 1.7907083767717547 acc: 0.83\n",
      "loss: 1.8015438428150645 acc: 0.87\n",
      "loss: 1.8192254971972646 acc: 0.8\n",
      "loss: 1.8478900161839755 acc: 0.84\n",
      "loss: 1.8224620748307154 acc: 0.85\n",
      "loss: 1.8273471239131458 acc: 0.81\n",
      "loss: 1.82809073145465 acc: 0.88\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 1.8281\t Accuracy 0.8800\n",
      "loss: 1.8348124848954808 acc: 0.89\n",
      "loss: 1.848831062626248 acc: 0.82\n",
      "loss: 1.814924932591376 acc: 0.91\n",
      "loss: 1.7907071271790307 acc: 0.89\n",
      "loss: 1.8060038557976297 acc: 0.87\n",
      "loss: 1.8322641722533732 acc: 0.84\n",
      "loss: 1.8377266057179835 acc: 0.81\n",
      "loss: 1.819738787867682 acc: 0.85\n",
      "loss: 1.828680661239412 acc: 0.84\n",
      "loss: 1.8542853221410778 acc: 0.79\n",
      "loss: 1.8383162468024998 acc: 0.78\n",
      "loss: 1.8181449884734422 acc: 0.84\n",
      "loss: 1.7753706312615285 acc: 0.88\n",
      "loss: 1.8035080899066944 acc: 0.84\n",
      "loss: 1.7940973720374558 acc: 0.89\n",
      "loss: 1.854527989814496 acc: 0.77\n",
      "loss: 1.8231920312115235 acc: 0.85\n",
      "loss: 1.7902014504423966 acc: 0.87\n",
      "loss: 1.8158380735325195 acc: 0.85\n",
      "loss: 1.8177005029522348 acc: 0.87\n",
      "loss: 1.8640003646121113 acc: 0.77\n",
      "loss: 1.8650829498168493 acc: 0.75\n",
      "loss: 1.8355138758176313 acc: 0.8\n",
      "loss: 1.842231332690783 acc: 0.86\n",
      "loss: 1.821972698796425 acc: 0.85\n",
      "loss: 1.8279537540255941 acc: 0.83\n",
      "loss: 1.7852628145906961 acc: 0.89\n",
      "loss: 1.8103474802568116 acc: 0.88\n",
      "loss: 1.7988553664826812 acc: 0.88\n",
      "loss: 1.8057182623855783 acc: 0.91\n",
      "loss: 1.859279637844657 acc: 0.8\n",
      "loss: 1.800671012614616 acc: 0.89\n",
      "loss: 1.846686718024195 acc: 0.87\n",
      "loss: 1.8010473956319077 acc: 0.89\n",
      "loss: 1.8210461086579535 acc: 0.87\n",
      "loss: 1.8649320723875074 acc: 0.78\n",
      "loss: 1.8151829511193756 acc: 0.88\n",
      "loss: 1.8280447029644407 acc: 0.8\n",
      "loss: 1.7908534999351808 acc: 0.91\n",
      "loss: 1.8031349077101486 acc: 0.87\n",
      "loss: 1.8205340081929406 acc: 0.84\n",
      "loss: 1.8489737907304127 acc: 0.79\n",
      "loss: 1.820900730742113 acc: 0.83\n",
      "loss: 1.767482851811568 acc: 0.94\n",
      "loss: 1.8403326552474635 acc: 0.79\n",
      "loss: 1.8564831118270524 acc: 0.82\n",
      "loss: 1.8427403768497812 acc: 0.81\n",
      "loss: 1.83913978992761 acc: 0.82\n",
      "loss: 1.8414815075599642 acc: 0.78\n",
      "loss: 1.7976927598002703 acc: 0.86\n",
      "loss: 1.799475759241053 acc: 0.88\n",
      "loss: 1.8146656694470362 acc: 0.88\n",
      "loss: 1.8353644961434723 acc: 0.87\n",
      "loss: 1.8242510315950218 acc: 0.94\n",
      "loss: 1.8090406142906068 acc: 0.85\n",
      "loss: 1.7554711684988815 acc: 0.86\n",
      "loss: 1.7663080122403014 acc: 0.86\n",
      "loss: 1.8314228404479158 acc: 0.86\n",
      "loss: 1.7548630657380946 acc: 0.94\n",
      "loss: 1.80578643485574 acc: 0.87\n",
      "loss: 1.8001871758817714 acc: 0.84\n",
      "loss: 1.8484016206760763 acc: 0.84\n",
      "loss: 1.8540436524121546 acc: 0.88\n",
      "loss: 1.8714508730332222 acc: 0.85\n",
      "loss: 1.7866781142128323 acc: 0.96\n",
      "loss: 1.8032642798079572 acc: 0.85\n",
      "loss: 1.7574997094123301 acc: 0.93\n",
      "loss: 1.7915267016568834 acc: 0.87\n",
      "loss: 1.7961456992502485 acc: 0.87\n",
      "loss: 1.8736475175105898 acc: 0.87\n",
      "loss: 1.823712050538323 acc: 0.88\n",
      "loss: 1.8292486283937044 acc: 0.76\n",
      "loss: 1.8613446273731897 acc: 0.87\n",
      "loss: 1.8305304619897482 acc: 0.9\n",
      "loss: 1.8519841826782106 acc: 0.79\n",
      "loss: 1.8551048839903999 acc: 0.81\n",
      "loss: 1.884496967890953 acc: 0.79\n",
      "loss: 1.8202506276942756 acc: 0.91\n",
      "loss: 1.7953195850787131 acc: 0.89\n",
      "loss: 1.8226377901829591 acc: 0.82\n",
      "loss: 1.7767601218755862 acc: 0.94\n",
      "loss: 1.744561065017151 acc: 0.94\n",
      "loss: 1.7941032645731414 acc: 0.92\n",
      "loss: 1.7893082093429027 acc: 0.88\n",
      "loss: 1.8153767946244905 acc: 0.9\n",
      "loss: 1.8515768284088672 acc: 0.91\n",
      "loss: 1.8434308314234817 acc: 0.89\n",
      "loss: 1.7429724401576268 acc: 0.91\n",
      "loss: 1.7186695968794397 acc: 0.97\n",
      "loss: 1.7406834340707869 acc: 0.96\n",
      "loss: 1.7471801217209206 acc: 0.96\n",
      "loss: 1.7992342986979586 acc: 0.87\n",
      "loss: 1.7814026835749195 acc: 0.84\n",
      "loss: 1.7293887472732743 acc: 0.86\n",
      "loss: 1.8061948224113404 acc: 0.9\n",
      "loss: 1.8144237271419255 acc: 0.92\n",
      "loss: 1.8826112474734993 acc: 0.79\n",
      "loss: 1.7181013864494523 acc: 0.96\n",
      "loss: 1.8665179572000201 acc: 0.85\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8237\t Average training accuracy 0.8407\n",
      "Epoch [3]\t Average validation loss 1.8063\t Average validation accuracy 0.8804\n",
      "\n",
      "loss: 1.8554651140433365 acc: 0.79\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 1.8555\t Accuracy 0.7900\n",
      "loss: 1.788979311708175 acc: 0.87\n",
      "loss: 1.7776842281807452 acc: 0.87\n",
      "loss: 1.8514357999353848 acc: 0.75\n",
      "loss: 1.843522049031756 acc: 0.84\n",
      "loss: 1.8440626323099634 acc: 0.83\n",
      "loss: 1.8399246175271233 acc: 0.88\n",
      "loss: 1.8365100320643082 acc: 0.83\n",
      "loss: 1.8426306791741973 acc: 0.85\n",
      "loss: 1.8066666544935928 acc: 0.87\n",
      "loss: 1.812423505001464 acc: 0.88\n",
      "loss: 1.861895567088962 acc: 0.84\n",
      "loss: 1.8037192042918164 acc: 0.94\n",
      "loss: 1.8412296441249227 acc: 0.83\n",
      "loss: 1.8694028051010347 acc: 0.83\n",
      "loss: 1.8069205785021152 acc: 0.86\n",
      "loss: 1.8448369014665564 acc: 0.84\n",
      "loss: 1.8616516111423633 acc: 0.81\n",
      "loss: 1.8523467584991735 acc: 0.82\n",
      "loss: 1.79889637442396 acc: 0.87\n",
      "loss: 1.7991463242223713 acc: 0.85\n",
      "loss: 1.8317066822835417 acc: 0.8\n",
      "loss: 1.8440496649622333 acc: 0.82\n",
      "loss: 1.8426358019086018 acc: 0.83\n",
      "loss: 1.7876070860140356 acc: 0.89\n",
      "loss: 1.8337132851010804 acc: 0.83\n",
      "loss: 1.8195100456282023 acc: 0.85\n",
      "loss: 1.8340713540927966 acc: 0.86\n",
      "loss: 1.861689858250013 acc: 0.8\n",
      "loss: 1.804606833276254 acc: 0.84\n",
      "loss: 1.816600899079418 acc: 0.85\n",
      "loss: 1.8156881241603384 acc: 0.83\n",
      "loss: 1.820165692862663 acc: 0.79\n",
      "loss: 1.821565654517862 acc: 0.85\n",
      "loss: 1.82446748633036 acc: 0.81\n",
      "loss: 1.840522972102149 acc: 0.79\n",
      "loss: 1.8148362027243288 acc: 0.86\n",
      "loss: 1.8158579270178217 acc: 0.86\n",
      "loss: 1.859104740460561 acc: 0.74\n",
      "loss: 1.8315795915386095 acc: 0.8\n",
      "loss: 1.8240617954609823 acc: 0.84\n",
      "loss: 1.8373366421482205 acc: 0.89\n",
      "loss: 1.825838839924511 acc: 0.86\n",
      "loss: 1.8413088211122446 acc: 0.8\n",
      "loss: 1.857985986575479 acc: 0.78\n",
      "loss: 1.8446398610659582 acc: 0.88\n",
      "loss: 1.8266443861805122 acc: 0.82\n",
      "loss: 1.8378325091484162 acc: 0.87\n",
      "loss: 1.8448448376870856 acc: 0.79\n",
      "loss: 1.8106656415133449 acc: 0.86\n",
      "loss: 1.8508667322991192 acc: 0.84\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 1.8509\t Accuracy 0.8400\n",
      "loss: 1.8591549083317702 acc: 0.78\n",
      "loss: 1.8486451209415113 acc: 0.82\n",
      "loss: 1.8150073718566382 acc: 0.81\n",
      "loss: 1.8146365388881176 acc: 0.89\n",
      "loss: 1.8235509004157742 acc: 0.86\n",
      "loss: 1.8332299610293357 acc: 0.84\n",
      "loss: 1.7966604938533868 acc: 0.85\n",
      "loss: 1.8136411549848088 acc: 0.85\n",
      "loss: 1.8045546891353865 acc: 0.92\n",
      "loss: 1.8423943641783018 acc: 0.85\n",
      "loss: 1.756273590601599 acc: 0.91\n",
      "loss: 1.8265535075479693 acc: 0.84\n",
      "loss: 1.800904584124802 acc: 0.85\n",
      "loss: 1.8386246706129368 acc: 0.81\n",
      "loss: 1.7891542616126932 acc: 0.89\n",
      "loss: 1.8204607191178672 acc: 0.83\n",
      "loss: 1.8328028195121513 acc: 0.81\n",
      "loss: 1.8153492096370503 acc: 0.84\n",
      "loss: 1.8169473374253713 acc: 0.87\n",
      "loss: 1.7884108654283912 acc: 0.88\n",
      "loss: 1.8211683372134575 acc: 0.83\n",
      "loss: 1.759662801460347 acc: 0.89\n",
      "loss: 1.8051495577310055 acc: 0.82\n",
      "loss: 1.8448847788789298 acc: 0.8\n",
      "loss: 1.8182778591665087 acc: 0.8\n",
      "loss: 1.8073157587633781 acc: 0.87\n",
      "loss: 1.8229969779590265 acc: 0.9\n",
      "loss: 1.8463858919594693 acc: 0.81\n",
      "loss: 1.7930085452318196 acc: 0.85\n",
      "loss: 1.7751698814176735 acc: 0.87\n",
      "loss: 1.8353101464885422 acc: 0.82\n",
      "loss: 1.8301391263008546 acc: 0.88\n",
      "loss: 1.8452359693607354 acc: 0.78\n",
      "loss: 1.7919298612983954 acc: 0.85\n",
      "loss: 1.8479376433941865 acc: 0.84\n",
      "loss: 1.8226885549938134 acc: 0.84\n",
      "loss: 1.8247637648575041 acc: 0.88\n",
      "loss: 1.7678862093296166 acc: 0.91\n",
      "loss: 1.8168458311915183 acc: 0.88\n",
      "loss: 1.8166012738069 acc: 0.88\n",
      "loss: 1.8248030521735998 acc: 0.77\n",
      "loss: 1.8135731458053161 acc: 0.83\n",
      "loss: 1.8489354537844969 acc: 0.81\n",
      "loss: 1.8282425344763473 acc: 0.78\n",
      "loss: 1.806643944088806 acc: 0.87\n",
      "loss: 1.8324019530155249 acc: 0.86\n",
      "loss: 1.822932804755649 acc: 0.89\n",
      "loss: 1.811868684252504 acc: 0.84\n",
      "loss: 1.8002773835724395 acc: 0.87\n",
      "loss: 1.8287350584722162 acc: 0.85\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 1.8287\t Accuracy 0.8500\n",
      "loss: 1.8894749139296112 acc: 0.81\n",
      "loss: 1.8292356397872058 acc: 0.88\n",
      "loss: 1.8235214561300033 acc: 0.84\n",
      "loss: 1.8401023486622803 acc: 0.79\n",
      "loss: 1.804594463570545 acc: 0.91\n",
      "loss: 1.8180555974354828 acc: 0.8\n",
      "loss: 1.7555847881732272 acc: 0.91\n",
      "loss: 1.8180196386434844 acc: 0.88\n",
      "loss: 1.8139433838021406 acc: 0.81\n",
      "loss: 1.8171662076694677 acc: 0.83\n",
      "loss: 1.8520593307865345 acc: 0.81\n",
      "loss: 1.807250201174923 acc: 0.81\n",
      "loss: 1.8435339959818828 acc: 0.87\n",
      "loss: 1.8219619266482934 acc: 0.81\n",
      "loss: 1.830049593394921 acc: 0.88\n",
      "loss: 1.8187938847284801 acc: 0.84\n",
      "loss: 1.827833316673305 acc: 0.88\n",
      "loss: 1.837255230186223 acc: 0.78\n",
      "loss: 1.8286893584816182 acc: 0.84\n",
      "loss: 1.7915270751407197 acc: 0.87\n",
      "loss: 1.8239509514935583 acc: 0.84\n",
      "loss: 1.84436132702126 acc: 0.8\n",
      "loss: 1.8149017618512995 acc: 0.83\n",
      "loss: 1.8286363307686546 acc: 0.85\n",
      "loss: 1.83367522550733 acc: 0.82\n",
      "loss: 1.8106477392960514 acc: 0.83\n",
      "loss: 1.789448270209079 acc: 0.84\n",
      "loss: 1.7952769557371773 acc: 0.85\n",
      "loss: 1.8546528677565641 acc: 0.74\n",
      "loss: 1.8453405454025873 acc: 0.83\n",
      "loss: 1.783216687784745 acc: 0.86\n",
      "loss: 1.852563517289863 acc: 0.8\n",
      "loss: 1.8437771147314945 acc: 0.79\n",
      "loss: 1.7923862729403248 acc: 0.89\n",
      "loss: 1.8211237435442431 acc: 0.86\n",
      "loss: 1.8166399581283048 acc: 0.9\n",
      "loss: 1.8109566303184637 acc: 0.87\n",
      "loss: 1.848087729516969 acc: 0.81\n",
      "loss: 1.8386225221425536 acc: 0.79\n",
      "loss: 1.8354893489065947 acc: 0.84\n",
      "loss: 1.7633895147840988 acc: 0.9\n",
      "loss: 1.7997546701792553 acc: 0.89\n",
      "loss: 1.8419178858421537 acc: 0.85\n",
      "loss: 1.8159127163747775 acc: 0.8\n",
      "loss: 1.8415950363299896 acc: 0.81\n",
      "loss: 1.8436288355665296 acc: 0.84\n",
      "loss: 1.8272546506022207 acc: 0.77\n",
      "loss: 1.8106892349704842 acc: 0.85\n",
      "loss: 1.8022237885885515 acc: 0.8\n",
      "loss: 1.7878571808678896 acc: 0.9\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 1.7879\t Accuracy 0.9000\n",
      "loss: 1.8249899521593707 acc: 0.87\n",
      "loss: 1.8067639685920318 acc: 0.9\n",
      "loss: 1.8559844603399878 acc: 0.82\n",
      "loss: 1.8302144551258572 acc: 0.87\n",
      "loss: 1.8210746869991636 acc: 0.85\n",
      "loss: 1.8299281257467568 acc: 0.83\n",
      "loss: 1.8524844238125533 acc: 0.82\n",
      "loss: 1.833422791646835 acc: 0.78\n",
      "loss: 1.842156110690024 acc: 0.84\n",
      "loss: 1.8286092244675698 acc: 0.84\n",
      "loss: 1.8065894587770608 acc: 0.86\n",
      "loss: 1.7999709683478364 acc: 0.83\n",
      "loss: 1.8439228557993816 acc: 0.85\n",
      "loss: 1.8199778192214169 acc: 0.89\n",
      "loss: 1.7905482105988098 acc: 0.91\n",
      "loss: 1.806864091911888 acc: 0.89\n",
      "loss: 1.834002180937775 acc: 0.82\n",
      "loss: 1.828985786400725 acc: 0.85\n",
      "loss: 1.794102643588487 acc: 0.83\n",
      "loss: 1.8283991377112938 acc: 0.8\n",
      "loss: 1.795163429962782 acc: 0.9\n",
      "loss: 1.822652900339602 acc: 0.85\n",
      "loss: 1.831854305832406 acc: 0.84\n",
      "loss: 1.8145600577285494 acc: 0.85\n",
      "loss: 1.8160577502055812 acc: 0.87\n",
      "loss: 1.827073707108052 acc: 0.83\n",
      "loss: 1.8777751270424219 acc: 0.76\n",
      "loss: 1.8043887295207153 acc: 0.85\n",
      "loss: 1.8190283317774418 acc: 0.76\n",
      "loss: 1.8018679132934472 acc: 0.89\n",
      "loss: 1.8384034126702307 acc: 0.8\n",
      "loss: 1.772728136302363 acc: 0.93\n",
      "loss: 1.822438146210142 acc: 0.82\n",
      "loss: 1.8028734781085876 acc: 0.85\n",
      "loss: 1.8287694811370534 acc: 0.81\n",
      "loss: 1.8495084572270721 acc: 0.84\n",
      "loss: 1.837578873916308 acc: 0.84\n",
      "loss: 1.8331482317985945 acc: 0.84\n",
      "loss: 1.7815995828458175 acc: 0.85\n",
      "loss: 1.8232563766227496 acc: 0.84\n",
      "loss: 1.8587393842646245 acc: 0.83\n",
      "loss: 1.8255269827900973 acc: 0.83\n",
      "loss: 1.7945564721586584 acc: 0.91\n",
      "loss: 1.8420281377412702 acc: 0.8\n",
      "loss: 1.8510313375593441 acc: 0.81\n",
      "loss: 1.8523774172507677 acc: 0.82\n",
      "loss: 1.819747331643846 acc: 0.88\n",
      "loss: 1.8529345319450155 acc: 0.84\n",
      "loss: 1.8209990597995516 acc: 0.86\n",
      "loss: 1.795917191314183 acc: 0.89\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 1.7959\t Accuracy 0.8900\n",
      "loss: 1.8332242893935446 acc: 0.86\n",
      "loss: 1.7915118895396256 acc: 0.86\n",
      "loss: 1.829350066479344 acc: 0.87\n",
      "loss: 1.8606329408237634 acc: 0.76\n",
      "loss: 1.8042672807432256 acc: 0.86\n",
      "loss: 1.7821680227773173 acc: 0.89\n",
      "loss: 1.8278719159610128 acc: 0.83\n",
      "loss: 1.7989300961640184 acc: 0.87\n",
      "loss: 1.8221606000465085 acc: 0.84\n",
      "loss: 1.8241041348628977 acc: 0.81\n",
      "loss: 1.7937945669101603 acc: 0.89\n",
      "loss: 1.8210524485521653 acc: 0.84\n",
      "loss: 1.8120767911478088 acc: 0.86\n",
      "loss: 1.832714603497486 acc: 0.87\n",
      "loss: 1.8073931354811257 acc: 0.86\n",
      "loss: 1.7670183794012158 acc: 0.9\n",
      "loss: 1.781426196085937 acc: 0.91\n",
      "loss: 1.7868518014400987 acc: 0.87\n",
      "loss: 1.783653249197574 acc: 0.88\n",
      "loss: 1.8284125590905038 acc: 0.84\n",
      "loss: 1.8221122931473857 acc: 0.85\n",
      "loss: 1.8011797312697109 acc: 0.83\n",
      "loss: 1.7882815865042847 acc: 0.89\n",
      "loss: 1.7954133824834164 acc: 0.86\n",
      "loss: 1.8304494642909013 acc: 0.84\n",
      "loss: 1.8307688985107702 acc: 0.84\n",
      "loss: 1.8302642179588346 acc: 0.8\n",
      "loss: 1.835674736772968 acc: 0.85\n",
      "loss: 1.8454951278809886 acc: 0.82\n",
      "loss: 1.8296712182986141 acc: 0.84\n",
      "loss: 1.803900743507328 acc: 0.86\n",
      "loss: 1.843643309993505 acc: 0.81\n",
      "loss: 1.7821533409916848 acc: 0.89\n",
      "loss: 1.803374726062446 acc: 0.86\n",
      "loss: 1.7877167553283695 acc: 0.88\n",
      "loss: 1.8245180180194893 acc: 0.76\n",
      "loss: 1.8187028604208177 acc: 0.82\n",
      "loss: 1.8769319516350935 acc: 0.8\n",
      "loss: 1.8147995290449348 acc: 0.84\n",
      "loss: 1.8072492207395876 acc: 0.89\n",
      "loss: 1.838179546306908 acc: 0.82\n",
      "loss: 1.8177901171757664 acc: 0.83\n",
      "loss: 1.817133511310105 acc: 0.88\n",
      "loss: 1.793331311028301 acc: 0.9\n",
      "loss: 1.8539483909022199 acc: 0.77\n",
      "loss: 1.836028531305714 acc: 0.89\n",
      "loss: 1.8352924207390282 acc: 0.91\n",
      "loss: 1.7890376343580763 acc: 0.87\n",
      "loss: 1.818091018236985 acc: 0.88\n",
      "loss: 1.8241706221598895 acc: 0.8\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 1.8242\t Accuracy 0.8000\n",
      "loss: 1.8229108775056444 acc: 0.84\n",
      "loss: 1.7762289937032214 acc: 0.9\n",
      "loss: 1.8261675707931448 acc: 0.83\n",
      "loss: 1.833351345898596 acc: 0.84\n",
      "loss: 1.8199662148948739 acc: 0.87\n",
      "loss: 1.8277323409905912 acc: 0.85\n",
      "loss: 1.7982327010348582 acc: 0.9\n",
      "loss: 1.837517728399566 acc: 0.84\n",
      "loss: 1.815881777284921 acc: 0.86\n",
      "loss: 1.8203561427023565 acc: 0.81\n",
      "loss: 1.8054393082529507 acc: 0.87\n",
      "loss: 1.8083539278310787 acc: 0.84\n",
      "loss: 1.8434123294116431 acc: 0.82\n",
      "loss: 1.8286481288327516 acc: 0.78\n",
      "loss: 1.844382026583252 acc: 0.84\n",
      "loss: 1.8368481669540728 acc: 0.81\n",
      "loss: 1.8187735932545857 acc: 0.88\n",
      "loss: 1.8105004453682787 acc: 0.88\n",
      "loss: 1.8222488337864804 acc: 0.88\n",
      "loss: 1.8866799338583198 acc: 0.74\n",
      "loss: 1.8493096577584804 acc: 0.82\n",
      "loss: 1.8207384524783572 acc: 0.82\n",
      "loss: 1.8181720083896886 acc: 0.92\n",
      "loss: 1.798631474681771 acc: 0.88\n",
      "loss: 1.8430937462050918 acc: 0.78\n",
      "loss: 1.7923372629998264 acc: 0.91\n",
      "loss: 1.8633404781859115 acc: 0.83\n",
      "loss: 1.8121588878905632 acc: 0.89\n",
      "loss: 1.8010740564055028 acc: 0.85\n",
      "loss: 1.8741535392412936 acc: 0.83\n",
      "loss: 1.8054189741406907 acc: 0.85\n",
      "loss: 1.838374451565053 acc: 0.82\n",
      "loss: 1.8407390466351172 acc: 0.83\n",
      "loss: 1.8490107987633462 acc: 0.86\n",
      "loss: 1.8005401838109438 acc: 0.83\n",
      "loss: 1.8168654482630304 acc: 0.84\n",
      "loss: 1.7989066948418884 acc: 0.87\n",
      "loss: 1.798358887653865 acc: 0.85\n",
      "loss: 1.8376859415450417 acc: 0.83\n",
      "loss: 1.7845475234048696 acc: 0.89\n",
      "loss: 1.9211371578104748 acc: 0.74\n",
      "loss: 1.8434691613284628 acc: 0.81\n",
      "loss: 1.7795717980396575 acc: 0.84\n",
      "loss: 1.7893298564770577 acc: 0.83\n",
      "loss: 1.8295706153661115 acc: 0.77\n",
      "loss: 1.8079101432774147 acc: 0.83\n",
      "loss: 1.8083217592066236 acc: 0.85\n",
      "loss: 1.830445070884671 acc: 0.76\n",
      "loss: 1.7979301402219454 acc: 0.86\n",
      "loss: 1.8672476750085358 acc: 0.77\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 1.8672\t Accuracy 0.7700\n",
      "loss: 1.791078711939886 acc: 0.87\n",
      "loss: 1.836745892804048 acc: 0.84\n",
      "loss: 1.8558263635531782 acc: 0.86\n",
      "loss: 1.8030318951484567 acc: 0.9\n",
      "loss: 1.8319249266764257 acc: 0.86\n",
      "loss: 1.864889671471612 acc: 0.83\n",
      "loss: 1.8356060795945843 acc: 0.8\n",
      "loss: 1.850947197847411 acc: 0.83\n",
      "loss: 1.8584737578946684 acc: 0.79\n",
      "loss: 1.7989411338638355 acc: 0.86\n",
      "loss: 1.8315711354072015 acc: 0.81\n",
      "loss: 1.8245384579190702 acc: 0.8\n",
      "loss: 1.834929323242121 acc: 0.86\n",
      "loss: 1.7979067555314003 acc: 0.78\n",
      "loss: 1.8324472312652198 acc: 0.85\n",
      "loss: 1.8354933905476443 acc: 0.83\n",
      "loss: 1.845980818203395 acc: 0.82\n",
      "loss: 1.8274043254612478 acc: 0.85\n",
      "loss: 1.805828848063334 acc: 0.89\n",
      "loss: 1.8285764371703672 acc: 0.86\n",
      "loss: 1.826640703286653 acc: 0.92\n",
      "loss: 1.8591411428181914 acc: 0.83\n",
      "loss: 1.8055492557372856 acc: 0.87\n",
      "loss: 1.839696843887271 acc: 0.8\n",
      "loss: 1.8481765833849142 acc: 0.84\n",
      "loss: 1.8104937763254838 acc: 0.91\n",
      "loss: 1.7963655400014926 acc: 0.93\n",
      "loss: 1.8229928645294442 acc: 0.86\n",
      "loss: 1.8302893249899055 acc: 0.87\n",
      "loss: 1.8596288807009091 acc: 0.85\n",
      "loss: 1.8238273851345383 acc: 0.87\n",
      "loss: 1.832055883875535 acc: 0.87\n",
      "loss: 1.8128810855954083 acc: 0.86\n",
      "loss: 1.792975709703438 acc: 0.91\n",
      "loss: 1.8555078231129274 acc: 0.84\n",
      "loss: 1.831345725930872 acc: 0.81\n",
      "loss: 1.805056478095971 acc: 0.85\n",
      "loss: 1.7811929131382511 acc: 0.91\n",
      "loss: 1.8156793733908594 acc: 0.93\n",
      "loss: 1.806545039272698 acc: 0.85\n",
      "loss: 1.8463378556238639 acc: 0.85\n",
      "loss: 1.8505570149525037 acc: 0.83\n",
      "loss: 1.7928281982395142 acc: 0.84\n",
      "loss: 1.832242075824235 acc: 0.82\n",
      "loss: 1.8255017730696794 acc: 0.78\n",
      "loss: 1.823710704235987 acc: 0.83\n",
      "loss: 1.78931715458395 acc: 0.93\n",
      "loss: 1.777466397212502 acc: 0.86\n",
      "loss: 1.8321031869868971 acc: 0.77\n",
      "loss: 1.8243560625604403 acc: 0.8\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 1.8244\t Accuracy 0.8000\n",
      "loss: 1.8436964784810064 acc: 0.82\n",
      "loss: 1.7951560065164285 acc: 0.88\n",
      "loss: 1.8030668893013828 acc: 0.9\n",
      "loss: 1.8275198252754268 acc: 0.83\n",
      "loss: 1.819665200395283 acc: 0.83\n",
      "loss: 1.8313811462409024 acc: 0.83\n",
      "loss: 1.8138565758577732 acc: 0.84\n",
      "loss: 1.7896208300934924 acc: 0.85\n",
      "loss: 1.825470919693829 acc: 0.84\n",
      "loss: 1.8064182534858184 acc: 0.91\n",
      "loss: 1.812364224173781 acc: 0.84\n",
      "loss: 1.812919723515858 acc: 0.92\n",
      "loss: 1.8276919439739225 acc: 0.83\n",
      "loss: 1.8708401111753696 acc: 0.71\n",
      "loss: 1.8197740989736753 acc: 0.84\n",
      "loss: 1.840758976120543 acc: 0.86\n",
      "loss: 1.829134567962898 acc: 0.91\n",
      "loss: 1.7953635318348304 acc: 0.89\n",
      "loss: 1.830541185149138 acc: 0.81\n",
      "loss: 1.808463064633537 acc: 0.87\n",
      "loss: 1.8151612841834128 acc: 0.83\n",
      "loss: 1.8153611107434733 acc: 0.87\n",
      "loss: 1.8195809160435112 acc: 0.84\n",
      "loss: 1.810089130995203 acc: 0.87\n",
      "loss: 1.8227151696141204 acc: 0.79\n",
      "loss: 1.8774320714436723 acc: 0.75\n",
      "loss: 1.8385046829264464 acc: 0.87\n",
      "loss: 1.8166870165607043 acc: 0.88\n",
      "loss: 1.768885498242541 acc: 0.9\n",
      "loss: 1.805741651884456 acc: 0.84\n",
      "loss: 1.7909511146328756 acc: 0.87\n",
      "loss: 1.8210737314316818 acc: 0.85\n",
      "loss: 1.8385658707412267 acc: 0.82\n",
      "loss: 1.802378092811877 acc: 0.88\n",
      "loss: 1.8245090307678469 acc: 0.88\n",
      "loss: 1.8218586277937876 acc: 0.81\n",
      "loss: 1.7847067511939307 acc: 0.86\n",
      "loss: 1.8112006574177082 acc: 0.82\n",
      "loss: 1.8112123076106839 acc: 0.84\n",
      "loss: 1.8497955883194448 acc: 0.79\n",
      "loss: 1.8145220383964393 acc: 0.84\n",
      "loss: 1.7730590198828906 acc: 0.89\n",
      "loss: 1.8424135040509118 acc: 0.79\n",
      "loss: 1.816195180241611 acc: 0.87\n",
      "loss: 1.8278464493480076 acc: 0.83\n",
      "loss: 1.8225004578524864 acc: 0.82\n",
      "loss: 1.7873093560689106 acc: 0.88\n",
      "loss: 1.832902544016734 acc: 0.86\n",
      "loss: 1.8262279033942845 acc: 0.82\n",
      "loss: 1.8134536524550635 acc: 0.87\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 1.8135\t Accuracy 0.8700\n",
      "loss: 1.8256945533362667 acc: 0.76\n",
      "loss: 1.8041972884543742 acc: 0.85\n",
      "loss: 1.8594742299884468 acc: 0.78\n",
      "loss: 1.8666617001906811 acc: 0.81\n",
      "loss: 1.7993933376179085 acc: 0.91\n",
      "loss: 1.8476132652259034 acc: 0.76\n",
      "loss: 1.8280921928397205 acc: 0.8\n",
      "loss: 1.8067234744491278 acc: 0.85\n",
      "loss: 1.8367181828814938 acc: 0.84\n",
      "loss: 1.794737226753356 acc: 0.87\n",
      "loss: 1.820646077410892 acc: 0.86\n",
      "loss: 1.819358473560112 acc: 0.85\n",
      "loss: 1.8262481695938235 acc: 0.83\n",
      "loss: 1.8481108479141721 acc: 0.81\n",
      "loss: 1.842595290311559 acc: 0.81\n",
      "loss: 1.8044902428594027 acc: 0.91\n",
      "loss: 1.8037748101610307 acc: 0.85\n",
      "loss: 1.820194869408943 acc: 0.86\n",
      "loss: 1.8129600235634384 acc: 0.82\n",
      "loss: 1.8627227534682376 acc: 0.83\n",
      "loss: 1.8254476433439402 acc: 0.83\n",
      "loss: 1.8152564343985989 acc: 0.87\n",
      "loss: 1.824397671857627 acc: 0.84\n",
      "loss: 1.794065064920808 acc: 0.85\n",
      "loss: 1.7658595208302876 acc: 0.87\n",
      "loss: 1.785056308228417 acc: 0.9\n",
      "loss: 1.8128412376137166 acc: 0.84\n",
      "loss: 1.8061167776215654 acc: 0.83\n",
      "loss: 1.8285672046528776 acc: 0.81\n",
      "loss: 1.8649285454389857 acc: 0.8\n",
      "loss: 1.789296362185175 acc: 0.87\n",
      "loss: 1.7796021239132573 acc: 0.91\n",
      "loss: 1.8113504136492482 acc: 0.86\n",
      "loss: 1.8190831866156056 acc: 0.87\n",
      "loss: 1.8471449387716756 acc: 0.75\n",
      "loss: 1.8166589619502305 acc: 0.87\n",
      "loss: 1.8004197820582286 acc: 0.86\n",
      "loss: 1.8571549720627283 acc: 0.84\n",
      "loss: 1.841684185115746 acc: 0.85\n",
      "loss: 1.819471178374228 acc: 0.87\n",
      "loss: 1.849960736493731 acc: 0.82\n",
      "loss: 1.8716295328767996 acc: 0.8\n",
      "loss: 1.7695783578534618 acc: 0.85\n",
      "loss: 1.8185990462792265 acc: 0.87\n",
      "loss: 1.807497455412416 acc: 0.88\n",
      "loss: 1.8072294617904314 acc: 0.89\n",
      "loss: 1.8290175299259386 acc: 0.87\n",
      "loss: 1.7898858192384604 acc: 0.87\n",
      "loss: 1.8571173082840966 acc: 0.77\n",
      "loss: 1.844250437409289 acc: 0.8\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 1.8443\t Accuracy 0.8000\n",
      "loss: 1.8335176544342935 acc: 0.81\n",
      "loss: 1.800086754731814 acc: 0.87\n",
      "loss: 1.8336668053351366 acc: 0.83\n",
      "loss: 1.8136997068403082 acc: 0.84\n",
      "loss: 1.7778389681248428 acc: 0.83\n",
      "loss: 1.869077314616776 acc: 0.79\n",
      "loss: 1.8430407328960214 acc: 0.93\n",
      "loss: 1.8154373456074016 acc: 0.84\n",
      "loss: 1.8016928357740565 acc: 0.88\n",
      "loss: 1.8266224006521774 acc: 0.83\n",
      "loss: 1.8048250208184524 acc: 0.81\n",
      "loss: 1.8423066280575437 acc: 0.79\n",
      "loss: 1.801151161082822 acc: 0.89\n",
      "loss: 1.8120209612728626 acc: 0.78\n",
      "loss: 1.8252569318924734 acc: 0.86\n",
      "loss: 1.8142122720146485 acc: 0.83\n",
      "loss: 1.8877545394956838 acc: 0.8\n",
      "loss: 1.8711828338385101 acc: 0.84\n",
      "loss: 1.8414463402009595 acc: 0.86\n",
      "loss: 1.8366062678526693 acc: 0.78\n",
      "loss: 1.8789178607361294 acc: 0.82\n",
      "loss: 1.833596470142858 acc: 0.81\n",
      "loss: 1.8360474521671943 acc: 0.76\n",
      "loss: 1.8696900716565588 acc: 0.77\n",
      "loss: 1.8219064266083476 acc: 0.9\n",
      "loss: 1.793922533844745 acc: 0.86\n",
      "loss: 1.8565589739726933 acc: 0.81\n",
      "loss: 1.8337403651942674 acc: 0.81\n",
      "loss: 1.8141100882198575 acc: 0.88\n",
      "loss: 1.8163317162580837 acc: 0.86\n",
      "loss: 1.8148713917018109 acc: 0.85\n",
      "loss: 1.8409218610929106 acc: 0.88\n",
      "loss: 1.8332695036780746 acc: 0.84\n",
      "loss: 1.7799264990755097 acc: 0.9\n",
      "loss: 1.8150445067238947 acc: 0.83\n",
      "loss: 1.8865300441704376 acc: 0.76\n",
      "loss: 1.7930856569082687 acc: 0.86\n",
      "loss: 1.8320045664876727 acc: 0.83\n",
      "loss: 1.8117320232044245 acc: 0.86\n",
      "loss: 1.823119153062167 acc: 0.79\n",
      "loss: 1.7856749919809456 acc: 0.9\n",
      "loss: 1.8182950546774566 acc: 0.88\n",
      "loss: 1.8578875209253172 acc: 0.8\n",
      "loss: 1.8057532506753984 acc: 0.86\n",
      "loss: 1.8771445640371807 acc: 0.78\n",
      "loss: 1.804359689826348 acc: 0.86\n",
      "loss: 1.8606637815397213 acc: 0.81\n",
      "loss: 1.8345590370347253 acc: 0.79\n",
      "loss: 1.8355326432130945 acc: 0.82\n",
      "loss: 1.7991094517076756 acc: 0.84\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 1.7991\t Accuracy 0.8400\n",
      "loss: 1.849584639862543 acc: 0.83\n",
      "loss: 1.8077870689964914 acc: 0.9\n",
      "loss: 1.832511603531576 acc: 0.85\n",
      "loss: 1.7755511625754479 acc: 0.88\n",
      "loss: 1.788806211785061 acc: 0.87\n",
      "loss: 1.8253626169991228 acc: 0.81\n",
      "loss: 1.7890318153103595 acc: 0.89\n",
      "loss: 1.8297388102016054 acc: 0.77\n",
      "loss: 1.8154535078709204 acc: 0.91\n",
      "loss: 1.8239246802641103 acc: 0.87\n",
      "loss: 1.8446526859203147 acc: 0.87\n",
      "loss: 1.855704071395938 acc: 0.79\n",
      "loss: 1.8097168389134857 acc: 0.91\n",
      "loss: 1.8054471044975589 acc: 0.85\n",
      "loss: 1.81992049277862 acc: 0.8\n",
      "loss: 1.8156147127161515 acc: 0.85\n",
      "loss: 1.807430192091295 acc: 0.83\n",
      "loss: 1.845687987053252 acc: 0.79\n",
      "loss: 1.8415297643789414 acc: 0.79\n",
      "loss: 1.8494910657172385 acc: 0.83\n",
      "loss: 1.8548108470063283 acc: 0.83\n",
      "loss: 1.820252496668846 acc: 0.8\n",
      "loss: 1.8226788424112434 acc: 0.83\n",
      "loss: 1.8104327574721717 acc: 0.88\n",
      "loss: 1.8417183369944357 acc: 0.85\n",
      "loss: 1.784277358392836 acc: 0.89\n",
      "loss: 1.8442308435748487 acc: 0.83\n",
      "loss: 1.8457800691151354 acc: 0.84\n",
      "loss: 1.841522317447389 acc: 0.84\n",
      "loss: 1.8303064935198945 acc: 0.81\n",
      "loss: 1.8025998129568734 acc: 0.9\n",
      "loss: 1.8272039827293012 acc: 0.83\n",
      "loss: 1.7979467897872423 acc: 0.86\n",
      "loss: 1.8249681196445757 acc: 0.9\n",
      "loss: 1.824364914681926 acc: 0.84\n",
      "loss: 1.8021208788947496 acc: 0.85\n",
      "loss: 1.7995345624405183 acc: 0.9\n",
      "loss: 1.8148090035012114 acc: 0.84\n",
      "loss: 1.8366764577041013 acc: 0.86\n",
      "loss: 1.8163368358034697 acc: 0.82\n",
      "loss: 1.796185429059683 acc: 0.9\n",
      "loss: 1.8547650247101435 acc: 0.81\n",
      "loss: 1.7970205212602954 acc: 0.83\n",
      "loss: 1.8119618344834985 acc: 0.85\n",
      "loss: 1.8141708567901693 acc: 0.87\n",
      "loss: 1.829756754816606 acc: 0.86\n",
      "loss: 1.8706354873146227 acc: 0.79\n",
      "loss: 1.8422346443344961 acc: 0.83\n",
      "loss: 1.8221826934556424 acc: 0.89\n",
      "loss: 1.7904659567153671 acc: 0.87\n",
      "loss: 1.787935559558192 acc: 0.91\n",
      "loss: 1.8035448063021644 acc: 0.89\n",
      "loss: 1.8204559429164389 acc: 0.88\n",
      "loss: 1.8203104683801907 acc: 0.94\n",
      "loss: 1.8001651174283888 acc: 0.88\n",
      "loss: 1.7487782700252907 acc: 0.85\n",
      "loss: 1.7529853321278384 acc: 0.85\n",
      "loss: 1.8243826892800952 acc: 0.82\n",
      "loss: 1.7450779172749507 acc: 0.93\n",
      "loss: 1.79626931349809 acc: 0.9\n",
      "loss: 1.7909070216946685 acc: 0.84\n",
      "loss: 1.8429810453112896 acc: 0.83\n",
      "loss: 1.8530480897827106 acc: 0.83\n",
      "loss: 1.867767740821356 acc: 0.85\n",
      "loss: 1.7774413841705095 acc: 0.93\n",
      "loss: 1.7901238720472492 acc: 0.84\n",
      "loss: 1.7578163515221044 acc: 0.9\n",
      "loss: 1.7808086813531236 acc: 0.88\n",
      "loss: 1.7832580780620932 acc: 0.87\n",
      "loss: 1.8635743090511576 acc: 0.87\n",
      "loss: 1.8142029949512066 acc: 0.89\n",
      "loss: 1.816608779627993 acc: 0.75\n",
      "loss: 1.8457718908456826 acc: 0.85\n",
      "loss: 1.8279206450127803 acc: 0.89\n",
      "loss: 1.8450709363851194 acc: 0.77\n",
      "loss: 1.8477471252217448 acc: 0.8\n",
      "loss: 1.875706554467509 acc: 0.79\n",
      "loss: 1.8137393197792346 acc: 0.9\n",
      "loss: 1.7927511102047353 acc: 0.89\n",
      "loss: 1.8120068790858412 acc: 0.85\n",
      "loss: 1.7775927768538424 acc: 0.95\n",
      "loss: 1.7386768691818912 acc: 0.93\n",
      "loss: 1.783756484871074 acc: 0.89\n",
      "loss: 1.7674216103507252 acc: 0.86\n",
      "loss: 1.8023043306107391 acc: 0.89\n",
      "loss: 1.855001365053616 acc: 0.9\n",
      "loss: 1.8337681224079663 acc: 0.87\n",
      "loss: 1.7273879335138753 acc: 0.93\n",
      "loss: 1.709492876180002 acc: 0.97\n",
      "loss: 1.7364657390777398 acc: 0.93\n",
      "loss: 1.7416977799701863 acc: 0.97\n",
      "loss: 1.7896692207592464 acc: 0.87\n",
      "loss: 1.773233521051732 acc: 0.85\n",
      "loss: 1.7183532313298315 acc: 0.84\n",
      "loss: 1.8016486458170289 acc: 0.89\n",
      "loss: 1.8100351719909538 acc: 0.92\n",
      "loss: 1.8857040264732492 acc: 0.8\n",
      "loss: 1.70363747860072 acc: 0.95\n",
      "loss: 1.858318741364835 acc: 0.87\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8226\t Average training accuracy 0.8433\n",
      "Epoch [4]\t Average validation loss 1.7981\t Average validation accuracy 0.8764\n",
      "\n",
      "loss: 1.8315978848110714 acc: 0.79\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 1.8316\t Accuracy 0.7900\n",
      "loss: 1.8133039753696352 acc: 0.82\n",
      "loss: 1.817024748657901 acc: 0.87\n",
      "loss: 1.8194974787847593 acc: 0.88\n",
      "loss: 1.8423486515607235 acc: 0.8\n",
      "loss: 1.8526064395159814 acc: 0.8\n",
      "loss: 1.8611033215187291 acc: 0.78\n",
      "loss: 1.8139297381466186 acc: 0.86\n",
      "loss: 1.8001772523258208 acc: 0.88\n",
      "loss: 1.7892485030844647 acc: 0.88\n",
      "loss: 1.825660659226905 acc: 0.88\n",
      "loss: 1.7963110329426 acc: 0.83\n",
      "loss: 1.8074057010441786 acc: 0.85\n",
      "loss: 1.8458428191511902 acc: 0.87\n",
      "loss: 1.8004156825019475 acc: 0.9\n",
      "loss: 1.8284103398579041 acc: 0.82\n",
      "loss: 1.8152609001546578 acc: 0.85\n",
      "loss: 1.8428872685197542 acc: 0.87\n",
      "loss: 1.8396563599038647 acc: 0.82\n",
      "loss: 1.836673817695673 acc: 0.82\n",
      "loss: 1.903691553151221 acc: 0.74\n",
      "loss: 1.8332639149676497 acc: 0.77\n",
      "loss: 1.807376709725149 acc: 0.89\n",
      "loss: 1.843839213611453 acc: 0.75\n",
      "loss: 1.8624405317179782 acc: 0.8\n",
      "loss: 1.815843811950723 acc: 0.82\n",
      "loss: 1.8439816071282016 acc: 0.86\n",
      "loss: 1.8194847737673046 acc: 0.86\n",
      "loss: 1.820978641462664 acc: 0.81\n",
      "loss: 1.8471157920393189 acc: 0.82\n",
      "loss: 1.8326765537867862 acc: 0.83\n",
      "loss: 1.813576393249295 acc: 0.84\n",
      "loss: 1.8049358219485843 acc: 0.9\n",
      "loss: 1.8292590458507039 acc: 0.83\n",
      "loss: 1.8350170674052593 acc: 0.85\n",
      "loss: 1.8189694965003407 acc: 0.85\n",
      "loss: 1.8323836002883318 acc: 0.8\n",
      "loss: 1.8526204993497881 acc: 0.83\n",
      "loss: 1.8491196925177042 acc: 0.82\n",
      "loss: 1.8344525236281815 acc: 0.84\n",
      "loss: 1.806755767735197 acc: 0.87\n",
      "loss: 1.7951485550259232 acc: 0.89\n",
      "loss: 1.823098258499119 acc: 0.88\n",
      "loss: 1.838722887761451 acc: 0.87\n",
      "loss: 1.808461836860709 acc: 0.89\n",
      "loss: 1.7899166652263545 acc: 0.87\n",
      "loss: 1.839212013131744 acc: 0.81\n",
      "loss: 1.7937208576317902 acc: 0.9\n",
      "loss: 1.8281381603986606 acc: 0.83\n",
      "loss: 1.834187817967285 acc: 0.76\n",
      "loss: 1.819707663286604 acc: 0.89\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 1.8197\t Accuracy 0.8900\n",
      "loss: 1.812081738839513 acc: 0.84\n",
      "loss: 1.8060580567739473 acc: 0.8\n",
      "loss: 1.8593006850844482 acc: 0.83\n",
      "loss: 1.8035810136075656 acc: 0.84\n",
      "loss: 1.830592087356438 acc: 0.85\n",
      "loss: 1.8152947836026578 acc: 0.84\n",
      "loss: 1.8337849207395123 acc: 0.79\n",
      "loss: 1.8087198998868874 acc: 0.83\n",
      "loss: 1.8211673417857301 acc: 0.82\n",
      "loss: 1.803722680462624 acc: 0.86\n",
      "loss: 1.8268330002996191 acc: 0.82\n",
      "loss: 1.7793642972490193 acc: 0.86\n",
      "loss: 1.8065155936421362 acc: 0.83\n",
      "loss: 1.851063121361734 acc: 0.84\n",
      "loss: 1.8264216358623255 acc: 0.83\n",
      "loss: 1.8133924492643965 acc: 0.84\n",
      "loss: 1.8104956322347405 acc: 0.86\n",
      "loss: 1.8547382080784356 acc: 0.83\n",
      "loss: 1.8287337079507677 acc: 0.83\n",
      "loss: 1.8558578726865966 acc: 0.82\n",
      "loss: 1.7954021479737239 acc: 0.88\n",
      "loss: 1.835511743939036 acc: 0.83\n",
      "loss: 1.8592060195831326 acc: 0.8\n",
      "loss: 1.8446384747383633 acc: 0.84\n",
      "loss: 1.822266499471883 acc: 0.86\n",
      "loss: 1.8497311959444613 acc: 0.83\n",
      "loss: 1.82672018846017 acc: 0.87\n",
      "loss: 1.855416986703822 acc: 0.85\n",
      "loss: 1.8373923514168178 acc: 0.81\n",
      "loss: 1.8117445448418428 acc: 0.85\n",
      "loss: 1.8213170669654206 acc: 0.81\n",
      "loss: 1.8383830841203783 acc: 0.8\n",
      "loss: 1.83082627582461 acc: 0.87\n",
      "loss: 1.841240611137142 acc: 0.8\n",
      "loss: 1.8417397219888787 acc: 0.83\n",
      "loss: 1.8102158789346294 acc: 0.85\n",
      "loss: 1.8223627918139405 acc: 0.85\n",
      "loss: 1.8040669325187202 acc: 0.9\n",
      "loss: 1.8298771029189282 acc: 0.87\n",
      "loss: 1.8114639306864098 acc: 0.9\n",
      "loss: 1.7931146867254038 acc: 0.87\n",
      "loss: 1.82084446430287 acc: 0.88\n",
      "loss: 1.8352170791690599 acc: 0.81\n",
      "loss: 1.8166454381963997 acc: 0.84\n",
      "loss: 1.8159568161038373 acc: 0.78\n",
      "loss: 1.8306155574010565 acc: 0.83\n",
      "loss: 1.7813932220464799 acc: 0.89\n",
      "loss: 1.8405791180137694 acc: 0.76\n",
      "loss: 1.8509785312864258 acc: 0.86\n",
      "loss: 1.784780961002551 acc: 0.86\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 1.7848\t Accuracy 0.8600\n",
      "loss: 1.8135015325387294 acc: 0.88\n",
      "loss: 1.8120883020432417 acc: 0.84\n",
      "loss: 1.8214694556938016 acc: 0.86\n",
      "loss: 1.7917168083373682 acc: 0.84\n",
      "loss: 1.8407208805239095 acc: 0.8\n",
      "loss: 1.802976418967703 acc: 0.81\n",
      "loss: 1.8060015037607948 acc: 0.83\n",
      "loss: 1.8430747959388498 acc: 0.84\n",
      "loss: 1.836858887693774 acc: 0.83\n",
      "loss: 1.8010077236778534 acc: 0.82\n",
      "loss: 1.8134750059483966 acc: 0.85\n",
      "loss: 1.8777834943711422 acc: 0.79\n",
      "loss: 1.7879279080769495 acc: 0.89\n",
      "loss: 1.7824780663181858 acc: 0.89\n",
      "loss: 1.8223109159337374 acc: 0.82\n",
      "loss: 1.7951806440968907 acc: 0.84\n",
      "loss: 1.8450636507068392 acc: 0.86\n",
      "loss: 1.8487803214563652 acc: 0.78\n",
      "loss: 1.8497346486120185 acc: 0.83\n",
      "loss: 1.8253315902430451 acc: 0.85\n",
      "loss: 1.8091875486910036 acc: 0.85\n",
      "loss: 1.7909500858366174 acc: 0.88\n",
      "loss: 1.8534679874194029 acc: 0.85\n",
      "loss: 1.7981662693217912 acc: 0.88\n",
      "loss: 1.80360610076832 acc: 0.9\n",
      "loss: 1.8085964230827536 acc: 0.87\n",
      "loss: 1.8114944214285973 acc: 0.86\n",
      "loss: 1.8163189192536382 acc: 0.84\n",
      "loss: 1.8121392619486487 acc: 0.82\n",
      "loss: 1.8157286991857229 acc: 0.85\n",
      "loss: 1.8063191365831042 acc: 0.89\n",
      "loss: 1.8688435002099026 acc: 0.8\n",
      "loss: 1.8279802303018524 acc: 0.85\n",
      "loss: 1.8468165961906513 acc: 0.87\n",
      "loss: 1.8381584303912748 acc: 0.84\n",
      "loss: 1.8458834676857876 acc: 0.84\n",
      "loss: 1.8187046789518049 acc: 0.86\n",
      "loss: 1.7895413970852059 acc: 0.85\n",
      "loss: 1.859365524536752 acc: 0.76\n",
      "loss: 1.832644466058088 acc: 0.78\n",
      "loss: 1.84709215801984 acc: 0.8\n",
      "loss: 1.8389954136342193 acc: 0.8\n",
      "loss: 1.8449967968600325 acc: 0.8\n",
      "loss: 1.822911285047316 acc: 0.87\n",
      "loss: 1.8332615122921814 acc: 0.82\n",
      "loss: 1.7924449869568795 acc: 0.86\n",
      "loss: 1.8045115962520653 acc: 0.86\n",
      "loss: 1.83396782765965 acc: 0.85\n",
      "loss: 1.8193778959159355 acc: 0.84\n",
      "loss: 1.8152753167528777 acc: 0.89\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 1.8153\t Accuracy 0.8900\n",
      "loss: 1.8513271375717983 acc: 0.81\n",
      "loss: 1.8438199235033377 acc: 0.82\n",
      "loss: 1.8215201611112972 acc: 0.82\n",
      "loss: 1.835014806613281 acc: 0.81\n",
      "loss: 1.7876620831877386 acc: 0.83\n",
      "loss: 1.784430444379069 acc: 0.87\n",
      "loss: 1.8253959484392341 acc: 0.91\n",
      "loss: 1.78503665665939 acc: 0.87\n",
      "loss: 1.8232730830663986 acc: 0.9\n",
      "loss: 1.7869068202962515 acc: 0.86\n",
      "loss: 1.832814894090197 acc: 0.84\n",
      "loss: 1.8297124742256745 acc: 0.8\n",
      "loss: 1.865104020875979 acc: 0.85\n",
      "loss: 1.841560856293296 acc: 0.88\n",
      "loss: 1.8255029232942268 acc: 0.85\n",
      "loss: 1.8446612395009878 acc: 0.84\n",
      "loss: 1.8239733232075266 acc: 0.83\n",
      "loss: 1.847853445821315 acc: 0.85\n",
      "loss: 1.7908891690627682 acc: 0.82\n",
      "loss: 1.7762406406674425 acc: 0.87\n",
      "loss: 1.8308358010860166 acc: 0.88\n",
      "loss: 1.7801927632894623 acc: 0.9\n",
      "loss: 1.8232160327123241 acc: 0.85\n",
      "loss: 1.8308938167408355 acc: 0.82\n",
      "loss: 1.8357730373102257 acc: 0.85\n",
      "loss: 1.8353964766781197 acc: 0.83\n",
      "loss: 1.843521117258135 acc: 0.84\n",
      "loss: 1.861294284658954 acc: 0.82\n",
      "loss: 1.8100985241203262 acc: 0.86\n",
      "loss: 1.798453752551839 acc: 0.86\n",
      "loss: 1.8522028536786015 acc: 0.81\n",
      "loss: 1.8205451724326867 acc: 0.82\n",
      "loss: 1.8474091374491932 acc: 0.83\n",
      "loss: 1.8089013869169883 acc: 0.85\n",
      "loss: 1.8348806197759904 acc: 0.84\n",
      "loss: 1.814252288896804 acc: 0.92\n",
      "loss: 1.8283529680949033 acc: 0.81\n",
      "loss: 1.832944053505841 acc: 0.8\n",
      "loss: 1.814843568550311 acc: 0.84\n",
      "loss: 1.8626860535857597 acc: 0.8\n",
      "loss: 1.8656240496337892 acc: 0.8\n",
      "loss: 1.813035071590579 acc: 0.87\n",
      "loss: 1.8192180139583491 acc: 0.8\n",
      "loss: 1.8150726629803589 acc: 0.87\n",
      "loss: 1.8273177553576525 acc: 0.84\n",
      "loss: 1.8246348464078925 acc: 0.83\n",
      "loss: 1.8360040202241443 acc: 0.83\n",
      "loss: 1.8425166879869976 acc: 0.81\n",
      "loss: 1.7933225093522247 acc: 0.84\n",
      "loss: 1.829005952696903 acc: 0.82\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 1.8290\t Accuracy 0.8200\n",
      "loss: 1.8222416761912101 acc: 0.82\n",
      "loss: 1.8244706478958053 acc: 0.89\n",
      "loss: 1.7868779080005093 acc: 0.9\n",
      "loss: 1.8420972554967525 acc: 0.82\n",
      "loss: 1.8212173812889583 acc: 0.78\n",
      "loss: 1.8125458154193168 acc: 0.85\n",
      "loss: 1.8496547451354888 acc: 0.81\n",
      "loss: 1.7733232418608802 acc: 0.88\n",
      "loss: 1.8384014086361924 acc: 0.84\n",
      "loss: 1.8131251360635512 acc: 0.88\n",
      "loss: 1.8797848554511032 acc: 0.74\n",
      "loss: 1.7994460655378481 acc: 0.85\n",
      "loss: 1.8359192357459562 acc: 0.83\n",
      "loss: 1.8534246809386323 acc: 0.79\n",
      "loss: 1.7880084740964362 acc: 0.86\n",
      "loss: 1.7768687821519469 acc: 0.93\n",
      "loss: 1.8077316957249543 acc: 0.84\n",
      "loss: 1.8771017280269304 acc: 0.8\n",
      "loss: 1.8472259797957176 acc: 0.78\n",
      "loss: 1.8547243038604906 acc: 0.83\n",
      "loss: 1.8246821701450076 acc: 0.87\n",
      "loss: 1.8034570213812595 acc: 0.87\n",
      "loss: 1.7877714753048708 acc: 0.86\n",
      "loss: 1.8100375769246966 acc: 0.84\n",
      "loss: 1.8490616498691383 acc: 0.81\n",
      "loss: 1.7942284786093985 acc: 0.83\n",
      "loss: 1.8321203199064806 acc: 0.84\n",
      "loss: 1.7879992078990172 acc: 0.95\n",
      "loss: 1.8304122642372707 acc: 0.81\n",
      "loss: 1.8549326621879043 acc: 0.81\n",
      "loss: 1.8047057635351993 acc: 0.86\n",
      "loss: 1.8230390856215237 acc: 0.82\n",
      "loss: 1.8268716167745893 acc: 0.88\n",
      "loss: 1.8021321151027154 acc: 0.88\n",
      "loss: 1.834450374772405 acc: 0.86\n",
      "loss: 1.7686648808885013 acc: 0.89\n",
      "loss: 1.8279168361878948 acc: 0.79\n",
      "loss: 1.8299405406595755 acc: 0.85\n",
      "loss: 1.797667791141747 acc: 0.9\n",
      "loss: 1.8174428319199256 acc: 0.87\n",
      "loss: 1.8317351892062184 acc: 0.83\n",
      "loss: 1.8098908122444357 acc: 0.85\n",
      "loss: 1.8370648037799564 acc: 0.82\n",
      "loss: 1.8352567452465565 acc: 0.81\n",
      "loss: 1.8784006297614213 acc: 0.71\n",
      "loss: 1.7976539432129883 acc: 0.9\n",
      "loss: 1.8287706618918391 acc: 0.84\n",
      "loss: 1.8102058878572864 acc: 0.89\n",
      "loss: 1.8058182731563042 acc: 0.86\n",
      "loss: 1.797074574609926 acc: 0.92\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 1.7971\t Accuracy 0.9200\n",
      "loss: 1.7830462696876137 acc: 0.86\n",
      "loss: 1.7709590815318452 acc: 0.86\n",
      "loss: 1.8392098459969346 acc: 0.81\n",
      "loss: 1.774764068768164 acc: 0.85\n",
      "loss: 1.8421532375239602 acc: 0.79\n",
      "loss: 1.8255477853711013 acc: 0.81\n",
      "loss: 1.8255344402057878 acc: 0.85\n",
      "loss: 1.7965275479488991 acc: 0.88\n",
      "loss: 1.8362129740299096 acc: 0.9\n",
      "loss: 1.8049983793535174 acc: 0.88\n",
      "loss: 1.8180565512517455 acc: 0.84\n",
      "loss: 1.857712852449211 acc: 0.77\n",
      "loss: 1.7653986237824855 acc: 0.88\n",
      "loss: 1.8247889249925462 acc: 0.8\n",
      "loss: 1.8308710790654996 acc: 0.86\n",
      "loss: 1.8192828523516744 acc: 0.84\n",
      "loss: 1.8186146386515147 acc: 0.86\n",
      "loss: 1.790777131596505 acc: 0.88\n",
      "loss: 1.8106956286244573 acc: 0.84\n",
      "loss: 1.775980447640401 acc: 0.93\n",
      "loss: 1.7977993419212694 acc: 0.88\n",
      "loss: 1.790127434255301 acc: 0.89\n",
      "loss: 1.8191640235335718 acc: 0.87\n",
      "loss: 1.8395631224453723 acc: 0.81\n",
      "loss: 1.7910957104929408 acc: 0.86\n",
      "loss: 1.773775694405372 acc: 0.9\n",
      "loss: 1.8098827240877382 acc: 0.87\n",
      "loss: 1.776645073425498 acc: 0.85\n",
      "loss: 1.8682150729885376 acc: 0.78\n",
      "loss: 1.878142020221874 acc: 0.79\n",
      "loss: 1.8044172992559169 acc: 0.88\n",
      "loss: 1.8325234461887987 acc: 0.82\n",
      "loss: 1.8178580444096126 acc: 0.83\n",
      "loss: 1.8064250717841397 acc: 0.85\n",
      "loss: 1.8466207805847399 acc: 0.79\n",
      "loss: 1.8316239260671063 acc: 0.81\n",
      "loss: 1.8533153745714055 acc: 0.84\n",
      "loss: 1.7945084432688143 acc: 0.85\n",
      "loss: 1.8658594520184206 acc: 0.79\n",
      "loss: 1.8367828393649928 acc: 0.82\n",
      "loss: 1.8198598038764353 acc: 0.83\n",
      "loss: 1.8851681303841672 acc: 0.79\n",
      "loss: 1.8410935060932097 acc: 0.79\n",
      "loss: 1.8317326086513994 acc: 0.84\n",
      "loss: 1.8001981817742385 acc: 0.85\n",
      "loss: 1.856534519689917 acc: 0.85\n",
      "loss: 1.8401039750697685 acc: 0.87\n",
      "loss: 1.817522269034101 acc: 0.84\n",
      "loss: 1.8202428987972603 acc: 0.84\n",
      "loss: 1.8328441504544872 acc: 0.87\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 1.8328\t Accuracy 0.8700\n",
      "loss: 1.8586684510020939 acc: 0.83\n",
      "loss: 1.7923485839595008 acc: 0.87\n",
      "loss: 1.7998097495007355 acc: 0.85\n",
      "loss: 1.8019693732167006 acc: 0.87\n",
      "loss: 1.8387223575042007 acc: 0.84\n",
      "loss: 1.8314019283518388 acc: 0.82\n",
      "loss: 1.8143308236332485 acc: 0.85\n",
      "loss: 1.8481343191345148 acc: 0.83\n",
      "loss: 1.8101274568698034 acc: 0.87\n",
      "loss: 1.796079368295952 acc: 0.84\n",
      "loss: 1.8289757976199608 acc: 0.86\n",
      "loss: 1.853143605108242 acc: 0.83\n",
      "loss: 1.8148269149963565 acc: 0.83\n",
      "loss: 1.7946372808673565 acc: 0.89\n",
      "loss: 1.7821883482265555 acc: 0.91\n",
      "loss: 1.8281685845636182 acc: 0.8\n",
      "loss: 1.8493517868754288 acc: 0.8\n",
      "loss: 1.7955849000443387 acc: 0.89\n",
      "loss: 1.8039418624105903 acc: 0.87\n",
      "loss: 1.7598298664509477 acc: 0.83\n",
      "loss: 1.8107652809306187 acc: 0.85\n",
      "loss: 1.8558581631632811 acc: 0.81\n",
      "loss: 1.8233442722183244 acc: 0.86\n",
      "loss: 1.8020183940345251 acc: 0.87\n",
      "loss: 1.8318871410169486 acc: 0.8\n",
      "loss: 1.806838162091507 acc: 0.85\n",
      "loss: 1.8001181170548404 acc: 0.88\n",
      "loss: 1.8330912837025284 acc: 0.79\n",
      "loss: 1.7975370133405932 acc: 0.89\n",
      "loss: 1.8013037455040963 acc: 0.86\n",
      "loss: 1.8366487823699138 acc: 0.81\n",
      "loss: 1.8471015102712705 acc: 0.85\n",
      "loss: 1.8382600134089822 acc: 0.82\n",
      "loss: 1.8180638239501605 acc: 0.89\n",
      "loss: 1.8189034115353726 acc: 0.82\n",
      "loss: 1.8410798945081346 acc: 0.87\n",
      "loss: 1.8653719943288953 acc: 0.81\n",
      "loss: 1.7969347057433285 acc: 0.86\n",
      "loss: 1.8181743959619845 acc: 0.84\n",
      "loss: 1.864142535583887 acc: 0.81\n",
      "loss: 1.839585017654774 acc: 0.83\n",
      "loss: 1.871454059657355 acc: 0.77\n",
      "loss: 1.8111564409854934 acc: 0.88\n",
      "loss: 1.8281257349627478 acc: 0.85\n",
      "loss: 1.8614698205568132 acc: 0.79\n",
      "loss: 1.8403882052916343 acc: 0.83\n",
      "loss: 1.7922664497129766 acc: 0.88\n",
      "loss: 1.8209917034515382 acc: 0.87\n",
      "loss: 1.83949590705816 acc: 0.86\n",
      "loss: 1.8188877653220823 acc: 0.88\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 1.8189\t Accuracy 0.8800\n",
      "loss: 1.8173893489578117 acc: 0.84\n",
      "loss: 1.8544709403821036 acc: 0.84\n",
      "loss: 1.8075663425846105 acc: 0.85\n",
      "loss: 1.8145417498117535 acc: 0.84\n",
      "loss: 1.8207584220953108 acc: 0.85\n",
      "loss: 1.8246634427576152 acc: 0.88\n",
      "loss: 1.8060965374076365 acc: 0.82\n",
      "loss: 1.8455610082086438 acc: 0.82\n",
      "loss: 1.8018944348272856 acc: 0.85\n",
      "loss: 1.8599109660655078 acc: 0.82\n",
      "loss: 1.8395574413007265 acc: 0.85\n",
      "loss: 1.8229236510321116 acc: 0.8\n",
      "loss: 1.7855507679350007 acc: 0.85\n",
      "loss: 1.8045275234972908 acc: 0.85\n",
      "loss: 1.8141024001346213 acc: 0.86\n",
      "loss: 1.816011461681225 acc: 0.83\n",
      "loss: 1.8104782086474085 acc: 0.87\n",
      "loss: 1.8121316313148421 acc: 0.8\n",
      "loss: 1.805527512694199 acc: 0.87\n",
      "loss: 1.803674692487125 acc: 0.84\n",
      "loss: 1.8178669289533083 acc: 0.86\n",
      "loss: 1.814779590836199 acc: 0.85\n",
      "loss: 1.8452321227958186 acc: 0.82\n",
      "loss: 1.80510158702223 acc: 0.88\n",
      "loss: 1.828297905207701 acc: 0.82\n",
      "loss: 1.8089490551596668 acc: 0.83\n",
      "loss: 1.8354550400610692 acc: 0.81\n",
      "loss: 1.8746871280074981 acc: 0.79\n",
      "loss: 1.8468690138750645 acc: 0.79\n",
      "loss: 1.824153081972131 acc: 0.82\n",
      "loss: 1.784441385671397 acc: 0.79\n",
      "loss: 1.7862526713596785 acc: 0.86\n",
      "loss: 1.809660603799938 acc: 0.87\n",
      "loss: 1.8181682191974522 acc: 0.8\n",
      "loss: 1.8265178700475693 acc: 0.8\n",
      "loss: 1.7796044919006626 acc: 0.84\n",
      "loss: 1.8491302177663562 acc: 0.79\n",
      "loss: 1.823003997091882 acc: 0.86\n",
      "loss: 1.8250074031625 acc: 0.87\n",
      "loss: 1.8429793424677985 acc: 0.77\n",
      "loss: 1.7935978375312296 acc: 0.91\n",
      "loss: 1.801023774310247 acc: 0.86\n",
      "loss: 1.8217355668196538 acc: 0.85\n",
      "loss: 1.8079344993515272 acc: 0.87\n",
      "loss: 1.8342365415288922 acc: 0.87\n",
      "loss: 1.7743996072435835 acc: 0.88\n",
      "loss: 1.810794531840306 acc: 0.84\n",
      "loss: 1.7975495661294054 acc: 0.86\n",
      "loss: 1.753028919394773 acc: 0.88\n",
      "loss: 1.8496705317470694 acc: 0.78\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 1.8497\t Accuracy 0.7800\n",
      "loss: 1.7918963779792443 acc: 0.88\n",
      "loss: 1.8309766568974337 acc: 0.83\n",
      "loss: 1.8194319933245597 acc: 0.86\n",
      "loss: 1.8236670627603115 acc: 0.83\n",
      "loss: 1.7973703586186678 acc: 0.87\n",
      "loss: 1.8245047603896858 acc: 0.82\n",
      "loss: 1.8269827490842767 acc: 0.87\n",
      "loss: 1.8042795004023608 acc: 0.89\n",
      "loss: 1.820514812313036 acc: 0.84\n",
      "loss: 1.8393137194414337 acc: 0.83\n",
      "loss: 1.806098249627838 acc: 0.87\n",
      "loss: 1.801853705986213 acc: 0.89\n",
      "loss: 1.786629721292815 acc: 0.88\n",
      "loss: 1.8224381900044062 acc: 0.85\n",
      "loss: 1.7817534737482241 acc: 0.94\n",
      "loss: 1.8334552293182045 acc: 0.81\n",
      "loss: 1.833314565594032 acc: 0.83\n",
      "loss: 1.8421299918455158 acc: 0.83\n",
      "loss: 1.8084330360224723 acc: 0.84\n",
      "loss: 1.8515658337578154 acc: 0.78\n",
      "loss: 1.8320919518094547 acc: 0.8\n",
      "loss: 1.829335006401163 acc: 0.83\n",
      "loss: 1.8307823740009226 acc: 0.82\n",
      "loss: 1.8454568242837035 acc: 0.83\n",
      "loss: 1.8256153401136508 acc: 0.87\n",
      "loss: 1.8106281231364059 acc: 0.85\n",
      "loss: 1.8625373222279966 acc: 0.82\n",
      "loss: 1.8089916500341483 acc: 0.81\n",
      "loss: 1.7800567347653145 acc: 0.89\n",
      "loss: 1.8259083344544254 acc: 0.86\n",
      "loss: 1.802489188877201 acc: 0.92\n",
      "loss: 1.8414160980936571 acc: 0.85\n",
      "loss: 1.8279992918933339 acc: 0.85\n",
      "loss: 1.8322274717619147 acc: 0.82\n",
      "loss: 1.8279490859975231 acc: 0.82\n",
      "loss: 1.8543338881512768 acc: 0.8\n",
      "loss: 1.788789915757304 acc: 0.89\n",
      "loss: 1.8450929888319108 acc: 0.84\n",
      "loss: 1.8014009996812488 acc: 0.9\n",
      "loss: 1.8385561710320055 acc: 0.84\n",
      "loss: 1.811581804560838 acc: 0.85\n",
      "loss: 1.828559849328839 acc: 0.79\n",
      "loss: 1.8247015106169289 acc: 0.82\n",
      "loss: 1.8076824623824268 acc: 0.88\n",
      "loss: 1.7752353152503133 acc: 0.87\n",
      "loss: 1.8222705640583658 acc: 0.82\n",
      "loss: 1.8641399671071257 acc: 0.78\n",
      "loss: 1.8133238612869997 acc: 0.86\n",
      "loss: 1.8394013625262513 acc: 0.8\n",
      "loss: 1.8028769758072156 acc: 0.85\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 1.8029\t Accuracy 0.8500\n",
      "loss: 1.832939463830917 acc: 0.84\n",
      "loss: 1.8188752920349247 acc: 0.82\n",
      "loss: 1.8269466559586618 acc: 0.8\n",
      "loss: 1.803093951234389 acc: 0.88\n",
      "loss: 1.8031051982816448 acc: 0.87\n",
      "loss: 1.830674429639426 acc: 0.84\n",
      "loss: 1.7903665387710723 acc: 0.89\n",
      "loss: 1.8165227128283348 acc: 0.83\n",
      "loss: 1.8231802311304415 acc: 0.86\n",
      "loss: 1.81868087531457 acc: 0.84\n",
      "loss: 1.8432726877729886 acc: 0.84\n",
      "loss: 1.8249511615776348 acc: 0.85\n",
      "loss: 1.8460905789005495 acc: 0.81\n",
      "loss: 1.8688725502579329 acc: 0.82\n",
      "loss: 1.8484563523227635 acc: 0.83\n",
      "loss: 1.787135769452453 acc: 0.92\n",
      "loss: 1.7906628493993615 acc: 0.9\n",
      "loss: 1.8256935666154606 acc: 0.85\n",
      "loss: 1.8191374190326621 acc: 0.84\n",
      "loss: 1.8378987834117613 acc: 0.86\n",
      "loss: 1.8342180914984374 acc: 0.82\n",
      "loss: 1.8549612194382994 acc: 0.83\n",
      "loss: 1.8238722490608044 acc: 0.86\n",
      "loss: 1.8586535463266127 acc: 0.82\n",
      "loss: 1.8049451632988291 acc: 0.87\n",
      "loss: 1.817561713125197 acc: 0.84\n",
      "loss: 1.853363749408305 acc: 0.8\n",
      "loss: 1.836736001543403 acc: 0.85\n",
      "loss: 1.7918543615549247 acc: 0.83\n",
      "loss: 1.802944606907139 acc: 0.82\n",
      "loss: 1.8246097465553164 acc: 0.87\n",
      "loss: 1.8443784096896432 acc: 0.83\n",
      "loss: 1.855427014872962 acc: 0.86\n",
      "loss: 1.821774586042903 acc: 0.87\n",
      "loss: 1.857910755368105 acc: 0.76\n",
      "loss: 1.8437251124563139 acc: 0.84\n",
      "loss: 1.828105701482809 acc: 0.81\n",
      "loss: 1.889008066371545 acc: 0.81\n",
      "loss: 1.827857236482638 acc: 0.86\n",
      "loss: 1.811977617451706 acc: 0.89\n",
      "loss: 1.8489534243957138 acc: 0.82\n",
      "loss: 1.821982449031741 acc: 0.86\n",
      "loss: 1.8139801421471942 acc: 0.85\n",
      "loss: 1.8204503777784138 acc: 0.84\n",
      "loss: 1.821840212440858 acc: 0.82\n",
      "loss: 1.7999616489813242 acc: 0.9\n",
      "loss: 1.8136904705608163 acc: 0.82\n",
      "loss: 1.808251990219757 acc: 0.9\n",
      "loss: 1.833014691395918 acc: 0.84\n",
      "loss: 1.8131071844774629 acc: 0.88\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 1.8131\t Accuracy 0.8800\n",
      "loss: 1.770420999946406 acc: 0.92\n",
      "loss: 1.8088749166104003 acc: 0.84\n",
      "loss: 1.8392164306075873 acc: 0.82\n",
      "loss: 1.8177904877461262 acc: 0.86\n",
      "loss: 1.8325887132976642 acc: 0.8\n",
      "loss: 1.8181435551735274 acc: 0.82\n",
      "loss: 1.7477632331393318 acc: 0.92\n",
      "loss: 1.8023236086879648 acc: 0.83\n",
      "loss: 1.8075532508967138 acc: 0.87\n",
      "loss: 1.8861946235714486 acc: 0.81\n",
      "loss: 1.8367885400751682 acc: 0.81\n",
      "loss: 1.8118679797321156 acc: 0.89\n",
      "loss: 1.814132044787528 acc: 0.87\n",
      "loss: 1.7821784263980809 acc: 0.84\n",
      "loss: 1.8199243074510512 acc: 0.87\n",
      "loss: 1.8139642182912867 acc: 0.84\n",
      "loss: 1.7917096077821808 acc: 0.88\n",
      "loss: 1.8226010384897788 acc: 0.8\n",
      "loss: 1.8409656572880644 acc: 0.88\n",
      "loss: 1.7888390805062324 acc: 0.89\n",
      "loss: 1.7932377613255128 acc: 0.89\n",
      "loss: 1.8565109817326242 acc: 0.77\n",
      "loss: 1.8077312836427937 acc: 0.87\n",
      "loss: 1.81070842958612 acc: 0.89\n",
      "loss: 1.8069277696350732 acc: 0.78\n",
      "loss: 1.812010642739582 acc: 0.85\n",
      "loss: 1.8147863359406295 acc: 0.86\n",
      "loss: 1.8134896597827386 acc: 0.83\n",
      "loss: 1.8227771873366598 acc: 0.83\n",
      "loss: 1.823054287808641 acc: 0.83\n",
      "loss: 1.8121555547427377 acc: 0.86\n",
      "loss: 1.7777704450823273 acc: 0.83\n",
      "loss: 1.8571205866609577 acc: 0.88\n",
      "loss: 1.823993994505544 acc: 0.86\n",
      "loss: 1.8207011633194443 acc: 0.84\n",
      "loss: 1.8044705983291036 acc: 0.93\n",
      "loss: 1.888651293433731 acc: 0.74\n",
      "loss: 1.8085803564884608 acc: 0.94\n",
      "loss: 1.8211225384383132 acc: 0.82\n",
      "loss: 1.7972012644805246 acc: 0.84\n",
      "loss: 1.8415634033188868 acc: 0.8\n",
      "loss: 1.7943416893404391 acc: 0.87\n",
      "loss: 1.8498910795494503 acc: 0.79\n",
      "loss: 1.806004050698065 acc: 0.87\n",
      "loss: 1.7935793736993304 acc: 0.87\n",
      "loss: 1.7903180800300995 acc: 0.89\n",
      "loss: 1.8129592929133815 acc: 0.84\n",
      "loss: 1.7763300050173958 acc: 0.91\n",
      "loss: 1.8038627809339838 acc: 0.86\n",
      "loss: 1.7884225943667698 acc: 0.87\n",
      "loss: 1.7871192480604265 acc: 0.88\n",
      "loss: 1.79135198518408 acc: 0.88\n",
      "loss: 1.828910729009794 acc: 0.87\n",
      "loss: 1.8097363774340067 acc: 0.92\n",
      "loss: 1.7969544875000694 acc: 0.85\n",
      "loss: 1.750672777705898 acc: 0.86\n",
      "loss: 1.7518566641460835 acc: 0.85\n",
      "loss: 1.8197308963773071 acc: 0.86\n",
      "loss: 1.7501090186705033 acc: 0.92\n",
      "loss: 1.79366902677205 acc: 0.85\n",
      "loss: 1.7885515242009256 acc: 0.84\n",
      "loss: 1.844252252720989 acc: 0.82\n",
      "loss: 1.8562310780323443 acc: 0.8\n",
      "loss: 1.8581441926451792 acc: 0.84\n",
      "loss: 1.773420683357205 acc: 0.92\n",
      "loss: 1.7891611433424304 acc: 0.85\n",
      "loss: 1.743414361347578 acc: 0.89\n",
      "loss: 1.7747733995417432 acc: 0.84\n",
      "loss: 1.7817227699596396 acc: 0.88\n",
      "loss: 1.8592205645457525 acc: 0.86\n",
      "loss: 1.8141564747697163 acc: 0.87\n",
      "loss: 1.8123276495749483 acc: 0.76\n",
      "loss: 1.852041859985972 acc: 0.83\n",
      "loss: 1.8318095238273235 acc: 0.85\n",
      "loss: 1.8482114670445458 acc: 0.8\n",
      "loss: 1.8395228220713409 acc: 0.79\n",
      "loss: 1.8634525852438175 acc: 0.78\n",
      "loss: 1.8165002306238571 acc: 0.9\n",
      "loss: 1.797468012028152 acc: 0.88\n",
      "loss: 1.8107344052613874 acc: 0.85\n",
      "loss: 1.7692863597762583 acc: 0.93\n",
      "loss: 1.7286791262049157 acc: 0.93\n",
      "loss: 1.779634359263146 acc: 0.87\n",
      "loss: 1.7642079577224479 acc: 0.87\n",
      "loss: 1.8038688102859266 acc: 0.88\n",
      "loss: 1.847457308522085 acc: 0.87\n",
      "loss: 1.8255063035273376 acc: 0.88\n",
      "loss: 1.737168573999834 acc: 0.91\n",
      "loss: 1.6966927258368938 acc: 0.97\n",
      "loss: 1.7236373824951432 acc: 0.94\n",
      "loss: 1.7280061083137337 acc: 0.97\n",
      "loss: 1.7947094210424277 acc: 0.9\n",
      "loss: 1.7822102745305088 acc: 0.84\n",
      "loss: 1.7240160168723833 acc: 0.83\n",
      "loss: 1.7922323531611881 acc: 0.89\n",
      "loss: 1.7977735070952106 acc: 0.92\n",
      "loss: 1.8857042058359659 acc: 0.77\n",
      "loss: 1.70029938692781 acc: 0.96\n",
      "loss: 1.8486706214098567 acc: 0.87\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8219\t Average training accuracy 0.8432\n",
      "Epoch [5]\t Average validation loss 1.7951\t Average validation accuracy 0.8692\n",
      "\n",
      "loss: 1.8101843733741365 acc: 0.86\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 1.8102\t Accuracy 0.8600\n",
      "loss: 1.828231141965347 acc: 0.84\n",
      "loss: 1.8183133944421197 acc: 0.88\n",
      "loss: 1.7803390020894398 acc: 0.89\n",
      "loss: 1.824921273276645 acc: 0.89\n",
      "loss: 1.811105983696433 acc: 0.82\n",
      "loss: 1.8042734685613928 acc: 0.87\n",
      "loss: 1.8617511755715892 acc: 0.82\n",
      "loss: 1.8670518142277734 acc: 0.83\n",
      "loss: 1.8255976503790612 acc: 0.9\n",
      "loss: 1.7884507485200323 acc: 0.9\n",
      "loss: 1.8804100203147573 acc: 0.79\n",
      "loss: 1.8328877869095792 acc: 0.85\n",
      "loss: 1.7997580361368366 acc: 0.84\n",
      "loss: 1.8129120157197314 acc: 0.86\n",
      "loss: 1.8321251103837577 acc: 0.84\n",
      "loss: 1.8376429721170777 acc: 0.83\n",
      "loss: 1.7921766954617928 acc: 0.85\n",
      "loss: 1.7866233355692245 acc: 0.89\n",
      "loss: 1.7819129766654087 acc: 0.84\n",
      "loss: 1.7813432174453678 acc: 0.92\n",
      "loss: 1.833726810175635 acc: 0.79\n",
      "loss: 1.8304892297807458 acc: 0.83\n",
      "loss: 1.8623741155635483 acc: 0.83\n",
      "loss: 1.8495256755868215 acc: 0.85\n",
      "loss: 1.783072008811993 acc: 0.92\n",
      "loss: 1.8450726739318046 acc: 0.73\n",
      "loss: 1.799108699914612 acc: 0.89\n",
      "loss: 1.8149660763414022 acc: 0.84\n",
      "loss: 1.8030715807434572 acc: 0.93\n",
      "loss: 1.803046974630547 acc: 0.85\n",
      "loss: 1.8199259296551418 acc: 0.88\n",
      "loss: 1.826293221910324 acc: 0.87\n",
      "loss: 1.7732161421374784 acc: 0.85\n",
      "loss: 1.8269981921250358 acc: 0.85\n",
      "loss: 1.7873517146758078 acc: 0.83\n",
      "loss: 1.871387694605332 acc: 0.78\n",
      "loss: 1.8313540811413689 acc: 0.79\n",
      "loss: 1.839191765176089 acc: 0.81\n",
      "loss: 1.855451281670631 acc: 0.77\n",
      "loss: 1.8479225331980529 acc: 0.8\n",
      "loss: 1.8010493423033815 acc: 0.85\n",
      "loss: 1.8117180496218723 acc: 0.86\n",
      "loss: 1.800904185950965 acc: 0.86\n",
      "loss: 1.8340549195843514 acc: 0.89\n",
      "loss: 1.8547471474925485 acc: 0.83\n",
      "loss: 1.8071165452844127 acc: 0.79\n",
      "loss: 1.825621740521764 acc: 0.83\n",
      "loss: 1.7836607376019324 acc: 0.87\n",
      "loss: 1.8415197885833121 acc: 0.81\n",
      "loss: 1.8017596619797982 acc: 0.9\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 1.8018\t Accuracy 0.9000\n",
      "loss: 1.783623066809592 acc: 0.87\n",
      "loss: 1.8592615282920777 acc: 0.82\n",
      "loss: 1.8222277637553737 acc: 0.83\n",
      "loss: 1.8676983755255978 acc: 0.81\n",
      "loss: 1.8249598494391979 acc: 0.83\n",
      "loss: 1.8090871013102368 acc: 0.89\n",
      "loss: 1.806705771123993 acc: 0.85\n",
      "loss: 1.8285997344176008 acc: 0.86\n",
      "loss: 1.8107188243458583 acc: 0.86\n",
      "loss: 1.798236594187125 acc: 0.88\n",
      "loss: 1.807412189237037 acc: 0.84\n",
      "loss: 1.8288274898634969 acc: 0.82\n",
      "loss: 1.8224509033688379 acc: 0.76\n",
      "loss: 1.7850938174502946 acc: 0.88\n",
      "loss: 1.8096399116417405 acc: 0.84\n",
      "loss: 1.851102182026621 acc: 0.76\n",
      "loss: 1.812411700682214 acc: 0.84\n",
      "loss: 1.794755350090432 acc: 0.9\n",
      "loss: 1.8473370297783402 acc: 0.83\n",
      "loss: 1.8449009312647933 acc: 0.84\n",
      "loss: 1.8026310920834754 acc: 0.88\n",
      "loss: 1.8580974340633543 acc: 0.8\n",
      "loss: 1.866425080670581 acc: 0.78\n",
      "loss: 1.8432269411549231 acc: 0.84\n",
      "loss: 1.866318502848682 acc: 0.83\n",
      "loss: 1.8358828556927855 acc: 0.83\n",
      "loss: 1.79915568778923 acc: 0.91\n",
      "loss: 1.80943393763256 acc: 0.86\n",
      "loss: 1.8590014875022154 acc: 0.78\n",
      "loss: 1.8117502557420033 acc: 0.87\n",
      "loss: 1.766359664060654 acc: 0.88\n",
      "loss: 1.7999361952894644 acc: 0.84\n",
      "loss: 1.8417303289945326 acc: 0.82\n",
      "loss: 1.8148245112560373 acc: 0.87\n",
      "loss: 1.84129043408267 acc: 0.85\n",
      "loss: 1.8435837009442946 acc: 0.84\n",
      "loss: 1.8039202960798422 acc: 0.88\n",
      "loss: 1.864016066646603 acc: 0.75\n",
      "loss: 1.8392538526347177 acc: 0.8\n",
      "loss: 1.841002618265524 acc: 0.82\n",
      "loss: 1.8187500734002233 acc: 0.88\n",
      "loss: 1.8365921034590613 acc: 0.85\n",
      "loss: 1.8271843152297467 acc: 0.84\n",
      "loss: 1.826817887333207 acc: 0.85\n",
      "loss: 1.8793751238114782 acc: 0.77\n",
      "loss: 1.817567481623062 acc: 0.87\n",
      "loss: 1.8696933120202368 acc: 0.8\n",
      "loss: 1.8115851278275692 acc: 0.78\n",
      "loss: 1.8377631844719964 acc: 0.86\n",
      "loss: 1.8169974479099829 acc: 0.86\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 1.8170\t Accuracy 0.8600\n",
      "loss: 1.831232968028333 acc: 0.82\n",
      "loss: 1.8089541181777522 acc: 0.83\n",
      "loss: 1.831054383259731 acc: 0.86\n",
      "loss: 1.8292872195664294 acc: 0.77\n",
      "loss: 1.822219187036544 acc: 0.83\n",
      "loss: 1.800435093160057 acc: 0.85\n",
      "loss: 1.7892558685895847 acc: 0.9\n",
      "loss: 1.8228514294851033 acc: 0.81\n",
      "loss: 1.8260088956528526 acc: 0.84\n",
      "loss: 1.8458091175708065 acc: 0.84\n",
      "loss: 1.85811764534467 acc: 0.85\n",
      "loss: 1.8234104170658074 acc: 0.86\n",
      "loss: 1.822879189357418 acc: 0.82\n",
      "loss: 1.801304472771054 acc: 0.85\n",
      "loss: 1.8195699700204448 acc: 0.83\n",
      "loss: 1.812400080991391 acc: 0.93\n",
      "loss: 1.816020438344481 acc: 0.82\n",
      "loss: 1.8206439038326343 acc: 0.87\n",
      "loss: 1.867282573124129 acc: 0.81\n",
      "loss: 1.8274169531557956 acc: 0.88\n",
      "loss: 1.7916251829983265 acc: 0.87\n",
      "loss: 1.8371217548894316 acc: 0.87\n",
      "loss: 1.8089889170448081 acc: 0.81\n",
      "loss: 1.8742086705916428 acc: 0.75\n",
      "loss: 1.81336674704551 acc: 0.87\n",
      "loss: 1.8404449142350257 acc: 0.84\n",
      "loss: 1.839845584847888 acc: 0.87\n",
      "loss: 1.8155737154487945 acc: 0.87\n",
      "loss: 1.7804839511243546 acc: 0.89\n",
      "loss: 1.8157134461424198 acc: 0.86\n",
      "loss: 1.849604952459107 acc: 0.76\n",
      "loss: 1.801292071750146 acc: 0.87\n",
      "loss: 1.8209473308752786 acc: 0.85\n",
      "loss: 1.8275568932016584 acc: 0.82\n",
      "loss: 1.8031448882571064 acc: 0.88\n",
      "loss: 1.7929563066891592 acc: 0.87\n",
      "loss: 1.8480871426285819 acc: 0.85\n",
      "loss: 1.8272856578474557 acc: 0.86\n",
      "loss: 1.8067042509253406 acc: 0.85\n",
      "loss: 1.8137355393857706 acc: 0.8\n",
      "loss: 1.8404451212517452 acc: 0.83\n",
      "loss: 1.7937941931758443 acc: 0.86\n",
      "loss: 1.8110824238420205 acc: 0.82\n",
      "loss: 1.8098102696379046 acc: 0.86\n",
      "loss: 1.8327996089932135 acc: 0.85\n",
      "loss: 1.8034285345501633 acc: 0.82\n",
      "loss: 1.808529431352211 acc: 0.85\n",
      "loss: 1.8132804521222772 acc: 0.88\n",
      "loss: 1.7983772679328007 acc: 0.87\n",
      "loss: 1.8237108182752926 acc: 0.78\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 1.8237\t Accuracy 0.7800\n",
      "loss: 1.8463867573780528 acc: 0.8\n",
      "loss: 1.8290749594948794 acc: 0.81\n",
      "loss: 1.7973599139668088 acc: 0.9\n",
      "loss: 1.8000346359106467 acc: 0.86\n",
      "loss: 1.8226616669796938 acc: 0.8\n",
      "loss: 1.8227099546776993 acc: 0.85\n",
      "loss: 1.7921450060764241 acc: 0.83\n",
      "loss: 1.8433052294568517 acc: 0.81\n",
      "loss: 1.8225183658781532 acc: 0.85\n",
      "loss: 1.8211063267385785 acc: 0.83\n",
      "loss: 1.844894076826363 acc: 0.83\n",
      "loss: 1.8315629108975864 acc: 0.88\n",
      "loss: 1.8573607903391167 acc: 0.79\n",
      "loss: 1.8184952654133488 acc: 0.83\n",
      "loss: 1.852546836830199 acc: 0.88\n",
      "loss: 1.8352014258750358 acc: 0.82\n",
      "loss: 1.8280359385019338 acc: 0.87\n",
      "loss: 1.8092619420632707 acc: 0.85\n",
      "loss: 1.7960560099287142 acc: 0.84\n",
      "loss: 1.7961660262105437 acc: 0.87\n",
      "loss: 1.855461993646754 acc: 0.8\n",
      "loss: 1.8134051411351053 acc: 0.89\n",
      "loss: 1.7700039843748814 acc: 0.89\n",
      "loss: 1.8310010187664145 acc: 0.84\n",
      "loss: 1.8772854668836363 acc: 0.8\n",
      "loss: 1.785804870636044 acc: 0.9\n",
      "loss: 1.8364174830017475 acc: 0.79\n",
      "loss: 1.856410068221441 acc: 0.74\n",
      "loss: 1.8368329249545914 acc: 0.83\n",
      "loss: 1.7985566061613114 acc: 0.88\n",
      "loss: 1.8192502931152603 acc: 0.84\n",
      "loss: 1.8103245532656476 acc: 0.83\n",
      "loss: 1.837973333133281 acc: 0.87\n",
      "loss: 1.8055076162682184 acc: 0.84\n",
      "loss: 1.8045336839926716 acc: 0.82\n",
      "loss: 1.831340144184616 acc: 0.86\n",
      "loss: 1.7719556927162348 acc: 0.9\n",
      "loss: 1.8406236951036579 acc: 0.89\n",
      "loss: 1.8125179658130532 acc: 0.88\n",
      "loss: 1.7801168635962004 acc: 0.89\n",
      "loss: 1.8259547610116564 acc: 0.86\n",
      "loss: 1.8258599814554526 acc: 0.82\n",
      "loss: 1.8090575898998742 acc: 0.86\n",
      "loss: 1.7744366878886546 acc: 0.84\n",
      "loss: 1.836197288023609 acc: 0.81\n",
      "loss: 1.8567852449990827 acc: 0.79\n",
      "loss: 1.8388345706541886 acc: 0.81\n",
      "loss: 1.8461496987364836 acc: 0.79\n",
      "loss: 1.8032600875102747 acc: 0.85\n",
      "loss: 1.7986153612093774 acc: 0.89\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 1.7986\t Accuracy 0.8900\n",
      "loss: 1.8073112645523193 acc: 0.84\n",
      "loss: 1.8110554376718344 acc: 0.87\n",
      "loss: 1.8101801976703433 acc: 0.87\n",
      "loss: 1.7991035875420622 acc: 0.85\n",
      "loss: 1.8012488942049387 acc: 0.87\n",
      "loss: 1.8293150751457097 acc: 0.8\n",
      "loss: 1.8170181632381748 acc: 0.81\n",
      "loss: 1.8012327715571816 acc: 0.85\n",
      "loss: 1.8294004253649951 acc: 0.84\n",
      "loss: 1.801988076775437 acc: 0.83\n",
      "loss: 1.8473265061911885 acc: 0.84\n",
      "loss: 1.8123283718813272 acc: 0.82\n",
      "loss: 1.7831857059062517 acc: 0.86\n",
      "loss: 1.8314628896589815 acc: 0.82\n",
      "loss: 1.7856978970687398 acc: 0.91\n",
      "loss: 1.8404424407802935 acc: 0.82\n",
      "loss: 1.8193956160813616 acc: 0.88\n",
      "loss: 1.8049029261367704 acc: 0.86\n",
      "loss: 1.8029370936680007 acc: 0.85\n",
      "loss: 1.825284399749527 acc: 0.79\n",
      "loss: 1.7987929226331227 acc: 0.85\n",
      "loss: 1.8644572185419295 acc: 0.81\n",
      "loss: 1.835321286541391 acc: 0.81\n",
      "loss: 1.7186372870081046 acc: 0.93\n",
      "loss: 1.8885289299358308 acc: 0.79\n",
      "loss: 1.815397146481889 acc: 0.83\n",
      "loss: 1.8054815678060299 acc: 0.84\n",
      "loss: 1.857427026922146 acc: 0.82\n",
      "loss: 1.8230864832248688 acc: 0.84\n",
      "loss: 1.8169590322391702 acc: 0.84\n",
      "loss: 1.8043037658116483 acc: 0.89\n",
      "loss: 1.8189281245865556 acc: 0.83\n",
      "loss: 1.795472320813147 acc: 0.87\n",
      "loss: 1.8294511363879922 acc: 0.84\n",
      "loss: 1.7784181607566674 acc: 0.84\n",
      "loss: 1.817234646551567 acc: 0.91\n",
      "loss: 1.8099923861955125 acc: 0.85\n",
      "loss: 1.8141098503957196 acc: 0.84\n",
      "loss: 1.8541621025896708 acc: 0.81\n",
      "loss: 1.8203721139691313 acc: 0.84\n",
      "loss: 1.8309960821639977 acc: 0.82\n",
      "loss: 1.8348923875233583 acc: 0.82\n",
      "loss: 1.7965695004958022 acc: 0.94\n",
      "loss: 1.8383177537046504 acc: 0.83\n",
      "loss: 1.8233372396765106 acc: 0.8\n",
      "loss: 1.828286495304353 acc: 0.84\n",
      "loss: 1.8382703006035372 acc: 0.82\n",
      "loss: 1.8584875312612041 acc: 0.79\n",
      "loss: 1.843257890052472 acc: 0.88\n",
      "loss: 1.8218053160610574 acc: 0.85\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 1.8218\t Accuracy 0.8500\n",
      "loss: 1.8237221140768338 acc: 0.84\n",
      "loss: 1.8015523161546991 acc: 0.87\n",
      "loss: 1.8518780972062034 acc: 0.85\n",
      "loss: 1.8625088868242048 acc: 0.8\n",
      "loss: 1.850474078673279 acc: 0.83\n",
      "loss: 1.8523046635925748 acc: 0.85\n",
      "loss: 1.833981565791779 acc: 0.79\n",
      "loss: 1.8236248234275927 acc: 0.82\n",
      "loss: 1.8466179246949281 acc: 0.8\n",
      "loss: 1.8276127849724262 acc: 0.86\n",
      "loss: 1.8470081682375332 acc: 0.79\n",
      "loss: 1.821033898761027 acc: 0.79\n",
      "loss: 1.8475015950549944 acc: 0.78\n",
      "loss: 1.8178868316571128 acc: 0.86\n",
      "loss: 1.8088646434805893 acc: 0.84\n",
      "loss: 1.827759093345104 acc: 0.87\n",
      "loss: 1.8273547966102621 acc: 0.86\n",
      "loss: 1.8357366520838128 acc: 0.81\n",
      "loss: 1.8002490111102756 acc: 0.91\n",
      "loss: 1.8174490049062106 acc: 0.84\n",
      "loss: 1.8134103835696722 acc: 0.84\n",
      "loss: 1.810182369466319 acc: 0.88\n",
      "loss: 1.8020264287971253 acc: 0.84\n",
      "loss: 1.7802227578438936 acc: 0.87\n",
      "loss: 1.8189094000669197 acc: 0.87\n",
      "loss: 1.8325886559099138 acc: 0.83\n",
      "loss: 1.788053286093921 acc: 0.85\n",
      "loss: 1.8179635676018124 acc: 0.85\n",
      "loss: 1.826767654761303 acc: 0.86\n",
      "loss: 1.8371987087509694 acc: 0.87\n",
      "loss: 1.8373934248049943 acc: 0.84\n",
      "loss: 1.808436948396985 acc: 0.84\n",
      "loss: 1.8139345891782164 acc: 0.85\n",
      "loss: 1.8095192613288014 acc: 0.84\n",
      "loss: 1.830667350653868 acc: 0.83\n",
      "loss: 1.8408501853846448 acc: 0.87\n",
      "loss: 1.8086004487782088 acc: 0.82\n",
      "loss: 1.8335879297333895 acc: 0.86\n",
      "loss: 1.8135621439479184 acc: 0.84\n",
      "loss: 1.8187838877241103 acc: 0.83\n",
      "loss: 1.7720900082439486 acc: 0.87\n",
      "loss: 1.8237564303874878 acc: 0.85\n",
      "loss: 1.822029915626832 acc: 0.81\n",
      "loss: 1.8189940442089694 acc: 0.85\n",
      "loss: 1.8181619419371002 acc: 0.89\n",
      "loss: 1.8099904856944715 acc: 0.87\n",
      "loss: 1.848984326249098 acc: 0.78\n",
      "loss: 1.8134924755571291 acc: 0.84\n",
      "loss: 1.8437812238039888 acc: 0.85\n",
      "loss: 1.8445640978414115 acc: 0.84\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 1.8446\t Accuracy 0.8400\n",
      "loss: 1.7887849940543594 acc: 0.89\n",
      "loss: 1.8140038789668134 acc: 0.81\n",
      "loss: 1.8224063980718832 acc: 0.79\n",
      "loss: 1.7971325889205059 acc: 0.87\n",
      "loss: 1.7971972850678983 acc: 0.83\n",
      "loss: 1.8308006751133796 acc: 0.83\n",
      "loss: 1.8568795164104983 acc: 0.78\n",
      "loss: 1.8466367882426158 acc: 0.81\n",
      "loss: 1.8241650059600978 acc: 0.85\n",
      "loss: 1.7913447531224675 acc: 0.87\n",
      "loss: 1.8050967096257515 acc: 0.85\n",
      "loss: 1.80531287160366 acc: 0.85\n",
      "loss: 1.7677481085438977 acc: 0.9\n",
      "loss: 1.8196711965673524 acc: 0.87\n",
      "loss: 1.8057057031475923 acc: 0.82\n",
      "loss: 1.8205827986159724 acc: 0.87\n",
      "loss: 1.8357911863089833 acc: 0.79\n",
      "loss: 1.8536943322524855 acc: 0.76\n",
      "loss: 1.8158931483465388 acc: 0.87\n",
      "loss: 1.7897692244875187 acc: 0.87\n",
      "loss: 1.8263547058513976 acc: 0.83\n",
      "loss: 1.8307934184969141 acc: 0.82\n",
      "loss: 1.8367425995932514 acc: 0.84\n",
      "loss: 1.8281356170878678 acc: 0.77\n",
      "loss: 1.8257269661168707 acc: 0.82\n",
      "loss: 1.8022266486960223 acc: 0.88\n",
      "loss: 1.862256612851284 acc: 0.77\n",
      "loss: 1.809720933808361 acc: 0.94\n",
      "loss: 1.8464224144174555 acc: 0.85\n",
      "loss: 1.7968901728562725 acc: 0.89\n",
      "loss: 1.8264809597590357 acc: 0.83\n",
      "loss: 1.8118635098731566 acc: 0.88\n",
      "loss: 1.8051935217202573 acc: 0.9\n",
      "loss: 1.8011434961263917 acc: 0.9\n",
      "loss: 1.829240481013037 acc: 0.84\n",
      "loss: 1.823098072314646 acc: 0.8\n",
      "loss: 1.8236137325826107 acc: 0.85\n",
      "loss: 1.8256903968596703 acc: 0.81\n",
      "loss: 1.8441544692536795 acc: 0.82\n",
      "loss: 1.8363072782290546 acc: 0.78\n",
      "loss: 1.802696732825254 acc: 0.86\n",
      "loss: 1.8254624414676692 acc: 0.86\n",
      "loss: 1.8279068052489378 acc: 0.81\n",
      "loss: 1.7913044804391491 acc: 0.92\n",
      "loss: 1.812546229569515 acc: 0.84\n",
      "loss: 1.8203914555422378 acc: 0.9\n",
      "loss: 1.7947914061976709 acc: 0.9\n",
      "loss: 1.844041160379404 acc: 0.8\n",
      "loss: 1.8441598529220602 acc: 0.82\n",
      "loss: 1.8195713910267686 acc: 0.83\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 1.8196\t Accuracy 0.8300\n",
      "loss: 1.8133348206474909 acc: 0.86\n",
      "loss: 1.805469380875426 acc: 0.84\n",
      "loss: 1.7981977702869498 acc: 0.91\n",
      "loss: 1.8271762691848155 acc: 0.91\n",
      "loss: 1.8173311674037205 acc: 0.88\n",
      "loss: 1.8244913201218247 acc: 0.81\n",
      "loss: 1.807963057950485 acc: 0.85\n",
      "loss: 1.8203045665755246 acc: 0.82\n",
      "loss: 1.847340386628798 acc: 0.8\n",
      "loss: 1.833532246729643 acc: 0.8\n",
      "loss: 1.8363075536458942 acc: 0.86\n",
      "loss: 1.8400961706370316 acc: 0.8\n",
      "loss: 1.7928009264586993 acc: 0.9\n",
      "loss: 1.8289581303000286 acc: 0.82\n",
      "loss: 1.843466601325271 acc: 0.84\n",
      "loss: 1.803945714283021 acc: 0.9\n",
      "loss: 1.815939344617599 acc: 0.8\n",
      "loss: 1.7882488247781723 acc: 0.89\n",
      "loss: 1.8591876741735018 acc: 0.82\n",
      "loss: 1.8163292820587962 acc: 0.83\n",
      "loss: 1.8273400821191692 acc: 0.84\n",
      "loss: 1.7568987241645875 acc: 0.9\n",
      "loss: 1.8044987028259947 acc: 0.87\n",
      "loss: 1.8340415149383107 acc: 0.82\n",
      "loss: 1.871785722159673 acc: 0.83\n",
      "loss: 1.813561547163275 acc: 0.9\n",
      "loss: 1.821733933073508 acc: 0.89\n",
      "loss: 1.8026715921991487 acc: 0.87\n",
      "loss: 1.8324089852965093 acc: 0.83\n",
      "loss: 1.8456764233110439 acc: 0.83\n",
      "loss: 1.7849132660271667 acc: 0.89\n",
      "loss: 1.8143085076112717 acc: 0.84\n",
      "loss: 1.8183200841039735 acc: 0.82\n",
      "loss: 1.8551677353145757 acc: 0.79\n",
      "loss: 1.7881678059463788 acc: 0.85\n",
      "loss: 1.7894271456125805 acc: 0.86\n",
      "loss: 1.7959664022000497 acc: 0.89\n",
      "loss: 1.8376670672288231 acc: 0.81\n",
      "loss: 1.7765907909822207 acc: 0.82\n",
      "loss: 1.7873842789819392 acc: 0.88\n",
      "loss: 1.8169274206270773 acc: 0.89\n",
      "loss: 1.7713280317815867 acc: 0.88\n",
      "loss: 1.7923630759437905 acc: 0.87\n",
      "loss: 1.805618461762706 acc: 0.88\n",
      "loss: 1.826494119392988 acc: 0.8\n",
      "loss: 1.807042742016754 acc: 0.88\n",
      "loss: 1.8068378576956294 acc: 0.88\n",
      "loss: 1.8281144741469872 acc: 0.83\n",
      "loss: 1.8018620027034984 acc: 0.85\n",
      "loss: 1.8189171630178818 acc: 0.83\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 1.8189\t Accuracy 0.8300\n",
      "loss: 1.805136495596373 acc: 0.84\n",
      "loss: 1.806086684301738 acc: 0.87\n",
      "loss: 1.853347921169722 acc: 0.82\n",
      "loss: 1.8539021758272463 acc: 0.86\n",
      "loss: 1.8495786267741747 acc: 0.82\n",
      "loss: 1.8352516170171038 acc: 0.83\n",
      "loss: 1.8395268936507625 acc: 0.85\n",
      "loss: 1.8365835699958646 acc: 0.85\n",
      "loss: 1.80824680479512 acc: 0.88\n",
      "loss: 1.8165795599245613 acc: 0.89\n",
      "loss: 1.809305430362533 acc: 0.89\n",
      "loss: 1.825918501008137 acc: 0.78\n",
      "loss: 1.8489998451575187 acc: 0.83\n",
      "loss: 1.8189201969856708 acc: 0.83\n",
      "loss: 1.7918491629413875 acc: 0.87\n",
      "loss: 1.8028327186943427 acc: 0.83\n",
      "loss: 1.819079359952112 acc: 0.86\n",
      "loss: 1.8169294976041943 acc: 0.87\n",
      "loss: 1.8335920665310357 acc: 0.81\n",
      "loss: 1.8024603563917057 acc: 0.87\n",
      "loss: 1.8293355133909237 acc: 0.88\n",
      "loss: 1.8344902706503445 acc: 0.81\n",
      "loss: 1.802520800785939 acc: 0.86\n",
      "loss: 1.8501641111403533 acc: 0.85\n",
      "loss: 1.8601507235259922 acc: 0.79\n",
      "loss: 1.8268946237435977 acc: 0.88\n",
      "loss: 1.8418596281371364 acc: 0.87\n",
      "loss: 1.8385615700532836 acc: 0.84\n",
      "loss: 1.7967934647200108 acc: 0.88\n",
      "loss: 1.794365772489696 acc: 0.87\n",
      "loss: 1.8308724327879908 acc: 0.87\n",
      "loss: 1.797293058939561 acc: 0.85\n",
      "loss: 1.8545531420243038 acc: 0.82\n",
      "loss: 1.833247489817439 acc: 0.81\n",
      "loss: 1.7977127058379614 acc: 0.91\n",
      "loss: 1.8640897352295134 acc: 0.83\n",
      "loss: 1.8127451955456562 acc: 0.84\n",
      "loss: 1.8319359557396389 acc: 0.89\n",
      "loss: 1.8357375107323262 acc: 0.87\n",
      "loss: 1.8682907497142198 acc: 0.77\n",
      "loss: 1.8649469473420566 acc: 0.82\n",
      "loss: 1.8233638204427285 acc: 0.85\n",
      "loss: 1.8150424767649067 acc: 0.82\n",
      "loss: 1.8240663989283379 acc: 0.86\n",
      "loss: 1.8178734767298252 acc: 0.84\n",
      "loss: 1.8472785216501253 acc: 0.81\n",
      "loss: 1.8015915952148942 acc: 0.81\n",
      "loss: 1.7996440193199823 acc: 0.83\n",
      "loss: 1.8201583461946784 acc: 0.85\n",
      "loss: 1.8680886953863785 acc: 0.81\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 1.8681\t Accuracy 0.8100\n",
      "loss: 1.8091829444309204 acc: 0.89\n",
      "loss: 1.8004477959704193 acc: 0.85\n",
      "loss: 1.8311469439812402 acc: 0.85\n",
      "loss: 1.760433230296898 acc: 0.87\n",
      "loss: 1.8135497761579058 acc: 0.88\n",
      "loss: 1.8280930679284757 acc: 0.83\n",
      "loss: 1.813035388756604 acc: 0.85\n",
      "loss: 1.8087063787469706 acc: 0.85\n",
      "loss: 1.7967587671413288 acc: 0.89\n",
      "loss: 1.8198981805354437 acc: 0.83\n",
      "loss: 1.7864135636049028 acc: 0.87\n",
      "loss: 1.8167274933384872 acc: 0.86\n",
      "loss: 1.8341846021111592 acc: 0.82\n",
      "loss: 1.8726763621516358 acc: 0.77\n",
      "loss: 1.8440517741725575 acc: 0.88\n",
      "loss: 1.822502393607605 acc: 0.8\n",
      "loss: 1.8023477862681327 acc: 0.81\n",
      "loss: 1.7770816807916512 acc: 0.9\n",
      "loss: 1.8460294024304778 acc: 0.82\n",
      "loss: 1.8169166723811279 acc: 0.86\n",
      "loss: 1.8082746032348884 acc: 0.85\n",
      "loss: 1.8387429370849582 acc: 0.79\n",
      "loss: 1.816188760210435 acc: 0.8\n",
      "loss: 1.8008593624336786 acc: 0.87\n",
      "loss: 1.7979209115598036 acc: 0.82\n",
      "loss: 1.800040612988854 acc: 0.86\n",
      "loss: 1.8404666640456504 acc: 0.83\n",
      "loss: 1.8147432149752092 acc: 0.87\n",
      "loss: 1.810922419361403 acc: 0.87\n",
      "loss: 1.819554333885455 acc: 0.82\n",
      "loss: 1.7977557041148475 acc: 0.86\n",
      "loss: 1.8231204653698663 acc: 0.84\n",
      "loss: 1.7915298193070361 acc: 0.9\n",
      "loss: 1.8142657120735886 acc: 0.87\n",
      "loss: 1.8275393287484745 acc: 0.82\n",
      "loss: 1.8523157457986306 acc: 0.84\n",
      "loss: 1.7810290291579196 acc: 0.87\n",
      "loss: 1.829655943738827 acc: 0.89\n",
      "loss: 1.8427998145806581 acc: 0.79\n",
      "loss: 1.853490484554151 acc: 0.82\n",
      "loss: 1.821459843922337 acc: 0.84\n",
      "loss: 1.8118524642518161 acc: 0.86\n",
      "loss: 1.825016169269224 acc: 0.85\n",
      "loss: 1.7789248335632755 acc: 0.88\n",
      "loss: 1.826705264563015 acc: 0.83\n",
      "loss: 1.8731569543734503 acc: 0.76\n",
      "loss: 1.810919151583891 acc: 0.83\n",
      "loss: 1.8863163709098787 acc: 0.79\n",
      "loss: 1.835169247202916 acc: 0.83\n",
      "loss: 1.809435332227758 acc: 0.9\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 1.8094\t Accuracy 0.9000\n",
      "loss: 1.8029215347058474 acc: 0.9\n",
      "loss: 1.7979503829571564 acc: 0.88\n",
      "loss: 1.7982270948897223 acc: 0.85\n",
      "loss: 1.846756655783351 acc: 0.87\n",
      "loss: 1.8210583791127288 acc: 0.91\n",
      "loss: 1.8076901489713135 acc: 0.88\n",
      "loss: 1.828683653550711 acc: 0.83\n",
      "loss: 1.829243774244094 acc: 0.8\n",
      "loss: 1.8545201570832608 acc: 0.85\n",
      "loss: 1.8578759787429617 acc: 0.79\n",
      "loss: 1.8235890400082315 acc: 0.86\n",
      "loss: 1.8174666389986833 acc: 0.84\n",
      "loss: 1.7812642050837642 acc: 0.88\n",
      "loss: 1.8351765857694804 acc: 0.81\n",
      "loss: 1.8378081766009067 acc: 0.87\n",
      "loss: 1.8137448488658863 acc: 0.87\n",
      "loss: 1.8156830915948412 acc: 0.85\n",
      "loss: 1.8358409872731616 acc: 0.88\n",
      "loss: 1.799103109546082 acc: 0.87\n",
      "loss: 1.8132511764224413 acc: 0.86\n",
      "loss: 1.8267611982314924 acc: 0.89\n",
      "loss: 1.832412432312921 acc: 0.82\n",
      "loss: 1.8038892952020336 acc: 0.93\n",
      "loss: 1.854571019938557 acc: 0.83\n",
      "loss: 1.8163441019607638 acc: 0.79\n",
      "loss: 1.8051394737571322 acc: 0.86\n",
      "loss: 1.831998385221806 acc: 0.84\n",
      "loss: 1.8359939069571993 acc: 0.81\n",
      "loss: 1.8018313455423014 acc: 0.86\n",
      "loss: 1.8230656358006985 acc: 0.84\n",
      "loss: 1.7946970042704762 acc: 0.88\n",
      "loss: 1.8348998623422204 acc: 0.84\n",
      "loss: 1.791990812035897 acc: 0.84\n",
      "loss: 1.8408020416163646 acc: 0.81\n",
      "loss: 1.8450416900782178 acc: 0.77\n",
      "loss: 1.7952931677110462 acc: 0.92\n",
      "loss: 1.8724275704054447 acc: 0.79\n",
      "loss: 1.832795428612566 acc: 0.87\n",
      "loss: 1.8127044221907662 acc: 0.85\n",
      "loss: 1.7864137532505147 acc: 0.84\n",
      "loss: 1.850157214412922 acc: 0.81\n",
      "loss: 1.855090322895954 acc: 0.82\n",
      "loss: 1.7998666466191673 acc: 0.86\n",
      "loss: 1.809566689118883 acc: 0.91\n",
      "loss: 1.8159913669297816 acc: 0.87\n",
      "loss: 1.8384037567665408 acc: 0.76\n",
      "loss: 1.830410599031255 acc: 0.81\n",
      "loss: 1.8433341266032204 acc: 0.83\n",
      "loss: 1.838786184389085 acc: 0.83\n",
      "loss: 1.7901252498538553 acc: 0.88\n",
      "loss: 1.7861642270871672 acc: 0.9\n",
      "loss: 1.7924499537648426 acc: 0.88\n",
      "loss: 1.8202345547232763 acc: 0.87\n",
      "loss: 1.8147402062993083 acc: 0.93\n",
      "loss: 1.7957046493580422 acc: 0.87\n",
      "loss: 1.7590926880652558 acc: 0.85\n",
      "loss: 1.7534483406287793 acc: 0.85\n",
      "loss: 1.8137981036100457 acc: 0.87\n",
      "loss: 1.7440357847537273 acc: 0.92\n",
      "loss: 1.788538265692699 acc: 0.88\n",
      "loss: 1.7895011375298049 acc: 0.84\n",
      "loss: 1.8411293275008136 acc: 0.83\n",
      "loss: 1.8503474179636823 acc: 0.82\n",
      "loss: 1.856415765980685 acc: 0.87\n",
      "loss: 1.7765589973155078 acc: 0.96\n",
      "loss: 1.787505366095715 acc: 0.84\n",
      "loss: 1.7507380645166126 acc: 0.88\n",
      "loss: 1.774763325548887 acc: 0.86\n",
      "loss: 1.7811525565150694 acc: 0.88\n",
      "loss: 1.86015253595754 acc: 0.86\n",
      "loss: 1.81094365233342 acc: 0.89\n",
      "loss: 1.8105777207158504 acc: 0.74\n",
      "loss: 1.8520976984141944 acc: 0.86\n",
      "loss: 1.8210034125195664 acc: 0.88\n",
      "loss: 1.8408939563102376 acc: 0.8\n",
      "loss: 1.8501255168348216 acc: 0.78\n",
      "loss: 1.8779011305556788 acc: 0.8\n",
      "loss: 1.8211529632576997 acc: 0.89\n",
      "loss: 1.7960857900720493 acc: 0.89\n",
      "loss: 1.8137664904916468 acc: 0.84\n",
      "loss: 1.7769983711875557 acc: 0.94\n",
      "loss: 1.7406953087522574 acc: 0.92\n",
      "loss: 1.778699534598962 acc: 0.9\n",
      "loss: 1.7743975031284271 acc: 0.88\n",
      "loss: 1.8000050193905504 acc: 0.89\n",
      "loss: 1.8489959118271657 acc: 0.9\n",
      "loss: 1.8322034698305378 acc: 0.9\n",
      "loss: 1.7261186762605545 acc: 0.92\n",
      "loss: 1.702210413730016 acc: 0.97\n",
      "loss: 1.735260281057888 acc: 0.94\n",
      "loss: 1.7333026490044672 acc: 0.98\n",
      "loss: 1.7849652194300285 acc: 0.92\n",
      "loss: 1.7731534214960218 acc: 0.88\n",
      "loss: 1.7174107239889076 acc: 0.84\n",
      "loss: 1.786946624966031 acc: 0.95\n",
      "loss: 1.8024700167062973 acc: 0.91\n",
      "loss: 1.8707050672370085 acc: 0.82\n",
      "loss: 1.69522509530329 acc: 0.97\n",
      "loss: 1.853929983965171 acc: 0.88\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8214\t Average training accuracy 0.8442\n",
      "Epoch [6]\t Average validation loss 1.7951\t Average validation accuracy 0.8804\n",
      "\n",
      "loss: 1.8502652700960847 acc: 0.85\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 1.8503\t Accuracy 0.8500\n",
      "loss: 1.8112318895452015 acc: 0.89\n",
      "loss: 1.812559377755476 acc: 0.84\n",
      "loss: 1.785433740171569 acc: 0.87\n",
      "loss: 1.8188262060360663 acc: 0.83\n",
      "loss: 1.8006703220320837 acc: 0.92\n",
      "loss: 1.7856318200350612 acc: 0.87\n",
      "loss: 1.836001673495387 acc: 0.82\n",
      "loss: 1.8133224393072696 acc: 0.85\n",
      "loss: 1.808754247908218 acc: 0.86\n",
      "loss: 1.8363854645341144 acc: 0.83\n",
      "loss: 1.7594714659688586 acc: 0.92\n",
      "loss: 1.8492150207485283 acc: 0.82\n",
      "loss: 1.8479683892959675 acc: 0.78\n",
      "loss: 1.847639928384001 acc: 0.78\n",
      "loss: 1.8239753695965177 acc: 0.87\n",
      "loss: 1.8215024737084031 acc: 0.82\n",
      "loss: 1.8739142248752034 acc: 0.79\n",
      "loss: 1.806167639417996 acc: 0.83\n",
      "loss: 1.840627174663534 acc: 0.84\n",
      "loss: 1.7994145846243461 acc: 0.84\n",
      "loss: 1.8047407397388513 acc: 0.87\n",
      "loss: 1.8042250122150028 acc: 0.88\n",
      "loss: 1.828424582226803 acc: 0.82\n",
      "loss: 1.850038809876929 acc: 0.8\n",
      "loss: 1.8085121813531626 acc: 0.83\n",
      "loss: 1.82683606299268 acc: 0.82\n",
      "loss: 1.8106523884976746 acc: 0.88\n",
      "loss: 1.8658185109686456 acc: 0.81\n",
      "loss: 1.8045956529758231 acc: 0.83\n",
      "loss: 1.8509805509452022 acc: 0.81\n",
      "loss: 1.8145556046334017 acc: 0.83\n",
      "loss: 1.8197991983280106 acc: 0.84\n",
      "loss: 1.781044759125643 acc: 0.87\n",
      "loss: 1.7951472233437291 acc: 0.84\n",
      "loss: 1.8095926976979952 acc: 0.82\n",
      "loss: 1.8362306590685396 acc: 0.82\n",
      "loss: 1.8397586141121567 acc: 0.79\n",
      "loss: 1.84039786466409 acc: 0.87\n",
      "loss: 1.83631735672492 acc: 0.89\n",
      "loss: 1.855618329804093 acc: 0.79\n",
      "loss: 1.8190116877915858 acc: 0.87\n",
      "loss: 1.838520605807674 acc: 0.83\n",
      "loss: 1.8094187219867004 acc: 0.88\n",
      "loss: 1.7872928971910855 acc: 0.84\n",
      "loss: 1.8193255598515048 acc: 0.84\n",
      "loss: 1.811020405158203 acc: 0.84\n",
      "loss: 1.8097604714486193 acc: 0.86\n",
      "loss: 1.8150857233109616 acc: 0.86\n",
      "loss: 1.8348055795117688 acc: 0.82\n",
      "loss: 1.8236871007608662 acc: 0.85\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 1.8237\t Accuracy 0.8500\n",
      "loss: 1.8468245831789227 acc: 0.8\n",
      "loss: 1.84094712680584 acc: 0.78\n",
      "loss: 1.825869445599944 acc: 0.84\n",
      "loss: 1.8138887365098189 acc: 0.88\n",
      "loss: 1.808495571193177 acc: 0.84\n",
      "loss: 1.8505569853013173 acc: 0.8\n",
      "loss: 1.8315693624436502 acc: 0.85\n",
      "loss: 1.842209116222194 acc: 0.89\n",
      "loss: 1.824189987070075 acc: 0.79\n",
      "loss: 1.8043995499012977 acc: 0.9\n",
      "loss: 1.8407700437698904 acc: 0.83\n",
      "loss: 1.8242716010382185 acc: 0.85\n",
      "loss: 1.8452254803654589 acc: 0.84\n",
      "loss: 1.8267473653364925 acc: 0.89\n",
      "loss: 1.8100059841365448 acc: 0.85\n",
      "loss: 1.8099749230184259 acc: 0.85\n",
      "loss: 1.8263677292889824 acc: 0.83\n",
      "loss: 1.8145875736605779 acc: 0.87\n",
      "loss: 1.7873588599625803 acc: 0.89\n",
      "loss: 1.8183903152642114 acc: 0.82\n",
      "loss: 1.8070499995515699 acc: 0.82\n",
      "loss: 1.8328139862370216 acc: 0.8\n",
      "loss: 1.8291651061773622 acc: 0.85\n",
      "loss: 1.8273695612543273 acc: 0.81\n",
      "loss: 1.8513389361018888 acc: 0.84\n",
      "loss: 1.8011085382874827 acc: 0.89\n",
      "loss: 1.772559149360155 acc: 0.94\n",
      "loss: 1.779386467193887 acc: 0.9\n",
      "loss: 1.8441658243125945 acc: 0.88\n",
      "loss: 1.857844500430133 acc: 0.78\n",
      "loss: 1.7989923602844098 acc: 0.87\n",
      "loss: 1.8548733112679858 acc: 0.85\n",
      "loss: 1.8421834801106474 acc: 0.82\n",
      "loss: 1.8142167847819735 acc: 0.87\n",
      "loss: 1.8471190507594275 acc: 0.78\n",
      "loss: 1.8220135961878035 acc: 0.83\n",
      "loss: 1.8088978525910167 acc: 0.86\n",
      "loss: 1.848749873618166 acc: 0.81\n",
      "loss: 1.8367149389411817 acc: 0.84\n",
      "loss: 1.8135968865173009 acc: 0.83\n",
      "loss: 1.7980010415912282 acc: 0.85\n",
      "loss: 1.8043881772339854 acc: 0.88\n",
      "loss: 1.8085477751564938 acc: 0.84\n",
      "loss: 1.8396912482565408 acc: 0.84\n",
      "loss: 1.8195987676373 acc: 0.77\n",
      "loss: 1.8095531142967096 acc: 0.84\n",
      "loss: 1.7970282989772821 acc: 0.84\n",
      "loss: 1.8105698487608661 acc: 0.89\n",
      "loss: 1.8324054630796518 acc: 0.82\n",
      "loss: 1.8241059860186644 acc: 0.83\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 1.8241\t Accuracy 0.8300\n",
      "loss: 1.850921185899749 acc: 0.87\n",
      "loss: 1.9022169240624849 acc: 0.77\n",
      "loss: 1.840578124181266 acc: 0.84\n",
      "loss: 1.8622080793572746 acc: 0.83\n",
      "loss: 1.8669604626041931 acc: 0.79\n",
      "loss: 1.8155725458951946 acc: 0.87\n",
      "loss: 1.837582858689376 acc: 0.84\n",
      "loss: 1.8150232906290937 acc: 0.81\n",
      "loss: 1.823769358057816 acc: 0.84\n",
      "loss: 1.8163992731603107 acc: 0.84\n",
      "loss: 1.8182695557378132 acc: 0.83\n",
      "loss: 1.834187196838624 acc: 0.84\n",
      "loss: 1.8022533214205978 acc: 0.88\n",
      "loss: 1.8091777096364254 acc: 0.83\n",
      "loss: 1.8003281235836834 acc: 0.87\n",
      "loss: 1.8130479526312682 acc: 0.84\n",
      "loss: 1.8130997993574078 acc: 0.87\n",
      "loss: 1.7801159004183558 acc: 0.9\n",
      "loss: 1.8268578306506853 acc: 0.87\n",
      "loss: 1.8374787669617296 acc: 0.79\n",
      "loss: 1.8526229463065034 acc: 0.8\n",
      "loss: 1.816270767439017 acc: 0.84\n",
      "loss: 1.8085388329119971 acc: 0.84\n",
      "loss: 1.7713911846106623 acc: 0.87\n",
      "loss: 1.8580329719683493 acc: 0.84\n",
      "loss: 1.7920674438815665 acc: 0.84\n",
      "loss: 1.7872941905425364 acc: 0.92\n",
      "loss: 1.8543941860285318 acc: 0.86\n",
      "loss: 1.833735752880345 acc: 0.89\n",
      "loss: 1.8305502172934882 acc: 0.83\n",
      "loss: 1.7814914709246086 acc: 0.88\n",
      "loss: 1.7741363402318284 acc: 0.86\n",
      "loss: 1.8606999511321967 acc: 0.81\n",
      "loss: 1.8062515316855825 acc: 0.81\n",
      "loss: 1.8174528943812882 acc: 0.81\n",
      "loss: 1.8289515223001045 acc: 0.81\n",
      "loss: 1.7843995695875499 acc: 0.86\n",
      "loss: 1.8387621677633985 acc: 0.81\n",
      "loss: 1.850704920922498 acc: 0.82\n",
      "loss: 1.8660290529253953 acc: 0.78\n",
      "loss: 1.8009518209018325 acc: 0.89\n",
      "loss: 1.8581623008252925 acc: 0.77\n",
      "loss: 1.8539192808940106 acc: 0.8\n",
      "loss: 1.8607599098326373 acc: 0.81\n",
      "loss: 1.823228554368149 acc: 0.83\n",
      "loss: 1.8198415822081313 acc: 0.83\n",
      "loss: 1.8049044895714172 acc: 0.88\n",
      "loss: 1.8477167520715212 acc: 0.8\n",
      "loss: 1.8553206515045313 acc: 0.84\n",
      "loss: 1.802711713812142 acc: 0.85\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 1.8027\t Accuracy 0.8500\n",
      "loss: 1.8261355394722265 acc: 0.83\n",
      "loss: 1.823975026360949 acc: 0.84\n",
      "loss: 1.8148105223328692 acc: 0.85\n",
      "loss: 1.870512224367406 acc: 0.78\n",
      "loss: 1.8056494901386932 acc: 0.84\n",
      "loss: 1.8065078375996668 acc: 0.88\n",
      "loss: 1.7986034967603906 acc: 0.83\n",
      "loss: 1.8310362937922589 acc: 0.85\n",
      "loss: 1.8352168885568776 acc: 0.84\n",
      "loss: 1.8319140514518244 acc: 0.83\n",
      "loss: 1.8333113987844487 acc: 0.87\n",
      "loss: 1.8398895041974144 acc: 0.79\n",
      "loss: 1.8040179226148576 acc: 0.9\n",
      "loss: 1.8063631869010441 acc: 0.85\n",
      "loss: 1.8195264606381212 acc: 0.8\n",
      "loss: 1.8055687071755124 acc: 0.87\n",
      "loss: 1.833927347898501 acc: 0.83\n",
      "loss: 1.8080843480778799 acc: 0.86\n",
      "loss: 1.7805635239200621 acc: 0.89\n",
      "loss: 1.805414674344133 acc: 0.92\n",
      "loss: 1.835770592972618 acc: 0.86\n",
      "loss: 1.7881410983276798 acc: 0.88\n",
      "loss: 1.841271618139739 acc: 0.88\n",
      "loss: 1.843163217605866 acc: 0.82\n",
      "loss: 1.8009278160139732 acc: 0.86\n",
      "loss: 1.77232095750095 acc: 0.86\n",
      "loss: 1.809731636846032 acc: 0.84\n",
      "loss: 1.8206135049416425 acc: 0.84\n",
      "loss: 1.7952066211069666 acc: 0.91\n",
      "loss: 1.8122353464656384 acc: 0.9\n",
      "loss: 1.8240704186415464 acc: 0.88\n",
      "loss: 1.8310300626116334 acc: 0.83\n",
      "loss: 1.828497844947368 acc: 0.84\n",
      "loss: 1.8012389264739537 acc: 0.81\n",
      "loss: 1.8306680012515932 acc: 0.89\n",
      "loss: 1.8453561028528953 acc: 0.83\n",
      "loss: 1.7876307405402536 acc: 0.89\n",
      "loss: 1.830508264812714 acc: 0.8\n",
      "loss: 1.8083482004331786 acc: 0.87\n",
      "loss: 1.8005220925377896 acc: 0.87\n",
      "loss: 1.821673102121633 acc: 0.82\n",
      "loss: 1.8130636285189865 acc: 0.86\n",
      "loss: 1.8350291045542502 acc: 0.8\n",
      "loss: 1.7960799553530706 acc: 0.84\n",
      "loss: 1.814294837571058 acc: 0.87\n",
      "loss: 1.859349336969693 acc: 0.8\n",
      "loss: 1.809631518049473 acc: 0.84\n",
      "loss: 1.7878379458082265 acc: 0.86\n",
      "loss: 1.7992602808336284 acc: 0.86\n",
      "loss: 1.8258627149334656 acc: 0.87\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 1.8259\t Accuracy 0.8700\n",
      "loss: 1.7692588796492796 acc: 0.9\n",
      "loss: 1.8014717036304446 acc: 0.9\n",
      "loss: 1.8366339658177255 acc: 0.8\n",
      "loss: 1.7853090189482925 acc: 0.78\n",
      "loss: 1.8217263365862433 acc: 0.83\n",
      "loss: 1.8136048630768 acc: 0.82\n",
      "loss: 1.813640723104922 acc: 0.81\n",
      "loss: 1.789144938718471 acc: 0.89\n",
      "loss: 1.7956557738341778 acc: 0.87\n",
      "loss: 1.8098816094694288 acc: 0.9\n",
      "loss: 1.8109879223136158 acc: 0.88\n",
      "loss: 1.8512694558319815 acc: 0.83\n",
      "loss: 1.8154921374608681 acc: 0.84\n",
      "loss: 1.8339168778082624 acc: 0.79\n",
      "loss: 1.8453696976649665 acc: 0.81\n",
      "loss: 1.7924804085072763 acc: 0.89\n",
      "loss: 1.8341803379974508 acc: 0.87\n",
      "loss: 1.831240483993085 acc: 0.88\n",
      "loss: 1.837623448410435 acc: 0.84\n",
      "loss: 1.8085984818413963 acc: 0.91\n",
      "loss: 1.8307860877717541 acc: 0.87\n",
      "loss: 1.8418099456743762 acc: 0.84\n",
      "loss: 1.8099712880643855 acc: 0.87\n",
      "loss: 1.8326912937351003 acc: 0.78\n",
      "loss: 1.8463820342513222 acc: 0.78\n",
      "loss: 1.864449614767079 acc: 0.78\n",
      "loss: 1.822663951932878 acc: 0.84\n",
      "loss: 1.850245928846441 acc: 0.79\n",
      "loss: 1.8030796142276229 acc: 0.87\n",
      "loss: 1.8155025083041412 acc: 0.88\n",
      "loss: 1.7752676521414756 acc: 0.86\n",
      "loss: 1.8503611514782619 acc: 0.87\n",
      "loss: 1.8440845081761026 acc: 0.83\n",
      "loss: 1.819957688359419 acc: 0.83\n",
      "loss: 1.8436398420261404 acc: 0.82\n",
      "loss: 1.7985847549832823 acc: 0.84\n",
      "loss: 1.8098274372188996 acc: 0.85\n",
      "loss: 1.8049352620950545 acc: 0.91\n",
      "loss: 1.823429728601454 acc: 0.83\n",
      "loss: 1.8234579049142843 acc: 0.82\n",
      "loss: 1.8232032266436278 acc: 0.87\n",
      "loss: 1.8266059513724593 acc: 0.81\n",
      "loss: 1.7876008932703138 acc: 0.88\n",
      "loss: 1.8328586368534463 acc: 0.78\n",
      "loss: 1.820354859415319 acc: 0.89\n",
      "loss: 1.8070102089831108 acc: 0.86\n",
      "loss: 1.8081515955448837 acc: 0.85\n",
      "loss: 1.806778443906956 acc: 0.89\n",
      "loss: 1.809068115456799 acc: 0.83\n",
      "loss: 1.8368946362440395 acc: 0.84\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 1.8369\t Accuracy 0.8400\n",
      "loss: 1.8315990293687696 acc: 0.84\n",
      "loss: 1.7982016214427612 acc: 0.85\n",
      "loss: 1.7962576683507518 acc: 0.89\n",
      "loss: 1.8386708746933527 acc: 0.84\n",
      "loss: 1.8416022700311974 acc: 0.84\n",
      "loss: 1.7987222329594754 acc: 0.88\n",
      "loss: 1.8442155242938358 acc: 0.86\n",
      "loss: 1.7721075610850456 acc: 0.89\n",
      "loss: 1.8444126965865808 acc: 0.8\n",
      "loss: 1.7572691375081604 acc: 0.85\n",
      "loss: 1.8381880579130445 acc: 0.86\n",
      "loss: 1.8212187789714671 acc: 0.85\n",
      "loss: 1.83199040471655 acc: 0.83\n",
      "loss: 1.8649944894586403 acc: 0.76\n",
      "loss: 1.8191370129144908 acc: 0.82\n",
      "loss: 1.7975252854518877 acc: 0.88\n",
      "loss: 1.7933874954356608 acc: 0.9\n",
      "loss: 1.81301196667356 acc: 0.88\n",
      "loss: 1.780323324954295 acc: 0.94\n",
      "loss: 1.789033908648673 acc: 0.91\n",
      "loss: 1.7747593940049093 acc: 0.85\n",
      "loss: 1.808316195422239 acc: 0.83\n",
      "loss: 1.788905057696617 acc: 0.82\n",
      "loss: 1.82399142127902 acc: 0.83\n",
      "loss: 1.8035610441635566 acc: 0.85\n",
      "loss: 1.8159594957892948 acc: 0.87\n",
      "loss: 1.850111828526053 acc: 0.86\n",
      "loss: 1.8066154013058209 acc: 0.9\n",
      "loss: 1.80438373537541 acc: 0.89\n",
      "loss: 1.8259568223591578 acc: 0.83\n",
      "loss: 1.810560102268521 acc: 0.84\n",
      "loss: 1.7882623376554099 acc: 0.85\n",
      "loss: 1.849748297262475 acc: 0.88\n",
      "loss: 1.8276015138625885 acc: 0.79\n",
      "loss: 1.8232062564364748 acc: 0.83\n",
      "loss: 1.8128658208757706 acc: 0.83\n",
      "loss: 1.8254531609654117 acc: 0.83\n",
      "loss: 1.8186563521439263 acc: 0.85\n",
      "loss: 1.8889058270438688 acc: 0.82\n",
      "loss: 1.8820036298913747 acc: 0.8\n",
      "loss: 1.8373387333291606 acc: 0.87\n",
      "loss: 1.7904618040836733 acc: 0.92\n",
      "loss: 1.7893148260571723 acc: 0.86\n",
      "loss: 1.840569298695125 acc: 0.83\n",
      "loss: 1.8468610477713097 acc: 0.78\n",
      "loss: 1.8110165599023333 acc: 0.89\n",
      "loss: 1.8089366043974673 acc: 0.85\n",
      "loss: 1.8061964794114374 acc: 0.86\n",
      "loss: 1.835529785984511 acc: 0.81\n",
      "loss: 1.7818756265647735 acc: 0.87\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 1.7819\t Accuracy 0.8700\n",
      "loss: 1.8439324366612266 acc: 0.81\n",
      "loss: 1.7849120478314062 acc: 0.86\n",
      "loss: 1.7850549287196438 acc: 0.85\n",
      "loss: 1.8191095472183778 acc: 0.83\n",
      "loss: 1.8277743354496343 acc: 0.84\n",
      "loss: 1.83820570770361 acc: 0.81\n",
      "loss: 1.8170057627947995 acc: 0.84\n",
      "loss: 1.85476173283263 acc: 0.83\n",
      "loss: 1.8211163817539193 acc: 0.82\n",
      "loss: 1.7970893820530103 acc: 0.84\n",
      "loss: 1.8234801580617508 acc: 0.82\n",
      "loss: 1.8269779330948939 acc: 0.82\n",
      "loss: 1.8110380194787123 acc: 0.83\n",
      "loss: 1.8388276602900482 acc: 0.85\n",
      "loss: 1.8220844879580187 acc: 0.86\n",
      "loss: 1.7974894283119314 acc: 0.88\n",
      "loss: 1.813277613744269 acc: 0.89\n",
      "loss: 1.8427548437436334 acc: 0.84\n",
      "loss: 1.8013969131237937 acc: 0.88\n",
      "loss: 1.811907942367898 acc: 0.86\n",
      "loss: 1.8507813494461984 acc: 0.81\n",
      "loss: 1.8076927965113603 acc: 0.83\n",
      "loss: 1.8238043185908686 acc: 0.85\n",
      "loss: 1.7762386078399033 acc: 0.96\n",
      "loss: 1.8023274503845281 acc: 0.85\n",
      "loss: 1.8248910721512184 acc: 0.82\n",
      "loss: 1.7800520596136367 acc: 0.88\n",
      "loss: 1.8224660810445048 acc: 0.83\n",
      "loss: 1.8216964887147975 acc: 0.79\n",
      "loss: 1.8024501148029477 acc: 0.82\n",
      "loss: 1.8281034224661554 acc: 0.8\n",
      "loss: 1.7700848597083285 acc: 0.86\n",
      "loss: 1.8260866511891982 acc: 0.83\n",
      "loss: 1.775432750838911 acc: 0.89\n",
      "loss: 1.8091644365734203 acc: 0.83\n",
      "loss: 1.8147031564000824 acc: 0.76\n",
      "loss: 1.8322885985807893 acc: 0.8\n",
      "loss: 1.8533073740560493 acc: 0.77\n",
      "loss: 1.8072135383058678 acc: 0.88\n",
      "loss: 1.7892465055954723 acc: 0.85\n",
      "loss: 1.8333333525575728 acc: 0.82\n",
      "loss: 1.8159336782065365 acc: 0.84\n",
      "loss: 1.8135256849415933 acc: 0.84\n",
      "loss: 1.8187231510076902 acc: 0.84\n",
      "loss: 1.7872060346083367 acc: 0.91\n",
      "loss: 1.8332592291702279 acc: 0.89\n",
      "loss: 1.8170344319271339 acc: 0.87\n",
      "loss: 1.8505035752385826 acc: 0.85\n",
      "loss: 1.845353082273922 acc: 0.82\n",
      "loss: 1.8324768649084253 acc: 0.84\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 1.8325\t Accuracy 0.8400\n",
      "loss: 1.836843808479948 acc: 0.84\n",
      "loss: 1.8480428199071437 acc: 0.84\n",
      "loss: 1.8242447398663668 acc: 0.85\n",
      "loss: 1.8250893676338489 acc: 0.83\n",
      "loss: 1.7909235486929556 acc: 0.9\n",
      "loss: 1.8053656232017858 acc: 0.87\n",
      "loss: 1.8782564305265206 acc: 0.81\n",
      "loss: 1.811867744292222 acc: 0.86\n",
      "loss: 1.8399830072184853 acc: 0.83\n",
      "loss: 1.8244660871854328 acc: 0.85\n",
      "loss: 1.7825502533919733 acc: 0.89\n",
      "loss: 1.7810766668839904 acc: 0.9\n",
      "loss: 1.8351843975157407 acc: 0.85\n",
      "loss: 1.8107058162339111 acc: 0.8\n",
      "loss: 1.877872660148992 acc: 0.82\n",
      "loss: 1.8215184147829648 acc: 0.87\n",
      "loss: 1.830896949441298 acc: 0.89\n",
      "loss: 1.7903086635204608 acc: 0.86\n",
      "loss: 1.7798334508440634 acc: 0.9\n",
      "loss: 1.8300345319004594 acc: 0.87\n",
      "loss: 1.822923493779323 acc: 0.84\n",
      "loss: 1.7977012743417293 acc: 0.86\n",
      "loss: 1.846138799225855 acc: 0.81\n",
      "loss: 1.813238266944743 acc: 0.82\n",
      "loss: 1.8055149616489155 acc: 0.84\n",
      "loss: 1.7898304307607382 acc: 0.83\n",
      "loss: 1.8184696828248597 acc: 0.8\n",
      "loss: 1.8064501991448196 acc: 0.89\n",
      "loss: 1.7936404417387686 acc: 0.89\n",
      "loss: 1.8320530649580087 acc: 0.84\n",
      "loss: 1.8190513986259342 acc: 0.84\n",
      "loss: 1.8158704645717296 acc: 0.83\n",
      "loss: 1.8593974521451122 acc: 0.81\n",
      "loss: 1.817854675532121 acc: 0.84\n",
      "loss: 1.8544547067897914 acc: 0.78\n",
      "loss: 1.8112871965838462 acc: 0.88\n",
      "loss: 1.8062820858020425 acc: 0.87\n",
      "loss: 1.8225421227286538 acc: 0.81\n",
      "loss: 1.8228817539573121 acc: 0.83\n",
      "loss: 1.8082428251687594 acc: 0.86\n",
      "loss: 1.8407602084407673 acc: 0.87\n",
      "loss: 1.835003889017319 acc: 0.89\n",
      "loss: 1.8019833598377237 acc: 0.89\n",
      "loss: 1.8055828972736552 acc: 0.85\n",
      "loss: 1.8122509200264452 acc: 0.86\n",
      "loss: 1.8207704995188356 acc: 0.86\n",
      "loss: 1.790184811695606 acc: 0.88\n",
      "loss: 1.8229954012596576 acc: 0.83\n",
      "loss: 1.7901382324807624 acc: 0.9\n",
      "loss: 1.7765817006684128 acc: 0.87\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 1.7766\t Accuracy 0.8700\n",
      "loss: 1.822275256161049 acc: 0.89\n",
      "loss: 1.8192428518734747 acc: 0.87\n",
      "loss: 1.814586008066282 acc: 0.88\n",
      "loss: 1.792887273065769 acc: 0.88\n",
      "loss: 1.8118514407213973 acc: 0.86\n",
      "loss: 1.853823097081905 acc: 0.75\n",
      "loss: 1.8079132756465497 acc: 0.81\n",
      "loss: 1.817152243886831 acc: 0.83\n",
      "loss: 1.8228330564610402 acc: 0.85\n",
      "loss: 1.7941906724763355 acc: 0.86\n",
      "loss: 1.78760174943084 acc: 0.88\n",
      "loss: 1.8539581287873639 acc: 0.76\n",
      "loss: 1.808785508704287 acc: 0.84\n",
      "loss: 1.8341239946306276 acc: 0.83\n",
      "loss: 1.8257027811104636 acc: 0.84\n",
      "loss: 1.8104816881243635 acc: 0.86\n",
      "loss: 1.7911823444217796 acc: 0.85\n",
      "loss: 1.8081261311793504 acc: 0.87\n",
      "loss: 1.8482486677141057 acc: 0.8\n",
      "loss: 1.8129319803794746 acc: 0.88\n",
      "loss: 1.8745271282994767 acc: 0.78\n",
      "loss: 1.802542452438926 acc: 0.83\n",
      "loss: 1.788638488975788 acc: 0.91\n",
      "loss: 1.7953644654712722 acc: 0.86\n",
      "loss: 1.8580184077270134 acc: 0.85\n",
      "loss: 1.79776144992259 acc: 0.87\n",
      "loss: 1.8139112013249765 acc: 0.84\n",
      "loss: 1.8331732357551687 acc: 0.81\n",
      "loss: 1.8070755900598447 acc: 0.91\n",
      "loss: 1.7782026740497878 acc: 0.87\n",
      "loss: 1.8502762916329354 acc: 0.74\n",
      "loss: 1.8408313805761916 acc: 0.82\n",
      "loss: 1.8182372084104588 acc: 0.85\n",
      "loss: 1.8354148186180428 acc: 0.79\n",
      "loss: 1.8573316854017348 acc: 0.81\n",
      "loss: 1.7960389567790571 acc: 0.86\n",
      "loss: 1.8418160543583284 acc: 0.82\n",
      "loss: 1.8389376750529711 acc: 0.81\n",
      "loss: 1.7699384429584584 acc: 0.89\n",
      "loss: 1.8182099275916854 acc: 0.89\n",
      "loss: 1.8063661004642473 acc: 0.87\n",
      "loss: 1.8425876645929122 acc: 0.86\n",
      "loss: 1.828583891176811 acc: 0.8\n",
      "loss: 1.820236461347251 acc: 0.83\n",
      "loss: 1.83886773794666 acc: 0.77\n",
      "loss: 1.8557276931512063 acc: 0.77\n",
      "loss: 1.8320194067419384 acc: 0.83\n",
      "loss: 1.8400226690248311 acc: 0.84\n",
      "loss: 1.8604661051360123 acc: 0.8\n",
      "loss: 1.8298833835464339 acc: 0.83\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 1.8299\t Accuracy 0.8300\n",
      "loss: 1.7951388954241583 acc: 0.87\n",
      "loss: 1.801675781137329 acc: 0.93\n",
      "loss: 1.7784028811545554 acc: 0.87\n",
      "loss: 1.7967901585643955 acc: 0.87\n",
      "loss: 1.8149985485473612 acc: 0.88\n",
      "loss: 1.8234317581362078 acc: 0.8\n",
      "loss: 1.8122803499701632 acc: 0.82\n",
      "loss: 1.8184467315876338 acc: 0.87\n",
      "loss: 1.8238757942362216 acc: 0.86\n",
      "loss: 1.8391947213270663 acc: 0.8\n",
      "loss: 1.877019801019198 acc: 0.8\n",
      "loss: 1.8643425074281708 acc: 0.76\n",
      "loss: 1.7926747644520469 acc: 0.89\n",
      "loss: 1.8250788255052512 acc: 0.83\n",
      "loss: 1.8189827410579105 acc: 0.8\n",
      "loss: 1.7841904059856515 acc: 0.91\n",
      "loss: 1.8211931210781822 acc: 0.84\n",
      "loss: 1.832526930744241 acc: 0.84\n",
      "loss: 1.853663963365925 acc: 0.83\n",
      "loss: 1.8061653872443284 acc: 0.87\n",
      "loss: 1.864347787242103 acc: 0.88\n",
      "loss: 1.848251868054675 acc: 0.78\n",
      "loss: 1.869959527672803 acc: 0.8\n",
      "loss: 1.8462652435210791 acc: 0.84\n",
      "loss: 1.8235148606691993 acc: 0.86\n",
      "loss: 1.8648770956921263 acc: 0.84\n",
      "loss: 1.8464356636563335 acc: 0.82\n",
      "loss: 1.7909924069653114 acc: 0.9\n",
      "loss: 1.8487966097161124 acc: 0.82\n",
      "loss: 1.8675924016750687 acc: 0.75\n",
      "loss: 1.831478952750768 acc: 0.84\n",
      "loss: 1.801235046384638 acc: 0.84\n",
      "loss: 1.8606589439741168 acc: 0.81\n",
      "loss: 1.8501680280984814 acc: 0.83\n",
      "loss: 1.87209201809005 acc: 0.79\n",
      "loss: 1.836560922280911 acc: 0.81\n",
      "loss: 1.8357639451059515 acc: 0.83\n",
      "loss: 1.8007902170085917 acc: 0.86\n",
      "loss: 1.8508613734743726 acc: 0.87\n",
      "loss: 1.805516674319703 acc: 0.89\n",
      "loss: 1.83544096250102 acc: 0.83\n",
      "loss: 1.8181933720593162 acc: 0.88\n",
      "loss: 1.8303391109051355 acc: 0.86\n",
      "loss: 1.7875835013881776 acc: 0.88\n",
      "loss: 1.8062794119172483 acc: 0.89\n",
      "loss: 1.7891634963831828 acc: 0.91\n",
      "loss: 1.81019027219175 acc: 0.81\n",
      "loss: 1.8383652964086574 acc: 0.84\n",
      "loss: 1.8103647084410621 acc: 0.84\n",
      "loss: 1.8541462301977734 acc: 0.83\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 1.8541\t Accuracy 0.8300\n",
      "loss: 1.8424422737878055 acc: 0.81\n",
      "loss: 1.7924848500412967 acc: 0.88\n",
      "loss: 1.7884783226932006 acc: 0.87\n",
      "loss: 1.8105842922660378 acc: 0.86\n",
      "loss: 1.8704435312550713 acc: 0.76\n",
      "loss: 1.8453560849360622 acc: 0.81\n",
      "loss: 1.8581756415932162 acc: 0.75\n",
      "loss: 1.8316781366911445 acc: 0.86\n",
      "loss: 1.836880357175808 acc: 0.87\n",
      "loss: 1.8227238482562456 acc: 0.82\n",
      "loss: 1.8440338004686527 acc: 0.82\n",
      "loss: 1.8233008618992264 acc: 0.84\n",
      "loss: 1.815196699977339 acc: 0.9\n",
      "loss: 1.8140931871900938 acc: 0.86\n",
      "loss: 1.7943940245805343 acc: 0.84\n",
      "loss: 1.8119856765301712 acc: 0.79\n",
      "loss: 1.8132702179329951 acc: 0.8\n",
      "loss: 1.8462708901312408 acc: 0.83\n",
      "loss: 1.772218750013971 acc: 0.86\n",
      "loss: 1.8107870722755184 acc: 0.83\n",
      "loss: 1.8303881336087733 acc: 0.85\n",
      "loss: 1.7814460848152083 acc: 0.85\n",
      "loss: 1.770944184070064 acc: 0.89\n",
      "loss: 1.8417728039738634 acc: 0.74\n",
      "loss: 1.8160897536912335 acc: 0.83\n",
      "loss: 1.825811952209104 acc: 0.83\n",
      "loss: 1.802356703602946 acc: 0.91\n",
      "loss: 1.8245362086551586 acc: 0.81\n",
      "loss: 1.8382576446152894 acc: 0.8\n",
      "loss: 1.8306520033538967 acc: 0.81\n",
      "loss: 1.8135845744742898 acc: 0.85\n",
      "loss: 1.8250648574066148 acc: 0.81\n",
      "loss: 1.8260464159035308 acc: 0.83\n",
      "loss: 1.821471729959969 acc: 0.85\n",
      "loss: 1.8240585636545756 acc: 0.86\n",
      "loss: 1.8399908374053076 acc: 0.86\n",
      "loss: 1.7845114025210007 acc: 0.87\n",
      "loss: 1.7884710321702142 acc: 0.9\n",
      "loss: 1.7987112262365577 acc: 0.85\n",
      "loss: 1.8109553515440233 acc: 0.85\n",
      "loss: 1.8143912881803044 acc: 0.92\n",
      "loss: 1.8152316800018233 acc: 0.88\n",
      "loss: 1.8083064161538442 acc: 0.85\n",
      "loss: 1.8274666193499598 acc: 0.87\n",
      "loss: 1.8611249269042545 acc: 0.79\n",
      "loss: 1.8299831038242713 acc: 0.82\n",
      "loss: 1.8311850297056411 acc: 0.81\n",
      "loss: 1.814343837202163 acc: 0.9\n",
      "loss: 1.8336168430355086 acc: 0.86\n",
      "loss: 1.7875588683322727 acc: 0.86\n",
      "loss: 1.793661483260039 acc: 0.89\n",
      "loss: 1.808700310243981 acc: 0.87\n",
      "loss: 1.829559187724453 acc: 0.87\n",
      "loss: 1.8267780076115496 acc: 0.92\n",
      "loss: 1.799366422421267 acc: 0.89\n",
      "loss: 1.7570361477154262 acc: 0.85\n",
      "loss: 1.7585848516892697 acc: 0.84\n",
      "loss: 1.8232476704502307 acc: 0.87\n",
      "loss: 1.745571376386097 acc: 0.93\n",
      "loss: 1.7995804769568138 acc: 0.87\n",
      "loss: 1.7981480255805942 acc: 0.83\n",
      "loss: 1.8434207624417018 acc: 0.83\n",
      "loss: 1.8577225014694299 acc: 0.83\n",
      "loss: 1.8635380007818327 acc: 0.86\n",
      "loss: 1.7737324678809891 acc: 0.95\n",
      "loss: 1.7963978449601399 acc: 0.82\n",
      "loss: 1.7564641105514527 acc: 0.91\n",
      "loss: 1.789529717715996 acc: 0.85\n",
      "loss: 1.7853300572767654 acc: 0.87\n",
      "loss: 1.8665749554358144 acc: 0.87\n",
      "loss: 1.8239757001391683 acc: 0.86\n",
      "loss: 1.8190594605839747 acc: 0.79\n",
      "loss: 1.8521752991729983 acc: 0.88\n",
      "loss: 1.8293577354134771 acc: 0.9\n",
      "loss: 1.8501778772572215 acc: 0.81\n",
      "loss: 1.852382305488402 acc: 0.78\n",
      "loss: 1.879411705602948 acc: 0.78\n",
      "loss: 1.8222833838495751 acc: 0.9\n",
      "loss: 1.7934496056747253 acc: 0.91\n",
      "loss: 1.8147415309128163 acc: 0.87\n",
      "loss: 1.7792722839750315 acc: 0.93\n",
      "loss: 1.7436419735943773 acc: 0.94\n",
      "loss: 1.7818042869942732 acc: 0.91\n",
      "loss: 1.77342458011964 acc: 0.87\n",
      "loss: 1.8130672741266871 acc: 0.9\n",
      "loss: 1.8480257976840182 acc: 0.89\n",
      "loss: 1.8463922747753423 acc: 0.89\n",
      "loss: 1.7247301187462114 acc: 0.92\n",
      "loss: 1.7061217993239262 acc: 0.98\n",
      "loss: 1.7388050733929459 acc: 0.94\n",
      "loss: 1.745106808796717 acc: 0.97\n",
      "loss: 1.8033146765070915 acc: 0.88\n",
      "loss: 1.7713137026831733 acc: 0.85\n",
      "loss: 1.7173618578360796 acc: 0.85\n",
      "loss: 1.8051240507271296 acc: 0.94\n",
      "loss: 1.8046965234381538 acc: 0.94\n",
      "loss: 1.8759454099269743 acc: 0.83\n",
      "loss: 1.7094161232522713 acc: 0.97\n",
      "loss: 1.8570494447618524 acc: 0.87\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8207\t Average training accuracy 0.8443\n",
      "Epoch [7]\t Average validation loss 1.8008\t Average validation accuracy 0.8806\n",
      "\n",
      "loss: 1.8202062569668076 acc: 0.83\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 1.8202\t Accuracy 0.8300\n",
      "loss: 1.8335341576813644 acc: 0.86\n",
      "loss: 1.8214239977464346 acc: 0.86\n",
      "loss: 1.825575977208485 acc: 0.79\n",
      "loss: 1.8363927764600338 acc: 0.81\n",
      "loss: 1.8097513422428206 acc: 0.91\n",
      "loss: 1.822935132218761 acc: 0.84\n",
      "loss: 1.8097562724065637 acc: 0.89\n",
      "loss: 1.825277526469907 acc: 0.86\n",
      "loss: 1.825589721845161 acc: 0.77\n",
      "loss: 1.7982541333561135 acc: 0.86\n",
      "loss: 1.7945008886413099 acc: 0.85\n",
      "loss: 1.808664607232385 acc: 0.9\n",
      "loss: 1.809456577647965 acc: 0.9\n",
      "loss: 1.8059630138323508 acc: 0.86\n",
      "loss: 1.864489164299246 acc: 0.81\n",
      "loss: 1.842754417679642 acc: 0.81\n",
      "loss: 1.800409414426518 acc: 0.9\n",
      "loss: 1.8263446363555378 acc: 0.76\n",
      "loss: 1.7900173738993927 acc: 0.89\n",
      "loss: 1.8107335554255226 acc: 0.89\n",
      "loss: 1.845924109857687 acc: 0.79\n",
      "loss: 1.8535201917760835 acc: 0.8\n",
      "loss: 1.8197572224169354 acc: 0.81\n",
      "loss: 1.7882418355011396 acc: 0.88\n",
      "loss: 1.8479502968120085 acc: 0.85\n",
      "loss: 1.835716727626547 acc: 0.84\n",
      "loss: 1.8428832627249867 acc: 0.77\n",
      "loss: 1.8028728083425143 acc: 0.84\n",
      "loss: 1.801236110138201 acc: 0.9\n",
      "loss: 1.8231192059258632 acc: 0.84\n",
      "loss: 1.795507569229037 acc: 0.85\n",
      "loss: 1.825413369717355 acc: 0.83\n",
      "loss: 1.8480383337252257 acc: 0.81\n",
      "loss: 1.8300880065533398 acc: 0.83\n",
      "loss: 1.869007736287808 acc: 0.85\n",
      "loss: 1.7946482715588599 acc: 0.9\n",
      "loss: 1.8302889138744984 acc: 0.82\n",
      "loss: 1.818805691255873 acc: 0.89\n",
      "loss: 1.7950474005614587 acc: 0.89\n",
      "loss: 1.8216191589149857 acc: 0.85\n",
      "loss: 1.8028804589751917 acc: 0.86\n",
      "loss: 1.7869406013592735 acc: 0.87\n",
      "loss: 1.800466630909506 acc: 0.9\n",
      "loss: 1.8468558661633834 acc: 0.83\n",
      "loss: 1.7877371170152134 acc: 0.81\n",
      "loss: 1.8088583841911146 acc: 0.89\n",
      "loss: 1.8286137076921078 acc: 0.83\n",
      "loss: 1.8238772660043554 acc: 0.88\n",
      "loss: 1.7949561578330826 acc: 0.9\n",
      "loss: 1.8725851154352264 acc: 0.8\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 1.8726\t Accuracy 0.8000\n",
      "loss: 1.8663827613290502 acc: 0.82\n",
      "loss: 1.8068792402783973 acc: 0.87\n",
      "loss: 1.7990741071391412 acc: 0.89\n",
      "loss: 1.8114831569012253 acc: 0.85\n",
      "loss: 1.7862038788591252 acc: 0.9\n",
      "loss: 1.7954503108618474 acc: 0.87\n",
      "loss: 1.8125078694138212 acc: 0.83\n",
      "loss: 1.8530221458771075 acc: 0.83\n",
      "loss: 1.8076950537819065 acc: 0.88\n",
      "loss: 1.8261961704511143 acc: 0.82\n",
      "loss: 1.8394259099488486 acc: 0.84\n",
      "loss: 1.839289357549073 acc: 0.86\n",
      "loss: 1.8275182764223792 acc: 0.88\n",
      "loss: 1.8356278093738267 acc: 0.79\n",
      "loss: 1.8361978724032024 acc: 0.84\n",
      "loss: 1.8075112984223767 acc: 0.83\n",
      "loss: 1.840866783517325 acc: 0.81\n",
      "loss: 1.8310165414000472 acc: 0.81\n",
      "loss: 1.8069647329984622 acc: 0.84\n",
      "loss: 1.8334163765183724 acc: 0.84\n",
      "loss: 1.823208729480392 acc: 0.83\n",
      "loss: 1.8047207892164008 acc: 0.82\n",
      "loss: 1.852962517431685 acc: 0.81\n",
      "loss: 1.8361736195362268 acc: 0.8\n",
      "loss: 1.8258993904080283 acc: 0.83\n",
      "loss: 1.8397694303862628 acc: 0.82\n",
      "loss: 1.735094214651905 acc: 0.9\n",
      "loss: 1.834217479133029 acc: 0.81\n",
      "loss: 1.8071678427864826 acc: 0.84\n",
      "loss: 1.7932686602631243 acc: 0.82\n",
      "loss: 1.8168812155081762 acc: 0.81\n",
      "loss: 1.820700904833909 acc: 0.86\n",
      "loss: 1.8314870831551908 acc: 0.85\n",
      "loss: 1.8010986702947382 acc: 0.89\n",
      "loss: 1.839979900050217 acc: 0.77\n",
      "loss: 1.8290022365390712 acc: 0.85\n",
      "loss: 1.781901012821939 acc: 0.91\n",
      "loss: 1.821211985784618 acc: 0.89\n",
      "loss: 1.830091793254393 acc: 0.8\n",
      "loss: 1.7643773248505001 acc: 0.92\n",
      "loss: 1.808867360641924 acc: 0.88\n",
      "loss: 1.8434920892081756 acc: 0.84\n",
      "loss: 1.8099958091132808 acc: 0.85\n",
      "loss: 1.8009691138520614 acc: 0.85\n",
      "loss: 1.81206158841206 acc: 0.88\n",
      "loss: 1.799261101131507 acc: 0.85\n",
      "loss: 1.8177636721965393 acc: 0.82\n",
      "loss: 1.8569046529438145 acc: 0.81\n",
      "loss: 1.8205557908583716 acc: 0.89\n",
      "loss: 1.7901496695728811 acc: 0.81\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 1.7901\t Accuracy 0.8100\n",
      "loss: 1.8022205036714536 acc: 0.86\n",
      "loss: 1.8367417163752149 acc: 0.8\n",
      "loss: 1.8113589001242891 acc: 0.91\n",
      "loss: 1.8473198969143585 acc: 0.8\n",
      "loss: 1.8197888297458749 acc: 0.84\n",
      "loss: 1.8505875598151649 acc: 0.78\n",
      "loss: 1.7925734484257116 acc: 0.86\n",
      "loss: 1.8429128439490523 acc: 0.84\n",
      "loss: 1.8035410425255296 acc: 0.83\n",
      "loss: 1.8252021660441706 acc: 0.79\n",
      "loss: 1.8108621394943827 acc: 0.87\n",
      "loss: 1.8418093630654715 acc: 0.83\n",
      "loss: 1.8336047441903878 acc: 0.86\n",
      "loss: 1.8524409518871903 acc: 0.84\n",
      "loss: 1.811927725892222 acc: 0.82\n",
      "loss: 1.8156448769318092 acc: 0.84\n",
      "loss: 1.8268452848104588 acc: 0.8\n",
      "loss: 1.7975655462271758 acc: 0.82\n",
      "loss: 1.7643260574726776 acc: 0.9\n",
      "loss: 1.8386149238451734 acc: 0.85\n",
      "loss: 1.8022127674758113 acc: 0.82\n",
      "loss: 1.8077357152937545 acc: 0.84\n",
      "loss: 1.7505842530543623 acc: 0.89\n",
      "loss: 1.8473881241683336 acc: 0.81\n",
      "loss: 1.821338918883678 acc: 0.76\n",
      "loss: 1.8261760636887905 acc: 0.84\n",
      "loss: 1.8597473835878142 acc: 0.82\n",
      "loss: 1.8057384194384856 acc: 0.81\n",
      "loss: 1.8150646023035093 acc: 0.86\n",
      "loss: 1.7990958371566481 acc: 0.87\n",
      "loss: 1.798673541971161 acc: 0.84\n",
      "loss: 1.849573563539073 acc: 0.87\n",
      "loss: 1.853709411380183 acc: 0.83\n",
      "loss: 1.802665125609061 acc: 0.94\n",
      "loss: 1.7993092118989795 acc: 0.87\n",
      "loss: 1.8076070054480227 acc: 0.88\n",
      "loss: 1.869054747673785 acc: 0.79\n",
      "loss: 1.8438991730732568 acc: 0.86\n",
      "loss: 1.829483676255296 acc: 0.82\n",
      "loss: 1.80159668447976 acc: 0.88\n",
      "loss: 1.8080662572421065 acc: 0.93\n",
      "loss: 1.8088599071014468 acc: 0.9\n",
      "loss: 1.8106510104450342 acc: 0.87\n",
      "loss: 1.8143892769709895 acc: 0.78\n",
      "loss: 1.781694623093703 acc: 0.9\n",
      "loss: 1.833873721437788 acc: 0.83\n",
      "loss: 1.8180086956741288 acc: 0.86\n",
      "loss: 1.7897695066907446 acc: 0.9\n",
      "loss: 1.8180537852553023 acc: 0.81\n",
      "loss: 1.8338701956450942 acc: 0.82\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 1.8339\t Accuracy 0.8200\n",
      "loss: 1.7584008690912254 acc: 0.9\n",
      "loss: 1.829874285420463 acc: 0.82\n",
      "loss: 1.7890466695131437 acc: 0.92\n",
      "loss: 1.8465246525416772 acc: 0.8\n",
      "loss: 1.8726655669571444 acc: 0.78\n",
      "loss: 1.7832087635872735 acc: 0.84\n",
      "loss: 1.790422164481347 acc: 0.86\n",
      "loss: 1.8051202085647517 acc: 0.84\n",
      "loss: 1.855568928237542 acc: 0.85\n",
      "loss: 1.7846654291999875 acc: 0.87\n",
      "loss: 1.803775315587079 acc: 0.91\n",
      "loss: 1.8139702508265818 acc: 0.85\n",
      "loss: 1.822663098434142 acc: 0.86\n",
      "loss: 1.8393487294281732 acc: 0.79\n",
      "loss: 1.815680769936054 acc: 0.83\n",
      "loss: 1.824882693426676 acc: 0.89\n",
      "loss: 1.859056537188956 acc: 0.75\n",
      "loss: 1.8291216312309262 acc: 0.84\n",
      "loss: 1.85436994900329 acc: 0.86\n",
      "loss: 1.809643993023201 acc: 0.84\n",
      "loss: 1.8294085492621943 acc: 0.86\n",
      "loss: 1.7952556925877454 acc: 0.82\n",
      "loss: 1.8484959448461453 acc: 0.85\n",
      "loss: 1.8217091018567708 acc: 0.86\n",
      "loss: 1.843925753781065 acc: 0.81\n",
      "loss: 1.800052938868119 acc: 0.87\n",
      "loss: 1.8180762863007183 acc: 0.84\n",
      "loss: 1.7961572759784739 acc: 0.89\n",
      "loss: 1.817970271436463 acc: 0.83\n",
      "loss: 1.8614657027246075 acc: 0.88\n",
      "loss: 1.7945846510872983 acc: 0.9\n",
      "loss: 1.8209031081224174 acc: 0.85\n",
      "loss: 1.7945032319081984 acc: 0.86\n",
      "loss: 1.7460359818813003 acc: 0.94\n",
      "loss: 1.858131530317627 acc: 0.84\n",
      "loss: 1.8101968865646532 acc: 0.81\n",
      "loss: 1.8125262875667414 acc: 0.84\n",
      "loss: 1.8146318005357 acc: 0.86\n",
      "loss: 1.8076833647561625 acc: 0.82\n",
      "loss: 1.8494489599076291 acc: 0.78\n",
      "loss: 1.8429169704274708 acc: 0.79\n",
      "loss: 1.8278088407633746 acc: 0.83\n",
      "loss: 1.780265587166975 acc: 0.86\n",
      "loss: 1.8162419209313276 acc: 0.83\n",
      "loss: 1.8584058856515535 acc: 0.81\n",
      "loss: 1.863924365360962 acc: 0.82\n",
      "loss: 1.8165160011983614 acc: 0.87\n",
      "loss: 1.830786637015569 acc: 0.82\n",
      "loss: 1.802951545209039 acc: 0.89\n",
      "loss: 1.8309379893328177 acc: 0.81\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 1.8309\t Accuracy 0.8100\n",
      "loss: 1.8123109104809298 acc: 0.85\n",
      "loss: 1.8156719148189635 acc: 0.86\n",
      "loss: 1.8658926445911659 acc: 0.77\n",
      "loss: 1.8103738455563902 acc: 0.87\n",
      "loss: 1.8315018933181961 acc: 0.88\n",
      "loss: 1.8260910151893146 acc: 0.86\n",
      "loss: 1.7893705383697078 acc: 0.9\n",
      "loss: 1.830959302845577 acc: 0.82\n",
      "loss: 1.7893424644573273 acc: 0.84\n",
      "loss: 1.791939512591081 acc: 0.9\n",
      "loss: 1.8058181834969675 acc: 0.84\n",
      "loss: 1.8265182825136048 acc: 0.8\n",
      "loss: 1.8506421294973325 acc: 0.82\n",
      "loss: 1.8269873253120748 acc: 0.81\n",
      "loss: 1.8346766047892884 acc: 0.86\n",
      "loss: 1.8470750084120875 acc: 0.79\n",
      "loss: 1.772283103884079 acc: 0.85\n",
      "loss: 1.7911934032073322 acc: 0.87\n",
      "loss: 1.852478751538318 acc: 0.81\n",
      "loss: 1.7860011211174165 acc: 0.88\n",
      "loss: 1.8703865952504881 acc: 0.78\n",
      "loss: 1.8268477291312157 acc: 0.84\n",
      "loss: 1.8348543815140446 acc: 0.83\n",
      "loss: 1.816023674601146 acc: 0.87\n",
      "loss: 1.7539571221319374 acc: 0.94\n",
      "loss: 1.8439307456597223 acc: 0.78\n",
      "loss: 1.8405567430159857 acc: 0.8\n",
      "loss: 1.8233407548862155 acc: 0.87\n",
      "loss: 1.8376459226820945 acc: 0.83\n",
      "loss: 1.8265650196900813 acc: 0.82\n",
      "loss: 1.7948074031057004 acc: 0.85\n",
      "loss: 1.8134771463100912 acc: 0.86\n",
      "loss: 1.8460049527034013 acc: 0.79\n",
      "loss: 1.8332649900690732 acc: 0.88\n",
      "loss: 1.8399925088732454 acc: 0.78\n",
      "loss: 1.827218584781065 acc: 0.8\n",
      "loss: 1.8153728188892069 acc: 0.82\n",
      "loss: 1.7873400207297234 acc: 0.88\n",
      "loss: 1.7948737010005973 acc: 0.92\n",
      "loss: 1.8279912854233802 acc: 0.82\n",
      "loss: 1.7978744416108197 acc: 0.81\n",
      "loss: 1.8293151234081588 acc: 0.83\n",
      "loss: 1.8232921888856382 acc: 0.81\n",
      "loss: 1.8097526571025184 acc: 0.89\n",
      "loss: 1.8151581512647579 acc: 0.8\n",
      "loss: 1.8446713649259345 acc: 0.87\n",
      "loss: 1.835994191630109 acc: 0.84\n",
      "loss: 1.8168430649030354 acc: 0.86\n",
      "loss: 1.7730488222350462 acc: 0.9\n",
      "loss: 1.8497686433896428 acc: 0.81\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 1.8498\t Accuracy 0.8100\n",
      "loss: 1.8090432441402706 acc: 0.86\n",
      "loss: 1.7900159962671254 acc: 0.87\n",
      "loss: 1.8072460571683646 acc: 0.88\n",
      "loss: 1.8216774214081546 acc: 0.91\n",
      "loss: 1.8257762432021687 acc: 0.82\n",
      "loss: 1.822630229386846 acc: 0.86\n",
      "loss: 1.8441001311463905 acc: 0.8\n",
      "loss: 1.7732085540835305 acc: 0.93\n",
      "loss: 1.8664594402681995 acc: 0.82\n",
      "loss: 1.7958842221950704 acc: 0.88\n",
      "loss: 1.8291941870825779 acc: 0.84\n",
      "loss: 1.8355454194173806 acc: 0.83\n",
      "loss: 1.8167685606826636 acc: 0.86\n",
      "loss: 1.8402024635886585 acc: 0.81\n",
      "loss: 1.8052863911670332 acc: 0.81\n",
      "loss: 1.8294370876791686 acc: 0.85\n",
      "loss: 1.7877513009190928 acc: 0.87\n",
      "loss: 1.8348688871189365 acc: 0.81\n",
      "loss: 1.821361577582177 acc: 0.88\n",
      "loss: 1.8267866482730801 acc: 0.83\n",
      "loss: 1.80015446259821 acc: 0.87\n",
      "loss: 1.7863228481823157 acc: 0.87\n",
      "loss: 1.7857136390820767 acc: 0.83\n",
      "loss: 1.8328607637485004 acc: 0.79\n",
      "loss: 1.819670421888539 acc: 0.8\n",
      "loss: 1.8487247552177757 acc: 0.8\n",
      "loss: 1.8064514875019848 acc: 0.87\n",
      "loss: 1.8453223500283997 acc: 0.8\n",
      "loss: 1.8642595778874864 acc: 0.77\n",
      "loss: 1.8429655783555048 acc: 0.8\n",
      "loss: 1.8127941476772023 acc: 0.87\n",
      "loss: 1.8222404973137463 acc: 0.82\n",
      "loss: 1.7867484633898496 acc: 0.86\n",
      "loss: 1.835018317868732 acc: 0.79\n",
      "loss: 1.8473128720283831 acc: 0.79\n",
      "loss: 1.8227333128562448 acc: 0.86\n",
      "loss: 1.7913255265898584 acc: 0.91\n",
      "loss: 1.837132042012534 acc: 0.85\n",
      "loss: 1.8065948266976173 acc: 0.8\n",
      "loss: 1.8158407307267808 acc: 0.83\n",
      "loss: 1.8008854424277942 acc: 0.86\n",
      "loss: 1.8184405027439836 acc: 0.83\n",
      "loss: 1.842747696813493 acc: 0.8\n",
      "loss: 1.825241673955461 acc: 0.83\n",
      "loss: 1.8509224009148133 acc: 0.79\n",
      "loss: 1.8658427002522684 acc: 0.77\n",
      "loss: 1.8482169040058865 acc: 0.81\n",
      "loss: 1.8109698539842012 acc: 0.8\n",
      "loss: 1.8510398974225402 acc: 0.8\n",
      "loss: 1.789152238859102 acc: 0.9\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 1.7892\t Accuracy 0.9000\n",
      "loss: 1.8405434849920146 acc: 0.84\n",
      "loss: 1.8363351702174504 acc: 0.85\n",
      "loss: 1.8127417520244518 acc: 0.81\n",
      "loss: 1.8110651555702326 acc: 0.87\n",
      "loss: 1.8163484480345005 acc: 0.86\n",
      "loss: 1.8440998156926398 acc: 0.79\n",
      "loss: 1.8301926568790814 acc: 0.81\n",
      "loss: 1.8349050167611436 acc: 0.84\n",
      "loss: 1.828622807089847 acc: 0.89\n",
      "loss: 1.853292635362254 acc: 0.82\n",
      "loss: 1.8172912666040884 acc: 0.88\n",
      "loss: 1.8323418344505071 acc: 0.88\n",
      "loss: 1.7875187619794235 acc: 0.85\n",
      "loss: 1.8367801216916484 acc: 0.85\n",
      "loss: 1.8183004948680093 acc: 0.83\n",
      "loss: 1.8276927845816022 acc: 0.84\n",
      "loss: 1.8219633501861103 acc: 0.83\n",
      "loss: 1.8048560309793342 acc: 0.83\n",
      "loss: 1.8500486918510322 acc: 0.79\n",
      "loss: 1.8060983890364304 acc: 0.84\n",
      "loss: 1.8217733775533345 acc: 0.87\n",
      "loss: 1.815761644872735 acc: 0.88\n",
      "loss: 1.8491161786728134 acc: 0.81\n",
      "loss: 1.8014368642088687 acc: 0.87\n",
      "loss: 1.8050025137236176 acc: 0.81\n",
      "loss: 1.8013504558487512 acc: 0.85\n",
      "loss: 1.8000729319589974 acc: 0.84\n",
      "loss: 1.8147607206622687 acc: 0.86\n",
      "loss: 1.8370471554359005 acc: 0.85\n",
      "loss: 1.8036256210349655 acc: 0.84\n",
      "loss: 1.7819531902686438 acc: 0.88\n",
      "loss: 1.8397939796491487 acc: 0.85\n",
      "loss: 1.8979596408212476 acc: 0.78\n",
      "loss: 1.8455991884059475 acc: 0.77\n",
      "loss: 1.8058464025177179 acc: 0.87\n",
      "loss: 1.8291345501878948 acc: 0.85\n",
      "loss: 1.8355985241638797 acc: 0.84\n",
      "loss: 1.8479680906234353 acc: 0.81\n",
      "loss: 1.8092337330458605 acc: 0.84\n",
      "loss: 1.7637764963607276 acc: 0.9\n",
      "loss: 1.7964642486492146 acc: 0.81\n",
      "loss: 1.8123836505424533 acc: 0.87\n",
      "loss: 1.831394981572568 acc: 0.88\n",
      "loss: 1.7919838137118551 acc: 0.86\n",
      "loss: 1.81617275523445 acc: 0.84\n",
      "loss: 1.7949132790385178 acc: 0.87\n",
      "loss: 1.7753526834103448 acc: 0.86\n",
      "loss: 1.8891216823533563 acc: 0.77\n",
      "loss: 1.7958599317409696 acc: 0.88\n",
      "loss: 1.8162898608994027 acc: 0.86\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 1.8163\t Accuracy 0.8600\n",
      "loss: 1.8061895625994355 acc: 0.85\n",
      "loss: 1.8178400773614798 acc: 0.85\n",
      "loss: 1.8263238296121542 acc: 0.86\n",
      "loss: 1.7744514513534295 acc: 0.87\n",
      "loss: 1.8606992808447977 acc: 0.79\n",
      "loss: 1.8363196160598236 acc: 0.79\n",
      "loss: 1.8165702008149185 acc: 0.86\n",
      "loss: 1.8181405282347456 acc: 0.86\n",
      "loss: 1.8159069415749862 acc: 0.86\n",
      "loss: 1.8536377587085977 acc: 0.8\n",
      "loss: 1.8459997164113218 acc: 0.83\n",
      "loss: 1.8396373117181661 acc: 0.78\n",
      "loss: 1.793366595106501 acc: 0.89\n",
      "loss: 1.805049125085838 acc: 0.87\n",
      "loss: 1.8618592767079358 acc: 0.76\n",
      "loss: 1.8452551680719478 acc: 0.85\n",
      "loss: 1.8324824107017628 acc: 0.82\n",
      "loss: 1.8169411102757105 acc: 0.81\n",
      "loss: 1.83052692877277 acc: 0.85\n",
      "loss: 1.826454143377475 acc: 0.83\n",
      "loss: 1.8297207704999945 acc: 0.84\n",
      "loss: 1.8222123117780515 acc: 0.79\n",
      "loss: 1.8186383580287262 acc: 0.85\n",
      "loss: 1.826018640119007 acc: 0.87\n",
      "loss: 1.815576429949182 acc: 0.88\n",
      "loss: 1.8221739048950858 acc: 0.84\n",
      "loss: 1.8494376891321473 acc: 0.82\n",
      "loss: 1.8318345305369548 acc: 0.85\n",
      "loss: 1.7989751239713951 acc: 0.87\n",
      "loss: 1.823274473195455 acc: 0.81\n",
      "loss: 1.8319351409199751 acc: 0.8\n",
      "loss: 1.8078041166171954 acc: 0.87\n",
      "loss: 1.805407500590708 acc: 0.86\n",
      "loss: 1.8302330406186196 acc: 0.81\n",
      "loss: 1.8417290899455419 acc: 0.78\n",
      "loss: 1.8345910043208096 acc: 0.81\n",
      "loss: 1.7874875569749598 acc: 0.86\n",
      "loss: 1.8232853966457656 acc: 0.85\n",
      "loss: 1.8287562023503199 acc: 0.82\n",
      "loss: 1.8182736744616639 acc: 0.84\n",
      "loss: 1.8206885380678361 acc: 0.83\n",
      "loss: 1.8489641566251465 acc: 0.81\n",
      "loss: 1.8184237286696214 acc: 0.82\n",
      "loss: 1.8225252767026126 acc: 0.85\n",
      "loss: 1.8335644313352415 acc: 0.88\n",
      "loss: 1.834797488917301 acc: 0.82\n",
      "loss: 1.8438629267979523 acc: 0.85\n",
      "loss: 1.835899782767427 acc: 0.81\n",
      "loss: 1.7984364222223683 acc: 0.82\n",
      "loss: 1.8218715118986564 acc: 0.87\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 1.8219\t Accuracy 0.8700\n",
      "loss: 1.8206279186649559 acc: 0.83\n",
      "loss: 1.8053030543433206 acc: 0.9\n",
      "loss: 1.809532080535195 acc: 0.84\n",
      "loss: 1.8532959183692486 acc: 0.81\n",
      "loss: 1.8491796684884807 acc: 0.86\n",
      "loss: 1.7986384351093136 acc: 0.92\n",
      "loss: 1.822098324933368 acc: 0.85\n",
      "loss: 1.8341552485087826 acc: 0.84\n",
      "loss: 1.82297236816799 acc: 0.86\n",
      "loss: 1.8586815637252472 acc: 0.84\n",
      "loss: 1.8457524137010006 acc: 0.8\n",
      "loss: 1.8280991029403284 acc: 0.83\n",
      "loss: 1.8270004157223538 acc: 0.82\n",
      "loss: 1.8147022175248793 acc: 0.83\n",
      "loss: 1.822014394317112 acc: 0.83\n",
      "loss: 1.8017695094908535 acc: 0.9\n",
      "loss: 1.7946632476398179 acc: 0.85\n",
      "loss: 1.8554951613840494 acc: 0.84\n",
      "loss: 1.8095356343201394 acc: 0.89\n",
      "loss: 1.8227101987728505 acc: 0.85\n",
      "loss: 1.8439265000156706 acc: 0.79\n",
      "loss: 1.8139939039166249 acc: 0.86\n",
      "loss: 1.8090916197864089 acc: 0.89\n",
      "loss: 1.8097954269853576 acc: 0.86\n",
      "loss: 1.8030840132855417 acc: 0.84\n",
      "loss: 1.776461439698836 acc: 0.92\n",
      "loss: 1.823058609778604 acc: 0.87\n",
      "loss: 1.801073335160539 acc: 0.83\n",
      "loss: 1.8211200571094839 acc: 0.86\n",
      "loss: 1.8492945301856247 acc: 0.83\n",
      "loss: 1.8215680099170712 acc: 0.84\n",
      "loss: 1.8000361503987319 acc: 0.9\n",
      "loss: 1.8295198183120434 acc: 0.82\n",
      "loss: 1.8387362440751298 acc: 0.82\n",
      "loss: 1.8393302941682776 acc: 0.79\n",
      "loss: 1.7937663033399898 acc: 0.87\n",
      "loss: 1.8201617871909743 acc: 0.87\n",
      "loss: 1.7929875295249382 acc: 0.84\n",
      "loss: 1.8703754427269592 acc: 0.77\n",
      "loss: 1.7964241468858022 acc: 0.85\n",
      "loss: 1.8257173210865643 acc: 0.91\n",
      "loss: 1.8330703035926823 acc: 0.79\n",
      "loss: 1.8107873078621362 acc: 0.85\n",
      "loss: 1.817273041760169 acc: 0.84\n",
      "loss: 1.8403242147547982 acc: 0.83\n",
      "loss: 1.8076546350869225 acc: 0.85\n",
      "loss: 1.8618810339497047 acc: 0.85\n",
      "loss: 1.8149757023748299 acc: 0.83\n",
      "loss: 1.8159337131106688 acc: 0.88\n",
      "loss: 1.8007633500539562 acc: 0.89\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 1.8008\t Accuracy 0.8900\n",
      "loss: 1.79614849591516 acc: 0.88\n",
      "loss: 1.8162694335726997 acc: 0.82\n",
      "loss: 1.8166693921381611 acc: 0.85\n",
      "loss: 1.838391471599619 acc: 0.82\n",
      "loss: 1.819594321365457 acc: 0.83\n",
      "loss: 1.8677431948861312 acc: 0.89\n",
      "loss: 1.8213926723484268 acc: 0.86\n",
      "loss: 1.8189497833562722 acc: 0.85\n",
      "loss: 1.8186685401170488 acc: 0.86\n",
      "loss: 1.8246977932139197 acc: 0.85\n",
      "loss: 1.780624829397133 acc: 0.88\n",
      "loss: 1.7966912894330003 acc: 0.94\n",
      "loss: 1.8255473909704074 acc: 0.85\n",
      "loss: 1.8706567919497485 acc: 0.82\n",
      "loss: 1.8168225453695266 acc: 0.85\n",
      "loss: 1.8143301827717904 acc: 0.81\n",
      "loss: 1.817102549250337 acc: 0.84\n",
      "loss: 1.793415441346009 acc: 0.85\n",
      "loss: 1.8086086019005159 acc: 0.85\n",
      "loss: 1.8249974272276974 acc: 0.77\n",
      "loss: 1.8300583519924318 acc: 0.78\n",
      "loss: 1.7703067367200815 acc: 0.87\n",
      "loss: 1.7636472103312764 acc: 0.89\n",
      "loss: 1.8057633949778205 acc: 0.85\n",
      "loss: 1.832121903083494 acc: 0.86\n",
      "loss: 1.84714080560945 acc: 0.82\n",
      "loss: 1.82215227544526 acc: 0.84\n",
      "loss: 1.8143824495375134 acc: 0.86\n",
      "loss: 1.8054036836828136 acc: 0.79\n",
      "loss: 1.817683210253829 acc: 0.84\n",
      "loss: 1.831662803522469 acc: 0.89\n",
      "loss: 1.857075585205405 acc: 0.78\n",
      "loss: 1.8526494820416497 acc: 0.84\n",
      "loss: 1.8209531057417139 acc: 0.89\n",
      "loss: 1.8332523305451034 acc: 0.85\n",
      "loss: 1.8413610302521837 acc: 0.82\n",
      "loss: 1.8413162086056127 acc: 0.8\n",
      "loss: 1.8794721677873614 acc: 0.86\n",
      "loss: 1.8036620888965857 acc: 0.88\n",
      "loss: 1.8583358074876601 acc: 0.83\n",
      "loss: 1.8180179920771418 acc: 0.82\n",
      "loss: 1.8457020309677992 acc: 0.83\n",
      "loss: 1.848879181885809 acc: 0.81\n",
      "loss: 1.8180314849996781 acc: 0.85\n",
      "loss: 1.8235170988578755 acc: 0.83\n",
      "loss: 1.8326641120295764 acc: 0.82\n",
      "loss: 1.8483655065928888 acc: 0.82\n",
      "loss: 1.8298954285213833 acc: 0.8\n",
      "loss: 1.8197860837527733 acc: 0.86\n",
      "loss: 1.8453871641615212 acc: 0.82\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 1.8454\t Accuracy 0.8200\n",
      "loss: 1.807093359178381 acc: 0.89\n",
      "loss: 1.8283894887727794 acc: 0.83\n",
      "loss: 1.830381393061572 acc: 0.83\n",
      "loss: 1.8118514662510492 acc: 0.88\n",
      "loss: 1.8424304192423642 acc: 0.84\n",
      "loss: 1.8155042932376009 acc: 0.85\n",
      "loss: 1.8323913912822878 acc: 0.85\n",
      "loss: 1.7994380674180235 acc: 0.88\n",
      "loss: 1.8398634608385587 acc: 0.83\n",
      "loss: 1.8008889564755848 acc: 0.9\n",
      "loss: 1.7892585905724434 acc: 0.85\n",
      "loss: 1.826396067045208 acc: 0.82\n",
      "loss: 1.830969593772047 acc: 0.82\n",
      "loss: 1.7983814886498575 acc: 0.83\n",
      "loss: 1.7913868808133082 acc: 0.85\n",
      "loss: 1.8416595344597884 acc: 0.88\n",
      "loss: 1.8572632930870743 acc: 0.77\n",
      "loss: 1.774965844554774 acc: 0.87\n",
      "loss: 1.8183991305394196 acc: 0.84\n",
      "loss: 1.8218769600668645 acc: 0.89\n",
      "loss: 1.8036949526576467 acc: 0.88\n",
      "loss: 1.8067423741750732 acc: 0.91\n",
      "loss: 1.852803485427068 acc: 0.77\n",
      "loss: 1.808896880219742 acc: 0.86\n",
      "loss: 1.80343851788644 acc: 0.85\n",
      "loss: 1.8305627277168295 acc: 0.84\n",
      "loss: 1.846518635966754 acc: 0.78\n",
      "loss: 1.835139081338028 acc: 0.84\n",
      "loss: 1.8114886277603688 acc: 0.87\n",
      "loss: 1.8063155486431375 acc: 0.87\n",
      "loss: 1.8086250034108764 acc: 0.87\n",
      "loss: 1.8372759893983923 acc: 0.83\n",
      "loss: 1.803405703437802 acc: 0.89\n",
      "loss: 1.8131942537570915 acc: 0.83\n",
      "loss: 1.8154747412020753 acc: 0.86\n",
      "loss: 1.7836038514907988 acc: 0.88\n",
      "loss: 1.8255819133748803 acc: 0.82\n",
      "loss: 1.8007407200406471 acc: 0.88\n",
      "loss: 1.82587987330034 acc: 0.81\n",
      "loss: 1.8083224634934356 acc: 0.81\n",
      "loss: 1.7737184264392587 acc: 0.87\n",
      "loss: 1.8377130901361607 acc: 0.83\n",
      "loss: 1.8399154792589132 acc: 0.82\n",
      "loss: 1.8008943129729753 acc: 0.83\n",
      "loss: 1.79760100039192 acc: 0.87\n",
      "loss: 1.7958769645261825 acc: 0.9\n",
      "loss: 1.8314312485067925 acc: 0.81\n",
      "loss: 1.8204866347923232 acc: 0.85\n",
      "loss: 1.8592541888252168 acc: 0.8\n",
      "loss: 1.794074725330119 acc: 0.87\n",
      "loss: 1.7894302818601262 acc: 0.88\n",
      "loss: 1.8009201479254306 acc: 0.88\n",
      "loss: 1.8335251324700081 acc: 0.86\n",
      "loss: 1.8236332344134996 acc: 0.89\n",
      "loss: 1.8006458648330295 acc: 0.8\n",
      "loss: 1.7520195452580263 acc: 0.83\n",
      "loss: 1.76382750262274 acc: 0.84\n",
      "loss: 1.8341847466757195 acc: 0.81\n",
      "loss: 1.7526896512924393 acc: 0.89\n",
      "loss: 1.7947242965013694 acc: 0.87\n",
      "loss: 1.792283645509982 acc: 0.85\n",
      "loss: 1.8504677769101228 acc: 0.82\n",
      "loss: 1.856818257074695 acc: 0.82\n",
      "loss: 1.8661428582755126 acc: 0.86\n",
      "loss: 1.7717610955163963 acc: 0.94\n",
      "loss: 1.7904477158576213 acc: 0.83\n",
      "loss: 1.7473925407096795 acc: 0.9\n",
      "loss: 1.7774371338455857 acc: 0.87\n",
      "loss: 1.7820214820585323 acc: 0.88\n",
      "loss: 1.8528940197036479 acc: 0.88\n",
      "loss: 1.8198222695791404 acc: 0.87\n",
      "loss: 1.82412149900786 acc: 0.74\n",
      "loss: 1.8479630917009642 acc: 0.89\n",
      "loss: 1.8301125518997288 acc: 0.86\n",
      "loss: 1.849493951363929 acc: 0.81\n",
      "loss: 1.849683220515602 acc: 0.77\n",
      "loss: 1.8700015120933051 acc: 0.81\n",
      "loss: 1.819982577228113 acc: 0.89\n",
      "loss: 1.7956938132801423 acc: 0.88\n",
      "loss: 1.8157538680517005 acc: 0.85\n",
      "loss: 1.775126750100004 acc: 0.94\n",
      "loss: 1.7440083149052603 acc: 0.91\n",
      "loss: 1.783277978251123 acc: 0.89\n",
      "loss: 1.7782467889179332 acc: 0.86\n",
      "loss: 1.8109927412272488 acc: 0.87\n",
      "loss: 1.8526119074965701 acc: 0.89\n",
      "loss: 1.840867269255441 acc: 0.89\n",
      "loss: 1.7348547276602917 acc: 0.91\n",
      "loss: 1.7044422871342704 acc: 0.98\n",
      "loss: 1.7369339264212196 acc: 0.96\n",
      "loss: 1.742644703941903 acc: 0.97\n",
      "loss: 1.7968364573288138 acc: 0.88\n",
      "loss: 1.7845263058345202 acc: 0.85\n",
      "loss: 1.7242506139656695 acc: 0.86\n",
      "loss: 1.805042064249183 acc: 0.89\n",
      "loss: 1.8009991024312 acc: 0.91\n",
      "loss: 1.8776660233042441 acc: 0.79\n",
      "loss: 1.7114146720770285 acc: 0.94\n",
      "loss: 1.847331065153147 acc: 0.88\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8208\t Average training accuracy 0.8432\n",
      "Epoch [8]\t Average validation loss 1.8000\t Average validation accuracy 0.8702\n",
      "\n",
      "loss: 1.8406666409646797 acc: 0.85\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 1.8407\t Accuracy 0.8500\n",
      "loss: 1.8078498444838809 acc: 0.82\n",
      "loss: 1.8168313137120933 acc: 0.84\n",
      "loss: 1.7775291420293586 acc: 0.87\n",
      "loss: 1.8338441612417673 acc: 0.8\n",
      "loss: 1.853174341982814 acc: 0.8\n",
      "loss: 1.8134323052259176 acc: 0.87\n",
      "loss: 1.797097822378972 acc: 0.89\n",
      "loss: 1.823435289700432 acc: 0.89\n",
      "loss: 1.8342188284161045 acc: 0.88\n",
      "loss: 1.8533302367302 acc: 0.78\n",
      "loss: 1.8184568636575005 acc: 0.85\n",
      "loss: 1.7911406482788153 acc: 0.84\n",
      "loss: 1.8140298508954686 acc: 0.84\n",
      "loss: 1.8457655853462904 acc: 0.82\n",
      "loss: 1.7797582967783736 acc: 0.91\n",
      "loss: 1.8362488118240095 acc: 0.86\n",
      "loss: 1.8111577780744952 acc: 0.83\n",
      "loss: 1.7886031554222759 acc: 0.9\n",
      "loss: 1.8322680855560207 acc: 0.78\n",
      "loss: 1.8385430618207017 acc: 0.82\n",
      "loss: 1.8136218947844742 acc: 0.81\n",
      "loss: 1.8255987765239814 acc: 0.87\n",
      "loss: 1.8145257944245676 acc: 0.84\n",
      "loss: 1.8160677426413978 acc: 0.86\n",
      "loss: 1.8489294847953948 acc: 0.82\n",
      "loss: 1.860982207364584 acc: 0.81\n",
      "loss: 1.83513695688138 acc: 0.85\n",
      "loss: 1.791371618251301 acc: 0.86\n",
      "loss: 1.7898624526250173 acc: 0.88\n",
      "loss: 1.8605684041132242 acc: 0.77\n",
      "loss: 1.7923783135522589 acc: 0.89\n",
      "loss: 1.7696418736289354 acc: 0.91\n",
      "loss: 1.843380107975139 acc: 0.85\n",
      "loss: 1.8413622534079663 acc: 0.81\n",
      "loss: 1.7977639596845814 acc: 0.89\n",
      "loss: 1.7831898527093437 acc: 0.85\n",
      "loss: 1.813418727300025 acc: 0.88\n",
      "loss: 1.7808079708892977 acc: 0.89\n",
      "loss: 1.8067133838925833 acc: 0.85\n",
      "loss: 1.7900798980413684 acc: 0.88\n",
      "loss: 1.8225533589362775 acc: 0.9\n",
      "loss: 1.8438606489635518 acc: 0.77\n",
      "loss: 1.808930453896233 acc: 0.8\n",
      "loss: 1.8119286148144371 acc: 0.88\n",
      "loss: 1.755748313201351 acc: 0.89\n",
      "loss: 1.8706521942736365 acc: 0.82\n",
      "loss: 1.814978844755652 acc: 0.87\n",
      "loss: 1.7987589112279785 acc: 0.85\n",
      "loss: 1.816275341112912 acc: 0.9\n",
      "loss: 1.796838828264948 acc: 0.88\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 1.7968\t Accuracy 0.8800\n",
      "loss: 1.8251119601523462 acc: 0.83\n",
      "loss: 1.8347720894102602 acc: 0.92\n",
      "loss: 1.8098120848095507 acc: 0.85\n",
      "loss: 1.8322075195865097 acc: 0.82\n",
      "loss: 1.8722221147391205 acc: 0.78\n",
      "loss: 1.802410690986319 acc: 0.89\n",
      "loss: 1.794908053360839 acc: 0.91\n",
      "loss: 1.8473879717222454 acc: 0.83\n",
      "loss: 1.815404275156904 acc: 0.87\n",
      "loss: 1.8191228541043751 acc: 0.84\n",
      "loss: 1.8183969067555852 acc: 0.86\n",
      "loss: 1.7810944427385402 acc: 0.85\n",
      "loss: 1.8504019343105957 acc: 0.77\n",
      "loss: 1.836806209776249 acc: 0.82\n",
      "loss: 1.7797978700297308 acc: 0.89\n",
      "loss: 1.8415690763276213 acc: 0.78\n",
      "loss: 1.8324798983098947 acc: 0.84\n",
      "loss: 1.8254694383569108 acc: 0.8\n",
      "loss: 1.8088765282486536 acc: 0.86\n",
      "loss: 1.852166643016248 acc: 0.78\n",
      "loss: 1.8220053703329415 acc: 0.82\n",
      "loss: 1.7393152734546293 acc: 0.9\n",
      "loss: 1.8542157965746413 acc: 0.83\n",
      "loss: 1.8155235764307023 acc: 0.89\n",
      "loss: 1.8442672537288205 acc: 0.81\n",
      "loss: 1.83364419404636 acc: 0.82\n",
      "loss: 1.8176450183875192 acc: 0.86\n",
      "loss: 1.7930261475574187 acc: 0.84\n",
      "loss: 1.8388126797338489 acc: 0.82\n",
      "loss: 1.8385635434326975 acc: 0.74\n",
      "loss: 1.814139488498765 acc: 0.86\n",
      "loss: 1.7882808903342404 acc: 0.88\n",
      "loss: 1.8297995309906674 acc: 0.83\n",
      "loss: 1.8020842754622137 acc: 0.84\n",
      "loss: 1.8213345389329847 acc: 0.88\n",
      "loss: 1.82006672325979 acc: 0.88\n",
      "loss: 1.8472047063308148 acc: 0.81\n",
      "loss: 1.824090489787765 acc: 0.83\n",
      "loss: 1.8275137669143038 acc: 0.82\n",
      "loss: 1.8318000696688284 acc: 0.85\n",
      "loss: 1.8415630052548337 acc: 0.79\n",
      "loss: 1.7996095389621811 acc: 0.88\n",
      "loss: 1.8112979556144095 acc: 0.88\n",
      "loss: 1.7999122283818578 acc: 0.86\n",
      "loss: 1.775995797424233 acc: 0.91\n",
      "loss: 1.8218650729044041 acc: 0.84\n",
      "loss: 1.8115944459447493 acc: 0.83\n",
      "loss: 1.805850490451097 acc: 0.85\n",
      "loss: 1.8406685310210522 acc: 0.8\n",
      "loss: 1.8798658600166085 acc: 0.83\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 1.8799\t Accuracy 0.8300\n",
      "loss: 1.8320317168370837 acc: 0.86\n",
      "loss: 1.796505067365598 acc: 0.84\n",
      "loss: 1.7722087561650257 acc: 0.91\n",
      "loss: 1.8026102760687466 acc: 0.86\n",
      "loss: 1.8256682814987326 acc: 0.83\n",
      "loss: 1.7867959426218016 acc: 0.88\n",
      "loss: 1.7848889335305569 acc: 0.88\n",
      "loss: 1.7820688205019184 acc: 0.89\n",
      "loss: 1.8580464531189975 acc: 0.78\n",
      "loss: 1.7914999262428128 acc: 0.91\n",
      "loss: 1.836729609954447 acc: 0.78\n",
      "loss: 1.8611818826776654 acc: 0.79\n",
      "loss: 1.792799610463373 acc: 0.87\n",
      "loss: 1.835115004735892 acc: 0.84\n",
      "loss: 1.8089087632527197 acc: 0.8\n",
      "loss: 1.7864148003702598 acc: 0.86\n",
      "loss: 1.8654720981596356 acc: 0.81\n",
      "loss: 1.8294853062813317 acc: 0.82\n",
      "loss: 1.848180204180479 acc: 0.84\n",
      "loss: 1.8401684106122556 acc: 0.85\n",
      "loss: 1.8216327639981273 acc: 0.82\n",
      "loss: 1.821835635506528 acc: 0.86\n",
      "loss: 1.774889929269194 acc: 0.91\n",
      "loss: 1.8645218421461889 acc: 0.85\n",
      "loss: 1.8121757357263413 acc: 0.87\n",
      "loss: 1.814835301633438 acc: 0.81\n",
      "loss: 1.8510056896225666 acc: 0.82\n",
      "loss: 1.7696955398945033 acc: 0.89\n",
      "loss: 1.7863670167870196 acc: 0.88\n",
      "loss: 1.840186469365911 acc: 0.81\n",
      "loss: 1.8317582571563475 acc: 0.86\n",
      "loss: 1.8264032443750438 acc: 0.85\n",
      "loss: 1.8372943284849041 acc: 0.82\n",
      "loss: 1.8362861700625503 acc: 0.84\n",
      "loss: 1.8515581662983964 acc: 0.82\n",
      "loss: 1.7644743787164767 acc: 0.92\n",
      "loss: 1.760567832124143 acc: 0.88\n",
      "loss: 1.8404658631394115 acc: 0.84\n",
      "loss: 1.7913855041905506 acc: 0.85\n",
      "loss: 1.8545833019978613 acc: 0.8\n",
      "loss: 1.7839522913318278 acc: 0.87\n",
      "loss: 1.79338704238767 acc: 0.88\n",
      "loss: 1.8605441580771955 acc: 0.78\n",
      "loss: 1.8383299198407657 acc: 0.76\n",
      "loss: 1.8398708243439736 acc: 0.82\n",
      "loss: 1.790779719133486 acc: 0.88\n",
      "loss: 1.8442561612292585 acc: 0.81\n",
      "loss: 1.7837139475371324 acc: 0.86\n",
      "loss: 1.8065690815183266 acc: 0.86\n",
      "loss: 1.8183409733843447 acc: 0.87\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 1.8183\t Accuracy 0.8700\n",
      "loss: 1.8183296979978811 acc: 0.83\n",
      "loss: 1.784302766138463 acc: 0.89\n",
      "loss: 1.8040381484644912 acc: 0.86\n",
      "loss: 1.8444310629546026 acc: 0.81\n",
      "loss: 1.834721004525488 acc: 0.85\n",
      "loss: 1.8647584457550475 acc: 0.8\n",
      "loss: 1.8161929289198175 acc: 0.9\n",
      "loss: 1.7922592204293941 acc: 0.84\n",
      "loss: 1.8291892259494742 acc: 0.84\n",
      "loss: 1.83379158651049 acc: 0.82\n",
      "loss: 1.8220959810559874 acc: 0.8\n",
      "loss: 1.8240626554107278 acc: 0.86\n",
      "loss: 1.8522048906142086 acc: 0.86\n",
      "loss: 1.7539669652710181 acc: 0.91\n",
      "loss: 1.783147655969174 acc: 0.85\n",
      "loss: 1.820583533051154 acc: 0.87\n",
      "loss: 1.8222880900892826 acc: 0.83\n",
      "loss: 1.847184446604268 acc: 0.82\n",
      "loss: 1.8484744591000464 acc: 0.73\n",
      "loss: 1.805741324574843 acc: 0.8\n",
      "loss: 1.7871869501223403 acc: 0.92\n",
      "loss: 1.8055137536098655 acc: 0.88\n",
      "loss: 1.8014603509678444 acc: 0.85\n",
      "loss: 1.8539725822814024 acc: 0.8\n",
      "loss: 1.8344128374970061 acc: 0.85\n",
      "loss: 1.8432501855105394 acc: 0.87\n",
      "loss: 1.8125276080324084 acc: 0.85\n",
      "loss: 1.9093525962672842 acc: 0.74\n",
      "loss: 1.803173593495395 acc: 0.86\n",
      "loss: 1.810481146982007 acc: 0.86\n",
      "loss: 1.80443903736376 acc: 0.83\n",
      "loss: 1.8327900173264564 acc: 0.85\n",
      "loss: 1.850910876765721 acc: 0.76\n",
      "loss: 1.825141077572681 acc: 0.82\n",
      "loss: 1.7837395498621504 acc: 0.94\n",
      "loss: 1.8261066075094943 acc: 0.82\n",
      "loss: 1.8273569148933066 acc: 0.85\n",
      "loss: 1.8263523964735713 acc: 0.87\n",
      "loss: 1.818914164970844 acc: 0.86\n",
      "loss: 1.851925894036153 acc: 0.86\n",
      "loss: 1.7987746516545269 acc: 0.87\n",
      "loss: 1.825819648729216 acc: 0.88\n",
      "loss: 1.8099772114250374 acc: 0.88\n",
      "loss: 1.826536615050305 acc: 0.82\n",
      "loss: 1.8215600159707415 acc: 0.89\n",
      "loss: 1.8091875064398024 acc: 0.84\n",
      "loss: 1.7949113328173647 acc: 0.87\n",
      "loss: 1.8278491580380072 acc: 0.77\n",
      "loss: 1.8075337911916063 acc: 0.82\n",
      "loss: 1.823332936733063 acc: 0.84\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 1.8233\t Accuracy 0.8400\n",
      "loss: 1.8099111496163156 acc: 0.89\n",
      "loss: 1.8262606001280575 acc: 0.77\n",
      "loss: 1.7864112906315475 acc: 0.86\n",
      "loss: 1.7785811023703146 acc: 0.88\n",
      "loss: 1.86676815800013 acc: 0.82\n",
      "loss: 1.7823738877513637 acc: 0.89\n",
      "loss: 1.8212556396348234 acc: 0.84\n",
      "loss: 1.8138294017167544 acc: 0.85\n",
      "loss: 1.7899104732111615 acc: 0.83\n",
      "loss: 1.8066948486179135 acc: 0.84\n",
      "loss: 1.8196983236774 acc: 0.85\n",
      "loss: 1.7816205366681115 acc: 0.85\n",
      "loss: 1.8068133556450914 acc: 0.86\n",
      "loss: 1.859303575824377 acc: 0.8\n",
      "loss: 1.811899640598792 acc: 0.82\n",
      "loss: 1.8147371447941116 acc: 0.84\n",
      "loss: 1.8387641404082202 acc: 0.81\n",
      "loss: 1.8327351772110578 acc: 0.85\n",
      "loss: 1.8370853691243374 acc: 0.88\n",
      "loss: 1.7988996604281147 acc: 0.86\n",
      "loss: 1.8017146231231451 acc: 0.82\n",
      "loss: 1.8195096714686394 acc: 0.84\n",
      "loss: 1.8125588636111976 acc: 0.82\n",
      "loss: 1.8583811737126608 acc: 0.86\n",
      "loss: 1.816554032005204 acc: 0.86\n",
      "loss: 1.809085572321638 acc: 0.88\n",
      "loss: 1.8197535605997064 acc: 0.88\n",
      "loss: 1.8559343640280728 acc: 0.84\n",
      "loss: 1.7891511327385121 acc: 0.87\n",
      "loss: 1.7944091063054395 acc: 0.85\n",
      "loss: 1.8251108719598994 acc: 0.8\n",
      "loss: 1.8331825178861119 acc: 0.82\n",
      "loss: 1.79313919703356 acc: 0.87\n",
      "loss: 1.837125915637271 acc: 0.82\n",
      "loss: 1.8046877098787075 acc: 0.86\n",
      "loss: 1.7950098470534561 acc: 0.88\n",
      "loss: 1.8373184105766271 acc: 0.84\n",
      "loss: 1.8027421363317169 acc: 0.84\n",
      "loss: 1.7998862445806671 acc: 0.81\n",
      "loss: 1.826387455183549 acc: 0.84\n",
      "loss: 1.8316370610963046 acc: 0.83\n",
      "loss: 1.8252173122526119 acc: 0.86\n",
      "loss: 1.8336214414918675 acc: 0.9\n",
      "loss: 1.8356050504051424 acc: 0.83\n",
      "loss: 1.8544215575992582 acc: 0.75\n",
      "loss: 1.8326357343570532 acc: 0.86\n",
      "loss: 1.7911909191224473 acc: 0.84\n",
      "loss: 1.839036286462794 acc: 0.79\n",
      "loss: 1.8226105451326715 acc: 0.85\n",
      "loss: 1.8327618577274092 acc: 0.83\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 1.8328\t Accuracy 0.8300\n",
      "loss: 1.8567232258598836 acc: 0.79\n",
      "loss: 1.7958503820567537 acc: 0.89\n",
      "loss: 1.8129411500497767 acc: 0.83\n",
      "loss: 1.8376986806399207 acc: 0.84\n",
      "loss: 1.8337265421591888 acc: 0.83\n",
      "loss: 1.8123899266726136 acc: 0.82\n",
      "loss: 1.7991129415109883 acc: 0.87\n",
      "loss: 1.8142909424068403 acc: 0.91\n",
      "loss: 1.842869264735361 acc: 0.86\n",
      "loss: 1.8447685239470066 acc: 0.84\n",
      "loss: 1.7834913262345138 acc: 0.86\n",
      "loss: 1.8407375990212735 acc: 0.83\n",
      "loss: 1.8302545034067645 acc: 0.83\n",
      "loss: 1.7953397227959649 acc: 0.88\n",
      "loss: 1.809755827081419 acc: 0.85\n",
      "loss: 1.7791610417137267 acc: 0.9\n",
      "loss: 1.7951055979347401 acc: 0.89\n",
      "loss: 1.8125011296970188 acc: 0.83\n",
      "loss: 1.8110191077095086 acc: 0.77\n",
      "loss: 1.7919659811121307 acc: 0.9\n",
      "loss: 1.7991860811402858 acc: 0.8\n",
      "loss: 1.8528426064505794 acc: 0.82\n",
      "loss: 1.8166263450352957 acc: 0.85\n",
      "loss: 1.7818756278684742 acc: 0.89\n",
      "loss: 1.817581404669787 acc: 0.82\n",
      "loss: 1.8219625300420221 acc: 0.81\n",
      "loss: 1.80639204057645 acc: 0.9\n",
      "loss: 1.861023981059415 acc: 0.83\n",
      "loss: 1.8164918673162251 acc: 0.84\n",
      "loss: 1.8425458253675995 acc: 0.79\n",
      "loss: 1.8935162595133184 acc: 0.73\n",
      "loss: 1.7994611107272882 acc: 0.94\n",
      "loss: 1.849275684428868 acc: 0.8\n",
      "loss: 1.8308912672512516 acc: 0.82\n",
      "loss: 1.812067386189488 acc: 0.88\n",
      "loss: 1.8429902799113222 acc: 0.89\n",
      "loss: 1.881575782531894 acc: 0.8\n",
      "loss: 1.8467169512709125 acc: 0.81\n",
      "loss: 1.817005451967411 acc: 0.84\n",
      "loss: 1.8038475890932708 acc: 0.78\n",
      "loss: 1.8303827073475352 acc: 0.81\n",
      "loss: 1.8771249493309659 acc: 0.75\n",
      "loss: 1.771420669533013 acc: 0.87\n",
      "loss: 1.8451983805366337 acc: 0.84\n",
      "loss: 1.843140769505093 acc: 0.85\n",
      "loss: 1.8055946295761478 acc: 0.89\n",
      "loss: 1.8576485142649326 acc: 0.81\n",
      "loss: 1.8023423767381614 acc: 0.89\n",
      "loss: 1.8409781373723382 acc: 0.83\n",
      "loss: 1.78667823946169 acc: 0.88\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 1.7867\t Accuracy 0.8800\n",
      "loss: 1.8030917307167817 acc: 0.85\n",
      "loss: 1.8429619783673448 acc: 0.82\n",
      "loss: 1.8062273586458966 acc: 0.84\n",
      "loss: 1.8313702047247415 acc: 0.84\n",
      "loss: 1.8196849786286093 acc: 0.88\n",
      "loss: 1.828061331752594 acc: 0.87\n",
      "loss: 1.857117404382603 acc: 0.84\n",
      "loss: 1.813500600569114 acc: 0.9\n",
      "loss: 1.8345420955794884 acc: 0.8\n",
      "loss: 1.855182683643589 acc: 0.76\n",
      "loss: 1.8076758373825441 acc: 0.86\n",
      "loss: 1.857197807845989 acc: 0.83\n",
      "loss: 1.7930798719007603 acc: 0.89\n",
      "loss: 1.8593943938126403 acc: 0.82\n",
      "loss: 1.778266520212944 acc: 0.86\n",
      "loss: 1.8199777798831343 acc: 0.85\n",
      "loss: 1.8326230854342986 acc: 0.81\n",
      "loss: 1.8313769659203623 acc: 0.81\n",
      "loss: 1.8586694991256525 acc: 0.85\n",
      "loss: 1.8475324181008204 acc: 0.83\n",
      "loss: 1.7638384687783057 acc: 0.88\n",
      "loss: 1.827702123839833 acc: 0.8\n",
      "loss: 1.8290073387134356 acc: 0.85\n",
      "loss: 1.8515970524488652 acc: 0.83\n",
      "loss: 1.8055770899114085 acc: 0.82\n",
      "loss: 1.835605386801694 acc: 0.81\n",
      "loss: 1.8441372735835333 acc: 0.75\n",
      "loss: 1.8248889152401195 acc: 0.89\n",
      "loss: 1.8284452827894384 acc: 0.87\n",
      "loss: 1.7985766170665698 acc: 0.85\n",
      "loss: 1.803032797173416 acc: 0.88\n",
      "loss: 1.8183464322296803 acc: 0.89\n",
      "loss: 1.7990891188431926 acc: 0.86\n",
      "loss: 1.8490109217168247 acc: 0.78\n",
      "loss: 1.8029579068316308 acc: 0.82\n",
      "loss: 1.8072007663689746 acc: 0.82\n",
      "loss: 1.810673131094035 acc: 0.81\n",
      "loss: 1.7953151826189937 acc: 0.81\n",
      "loss: 1.7525140577021434 acc: 0.95\n",
      "loss: 1.8483112714322345 acc: 0.75\n",
      "loss: 1.8036476048716619 acc: 0.84\n",
      "loss: 1.8102380201846615 acc: 0.83\n",
      "loss: 1.8128378052668719 acc: 0.8\n",
      "loss: 1.806459432501305 acc: 0.86\n",
      "loss: 1.8027687822933314 acc: 0.82\n",
      "loss: 1.8274236958616663 acc: 0.83\n",
      "loss: 1.8128680363942828 acc: 0.82\n",
      "loss: 1.848770051391899 acc: 0.85\n",
      "loss: 1.846040130859721 acc: 0.79\n",
      "loss: 1.777855502055507 acc: 0.84\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 1.7779\t Accuracy 0.8400\n",
      "loss: 1.8392978846923338 acc: 0.79\n",
      "loss: 1.834422892861763 acc: 0.83\n",
      "loss: 1.8608967190217316 acc: 0.84\n",
      "loss: 1.8230103066067773 acc: 0.85\n",
      "loss: 1.800800834665534 acc: 0.9\n",
      "loss: 1.8397143047109028 acc: 0.83\n",
      "loss: 1.8128466770213842 acc: 0.85\n",
      "loss: 1.8202774082344988 acc: 0.84\n",
      "loss: 1.7818355814427342 acc: 0.89\n",
      "loss: 1.8112820897112847 acc: 0.85\n",
      "loss: 1.8241084916090267 acc: 0.87\n",
      "loss: 1.8089518741800512 acc: 0.86\n",
      "loss: 1.8491921244983671 acc: 0.83\n",
      "loss: 1.8415716082402076 acc: 0.8\n",
      "loss: 1.8426779213119682 acc: 0.84\n",
      "loss: 1.802904963064445 acc: 0.87\n",
      "loss: 1.8069136686557368 acc: 0.83\n",
      "loss: 1.8531917708280536 acc: 0.8\n",
      "loss: 1.8501558100681217 acc: 0.83\n",
      "loss: 1.8040128387705194 acc: 0.84\n",
      "loss: 1.8381445676444665 acc: 0.81\n",
      "loss: 1.8243501915290004 acc: 0.85\n",
      "loss: 1.7898987867822242 acc: 0.89\n",
      "loss: 1.81143000898114 acc: 0.86\n",
      "loss: 1.8143068910131737 acc: 0.84\n",
      "loss: 1.7855929020269576 acc: 0.88\n",
      "loss: 1.7937483286584486 acc: 0.82\n",
      "loss: 1.8142601180174591 acc: 0.85\n",
      "loss: 1.806094002905753 acc: 0.82\n",
      "loss: 1.8236739974548632 acc: 0.86\n",
      "loss: 1.7955412652341078 acc: 0.87\n",
      "loss: 1.8003456784700231 acc: 0.89\n",
      "loss: 1.8335081146613297 acc: 0.85\n",
      "loss: 1.8107509256320742 acc: 0.8\n",
      "loss: 1.8317212327607737 acc: 0.83\n",
      "loss: 1.8206236317149171 acc: 0.88\n",
      "loss: 1.871735660048833 acc: 0.79\n",
      "loss: 1.8043596222043428 acc: 0.87\n",
      "loss: 1.8150357821482086 acc: 0.9\n",
      "loss: 1.8038215766879504 acc: 0.84\n",
      "loss: 1.8056260609784254 acc: 0.8\n",
      "loss: 1.831217187099538 acc: 0.81\n",
      "loss: 1.8178193348982494 acc: 0.91\n",
      "loss: 1.8436029970095322 acc: 0.83\n",
      "loss: 1.8210042762652048 acc: 0.84\n",
      "loss: 1.828215027139297 acc: 0.82\n",
      "loss: 1.8284165903595389 acc: 0.85\n",
      "loss: 1.8382310217352533 acc: 0.82\n",
      "loss: 1.867693128685928 acc: 0.81\n",
      "loss: 1.8739386013731612 acc: 0.8\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 1.8739\t Accuracy 0.8000\n",
      "loss: 1.794758134885309 acc: 0.88\n",
      "loss: 1.8232972280452053 acc: 0.85\n",
      "loss: 1.8246392907036184 acc: 0.84\n",
      "loss: 1.8300619282539712 acc: 0.85\n",
      "loss: 1.8590153293445288 acc: 0.83\n",
      "loss: 1.7724675898562368 acc: 0.9\n",
      "loss: 1.8053738866971516 acc: 0.81\n",
      "loss: 1.82568455309065 acc: 0.83\n",
      "loss: 1.8529328581102957 acc: 0.82\n",
      "loss: 1.8724796435693927 acc: 0.82\n",
      "loss: 1.8366575562837475 acc: 0.85\n",
      "loss: 1.829497655344632 acc: 0.86\n",
      "loss: 1.7646056941162893 acc: 0.85\n",
      "loss: 1.8593529753676898 acc: 0.78\n",
      "loss: 1.8180233648952862 acc: 0.81\n",
      "loss: 1.8538973790596245 acc: 0.79\n",
      "loss: 1.8011047615099198 acc: 0.89\n",
      "loss: 1.7905429372735977 acc: 0.79\n",
      "loss: 1.8057938482223337 acc: 0.83\n",
      "loss: 1.8214815972233325 acc: 0.83\n",
      "loss: 1.8009484177344235 acc: 0.87\n",
      "loss: 1.7952149427422401 acc: 0.9\n",
      "loss: 1.83876653742765 acc: 0.85\n",
      "loss: 1.8316155350247354 acc: 0.86\n",
      "loss: 1.8349021094009352 acc: 0.85\n",
      "loss: 1.8168086236600882 acc: 0.85\n",
      "loss: 1.7887614412859887 acc: 0.9\n",
      "loss: 1.849259401735963 acc: 0.84\n",
      "loss: 1.7834198285716736 acc: 0.86\n",
      "loss: 1.8336804947414245 acc: 0.85\n",
      "loss: 1.8284207830717634 acc: 0.79\n",
      "loss: 1.84915565250491 acc: 0.79\n",
      "loss: 1.8243454455909895 acc: 0.81\n",
      "loss: 1.817543560953541 acc: 0.86\n",
      "loss: 1.8205780077969649 acc: 0.85\n",
      "loss: 1.8262147050227453 acc: 0.85\n",
      "loss: 1.8023405034662858 acc: 0.86\n",
      "loss: 1.862536295165789 acc: 0.85\n",
      "loss: 1.8109023289097201 acc: 0.9\n",
      "loss: 1.810085005122167 acc: 0.84\n",
      "loss: 1.8012738067491534 acc: 0.85\n",
      "loss: 1.7639313729737691 acc: 0.91\n",
      "loss: 1.7663153881096272 acc: 0.9\n",
      "loss: 1.8107271914573653 acc: 0.86\n",
      "loss: 1.8361027925153415 acc: 0.84\n",
      "loss: 1.8225312772672473 acc: 0.85\n",
      "loss: 1.8273628411249871 acc: 0.88\n",
      "loss: 1.8183470422497772 acc: 0.87\n",
      "loss: 1.8257691027026612 acc: 0.84\n",
      "loss: 1.8555392961130417 acc: 0.8\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 1.8555\t Accuracy 0.8000\n",
      "loss: 1.8103399356810175 acc: 0.8\n",
      "loss: 1.8604539745260633 acc: 0.83\n",
      "loss: 1.7825961690054561 acc: 0.89\n",
      "loss: 1.845789104350298 acc: 0.8\n",
      "loss: 1.8026381811213728 acc: 0.83\n",
      "loss: 1.8230862871674216 acc: 0.87\n",
      "loss: 1.8381932149533566 acc: 0.83\n",
      "loss: 1.8620564733017508 acc: 0.8\n",
      "loss: 1.807108994344634 acc: 0.86\n",
      "loss: 1.8343260995146418 acc: 0.83\n",
      "loss: 1.8458664521141444 acc: 0.82\n",
      "loss: 1.8495463222064261 acc: 0.81\n",
      "loss: 1.8584199974149633 acc: 0.85\n",
      "loss: 1.7637621674907067 acc: 0.91\n",
      "loss: 1.8351556886055678 acc: 0.83\n",
      "loss: 1.7769634202065037 acc: 0.89\n",
      "loss: 1.8647037703238905 acc: 0.83\n",
      "loss: 1.7708780604210848 acc: 0.9\n",
      "loss: 1.7711568774217534 acc: 0.92\n",
      "loss: 1.8215777127913035 acc: 0.86\n",
      "loss: 1.7846864722439522 acc: 0.9\n",
      "loss: 1.843295767160648 acc: 0.77\n",
      "loss: 1.8265329498997844 acc: 0.82\n",
      "loss: 1.8473321110074026 acc: 0.87\n",
      "loss: 1.815221623988478 acc: 0.91\n",
      "loss: 1.8375193745605436 acc: 0.83\n",
      "loss: 1.8309344953223246 acc: 0.85\n",
      "loss: 1.8630751737635987 acc: 0.84\n",
      "loss: 1.7983240975947659 acc: 0.83\n",
      "loss: 1.8481738760154256 acc: 0.81\n",
      "loss: 1.82139727281799 acc: 0.85\n",
      "loss: 1.8369205323255655 acc: 0.85\n",
      "loss: 1.8341069224727677 acc: 0.83\n",
      "loss: 1.8166255919294403 acc: 0.87\n",
      "loss: 1.7972075625410953 acc: 0.78\n",
      "loss: 1.7960787542601766 acc: 0.88\n",
      "loss: 1.8042735763606823 acc: 0.88\n",
      "loss: 1.8127642528878487 acc: 0.81\n",
      "loss: 1.818469201133308 acc: 0.87\n",
      "loss: 1.814626273223594 acc: 0.86\n",
      "loss: 1.8250365503032437 acc: 0.84\n",
      "loss: 1.8406602717899505 acc: 0.84\n",
      "loss: 1.8410443860455654 acc: 0.87\n",
      "loss: 1.840450349693909 acc: 0.86\n",
      "loss: 1.8329776116122256 acc: 0.79\n",
      "loss: 1.8215170813418171 acc: 0.85\n",
      "loss: 1.8281692477806564 acc: 0.86\n",
      "loss: 1.8376369342834686 acc: 0.82\n",
      "loss: 1.783300450856634 acc: 0.85\n",
      "loss: 1.788274605401682 acc: 0.88\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 1.7883\t Accuracy 0.8800\n",
      "loss: 1.8198817219976484 acc: 0.82\n",
      "loss: 1.795444242650118 acc: 0.84\n",
      "loss: 1.8137996700647745 acc: 0.86\n",
      "loss: 1.8041125683550672 acc: 0.87\n",
      "loss: 1.7839832188555789 acc: 0.92\n",
      "loss: 1.8163009204938974 acc: 0.87\n",
      "loss: 1.7886751605491917 acc: 0.85\n",
      "loss: 1.7682711442544337 acc: 0.85\n",
      "loss: 1.8060042259922684 acc: 0.85\n",
      "loss: 1.794324929454655 acc: 0.85\n",
      "loss: 1.7927190723556548 acc: 0.9\n",
      "loss: 1.8311353012421177 acc: 0.81\n",
      "loss: 1.8349809649196025 acc: 0.86\n",
      "loss: 1.826551201054688 acc: 0.86\n",
      "loss: 1.8079844806993683 acc: 0.82\n",
      "loss: 1.8540601597878754 acc: 0.82\n",
      "loss: 1.8136401976367218 acc: 0.82\n",
      "loss: 1.8256552296025401 acc: 0.83\n",
      "loss: 1.8395955844431513 acc: 0.83\n",
      "loss: 1.819702667374263 acc: 0.81\n",
      "loss: 1.820801192846659 acc: 0.78\n",
      "loss: 1.7920146587522705 acc: 0.9\n",
      "loss: 1.820434985662718 acc: 0.82\n",
      "loss: 1.846537411298081 acc: 0.77\n",
      "loss: 1.811255616442146 acc: 0.87\n",
      "loss: 1.8455074836803107 acc: 0.82\n",
      "loss: 1.8438813943395151 acc: 0.83\n",
      "loss: 1.856561793981867 acc: 0.82\n",
      "loss: 1.81021491820283 acc: 0.79\n",
      "loss: 1.854172162840273 acc: 0.83\n",
      "loss: 1.8940838035664482 acc: 0.81\n",
      "loss: 1.8115001573855407 acc: 0.82\n",
      "loss: 1.8092928846816916 acc: 0.86\n",
      "loss: 1.793231971401072 acc: 0.82\n",
      "loss: 1.7838236082248387 acc: 0.87\n",
      "loss: 1.8410933513891516 acc: 0.82\n",
      "loss: 1.8361472278520725 acc: 0.82\n",
      "loss: 1.8457866235651494 acc: 0.85\n",
      "loss: 1.7951850431407066 acc: 0.84\n",
      "loss: 1.8268339624324474 acc: 0.84\n",
      "loss: 1.8164526611931584 acc: 0.89\n",
      "loss: 1.8106740666586132 acc: 0.84\n",
      "loss: 1.8238753740931009 acc: 0.85\n",
      "loss: 1.809350651668548 acc: 0.85\n",
      "loss: 1.8208526068537738 acc: 0.81\n",
      "loss: 1.8370136581274448 acc: 0.84\n",
      "loss: 1.818683263853034 acc: 0.89\n",
      "loss: 1.8339418006997534 acc: 0.88\n",
      "loss: 1.814796194006787 acc: 0.89\n",
      "loss: 1.7844195575745472 acc: 0.86\n",
      "loss: 1.7864670551426152 acc: 0.88\n",
      "loss: 1.7981295254733753 acc: 0.89\n",
      "loss: 1.830046030173481 acc: 0.87\n",
      "loss: 1.8142423755116273 acc: 0.95\n",
      "loss: 1.7901947257663813 acc: 0.89\n",
      "loss: 1.7598788007287425 acc: 0.83\n",
      "loss: 1.768607957861421 acc: 0.84\n",
      "loss: 1.8222510119759618 acc: 0.86\n",
      "loss: 1.7469459632661843 acc: 0.93\n",
      "loss: 1.7877106687605484 acc: 0.88\n",
      "loss: 1.7903405328692434 acc: 0.84\n",
      "loss: 1.8459533991817696 acc: 0.8\n",
      "loss: 1.851895453408929 acc: 0.84\n",
      "loss: 1.8659351343330601 acc: 0.87\n",
      "loss: 1.7745877402638448 acc: 0.93\n",
      "loss: 1.7919036693611314 acc: 0.86\n",
      "loss: 1.7489009703176694 acc: 0.9\n",
      "loss: 1.7834569998705758 acc: 0.85\n",
      "loss: 1.7747226328742074 acc: 0.9\n",
      "loss: 1.8624338340416964 acc: 0.88\n",
      "loss: 1.8085517262049342 acc: 0.9\n",
      "loss: 1.8196307636512747 acc: 0.76\n",
      "loss: 1.8478690788293246 acc: 0.88\n",
      "loss: 1.8315550377622498 acc: 0.86\n",
      "loss: 1.8460280983772637 acc: 0.81\n",
      "loss: 1.8446820263932289 acc: 0.78\n",
      "loss: 1.8726411936717668 acc: 0.81\n",
      "loss: 1.8090032001302654 acc: 0.93\n",
      "loss: 1.7853472685147316 acc: 0.89\n",
      "loss: 1.8133284796296545 acc: 0.83\n",
      "loss: 1.7723749619435776 acc: 0.93\n",
      "loss: 1.7379641823117857 acc: 0.92\n",
      "loss: 1.7771131635330475 acc: 0.88\n",
      "loss: 1.7737322941707214 acc: 0.85\n",
      "loss: 1.8038750487660917 acc: 0.89\n",
      "loss: 1.8495335378193136 acc: 0.9\n",
      "loss: 1.835301267394463 acc: 0.88\n",
      "loss: 1.7327294983687316 acc: 0.91\n",
      "loss: 1.7003503777386326 acc: 0.96\n",
      "loss: 1.7362047253459734 acc: 0.94\n",
      "loss: 1.7312508147392924 acc: 0.98\n",
      "loss: 1.7917148810527161 acc: 0.91\n",
      "loss: 1.7784200719219136 acc: 0.85\n",
      "loss: 1.7233817945636198 acc: 0.88\n",
      "loss: 1.8103875413712498 acc: 0.87\n",
      "loss: 1.7979488683974436 acc: 0.92\n",
      "loss: 1.8763062437825684 acc: 0.8\n",
      "loss: 1.7019542173060327 acc: 0.96\n",
      "loss: 1.8446723427069498 acc: 0.86\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8202\t Average training accuracy 0.8436\n",
      "Epoch [9]\t Average validation loss 1.7967\t Average validation accuracy 0.8778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train with momentum\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "loss2, acc2 = runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.8155867745567582 acc: 0.85\n",
      "loss: 1.837620336305423 acc: 0.89\n",
      "loss: 1.8411961540753672 acc: 0.83\n",
      "loss: 1.8689947289145963 acc: 0.8\n",
      "loss: 1.8823193771892812 acc: 0.81\n",
      "loss: 1.885617824595681 acc: 0.77\n",
      "loss: 1.8601998194586296 acc: 0.78\n",
      "loss: 1.8217576726391422 acc: 0.87\n",
      "loss: 1.8358411032336903 acc: 0.87\n",
      "loss: 1.8607495865032917 acc: 0.8\n",
      "loss: 1.8798843412778559 acc: 0.79\n",
      "loss: 1.8840342561640515 acc: 0.8\n",
      "loss: 1.9292142415956843 acc: 0.76\n",
      "loss: 1.8537818805412751 acc: 0.82\n",
      "loss: 1.8729220765092705 acc: 0.74\n",
      "loss: 1.8927451014543433 acc: 0.83\n",
      "loss: 1.8572141973482263 acc: 0.84\n",
      "loss: 1.8883884702103915 acc: 0.82\n",
      "loss: 1.8516235794249862 acc: 0.86\n",
      "loss: 1.8752958377922346 acc: 0.76\n",
      "loss: 1.89453375891051 acc: 0.8\n",
      "loss: 1.8968038173143154 acc: 0.74\n",
      "loss: 1.8751879611570827 acc: 0.82\n",
      "loss: 1.840807264305317 acc: 0.8\n",
      "loss: 1.8558418667843655 acc: 0.84\n",
      "loss: 1.8833637392651028 acc: 0.81\n",
      "loss: 1.8811931126348072 acc: 0.77\n",
      "loss: 1.845184095716588 acc: 0.82\n",
      "loss: 1.8320519613275303 acc: 0.83\n",
      "loss: 1.8756265984791545 acc: 0.84\n",
      "loss: 1.8293585545564937 acc: 0.9\n",
      "loss: 1.8780141705170506 acc: 0.8\n",
      "loss: 1.835255635715113 acc: 0.81\n",
      "loss: 1.8161511833833963 acc: 0.84\n",
      "loss: 1.844385714497025 acc: 0.87\n",
      "loss: 1.8771192569316761 acc: 0.8\n",
      "loss: 1.8055821550773 acc: 0.82\n",
      "loss: 1.8893074697858094 acc: 0.74\n",
      "loss: 1.8794065326405391 acc: 0.79\n",
      "loss: 1.8925170664922462 acc: 0.81\n",
      "loss: 1.8381776623846735 acc: 0.78\n",
      "loss: 1.865049527983301 acc: 0.79\n",
      "loss: 1.8639311300228758 acc: 0.78\n",
      "loss: 1.8997891005623695 acc: 0.78\n",
      "loss: 1.8939655820458343 acc: 0.75\n",
      "loss: 1.8593936772484778 acc: 0.8\n",
      "loss: 1.8373689302798233 acc: 0.84\n",
      "loss: 1.8414009071017556 acc: 0.85\n",
      "loss: 1.8922092753740016 acc: 0.77\n",
      "loss: 1.8562895194821258 acc: 0.78\n",
      "loss: 1.7683379018928438 acc: 0.9\n",
      "loss: 1.793412906509547 acc: 0.88\n",
      "loss: 1.7772710109171828 acc: 0.88\n",
      "loss: 1.7346629223620331 acc: 0.97\n",
      "loss: 1.7055025578215037 acc: 0.96\n",
      "loss: 1.7780973758854057 acc: 0.9\n",
      "loss: 1.7889385468150747 acc: 0.82\n",
      "loss: 1.7754476787562519 acc: 0.9\n",
      "loss: 1.8024814262992543 acc: 0.84\n",
      "loss: 1.8235954055909263 acc: 0.8\n",
      "loss: 1.7784201546476286 acc: 0.84\n",
      "loss: 1.6673404447917404 acc: 0.91\n",
      "loss: 1.6900210377449187 acc: 0.96\n",
      "loss: 1.6833857867512652 acc: 0.96\n",
      "loss: 1.7622774194359077 acc: 0.92\n",
      "loss: 1.8650399658362864 acc: 0.81\n",
      "loss: 1.858479402051434 acc: 0.81\n",
      "loss: 1.759809441920326 acc: 0.85\n",
      "loss: 1.7743818211051208 acc: 0.87\n",
      "loss: 1.7638495996317145 acc: 0.94\n",
      "loss: 1.7587045705386826 acc: 0.92\n",
      "loss: 1.7636389210803207 acc: 0.91\n",
      "loss: 1.8202809360811787 acc: 0.89\n",
      "loss: 1.7545289942505238 acc: 0.98\n",
      "loss: 1.8844498705323625 acc: 0.86\n",
      "loss: 1.805478899091441 acc: 0.89\n",
      "loss: 1.7926955293213198 acc: 0.93\n",
      "loss: 1.6669958490787014 acc: 0.97\n",
      "loss: 1.756705948637784 acc: 0.87\n",
      "loss: 1.7058176202449673 acc: 0.93\n",
      "loss: 1.7383924099703791 acc: 0.89\n",
      "loss: 1.7379243168441059 acc: 0.92\n",
      "loss: 1.7964833438900805 acc: 0.91\n",
      "loss: 1.7549143144067245 acc: 0.89\n",
      "loss: 1.7051681064324429 acc: 0.91\n",
      "loss: 1.728753312132284 acc: 0.95\n",
      "loss: 1.7517618857474202 acc: 0.98\n",
      "loss: 1.67976958057596 acc: 0.99\n",
      "loss: 1.671891255831556 acc: 0.98\n",
      "loss: 1.713630298767895 acc: 0.97\n",
      "loss: 1.773506065865959 acc: 0.82\n",
      "loss: 1.7103923615707828 acc: 0.91\n",
      "loss: 1.7866717234249418 acc: 0.9\n",
      "loss: 1.73422201964036 acc: 0.95\n",
      "loss: 1.7405502490259435 acc: 0.92\n",
      "loss: 1.7008857271796156 acc: 0.92\n",
      "loss: 1.8518139894595134 acc: 0.87\n",
      "loss: 1.8986754773493093 acc: 0.76\n",
      "loss: 1.9053220594808826 acc: 0.71\n",
      "loss: 1.8579189734249693 acc: 0.85\n",
      "Final test accuracy 0.8533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = runner.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAs/ElEQVR4nO3de3xV5Z3v8c8vN3Ih3AQCEhBa7nc04nWsiChWkWpnWmx79Djt8TijHduZVyvTq70cj1M7Z6YdbSl10E5Hi62CIiCCVqvWCwQLAQIoAkpMgAByJ+T2O3/sHdiEnWQHsvbaSb7v12u/9l7Pep61fjsbnt9ez9rrWebuiIiINJYWdgAiIpKalCBERCQuJQgREYlLCUJEROJSghARkbiUIEREJK7AEoSZzTOz3Wa2von1ZmY/N7MtZlZiZufHrJtuZpuj62YHFaOIiDQtyCOIx4Dpzay/DhgWfdwB/BLAzNKBh6PrRwO3mNnoAOMUEZE4AksQ7v4qsK+ZKjOB//KIt4AeZtYfmAxscfet7l4NzI/WFRGRJMoIcd8DgB0xy2XRsnjlFzW1ETO7g8gRCHl5eReMHDmy7SMVEemgVq9evcfd+8RbF2aCsDhl3kx5XO4+F5gLUFRU5MXFxW0TnYhIJ2BmHzS1LswEUQYMjFkuBMqBrCbKRUQkicL8mesi4Nbor5kuBg64ewWwChhmZkPMLAuYFa0rIiJJFNgRhJn9DrgS6G1mZcD3gUwAd58DLAU+DWwBjgK3R9fVmtndwAtAOjDP3TcEFaeIiMQXWIJw91taWO/AXU2sW0okgYhICGpqaigrK6OqqirsUKSNZGdnU1hYSGZmZsJtwjwHISIpqqysjPz8fAYPHoxZvN+NSHvi7uzdu5eysjKGDBmScDtNtSEip6mqquKcc85RcuggzIxzzjmn1UeEShAiEpeSQ8dyJp+nEoSIiMSlBCEikqA1a9awdGlyfz+zbNkyRowYwdChQ3nggQeSum+dpBaRs1L04xXsOVx9WnnvrlkUf2daCBEFZ82aNRQXF/PpT386Kfurq6vjrrvuYsWKFRQWFnLhhRdy4403Mnp0cuYv1RGEiJyVeMmhufJEbd++nZEjR/KVr3yFsWPH8sUvfpEXX3yRyy67jGHDhrFy5Ur27dvHZz7zGcaPH8/FF19MSUkJAPfddx+33XYb11xzDYMHD2bBggV885vfZNy4cUyfPp2amhoAVq9ezac+9SkuuOACrr32WioqKgC48soruffee5k8eTLDhw/ntddeo7q6mu9973s8+eSTTJw4kSeffJL77ruPn/70pydiHjt2LNu3b08o9kSsXLmSoUOH8olPfIKsrCxmzZrFs88+e1Z/19bQEYSINOsHz22gtPzgGbX9/K/ejFs++txufH/GmBbbb9myhT/84Q/MnTuXCy+8kCeeeILXX3+dRYsWcf/99zNw4EAmTZrEM888wx//+EduvfVW1qxZA8D777/Pyy+/TGlpKZdccglPP/00P/nJT7jppptYsmQJ119/PV/96ld59tln6dOnD08++STf/va3mTdvHgC1tbWsXLmSpUuX8oMf/IAXX3yRH/7whxQXF/PQQw8BkUR0prE/88wzvPzyy3z9618/rW1ubi5vvPEGH330EQMHnpx5qLCwkLfffrvFv1tbUYIQkZQ1ZMgQxo0bB8CYMWOYOnUqZsa4cePYvn07H3zwAU8//TQAV111FXv37uXAgQMAXHfddWRmZjJu3Djq6uqYPj1ye5qGtps3b2b9+vVMmxYZBqurq6N///4n9n3zzTcDcMEFF7B9+/Y2jx1gypQpJxJaPJHriU+VzF+XKUGISLNa+qY/ePaSJtc9+b8vOat9d+nS5cTrtLS0E8tpaWnU1taSkXF6F9bQgcbWzczMPFHe0NbdGTNmDG++Gf8op6F9eno6tbW1cetkZGRQX19/Yjn2OoOWYgdaPIIoLCxkx46Tdz8oKyvj3HPPjRtLEHQOQkTarSuuuILHH38cgFdeeYXevXvTrVu3hNqOGDGCysrKEwmipqaGDRuan/YtPz+fQ4cOnVgePHgw77zzDgDvvPMO27Zta1X8DUcQjR9vvPEGABdeeCHvvfce27Zto7q6mvnz53PjjTe2ah9nQwlCRM5K765ZrSpvS/fddx/FxcWMHz+e2bNn85vf/CbhtllZWTz11FPce++9TJgwgYkTJ57omJsyZcoUSktLT5yk/uxnP8u+ffuYOHEiv/zlLxk+fPjZvqVTZGRk8NBDD3HttdcyatQoPve5zzFmTMvnbtqKxRvjaq90wyCRtrFx40ZGjRoVdhjSxuJ9rma22t2L4tXXEYSIiMSlBCEiInEpQYiISFxKECIiElegCcLMppvZZjPbYmaz46zvaWYLzazEzFaa2diYddvNbJ2ZrTEznXkWEUmyIO9JnQ48DEwDyoBVZrbI3Utjqn0LWOPuN5nZyGj9qTHrp7j7nqBiFBGRpgV5BDEZ2OLuW929GpgPzGxUZzTwEoC7bwIGm1lBgDGJiJyxVJ3u++OPP+amm25i/PjxTJ48mfXr17fJvoOcamMAsCNmuQy4qFGdtcDNwOtmNhk4DygEdgEOLDczB37l7nPj7cTM7gDuABg0aFCbvgERScCDw+DI7tPL8/rCN95LfjwBStXpvu+//34mTpzIwoUL2bRpE3fddRcvvfTSWe8/yCOIeDNKNb4q7wGgp5mtAb4K/AVomPTkMnc/H7gOuMvMroi3E3ef6+5F7l7Up0+ftolcRBIXLzk0V54gTfed+HTfpaWlTJ0aGZ0fOXIk27dvZ9euXWf194dgjyDKgIExy4VAeWwFdz8I3A5gkZm0tkUfuHt59Hm3mS0kMmT1aoDxikg8z8+GnevOrO2j18cv7zcOrmv57mia7jux6b4nTJjAggULuPzyy1m5ciUffPABZWVlFBSc3Yh9kAliFTDMzIYAHwGzgC/EVjCzHsDR6DmKrwCvuvtBM8sD0tz9UPT1NcAPA4xVRFKQpvtObLrv2bNnc8899zBx4kTGjRvHpEmT4s5021qBJQh3rzWzu4EXgHRgnrtvMLM7o+vnAKOA/zKzOqAU+HK0eQGwMPqHyACecPdlQcUqIs1o6Zv+fd2bXnd701OBJ0LTfSc23Xe3bt149NFHgUhSGTJkCEOGDIkbc2sEej8Id18KLG1UNifm9ZvAsDjttgITgoxNRNq/hum+v/vd757VdN+XXHIJNTU1vPvuu83Olhpvuu/FixcDZzfdd1Nip/seMGAA8+fP54knnjit3v79+8nNzSUrK4tHHnmEK664IuG/Q3N0JbWInJ28vq0rb0OdebrvOXPmMGdO5Pv2xo0bGTNmDCNHjuT555/nZz/7WZvsX9N9i8hpNN13x6TpvkVEpE0oQYiISFxKECISV0cafpYz+zyVIETkNNnZ2ezdu1dJooNwd/bu3Ut2dnar2gX6M1cRaZ8KCwspKyujsrIy7FCkjWRnZ1NYWNiqNkoQInKazMzMNrnQSto3DTGJiEhcShAiIhKXEoSIiMSlBCEiInEpQYiISFxKECIiEpcShIiIxKUEISIicQWaIMxsupltNrMtZjY7zvqeZrbQzErMbKWZjU20rYiIBCuwBGFm6cDDwHXAaOAWMxvdqNq3gDXuPh64FfhZK9qKiEiAgjyCmAxscfet7l4NzAdmNqozGngJwN03AYPNrCDBtiIiEqAgE8QAYEfMclm0LNZa4GYAM5sMnAcUJtiWaLs7zKzYzIo1sZiISNsJMkFYnLLGcwc/APQ0szXAV4G/ALUJto0Uus919yJ3L+rTp89ZhCsiIrGCnM21DBgYs1wIlMdWcPeDwO0AZmbAtugjt6W2IiISrCCPIFYBw8xsiJllAbOARbEVzKxHdB3AV4BXo0mjxbYiIhKswI4g3L3WzO4GXgDSgXnuvsHM7oyunwOMAv7LzOqAUuDLzbUNKlYRETmddaRbChYVFXlxcXHYYYiItBtmttrdi+Kt05XUIiISlxKEiIjEpQQhIiJxKUGIiEhcShAiIhKXEoSIiMSlBCEiInEpQYiISFxKECIiEpcShIiIxKUEISIicSlBiIhIXEoQIiISlxKEiIjEpQQhIiJxKUGIiEhcgSYIM5tuZpvNbIuZzY6zvruZPWdma81sg5ndHrNuu5mtM7M1Zqa7AImIJFlgtxw1s3TgYWAaUAasMrNF7l4aU+0uoNTdZ5hZH2CzmT3u7tXR9VPcfU9QMYqISNOCPIKYDGxx963RDn8+MLNRHQfyzcyArsA+oDbAmEREJEGBHUEAA4AdMctlwEWN6jwELALKgXzg8+5eH13nwHIzc+BX7j433k7M7A7gDoBBgwa1XfQindWDw+DI7tPL8/rCN97rePuVJgWZICxOmTdavhZYA1wFfBJYYWavuftB4DJ3LzezvtHyTe7+6mkbjCSOuQBFRUWNty9ydjpjZxlvv82Vt/f9Quf8nBMQZIIoAwbGLBcSOVKIdTvwgLs7sMXMtgEjgZXuXg7g7rvNbCGRIavTEoR0AuosTy13h7oaqK+F+hqor2vb5ea8/m+Jxe9t/F1t9WOQlglpGZCWDukNrzNbuRzzSI/ZXms+Z3fw+jiPxuUxyzTRJsykmIAgE8QqYJiZDQE+AmYBX2hU50NgKvCamRUAI4CtZpYHpLn7oejra4AfBhirJCKsjjrR/0TuUFcN1Ueg5ihUH408n3h9pNHz0UZ145Q350d9wWIPlKOvEy07pdziro7rBz1aqBCgF+8LZ7/P3RPgxlv4g/+w96kdfScSWIJw91ozuxt4AUgH5rn7BjO7M7p+DvAj4DEzW0fkU7rX3feY2SeAhZFz12QAT7j7sqBilQS15ttOXS3UVkU67Nqq6ON4zCO6XNdoOd665vx80qmdute17j1lZENmLmTlRZ9zITMP8vpEXu95t+m2F9958vWJb8wep4wmyrzpsrfnNL3fK/858q234Rt1embbLv/r8Kb3/e2dTa87TUuZrpH/U9D0uq+XRo9wYh510aOf+ppWLtfGlEW39epPmt73pV8FS4skc0uLecQs03hdbJ2m1qXBwv/dur9RkgV5BIG7LwWWNiqbE/O6nMjRQeN2W4EJQcYmLTh+GA7vijwO7Yw8N+dfR0Y79mhCaG1HHU96l0gH3pxzzz/ZqWflNursYzv93FMTQENZWnrz27+ve9PrpgV4UNtsgjjtkqLkycwJZ7/dBwS7/eYSxNXfD26/nTlBSADOZpinvh6O7oXD0Q7/0K7o693RJLA7snxoV2S4pTWGTYt05ulZkeeMbMjoEvOILqfHLmdDRlZM3Zj26VmQFv0VdnOd9F//Z+vilJbl9W3631hH3K80SQmivWlumGfHqmgH36izbzgSOLw7/jf7Lt2ga1/o2g/OnQRdCyKP/H4ny/P7wU+GNB3Xjf/RNu8v1XTGzjKkX88UHf8Fe6qqTyvvnZFF0FMpVHp3+tiBuOVbt+7leG09VTV1HK+tjz7qOF5TT1X0uaGsqia6rrY+Wl538rnxNmrqeMWa3m+fgN9zIpQgOpL/vDpmwSLj6PnRzr5g7MnXp3T+BZGhmFTWCTvLVPiJY7LtOXx6cohXXl/vHK2p4+jxWo5U13HkeC1Hq+s4Ul3L0eMNz5F1R6trOXI8+lxdd0r5ybp1HDr+y6YDm/tWi7FnZaTRJSON7Mx0ukRfd8lIp0tmGtkZ6fTMyzpZFlPvwteb3u/2FvcaPCWIjuQLvz/Z+ef2hvQ2/njD6qg7YWcZpqIfr4jbWffumkXxd6ad8Xbdnaqaeg4cq+FgVQ0HjtVw4OjJ18254icvn+jsj9Ukfn4rPc3Iy0onr0sGuTHPBfnZ5PbOIC8rndysDOb9eVuT23jiKxfRJTPSuWdnnuzkGxJAVnoaaWmtPCEf9cjrTe83FShBdCTDrw12++qoO4XmvsnX1TsHYzr4g8dqT+nwDx6LPlfVnliOrV9Td2Y/Ez1/UA9yu5zs0PO6RJ67Nur4TzxnZZDbJZ2s9DTMWu68m0sQlw7tfUYxdwRKEO3Jkb1hRyBJEtS3eOBEJ78/2pkfOFbD/qPVJzr35nzyW0ubXZ+RZnTLyaR7TibdsjPolpPJgJ450eVoeU5Go+XI8/k/WtHkdv991qQzeq+prnfXrCY/51SgBNFeVB+BJz7X9Hr90qNDaWk83t05dLyWA0dPdvKRjj52uTpu+aGqM58P82tXDzutY4/t8HOz0hP6xp5qwuqozzbZB63FBGFmNwBLYybRk2Srq4Hf3wrl78DnH4dRN4QdkQSkpq6e8v3Hmq0z6YfLOVhVS11908M1WelpdMvJpEdupBMv6JbNiIL8E516Q3nDc+SRRfecTIZ/5/kmt/u1q5u5iO4shfltOtU76rAkcgQxC/iZmT0NPOruGwOOSWLV18Mzfw9bXoQZP1dy6AAOHKthx76jfLjvKB/sjTx/uO8IH+47Svn+qmY7foAbxp97slOP6eBPdPo5WWRnJjb2nkrUSaeeFhOEu3/JzLoBtwCPRqfffhT4nbsfCjrATs0dln8H1v0ervouXHBb2BF1OmdyLqCu3tl5sIoP9h5hxylJIPLYf/TUcf5eeVkM6pXLpIE9+czEXAb2yuWbT5U0GdOPPjP27N5UC1J9XFySJ6FzEO5+MHoEkQN8DbgJ+IaZ/dzdO+gVUingz/8Obz0MF90Jf/VPYUfTKTV3LmDTzoN8sPfoKUlgx76jlH18jOq6kyOyGWnGgJ45DOqVy/Xj+nPeObkM6hVJBIN65ZKfnXna9ptLEEHTN3lpkMg5iBnA3xK5X8NvgcnRKbhzgY2AEkQQ3vltZObMsX8N1/7fRrOBSjLU1jV/2m36v7924nV+dgbnnZPLyP75XDOmH4N65Z5IBP27Z5OR3rqbN+pbvKSCRI4g/gb4t8Y363H3o2b2t8GE1cltWgrP/QN88ir4zC9Pzkkkgaivd8o+Psa7uw6xedehyPPOQ2ytbH4+qv+4ZdKJRNA9J7NNx/z1LV5SQSIJ4vtARcOCmeUABe6+3d1fCiyyzuqDN+Cp26H/RPjcbyOT2UmbcHd2HTweSQI7I8ngvV2HeHfX4VOuzh3QI4dhBV351PA+/OrVrU1ub8aEc5MRtkhoEkkQfwAujVmui5ZdGEhEndmuDfDELOg+EL74B+jSNeyIUsKZnCjee/hkInh39+ETCSH2GoDeXbswol9XZk0eyPCCfIYX5DOsoCvdYs4JNJcgRDq6RBJEhruf+N/p7tVmpq+1be3jD+C3N0cmzvsfCyCv817e31hzJ4oPHKs5cRTQMDT03u5Dp7TpnpPJiIJ8bpxwLiP65Z9IBr3yWv5nrHMB0pklkiAqzexGd18EYGYzgT3BhtXJHK6E394Etcfg9mXQY1DYEbUbE36w/MTr3Kx0hhXkc9XIvgwvyD+RDPrmdznj8wM6FyCdWSIJ4k7gcTN7iMg9BHcAtyaycTObDvyMyC1HH3H3Bxqt7w78NzAoGstP3f3RRNp2GMcPweN/DQc/glufhYLRYUeUMtydjRXNX2rzzekjGBE9IhjQI+eMZ9UUkdMlcqHc+8DFZtYVsEQvjjOzdOBhYBpQBqwys0XuXhpT7S6g1N1nmFkfYLOZPU7kPEdLbdu/2uPw5Jdg5zqY9QQMujjsiEJXW1fPqu0fs7x0JytKd1H2cfPTTvz9lUOTFJlI55PQhXJmdj0wBshuOFR395ZuyDsZ2BK9vzRmNh+YCcR28g7kW2SjXYF9QC1wUQJt27f6elh4J2x9BWb+AkZMDzui0BytruXVdytZXrqLP27azf6jNWRlpHH50N7cPWUosxesCztEkU4pkQvl5gC5wBTgEeCvgZUJbHsAkeGoBmVEOv5YDwGLgHIgH/i8u9ebWSJtG+K7A7gDYNCgdjJ27w7L7oUNCyI3vp/0xbAjSro9h4/z0sZdLN+wi9e37OF4bT3dczKZOrIv00YXcMXwPuR1ifzz/OnyzTpRLBKCRI4gLnX38WZW4u4/MLN/BRYk0C7eYHDjWciuBdYAVxG5UnuFmb2WYNtIoftcYC5AUVHRmd2NJNle/SmsnAuX3A2X3RN2NEmzbc8Rlm+IDB2t/vBj3CPXHNwyeRDXjCngwsG9yIxzxbFOFIuEI5EEURV9Pmpm5wJ7gWbuXn9CGTAwZrmQyJFCrNuBB9zdgS1mtg0YmWDb9qn4UXj5xzB+Fkz7UdjRBKq+3llbtp8VpbtYXrqLLbsPAzDm3G7cM3UY00YXMLp/t3Y366hIZ5FIgnjOzHoADwLvEPkm/+sE2q0ChpnZEOAjItOGf6FRnQ+BqcBrZlYAjAC2AvsTaNv+lC6CJf8Iw66BmQ91yCk0jtfW8eb7e1leuosXS3ex+9Bx0tOMi4b04ksXDeLq0QUU9swNO0wRSUCzCcLM0oCX3H0/8LSZLQay3f1ASxt291ozuxt4gchPVee5+wYzuzO6fg7wI+AxM1tHZFjpXnffE933aW3P9E2mhG2vwdNfhgEXwN88Bumnz+CZ6pq6ovmcvCy+N2M0y0t38afNlRw+XktuVjpXjujDtNEFTBnRlx65Ol8g0t5YZHSnmQpmb7r7JUmK56wUFRV5cXFx2GGcrqIEHrseup0Ltz8Pub3CjuiMDJ69pNn1vbt2Ydrovlwzuh+XfPIcsjPTkxSZiJwpM1vt7kXx1iUyxLTczD4LLPCWsomcbt9W+O/PQpdu8KUF7TY5tOTpv7uUSQN76EI1kQ4kkQTxj0AeUGtmVUSGgtzduwUaWUdweHdkfqX6Gvifi6H7gLAjOmNVMbOdxnPBeT2TFImIJEsiV1LnJyOQDqfqYOTI4fAuuO056DMi7IjO2Ovv7eHbz+hiNZHOJpEL5a6IV974BkISo6YK5n8BdpfCLU9CYdzhvZS39/BxfrxkIwv/8hFDeueFHY6IJFkiQ0zfiHmdTWQKjdVELm6TxurrYMFXYPtrcPOvYdjVYUfUau7OH1aXcf/SjRw5Xss/XDWUv58ylMv/5Y+6olmkE0lkiGlG7LKZDQR+ElhE7Zk7LPkn2Phc5D7S4z8XdkSt9n7lYb61YB1vb9vHhYN7cv9N4xhWEBll1BXNIp1LQpP1NVIGjG3rQDqEV/4vrH4ULv86XPL3YUfTKsdr6/jFy+/zy1feJzszjQduHsfnigbqV0kinVgi5yD+g5PzIKUBE4G1AcbUPjw4DI7sPr08Ixumfj/58ZyFt7bu5VsL17G18ggzJ57Ld64fTZ/8LmGHJSIhS+QIIvbKs1rgd+7+54DiaT/iJQeA2ipoJ3MLfXykmvuXbuQPq8sY2CuH3/ztZD41vE/YYYlIikgkQTwFVLl7HURuBGRmue5+NNjQJCjuzjNrPuLHizey/1gNd37qk9wzdRg5WbryWUROSiRBvARcDRyOLucAy4FLgwpKgrN9zxG+88x6Xt+yh4kDe/DfN49jVH9d8ygip0skQWS7e0NywN0Pm5mm42xnqmvr+fVrW/n5S++RlZ7Gj2aO4QsXnUe6TkKLSBMSSRBHzOx8d38HwMwuAJq/UbCklOLt+/jWwnW8u+swnx7Xj+/PGENBt+ywwxKRFJdIgvga8Acza7hhT3/g84FF1F7k9Y1/ojqvb/JjacKBYzX8y7JNPPH2h5zbPZv/vK2IqaMKwg5LRNqJRC6UW2VmI4nczMeATe5eE3hkqe4b74UdQZPcnSXrKvjBc6XsPXycL18+hH+cNvzEPZ5FRBKRyHUQdwGPu/v66HJPM7vF3X8ReHTSajv2HeV7z67n5c2VjBvQnUf/54WMHdA97LBEpB1K5Cvl/3L3hxsW3P1jM/tfQIsJwsymAz8jcle4R9z9gUbrvwF8MSaWUUAfd99nZtuBQ0AdUNvUDS0koraunnl/3sa/rXgPM/juDaO57ZLzyEjveLc1FZHkSCRBpJmZNdwsyMzSgRZnZ4vWexiYRmR6jlVmtsjdSxvquPuDRO51jZnNAL7u7vtiNjOl4RakEtHUbT/T04y6eufqUQX8cOYYzu2RE0J0ItKRJJIgXgB+b2ZziEy5cSfwfALtJgNb3H0rgJnNB2YCpU3UvwX4XQLb7dTiJQeAunpnzpfO59ox/bB2ciW3iKS2RMYf7iVysdzfAXcBJUQulmvJAGBHzHJZtOw00esqpgNPxxQ7kdudrjazO5raiZndYWbFZlZcWVmZQFgd1/Sx/ZUcRKTNtJgg3L0eeAvYChQBU4GNCWw7Xk/V1D2tZwB/bjS8dJm7nw9cB9zVzI2L5rp7kbsX9emjeYRERNpKk0NMZjYcmEVk6Gcv8CSAu09JcNtlwMCY5UKgvIm6s2g0vOTu5dHn3Wa2kMiQle5iJyKSJM0dQWwicrQww90vd/f/IPKLokStAoaZ2RAzyyKSBBY1rmRm3YFPAc/GlOWZWX7Da+AaYH0r9i0iImepuQTxWWAn8LKZ/drMphJ/2Cgud68F7iZyknsj8Ht332Bmd5rZnTFVbwKWu/uRmLIC4HUzWwusBJa4+7JE992RNXV7T932U0TamkV/vdp0hcg3+M8QGWq6CvgNsNDdlwceXSsVFRV5cXFxyxXbMXfn8n95meEFXXn09slhhyMi7ZyZrW7qOrNETlIfcffH3f0GIucR1gCz2zZESdRfduzno/3HuH78uWGHIiIdXKsus3X3fe7+K3e/KqiApHlLSirISk/jmjGadE9EgqV5GNqR+npnSUkFVwzvQ7fszLDDEZEOTgmiHVn94cfsPFjFjAn9ww5FRDoBJYh2ZPHacrpkpOmeDiKSFEoQ7URdvbN0/U6mjOhLV93XQUSSQAminVi5bR+Vh45zg4aXRCRJlCDaicUl5eRkpnPVyNS5pamIdGxKEO1AbV09y9bv5KpRfcnN0vCSiCSHEkQ78NbWfew9Us2M8RpeEpHkUYJoBxaXlJOXlc6VIzS8JCLJowSR4mrq6lm2YSfTRheQnZkedjgi0okoQaS417fsYf/RGs29JCJJpwSR4paUVJCfncEVw3uHHYqIdDJKECnseG0dL2zYyTWj+9ElQ8NLIpJcShAp7LV393CoqlYXx4lIKAJNEGY23cw2m9kWMzvtHhJm9g0zWxN9rDezOjPrlUjbzmBxSTndczK57JMaXhKR5AssQZhZOvAwcB0wGrjFzEbH1nH3B919ortPBP4Z+JO770ukbUdXVVPHitJdTB/Tj6wMHeiJSPIF2fNMBra4+1Z3rwbmAzObqX8L8LszbNvhvLK5kiPVdRpeEpHQBJkgBgA7YpbLomWnMbNcYDrw9Bm0vcPMis2suLKy8qyDThWLS8rplZfFJZ84J+xQRKSTCjJBWJwyb6LuDODP7r6vtW3dfa67F7l7UZ8+fc4gzNRztLqWlzbuZvrYfmSka3hJRMIRZO9TBgyMWS4EypuoO4uTw0utbdvhvLypkmM1ddyguZdEJERBJohVwDAzG2JmWUSSwKLGlcysO/Ap4NnWtu2oFpeU0ye/CxcN0fCSiIQnsLmj3b3WzO4GXgDSgXnuvsHM7oyunxOtehOw3N2PtNQ2qFhTyeHjtfxx025mXTiQ9LR4I20iIskR6M0F3H0psLRR2ZxGy48BjyXStjN4aeMujtfWa+4lEQmdzoCmmMUlFfTrlk3ReT3DDkVEOjkliBRysKqGP22u5NPj+pOm4SURCZkSRApZsWEX1XX1ujhORFKCEkQKWVxSzoAeOUwa2CPsUERElCBSxYGjNbz23h6uH98fMw0viUj4lCBSxAsbdlJb77o4TkRShhJEiniupJxBvXIZN6B72KGIiABKEClh7+HjvPH+Xg0viUhKUYJIAS9s2EWdhpdEJMUoQaSAxSXlfKJ3HqP7dws7FBGRE5QgQlZ56Dhvbd3LDRpeEpEUowQRsufXV1DvaO4lEUk5ShAhW1xSwbC+XRnRLz/sUERETqEEEaJdB6tYtX0fN+joQURSkBJEiJaUVOAO1+vXSyKSgpQgQrS4pJyR/fIZ2rdr2KGIiJxGCSIkH+0/xjsf7mfGBA0viUhqCjRBmNl0M9tsZlvMbHYTda40szVmtsHM/hRTvt3M1kXXFQcZZxiWllQA6OI4EUlZgd1y1MzSgYeBaUAZsMrMFrl7aUydHsAvgOnu/qGZ9W20mSnuvieoGMO0uKSccQO6c945eWGHIiISV5BHEJOBLe6+1d2rgfnAzEZ1vgAscPcPAdx9d4DxpIwP9x5lbdkBHT2ISEoLMkEMAHbELJdFy2INB3qa2StmttrMbo1Z58DyaPkdTe3EzO4ws2IzK66srGyz4IO0ZF1keOnT45QgRCR1BTbEBMSbN8Lj7P8CYCqQA7xpZm+5+7vAZe5eHh12WmFmm9z91dM26D4XmAtQVFTUePspaXFJORMH9mBgr9ywQxERaVKQRxBlwMCY5UKgPE6dZe5+JHqu4VVgAoC7l0efdwMLiQxZtXvb9hxhQ/lBDS+JSMoLMkGsAoaZ2RAzywJmAYsa1XkW+CszyzCzXOAiYKOZ5ZlZPoCZ5QHXAOsDjDVpFq+N5EhdHCciqS6wISZ3rzWzu4EXgHRgnrtvMLM7o+vnuPtGM1sGlAD1wCPuvt7MPgEsjM5umgE84e7Lgoo1mZasq6DovJ70754TdigiIs0K8hwE7r4UWNqobE6j5QeBBxuVbSU61NSRbNl9iE07D3HfjNFhhyIi0iJdSZ1Ez62twEy/XhKR9kEJIkncncUl5Vw0pBd9u2WHHY6ISIuUIJJk865DvF95RDcGEpF2QwkiSRavrSDN4Lqx/cIORUQkIUoQSdAwvHTpJ3vTu2uXsMMREUmIEkQSbCg/yPa9R3VxnIi0K0oQSbC4pIKMNOPaMRpeEpH2QwkiYA3DS5cN7U3PvKywwxERSZgSRMDWlh2g7ONjGl4SkXZHCSJgi9eWk5luXKPhJRFpZ5QgAlRf7yxdV8EVw/rQPScz7HBERFpFCSJAf9nxMeUHqrhhgoaXRKT9UYII0HNrK8jKSOPqUQVhhyIi0mpKEAGpiw4vTRnRh/xsDS+JSPujBBGQ4u372H3ouOZeEpF2SwkiIItLKsjOTGPqyL5hhyIickYCTRBmNt3MNpvZFjOb3USdK81sjZltMLM/taZtqqqtq+f59RVMHVlAXpdA78kkIhKYwHovM0sHHgamAWXAKjNb5O6lMXV6AL8Aprv7h2bWN9G2qeztbfvYc7haF8eJSLsW5BHEZGCLu29192pgPjCzUZ0vAAvc/UMAd9/dirYpa3FJBblZ6Vw5QsNLItJ+BZkgBgA7YpbLomWxhgM9zewVM1ttZre2oi0AZnaHmRWbWXFlZWUbhX7maurqWba+gqtHFZCTlR52OCIiZyzIAXKLU+Zx9n8BMBXIAd40s7cSbBspdJ8LzAUoKiqKWyeZ3nh/Lx8frdHwkoi0e0EmiDJgYMxyIVAep84edz8CHDGzV4EJCbZNSYvXlpPfJYMrhvcJOxQRkbMS5BDTKmCYmQ0xsyxgFrCoUZ1ngb8yswwzywUuAjYm2DblVNfW88KGnUwbXUB2poaXRKR9C+wIwt1rzexu4AUgHZjn7hvM7M7o+jnuvtHMlgElQD3wiLuvB4jXNqhY28rrWyo5WFWruZdEpEMI9Ef67r4UWNqobE6j5QeBBxNpm+oWr62ge04mlw/V8JKItH+6krqNVNXUsbx0F9eOKSArQ39WEWn/1JO1kVffreTw8VrNvSQiHYYSRBtZXFJBz9xMLv3kOWGHIiLSJpQg2sCx6jpe3LiL6WP7k5muP6mIdAzqzdrAy5t3c7S6jhm6OE5EOhAliDawpKSC3l2zmDykV9ihiIi0Gc1FfYaKfryCPYerTykb+u3n6d01i+LvTAspKhGRtqMjiDPUODm0VC4i0t4oQYiISFzmHvoEqG3GzA4Bm5Oxr6x+Qy9oal31zi2rkxED0BvYk6R9pQq9585B7zl5znP3uNM/dLRzEJvdvSjsIJLFzIo70/sFvefOQu85NWiISURE4lKCEBGRuDpagpgbdgBJ1tneL+g9dxZ6zymgQ52kFhGRttPRjiBERKSNKEGIiEhcHSJBmNl0M9tsZlvMbHbY8QTNzAaa2ctmttHMNpjZPWHHlCxmlm5mfzGzxWHHkgxm1sPMnjKzTdHP+5KwYwqamX09+u96vZn9zsyyw46prZnZPDPbbWbrY8p6mdkKM3sv+twzzBihAyQIM0sHHgauA0YDt5jZ6HCjClwt8E/uPgq4GLirE7znBvcAG8MOIol+Bixz95HABDr4ezezAcA/AEXuPpbIPelnhRtVIB4Dpjcqmw285O7DgJeiy6Fq9wkCmAxscfet7l4NzAdmhhxToNy9wt3fib4+RKTTGBBuVMEzs0LgeuCRsGNJBjPrBlwB/CeAu1e7+/5Qg0qODCDHzDKAXKA85HjanLu/CuxrVDwT+E309W+AzyQzpng6QoIYAOyIWS6jE3SWDcxsMDAJeDvkUJLh34FvAvUhx5EsnwAqgUejw2qPmFle2EEFyd0/An4KfAhUAAfcfXm4USVNgbtXQORLINA35Hg6RIKwOGWd4re7ZtYVeBr4mrsfDDueIJnZDcBud0/WPFepIAM4H/ilu08CjpACww5Bio67zwSGAOcCeWb2pXCj6rw6QoIoAwbGLBfSAQ9JGzOzTCLJ4XF3XxB2PElwGXCjmW0nMox4lZn9d7ghBa4MKHP3hqPDp4gkjI7samCbu1e6ew2wALg05JiSZZeZ9QeIPu8OOZ4OkSBWAcPMbIiZZRE5obUo5JgCZWZGZFx6o7v/v7DjSQZ3/2d3L3T3wUQ+4z+6e4f+ZunuO4EdZjYiWjQVKA0xpGT4ELjYzHKj/86n0sFPzMdYBNwWfX0b8GyIsQAdYDZXd681s7uBF4j84mGeu28IOaygXQb8D2Cdma2Jln3L3ZeGF5IE5KvA49EvP1uB20OOJ1Du/raZPQW8Q+TXen8hBaegOFtm9jvgSqC3mZUB3wceAH5vZl8mkij/JrwIIzTVhoiIxNURhphERCQAShAiIhKXEoSIiMSlBCEiInEpQYiISFxKECKtYGZ1ZrYm5tFmVzab2eDY2T1Fwtbur4MQSbJj7j4x7CBEkkFHECJtwMy2m9m/mNnK6GNotPw8M3vJzEqiz4Oi5QVmttDM1kYfDdNJpJvZr6P3Q1huZjmhvSnp9JQgRFonp9EQ0+dj1h1098nAQ0RmniX6+r/cfTzwOPDzaPnPgT+5+wQi8ys1XP0/DHjY3ccA+4HPBvpuRJqhK6lFWsHMDrt71zjl24Gr3H1rdCLFne5+jpntAfq7e020vMLde5tZJVDo7sdjtjEYWBG9YQxmdi+Q6e4/TsJbEzmNjiBE2o438bqpOvEcj3ldh84TSoiUIETazudjnt+Mvn6Dk7fM/CLwevT1S8DfwYn7bHdLVpAiidK3E5HWyYmZQRci94tu+KlrFzN7m8gXr1uiZf8AzDOzbxC5O1zDbKz3AHOjM3fWEUkWFUEHL9IaOgch0gai5yCK3H1P2LGItBUNMYmISFw6ghARkbh0BCEiInEpQYiISFxKECIiEpcShIiIxKUEISIicf1/ShMxjmBZizsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({\n",
    "    \"momentum=0\": [loss1, acc1],\n",
    "    \"momentum=0.9\": [loss2, acc2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3)\n",
      "(1, 3)\n",
      "[[-0.30299694]\n",
      " [ 0.13746476]\n",
      " [-0.54639307]]\n",
      "[[ 0.53133751  1.04090438 -0.37768692 -0.8025295   0.54392463  0.19368977\n",
      "   0.96712941 -0.6222014   0.31706225  0.25087925]\n",
      " [-0.23243148  0.69716104  0.02804984 -1.32929883  0.08165972 -0.31759476\n",
      "   0.04190669 -0.98785723  0.24905791 -0.07940176]\n",
      " [-1.31209006 -0.49733679  0.61494164  0.46524103  0.17914572 -0.5483098\n",
      "  -1.55780342 -0.27046255 -0.33526063  0.33773651]]\n",
      "[[ 0.22834057  0.73790744 -0.68068385 -1.10552644  0.24092769 -0.10930717\n",
      "   0.66413247 -0.92519834  0.01406531 -0.05211768]\n",
      " [-0.09496672  0.83462581  0.1655146  -1.19183407  0.21912449 -0.18013\n",
      "   0.17937145 -0.85039246  0.38652268  0.05806301]\n",
      " [-1.85848312 -1.04372985  0.06854858 -0.08115203 -0.36724735 -1.09470286\n",
      "  -2.10419649 -0.81685561 -0.88165369 -0.20865655]]\n",
      "prob_M shape: (3, 10)\n",
      "t_M [[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "labels [1 1 0 1 0 1 0 0 1 2]\n",
      "t_M shape: [[0 0 1 0 1 0 1 1 0 0]\n",
      " [1 1 0 1 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/j1y8x0s17rggg6mtzyntr9500000gn/T/ipykernel_41381/1251500387.py:24: DeprecationWarning: This function is deprecated. Please call randint(0, 2 + 1) instead\n",
      "  labels = np.random.random_integers(0, num_output-1, (N, ))\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "import numpy as np\n",
    "num_input = 6 # m\n",
    "num_output = 3 # K\n",
    "N = 10\n",
    "raw_std = (2 / (num_input + num_output))**0.5\n",
    "init_std = raw_std * (2**0.5)\n",
    "W = np.random.normal(0, init_std, (num_input, num_output))\n",
    "b = np.random.normal(0, init_std, (1, num_output))\n",
    "Input = np.random.normal(0, init_std, (N, num_input)) # shape (N, m)\n",
    "print(W.shape)\n",
    "print(b.shape)\n",
    "\n",
    "X = Input.transpose() # shape(m, N)\n",
    "W_T = W.transpose() # shape(K,m)\n",
    "b_T = b.transpose() # shape(K,1)\n",
    "print(b_T)\n",
    "prob_M = np.dot(W_T,X)  # shape(K,N)\n",
    "print(prob_M)\n",
    "prob_M = prob_M + b_T\n",
    "print(prob_M)\n",
    "\n",
    "print('prob_M shape:', prob_M.shape)\n",
    "labels = np.random.random_integers(0, num_output-1, (N, )) \n",
    "t_M = np.zeros([num_output, N], dtype=int) # shape(K,N)\n",
    "print ('t_M',t_M)\n",
    "print ('labels',labels)\n",
    "\n",
    "for i in range(N):\n",
    "    t_M[labels[i]][i] = 1 # 将第i个元素的k类行设置为1\n",
    "print('t_M shape:', t_M) \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "132e6111d8ffa10be32dcc2d80c102638828bf77ba52c5acca41a3a150f1c1d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

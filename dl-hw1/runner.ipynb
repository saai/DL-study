{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver import Solver\n",
    "from visualize import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.6284226579832746 acc: 0.03\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.6284\t Accuracy 0.0300\n",
      "loss: 2.5346140266019255 acc: 0.05\n",
      "loss: 2.5126736634202387 acc: 0.08\n",
      "loss: 2.4571799915103205 acc: 0.1\n",
      "loss: 2.5163159530511963 acc: 0.06\n",
      "loss: 2.4501279924560664 acc: 0.13\n",
      "loss: 2.4395383949367004 acc: 0.06\n",
      "loss: 2.4287045326563788 acc: 0.1\n",
      "loss: 2.4011963022494216 acc: 0.05\n",
      "loss: 2.422934337111299 acc: 0.09\n",
      "loss: 2.377753404676225 acc: 0.12\n",
      "loss: 2.3909167111319114 acc: 0.11\n",
      "loss: 2.362186492225473 acc: 0.14\n",
      "loss: 2.391908658356875 acc: 0.07\n",
      "loss: 2.395333673072955 acc: 0.09\n",
      "loss: 2.3745928031895405 acc: 0.06\n",
      "loss: 2.3516163410250117 acc: 0.09\n",
      "loss: 2.305370344092499 acc: 0.14\n",
      "loss: 2.3373370879331326 acc: 0.13\n",
      "loss: 2.337777004749617 acc: 0.15\n",
      "loss: 2.2981196285010546 acc: 0.17\n",
      "loss: 2.3519742608610565 acc: 0.09\n",
      "loss: 2.2857744373686635 acc: 0.14\n",
      "loss: 2.3102156119448383 acc: 0.12\n",
      "loss: 2.2639872302531803 acc: 0.17\n",
      "loss: 2.311024288185274 acc: 0.14\n",
      "loss: 2.228039982368858 acc: 0.16\n",
      "loss: 2.244078451323858 acc: 0.15\n",
      "loss: 2.257132978774459 acc: 0.13\n",
      "loss: 2.267045868460237 acc: 0.22\n",
      "loss: 2.149972759666787 acc: 0.27\n",
      "loss: 2.2222248471433517 acc: 0.23\n",
      "loss: 2.276505459679874 acc: 0.16\n",
      "loss: 2.217194446066002 acc: 0.18\n",
      "loss: 2.2186150593910186 acc: 0.21\n",
      "loss: 2.1913854276091924 acc: 0.2\n",
      "loss: 2.187220114702023 acc: 0.22\n",
      "loss: 2.162523799458545 acc: 0.25\n",
      "loss: 2.1694236570451806 acc: 0.21\n",
      "loss: 2.165994720745422 acc: 0.28\n",
      "loss: 2.2559790892422895 acc: 0.14\n",
      "loss: 2.1575154422616936 acc: 0.27\n",
      "loss: 2.136509656453064 acc: 0.37\n",
      "loss: 2.137015002908962 acc: 0.3\n",
      "loss: 2.128801810445241 acc: 0.36\n",
      "loss: 2.1111294433861385 acc: 0.27\n",
      "loss: 2.243809194634585 acc: 0.24\n",
      "loss: 2.1390478235151917 acc: 0.27\n",
      "loss: 2.1081734877418743 acc: 0.31\n",
      "loss: 2.1182494707597517 acc: 0.35\n",
      "loss: 2.146639647295439 acc: 0.27\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 2.1466\t Accuracy 0.2700\n",
      "loss: 2.0548292157458223 acc: 0.34\n",
      "loss: 2.091326199017378 acc: 0.36\n",
      "loss: 2.0362721281150034 acc: 0.4\n",
      "loss: 2.139282266484035 acc: 0.31\n",
      "loss: 2.1326626471746017 acc: 0.32\n",
      "loss: 2.2181967110642256 acc: 0.21\n",
      "loss: 2.1130964944743273 acc: 0.25\n",
      "loss: 2.121873122762052 acc: 0.36\n",
      "loss: 2.0900163600379056 acc: 0.33\n",
      "loss: 2.1031565393955294 acc: 0.33\n",
      "loss: 2.1479890874518346 acc: 0.29\n",
      "loss: 2.068138074349383 acc: 0.3\n",
      "loss: 2.123400660503046 acc: 0.28\n",
      "loss: 2.0665356681036955 acc: 0.34\n",
      "loss: 2.0655072762204534 acc: 0.33\n",
      "loss: 2.047275366445805 acc: 0.4\n",
      "loss: 2.108348136542398 acc: 0.39\n",
      "loss: 2.0705540514207255 acc: 0.36\n",
      "loss: 2.100387543714622 acc: 0.35\n",
      "loss: 2.033836327833638 acc: 0.44\n",
      "loss: 2.0878799280107194 acc: 0.4\n",
      "loss: 2.0458161725344666 acc: 0.42\n",
      "loss: 2.0316295635091812 acc: 0.43\n",
      "loss: 2.057124604830693 acc: 0.41\n",
      "loss: 2.030816378761945 acc: 0.44\n",
      "loss: 2.0688391062039138 acc: 0.35\n",
      "loss: 2.0833119620887137 acc: 0.35\n",
      "loss: 2.077003134795102 acc: 0.39\n",
      "loss: 2.0069236581153262 acc: 0.46\n",
      "loss: 2.008925719356757 acc: 0.46\n",
      "loss: 2.0357275047743255 acc: 0.44\n",
      "loss: 2.0602583408423163 acc: 0.38\n",
      "loss: 1.9786604694567338 acc: 0.48\n",
      "loss: 2.038259091790211 acc: 0.41\n",
      "loss: 2.024147334933917 acc: 0.42\n",
      "loss: 2.0448090666526726 acc: 0.42\n",
      "loss: 2.0582716499683484 acc: 0.45\n",
      "loss: 1.99984255223691 acc: 0.46\n",
      "loss: 2.0835165784026866 acc: 0.35\n",
      "loss: 2.0292528797999694 acc: 0.49\n",
      "loss: 2.089610606493801 acc: 0.42\n",
      "loss: 1.9954934057817206 acc: 0.51\n",
      "loss: 2.0080811162015273 acc: 0.41\n",
      "loss: 2.0893455615697407 acc: 0.43\n",
      "loss: 2.018053559108595 acc: 0.41\n",
      "loss: 1.9906864608666108 acc: 0.44\n",
      "loss: 2.0673292891519486 acc: 0.34\n",
      "loss: 2.010340150818666 acc: 0.45\n",
      "loss: 2.05420636780286 acc: 0.44\n",
      "loss: 2.0704311326222826 acc: 0.45\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 2.0704\t Accuracy 0.4500\n",
      "loss: 1.9772645246137057 acc: 0.55\n",
      "loss: 1.9955007474821824 acc: 0.54\n",
      "loss: 2.0124751844682196 acc: 0.5\n",
      "loss: 2.0126893990758092 acc: 0.42\n",
      "loss: 2.0268820056270163 acc: 0.45\n",
      "loss: 2.0272236941006274 acc: 0.46\n",
      "loss: 1.9945098004206852 acc: 0.46\n",
      "loss: 2.0043532411513634 acc: 0.46\n",
      "loss: 1.9308731686284109 acc: 0.58\n",
      "loss: 2.0325815915343632 acc: 0.45\n",
      "loss: 1.9543076849401464 acc: 0.51\n",
      "loss: 2.0563702585537755 acc: 0.46\n",
      "loss: 1.9762519482854521 acc: 0.55\n",
      "loss: 1.972884468322832 acc: 0.48\n",
      "loss: 2.0471438618809383 acc: 0.47\n",
      "loss: 2.0197077269884773 acc: 0.44\n",
      "loss: 1.9689031144903688 acc: 0.52\n",
      "loss: 2.001983934111628 acc: 0.48\n",
      "loss: 1.9573962304866817 acc: 0.48\n",
      "loss: 1.9595340268048478 acc: 0.54\n",
      "loss: 1.9550703477134932 acc: 0.55\n",
      "loss: 2.0243563878611313 acc: 0.43\n",
      "loss: 1.9830615506064575 acc: 0.54\n",
      "loss: 1.9632550680614727 acc: 0.56\n",
      "loss: 1.9360200638779255 acc: 0.61\n",
      "loss: 1.974753624864747 acc: 0.55\n",
      "loss: 2.0010898823500325 acc: 0.45\n",
      "loss: 1.948924690233045 acc: 0.51\n",
      "loss: 1.995897799411584 acc: 0.51\n",
      "loss: 1.9629858736969477 acc: 0.51\n",
      "loss: 1.955755895535076 acc: 0.52\n",
      "loss: 1.9849563259383576 acc: 0.51\n",
      "loss: 2.002238382129604 acc: 0.48\n",
      "loss: 1.957601034940876 acc: 0.53\n",
      "loss: 2.0096864862694472 acc: 0.48\n",
      "loss: 1.9959178531343036 acc: 0.54\n",
      "loss: 1.9874210477780576 acc: 0.51\n",
      "loss: 1.9709034829590935 acc: 0.56\n",
      "loss: 1.9111363444818168 acc: 0.57\n",
      "loss: 1.991688908627913 acc: 0.54\n",
      "loss: 1.986579572779824 acc: 0.47\n",
      "loss: 1.9790144005383814 acc: 0.49\n",
      "loss: 1.9170332615465846 acc: 0.6\n",
      "loss: 2.0099708584160996 acc: 0.46\n",
      "loss: 1.951687676971043 acc: 0.58\n",
      "loss: 1.9544960192373586 acc: 0.53\n",
      "loss: 1.9399761617784759 acc: 0.54\n",
      "loss: 1.9932452340796258 acc: 0.59\n",
      "loss: 1.9790663484026851 acc: 0.48\n",
      "loss: 1.9306194257160392 acc: 0.59\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 1.9306\t Accuracy 0.5900\n",
      "loss: 1.9487754915205193 acc: 0.59\n",
      "loss: 1.9541759517682937 acc: 0.58\n",
      "loss: 1.951061151511614 acc: 0.63\n",
      "loss: 1.9897529508303082 acc: 0.54\n",
      "loss: 2.028180848696478 acc: 0.47\n",
      "loss: 2.033280386575574 acc: 0.5\n",
      "loss: 1.9838660431918118 acc: 0.57\n",
      "loss: 1.9266515815173284 acc: 0.59\n",
      "loss: 2.0208642745318715 acc: 0.48\n",
      "loss: 1.9573232755024654 acc: 0.54\n",
      "loss: 1.9641758827819584 acc: 0.57\n",
      "loss: 1.9491149291684564 acc: 0.58\n",
      "loss: 1.96679348475182 acc: 0.53\n",
      "loss: 1.9735793968141955 acc: 0.51\n",
      "loss: 1.9506972971832897 acc: 0.56\n",
      "loss: 1.9759208896151352 acc: 0.51\n",
      "loss: 1.9442409636482152 acc: 0.49\n",
      "loss: 1.9782997594416367 acc: 0.51\n",
      "loss: 1.9314494747567401 acc: 0.57\n",
      "loss: 2.004975380146178 acc: 0.45\n",
      "loss: 2.0199940262991287 acc: 0.53\n",
      "loss: 1.9164633640974853 acc: 0.59\n",
      "loss: 1.9793040825863275 acc: 0.5\n",
      "loss: 1.9438840535017368 acc: 0.51\n",
      "loss: 1.961490212457288 acc: 0.56\n",
      "loss: 1.9842328843720562 acc: 0.56\n",
      "loss: 1.935279754838005 acc: 0.53\n",
      "loss: 1.891719143674548 acc: 0.66\n",
      "loss: 1.9416369430896636 acc: 0.58\n",
      "loss: 1.9447426757884738 acc: 0.6\n",
      "loss: 1.924599347150646 acc: 0.51\n",
      "loss: 1.967471968796085 acc: 0.52\n",
      "loss: 1.9201917467739806 acc: 0.6\n",
      "loss: 1.905473466489986 acc: 0.62\n",
      "loss: 1.984342663577289 acc: 0.49\n",
      "loss: 1.947588103074209 acc: 0.56\n",
      "loss: 1.913432248515231 acc: 0.6\n",
      "loss: 1.9000640759907443 acc: 0.7\n",
      "loss: 1.925736869468767 acc: 0.59\n",
      "loss: 2.016255298497184 acc: 0.42\n",
      "loss: 1.944750752959144 acc: 0.58\n",
      "loss: 1.9247171887111718 acc: 0.62\n",
      "loss: 1.9422135953277646 acc: 0.59\n",
      "loss: 1.9948779334319346 acc: 0.53\n",
      "loss: 1.883306174728238 acc: 0.65\n",
      "loss: 1.9414317124638034 acc: 0.55\n",
      "loss: 1.9629170314736615 acc: 0.57\n",
      "loss: 1.9774141333731683 acc: 0.47\n",
      "loss: 1.989049436572222 acc: 0.53\n",
      "loss: 1.9533425864172913 acc: 0.55\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 1.9533\t Accuracy 0.5500\n",
      "loss: 1.9216906971739744 acc: 0.61\n",
      "loss: 1.9176129353427194 acc: 0.58\n",
      "loss: 1.9490665820583877 acc: 0.54\n",
      "loss: 1.8728066490654984 acc: 0.62\n",
      "loss: 2.000911453805533 acc: 0.49\n",
      "loss: 1.9549862853780717 acc: 0.52\n",
      "loss: 1.9353631834721154 acc: 0.62\n",
      "loss: 1.9452345565106859 acc: 0.56\n",
      "loss: 1.9170200058675797 acc: 0.65\n",
      "loss: 1.902248501251846 acc: 0.63\n",
      "loss: 1.9080512791554183 acc: 0.71\n",
      "loss: 1.924950738136679 acc: 0.59\n",
      "loss: 1.9360522277523902 acc: 0.58\n",
      "loss: 1.9129596024030497 acc: 0.64\n",
      "loss: 1.9676268930491405 acc: 0.55\n",
      "loss: 1.9349587088050877 acc: 0.56\n",
      "loss: 1.935994992588861 acc: 0.58\n",
      "loss: 1.9369256641685644 acc: 0.56\n",
      "loss: 1.9101430962037909 acc: 0.61\n",
      "loss: 1.9306782229757724 acc: 0.58\n",
      "loss: 1.932402328219959 acc: 0.6\n",
      "loss: 1.9201711042449419 acc: 0.53\n",
      "loss: 1.9445520329382242 acc: 0.5\n",
      "loss: 1.9471449800371472 acc: 0.59\n",
      "loss: 1.9482650401785961 acc: 0.61\n",
      "loss: 1.935528687903873 acc: 0.63\n",
      "loss: 1.9703944630838288 acc: 0.57\n",
      "loss: 1.933913208876789 acc: 0.59\n",
      "loss: 1.8732174588439983 acc: 0.7\n",
      "loss: 1.959167651608187 acc: 0.55\n",
      "loss: 1.9307856755954182 acc: 0.57\n",
      "loss: 1.921046467317517 acc: 0.6\n",
      "loss: 1.936760262689993 acc: 0.57\n",
      "loss: 1.9252334578298163 acc: 0.61\n",
      "loss: 1.936364079424682 acc: 0.63\n",
      "loss: 1.9276981643531947 acc: 0.62\n",
      "loss: 1.946269339063367 acc: 0.55\n",
      "loss: 1.9288395983613251 acc: 0.64\n",
      "loss: 1.9044257777198697 acc: 0.6\n",
      "loss: 1.987279514669001 acc: 0.56\n",
      "loss: 1.922228489260741 acc: 0.63\n",
      "loss: 1.878338839572096 acc: 0.74\n",
      "loss: 1.9411110278512316 acc: 0.69\n",
      "loss: 1.9335509000405715 acc: 0.64\n",
      "loss: 1.9652751647086688 acc: 0.6\n",
      "loss: 1.9418256013501736 acc: 0.6\n",
      "loss: 1.9358676243006343 acc: 0.55\n",
      "loss: 1.9304570469569209 acc: 0.58\n",
      "loss: 1.9222318392177673 acc: 0.67\n",
      "loss: 1.8979074959827946 acc: 0.65\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 1.8979\t Accuracy 0.6500\n",
      "loss: 1.8834531324646955 acc: 0.63\n",
      "loss: 1.9143714650923886 acc: 0.71\n",
      "loss: 1.9666628060627038 acc: 0.53\n",
      "loss: 1.909680857384388 acc: 0.69\n",
      "loss: 1.8854392978926002 acc: 0.65\n",
      "loss: 1.917902261930018 acc: 0.6\n",
      "loss: 1.8839095015215797 acc: 0.68\n",
      "loss: 1.94700965809845 acc: 0.63\n",
      "loss: 1.9022448408321642 acc: 0.7\n",
      "loss: 1.902403167020349 acc: 0.7\n",
      "loss: 1.9187397484875877 acc: 0.65\n",
      "loss: 1.9314194020461337 acc: 0.61\n",
      "loss: 1.9370584428695559 acc: 0.58\n",
      "loss: 1.9263337324010368 acc: 0.65\n",
      "loss: 1.9051347501041007 acc: 0.67\n",
      "loss: 1.8832549697322813 acc: 0.69\n",
      "loss: 1.8961188723671052 acc: 0.64\n",
      "loss: 1.8603448215731948 acc: 0.67\n",
      "loss: 1.8639109263549636 acc: 0.69\n",
      "loss: 1.9669098355373256 acc: 0.56\n",
      "loss: 1.902613656238676 acc: 0.64\n",
      "loss: 1.8994891329514554 acc: 0.6\n",
      "loss: 1.9470578019259148 acc: 0.6\n",
      "loss: 1.9070295697932749 acc: 0.66\n",
      "loss: 1.8823035413356897 acc: 0.69\n",
      "loss: 1.9271712793343545 acc: 0.63\n",
      "loss: 1.9391489691746364 acc: 0.69\n",
      "loss: 1.9167809158495988 acc: 0.6\n",
      "loss: 1.9458186965318491 acc: 0.56\n",
      "loss: 1.8754374643337863 acc: 0.66\n",
      "loss: 1.9773028753379154 acc: 0.6\n",
      "loss: 1.9045319076730873 acc: 0.64\n",
      "loss: 1.9143570523263727 acc: 0.67\n",
      "loss: 1.885951775627781 acc: 0.7\n",
      "loss: 1.8798015140083622 acc: 0.72\n",
      "loss: 1.9254942895741665 acc: 0.63\n",
      "loss: 1.9420304117265264 acc: 0.64\n",
      "loss: 1.917225279649284 acc: 0.62\n",
      "loss: 1.8861197215671108 acc: 0.61\n",
      "loss: 1.932203551318863 acc: 0.61\n",
      "loss: 1.9521092002376959 acc: 0.6\n",
      "loss: 1.9673811318105792 acc: 0.52\n",
      "loss: 1.8466318315998058 acc: 0.71\n",
      "loss: 1.9006402940180918 acc: 0.67\n",
      "loss: 1.8961570981378009 acc: 0.71\n",
      "loss: 1.9102209510168284 acc: 0.7\n",
      "loss: 1.8807737299367215 acc: 0.63\n",
      "loss: 1.9314879100222269 acc: 0.63\n",
      "loss: 1.920834015314263 acc: 0.69\n",
      "loss: 1.9248094403880285 acc: 0.64\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 1.9248\t Accuracy 0.6400\n",
      "loss: 2.0146493371524623 acc: 0.63\n",
      "loss: 1.9277970658480574 acc: 0.58\n",
      "loss: 1.8472195103190194 acc: 0.77\n",
      "loss: 1.9111884937558008 acc: 0.6\n",
      "loss: 1.9241796761602972 acc: 0.55\n",
      "loss: 1.9149405312426606 acc: 0.67\n",
      "loss: 1.875464690710957 acc: 0.65\n",
      "loss: 1.9179888564571221 acc: 0.65\n",
      "loss: 1.8855636174237014 acc: 0.65\n",
      "loss: 1.9061771419137878 acc: 0.61\n",
      "loss: 1.9116310717431046 acc: 0.62\n",
      "loss: 1.9372412442169695 acc: 0.61\n",
      "loss: 1.8841680619974097 acc: 0.65\n",
      "loss: 1.9097405962424616 acc: 0.74\n",
      "loss: 1.8977750831677163 acc: 0.71\n",
      "loss: 1.9538963019573097 acc: 0.58\n",
      "loss: 1.9113459625635074 acc: 0.66\n",
      "loss: 1.9538452458024138 acc: 0.58\n",
      "loss: 1.8751216897948013 acc: 0.73\n",
      "loss: 1.8726383485746312 acc: 0.71\n",
      "loss: 1.8489107081639276 acc: 0.75\n",
      "loss: 1.8991808238027459 acc: 0.64\n",
      "loss: 1.932211185791559 acc: 0.6\n",
      "loss: 1.911280511133573 acc: 0.61\n",
      "loss: 1.8924817457583947 acc: 0.63\n",
      "loss: 1.9525140549756643 acc: 0.62\n",
      "loss: 1.9229600425838194 acc: 0.62\n",
      "loss: 1.9183579241957458 acc: 0.59\n",
      "loss: 1.8925968841661416 acc: 0.68\n",
      "loss: 1.9267273405954484 acc: 0.65\n",
      "loss: 1.9345893330429562 acc: 0.57\n",
      "loss: 1.9047556473832956 acc: 0.62\n",
      "loss: 1.8761498419878238 acc: 0.68\n",
      "loss: 1.9389253144338383 acc: 0.58\n",
      "loss: 1.9406743989012472 acc: 0.67\n",
      "loss: 1.9154773733120327 acc: 0.73\n",
      "loss: 1.9321791865009361 acc: 0.67\n",
      "loss: 1.8685995118962382 acc: 0.72\n",
      "loss: 1.9077487382917022 acc: 0.65\n",
      "loss: 1.8497167527205993 acc: 0.72\n",
      "loss: 1.9063433304657744 acc: 0.7\n",
      "loss: 1.8794193216002821 acc: 0.67\n",
      "loss: 1.8874596063866047 acc: 0.65\n",
      "loss: 1.9401807200485899 acc: 0.62\n",
      "loss: 1.892626655820817 acc: 0.61\n",
      "loss: 1.8813302229878692 acc: 0.67\n",
      "loss: 1.8601366832707873 acc: 0.74\n",
      "loss: 1.9126462818672751 acc: 0.63\n",
      "loss: 1.8932446645620384 acc: 0.65\n",
      "loss: 1.9167779312938058 acc: 0.67\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 1.9168\t Accuracy 0.6700\n",
      "loss: 1.9102572263655007 acc: 0.62\n",
      "loss: 1.8575768359606954 acc: 0.79\n",
      "loss: 1.9043661067680748 acc: 0.66\n",
      "loss: 1.88288200552426 acc: 0.71\n",
      "loss: 1.8978406902654315 acc: 0.71\n",
      "loss: 1.8888588820655423 acc: 0.72\n",
      "loss: 1.892631194624696 acc: 0.73\n",
      "loss: 1.8703045967412586 acc: 0.68\n",
      "loss: 1.9161001419925183 acc: 0.64\n",
      "loss: 1.9357932377432376 acc: 0.6\n",
      "loss: 1.8932989419146469 acc: 0.59\n",
      "loss: 1.9416713064874083 acc: 0.65\n",
      "loss: 1.8649108883405348 acc: 0.74\n",
      "loss: 1.9154221934223963 acc: 0.68\n",
      "loss: 1.8899561418847837 acc: 0.71\n",
      "loss: 1.9495872797131042 acc: 0.61\n",
      "loss: 1.8877342683117673 acc: 0.66\n",
      "loss: 1.8836743959950701 acc: 0.69\n",
      "loss: 1.9556891217039105 acc: 0.63\n",
      "loss: 1.918572060391606 acc: 0.63\n",
      "loss: 1.91004186982452 acc: 0.64\n",
      "loss: 1.9007812594025495 acc: 0.71\n",
      "loss: 1.9015719576940788 acc: 0.7\n",
      "loss: 1.8570371009953042 acc: 0.72\n",
      "loss: 1.8933867887207547 acc: 0.69\n",
      "loss: 1.8997414912387822 acc: 0.67\n",
      "loss: 1.9629891465087888 acc: 0.58\n",
      "loss: 1.928356601488157 acc: 0.67\n",
      "loss: 1.8916436114044286 acc: 0.73\n",
      "loss: 1.9093730894790228 acc: 0.72\n",
      "loss: 1.9625647034568547 acc: 0.58\n",
      "loss: 1.900131196592633 acc: 0.62\n",
      "loss: 1.89643142371626 acc: 0.69\n",
      "loss: 1.8596423571150387 acc: 0.66\n",
      "loss: 1.9010672651603848 acc: 0.73\n",
      "loss: 1.89945942264739 acc: 0.68\n",
      "loss: 1.898153487051591 acc: 0.67\n",
      "loss: 1.8855748613416654 acc: 0.73\n",
      "loss: 1.926234457930352 acc: 0.68\n",
      "loss: 1.871245478484492 acc: 0.7\n",
      "loss: 1.8945064043299242 acc: 0.71\n",
      "loss: 1.9345669138652983 acc: 0.65\n",
      "loss: 1.9237560407645176 acc: 0.62\n",
      "loss: 1.9102150446684993 acc: 0.72\n",
      "loss: 1.9163354365377245 acc: 0.71\n",
      "loss: 1.8872490221402873 acc: 0.68\n",
      "loss: 1.8992164216839038 acc: 0.72\n",
      "loss: 1.8877708873650227 acc: 0.71\n",
      "loss: 1.9126652128991193 acc: 0.6\n",
      "loss: 1.9005554802132374 acc: 0.71\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 1.9006\t Accuracy 0.7100\n",
      "loss: 1.9337423025305336 acc: 0.65\n",
      "loss: 1.908026409025496 acc: 0.7\n",
      "loss: 1.895399698527701 acc: 0.67\n",
      "loss: 1.9153949783142297 acc: 0.64\n",
      "loss: 1.9158825968844588 acc: 0.64\n",
      "loss: 1.9088762858949422 acc: 0.7\n",
      "loss: 1.8922453291531687 acc: 0.68\n",
      "loss: 1.8925102525908397 acc: 0.71\n",
      "loss: 1.895720112627277 acc: 0.71\n",
      "loss: 1.8976458744756064 acc: 0.68\n",
      "loss: 1.8798194413299962 acc: 0.68\n",
      "loss: 1.8560788196439042 acc: 0.71\n",
      "loss: 1.8732733246591173 acc: 0.74\n",
      "loss: 1.8782827692264061 acc: 0.68\n",
      "loss: 1.9057986897489354 acc: 0.72\n",
      "loss: 1.9172191695908953 acc: 0.62\n",
      "loss: 1.8860551822297122 acc: 0.75\n",
      "loss: 1.8649744050633636 acc: 0.73\n",
      "loss: 1.8825842819938448 acc: 0.69\n",
      "loss: 1.897885981413338 acc: 0.75\n",
      "loss: 1.9302248279805985 acc: 0.66\n",
      "loss: 1.8684967692129972 acc: 0.71\n",
      "loss: 1.904259110344438 acc: 0.68\n",
      "loss: 1.8911402024120632 acc: 0.71\n",
      "loss: 1.9025069620908683 acc: 0.68\n",
      "loss: 1.859062496936066 acc: 0.73\n",
      "loss: 1.8665555149788242 acc: 0.74\n",
      "loss: 1.8887819646017723 acc: 0.77\n",
      "loss: 1.8481536029958365 acc: 0.7\n",
      "loss: 1.8929424027549282 acc: 0.65\n",
      "loss: 1.8886913774539413 acc: 0.65\n",
      "loss: 1.8718253296006195 acc: 0.73\n",
      "loss: 1.9388361774181229 acc: 0.62\n",
      "loss: 1.8736507843247479 acc: 0.75\n",
      "loss: 1.9126386650119955 acc: 0.72\n",
      "loss: 1.91357060163826 acc: 0.67\n",
      "loss: 1.9223250332880588 acc: 0.68\n",
      "loss: 1.9108248787232178 acc: 0.63\n",
      "loss: 1.8905554680319678 acc: 0.71\n",
      "loss: 1.9310592367216903 acc: 0.67\n",
      "loss: 1.8812453940156937 acc: 0.68\n",
      "loss: 1.8819092192180384 acc: 0.71\n",
      "loss: 1.8904869022001192 acc: 0.65\n",
      "loss: 1.8856521788747225 acc: 0.69\n",
      "loss: 1.9103867667199483 acc: 0.65\n",
      "loss: 1.9328048401956326 acc: 0.6\n",
      "loss: 1.9306365853776482 acc: 0.65\n",
      "loss: 1.9027651294363397 acc: 0.7\n",
      "loss: 1.8778593805347596 acc: 0.73\n",
      "loss: 1.8478509489378863 acc: 0.77\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 1.8479\t Accuracy 0.7700\n",
      "loss: 1.885478061879661 acc: 0.74\n",
      "loss: 1.9108986896650613 acc: 0.66\n",
      "loss: 1.888527336541205 acc: 0.69\n",
      "loss: 1.897273886690756 acc: 0.72\n",
      "loss: 1.9140722884361836 acc: 0.65\n",
      "loss: 1.875029741797636 acc: 0.68\n",
      "loss: 1.8917732695855236 acc: 0.64\n",
      "loss: 1.9193245427596022 acc: 0.63\n",
      "loss: 1.889895108228447 acc: 0.71\n",
      "loss: 1.8915681860288385 acc: 0.72\n",
      "loss: 1.8738857230141175 acc: 0.69\n",
      "loss: 1.9092759384259026 acc: 0.67\n",
      "loss: 1.8718063483770937 acc: 0.69\n",
      "loss: 1.8833932703656948 acc: 0.69\n",
      "loss: 1.8948869799640022 acc: 0.68\n",
      "loss: 1.9216404120436328 acc: 0.57\n",
      "loss: 1.9157474912278667 acc: 0.7\n",
      "loss: 1.8488465497590798 acc: 0.74\n",
      "loss: 1.8544026733953762 acc: 0.71\n",
      "loss: 1.9111087128179098 acc: 0.64\n",
      "loss: 1.875907269733909 acc: 0.81\n",
      "loss: 1.9120725119141033 acc: 0.67\n",
      "loss: 1.9163812422991047 acc: 0.73\n",
      "loss: 1.9054069130329685 acc: 0.69\n",
      "loss: 1.9399372503143102 acc: 0.63\n",
      "loss: 1.852916062752586 acc: 0.74\n",
      "loss: 1.9042568065371512 acc: 0.65\n",
      "loss: 1.8875388497451622 acc: 0.69\n",
      "loss: 1.880639487916044 acc: 0.63\n",
      "loss: 1.9471280418222172 acc: 0.58\n",
      "loss: 1.8895173802976821 acc: 0.64\n",
      "loss: 1.86371832230322 acc: 0.8\n",
      "loss: 1.8711798865121148 acc: 0.77\n",
      "loss: 1.9002994404162934 acc: 0.7\n",
      "loss: 1.912518203890711 acc: 0.65\n",
      "loss: 1.912681997850091 acc: 0.69\n",
      "loss: 1.905600948250509 acc: 0.69\n",
      "loss: 1.9283968267446858 acc: 0.65\n",
      "loss: 1.9161305650945295 acc: 0.64\n",
      "loss: 1.8830498356893708 acc: 0.72\n",
      "loss: 1.8929522850348648 acc: 0.72\n",
      "loss: 1.9168015239536742 acc: 0.64\n",
      "loss: 1.839297598330256 acc: 0.74\n",
      "loss: 1.867304865971012 acc: 0.68\n",
      "loss: 1.8287175828728923 acc: 0.78\n",
      "loss: 1.9078174751121377 acc: 0.68\n",
      "loss: 1.9365647159893917 acc: 0.66\n",
      "loss: 1.8855171769674266 acc: 0.73\n",
      "loss: 1.9236211389076658 acc: 0.68\n",
      "loss: 1.8577239726729065 acc: 0.71\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 1.8577\t Accuracy 0.7100\n",
      "loss: 1.9198707697431763 acc: 0.75\n",
      "loss: 1.8789140643345659 acc: 0.77\n",
      "loss: 1.9194992571447136 acc: 0.67\n",
      "loss: 1.8212357777821184 acc: 0.78\n",
      "loss: 1.8604389654342384 acc: 0.79\n",
      "loss: 1.8465888082082458 acc: 0.76\n",
      "loss: 1.9704409102741585 acc: 0.62\n",
      "loss: 1.8658205705331934 acc: 0.77\n",
      "loss: 1.8903056219165735 acc: 0.72\n",
      "loss: 1.8683307606803607 acc: 0.69\n",
      "loss: 1.8872575850790738 acc: 0.66\n",
      "loss: 1.8907874665461213 acc: 0.71\n",
      "loss: 1.924862154222886 acc: 0.68\n",
      "loss: 1.8308116250279307 acc: 0.73\n",
      "loss: 1.8850174627384855 acc: 0.68\n",
      "loss: 1.8678132233364926 acc: 0.76\n",
      "loss: 1.9321870053091827 acc: 0.63\n",
      "loss: 1.9121652114286787 acc: 0.68\n",
      "loss: 1.8674202991127544 acc: 0.76\n",
      "loss: 1.9064067175358732 acc: 0.66\n",
      "loss: 1.8802976219613277 acc: 0.7\n",
      "loss: 1.875037423316226 acc: 0.7\n",
      "loss: 1.8258799770498948 acc: 0.79\n",
      "loss: 1.9053736205174467 acc: 0.69\n",
      "loss: 1.9077796796563453 acc: 0.69\n",
      "loss: 1.8534766782875698 acc: 0.73\n",
      "loss: 1.8882957028621201 acc: 0.72\n",
      "loss: 1.8495725550718038 acc: 0.76\n",
      "loss: 1.8536853340390183 acc: 0.74\n",
      "loss: 1.8580222535399644 acc: 0.76\n",
      "loss: 1.8597412491895107 acc: 0.78\n",
      "loss: 1.8829108270230202 acc: 0.72\n",
      "loss: 1.8876151306807905 acc: 0.67\n",
      "loss: 1.891511146160926 acc: 0.73\n",
      "loss: 1.8609336651432042 acc: 0.78\n",
      "loss: 1.8443520730168041 acc: 0.76\n",
      "loss: 1.8993886402486087 acc: 0.72\n",
      "loss: 1.8870612678112215 acc: 0.72\n",
      "loss: 1.8602739280033054 acc: 0.7\n",
      "loss: 1.8768411370879472 acc: 0.72\n",
      "loss: 1.9170370398236067 acc: 0.7\n",
      "loss: 1.915802841916709 acc: 0.69\n",
      "loss: 1.8877201941563646 acc: 0.7\n",
      "loss: 1.8296146957396286 acc: 0.76\n",
      "loss: 1.8944101122562114 acc: 0.71\n",
      "loss: 1.9038303766125133 acc: 0.72\n",
      "loss: 1.8964460471159168 acc: 0.66\n",
      "loss: 1.9423009363774804 acc: 0.66\n",
      "loss: 1.8784457403510642 acc: 0.7\n",
      "loss: 1.885920735880381 acc: 0.76\n",
      "loss: 1.8519859559057525 acc: 0.78\n",
      "loss: 1.8507319096800119 acc: 0.77\n",
      "loss: 1.86856113648621 acc: 0.77\n",
      "loss: 1.8993861743115645 acc: 0.7\n",
      "loss: 1.8691619940820112 acc: 0.78\n",
      "loss: 1.8203988704358767 acc: 0.77\n",
      "loss: 1.8253667676457337 acc: 0.76\n",
      "loss: 1.8704245416345133 acc: 0.75\n",
      "loss: 1.8129218216967546 acc: 0.85\n",
      "loss: 1.8635441835310735 acc: 0.73\n",
      "loss: 1.827662883635809 acc: 0.79\n",
      "loss: 1.887901397378688 acc: 0.76\n",
      "loss: 1.9069900699485467 acc: 0.74\n",
      "loss: 1.9133170054928286 acc: 0.67\n",
      "loss: 1.8670154636040512 acc: 0.74\n",
      "loss: 1.867947583649551 acc: 0.75\n",
      "loss: 1.8246537495312898 acc: 0.78\n",
      "loss: 1.8504545102158623 acc: 0.8\n",
      "loss: 1.8286016938306862 acc: 0.82\n",
      "loss: 1.9461246118720588 acc: 0.66\n",
      "loss: 1.881512520208712 acc: 0.72\n",
      "loss: 1.8800555579457676 acc: 0.7\n",
      "loss: 1.8810864887776075 acc: 0.74\n",
      "loss: 1.888379913313988 acc: 0.78\n",
      "loss: 1.9104456363912228 acc: 0.65\n",
      "loss: 1.9039303604432896 acc: 0.71\n",
      "loss: 1.9111578923024246 acc: 0.71\n",
      "loss: 1.9017552514914207 acc: 0.73\n",
      "loss: 1.8315112125178123 acc: 0.73\n",
      "loss: 1.870093571651227 acc: 0.76\n",
      "loss: 1.839705163855338 acc: 0.76\n",
      "loss: 1.8232598401936735 acc: 0.8\n",
      "loss: 1.8307421634940886 acc: 0.83\n",
      "loss: 1.8573975347283909 acc: 0.73\n",
      "loss: 1.8664214595236124 acc: 0.79\n",
      "loss: 1.901072440504399 acc: 0.76\n",
      "loss: 1.9178283341997941 acc: 0.68\n",
      "loss: 1.8708333949586065 acc: 0.7\n",
      "loss: 1.73917472435638 acc: 0.9\n",
      "loss: 1.7915056679281693 acc: 0.86\n",
      "loss: 1.818253771966125 acc: 0.86\n",
      "loss: 1.8543643556611387 acc: 0.76\n",
      "loss: 1.8752743292993121 acc: 0.76\n",
      "loss: 1.7944601789002237 acc: 0.68\n",
      "loss: 1.8790368654018896 acc: 0.8\n",
      "loss: 1.850996869660363 acc: 0.79\n",
      "loss: 1.9300718016965428 acc: 0.7\n",
      "loss: 1.7471264192788736 acc: 0.9\n",
      "loss: 1.9825034293901296 acc: 0.55\n",
      "\n",
      "Epoch [0]\t Average training loss 1.9664\t Average training accuracy 0.5709\n",
      "Epoch [0]\t Average validation loss 1.8634\t Average validation accuracy 0.7554\n",
      "\n",
      "loss: 1.8209372611821115 acc: 0.74\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 1.8209\t Accuracy 0.7400\n",
      "loss: 1.9133692566659803 acc: 0.66\n",
      "loss: 1.8323488697377825 acc: 0.75\n",
      "loss: 1.8845619311822057 acc: 0.76\n",
      "loss: 1.8563915473601216 acc: 0.73\n",
      "loss: 1.9091239143034289 acc: 0.67\n",
      "loss: 1.9213088329771548 acc: 0.67\n",
      "loss: 1.9112636312820004 acc: 0.71\n",
      "loss: 1.8869217199733452 acc: 0.69\n",
      "loss: 1.8586537898568982 acc: 0.75\n",
      "loss: 1.8557376434638109 acc: 0.76\n",
      "loss: 1.8732915839877597 acc: 0.74\n",
      "loss: 1.8951866529780304 acc: 0.71\n",
      "loss: 1.948567280387797 acc: 0.57\n",
      "loss: 1.869868792233885 acc: 0.73\n",
      "loss: 1.8696641228773923 acc: 0.76\n",
      "loss: 1.8833488365011926 acc: 0.74\n",
      "loss: 1.8207832481746455 acc: 0.76\n",
      "loss: 1.8724397189254245 acc: 0.74\n",
      "loss: 1.896860908600306 acc: 0.7\n",
      "loss: 1.8865269090783192 acc: 0.74\n",
      "loss: 1.9233255733566994 acc: 0.65\n",
      "loss: 1.8816838943600234 acc: 0.7\n",
      "loss: 1.8497644341426507 acc: 0.74\n",
      "loss: 1.868820098511892 acc: 0.69\n",
      "loss: 1.8909079098680592 acc: 0.75\n",
      "loss: 1.8935365600821308 acc: 0.76\n",
      "loss: 1.9075289532046107 acc: 0.7\n",
      "loss: 1.8804764363670983 acc: 0.71\n",
      "loss: 1.8678322118348663 acc: 0.76\n",
      "loss: 1.8519756771603315 acc: 0.69\n",
      "loss: 1.8415379364731068 acc: 0.74\n",
      "loss: 1.8994439962810798 acc: 0.67\n",
      "loss: 1.8777879378738256 acc: 0.75\n",
      "loss: 1.8580437162615857 acc: 0.75\n",
      "loss: 1.8590365714448092 acc: 0.72\n",
      "loss: 1.834173371092442 acc: 0.79\n",
      "loss: 1.8897921842683936 acc: 0.72\n",
      "loss: 1.89121137282746 acc: 0.69\n",
      "loss: 1.8593914716366036 acc: 0.72\n",
      "loss: 1.8958481276258383 acc: 0.7\n",
      "loss: 1.8945679280983965 acc: 0.73\n",
      "loss: 1.8982171422786533 acc: 0.66\n",
      "loss: 1.871432491151641 acc: 0.79\n",
      "loss: 1.8456459711901194 acc: 0.71\n",
      "loss: 1.829084584628973 acc: 0.79\n",
      "loss: 1.9123746420195256 acc: 0.68\n",
      "loss: 1.8456391507136425 acc: 0.76\n",
      "loss: 1.8692346142157268 acc: 0.73\n",
      "loss: 1.880874010740878 acc: 0.74\n",
      "loss: 1.8617621325059122 acc: 0.76\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 1.8618\t Accuracy 0.7600\n",
      "loss: 1.8602449878376555 acc: 0.73\n",
      "loss: 1.8674841175482837 acc: 0.72\n",
      "loss: 1.8507963628726847 acc: 0.71\n",
      "loss: 1.8881191150629093 acc: 0.74\n",
      "loss: 1.8970855889609197 acc: 0.69\n",
      "loss: 1.8738121727199184 acc: 0.74\n",
      "loss: 1.8959876812960217 acc: 0.7\n",
      "loss: 1.8906393616038428 acc: 0.77\n",
      "loss: 1.8683275232089287 acc: 0.8\n",
      "loss: 1.838814935222632 acc: 0.76\n",
      "loss: 1.8500362854960566 acc: 0.73\n",
      "loss: 1.8806087974268981 acc: 0.71\n",
      "loss: 1.8764740943548768 acc: 0.76\n",
      "loss: 1.8704865806063242 acc: 0.73\n",
      "loss: 1.8697397863142382 acc: 0.66\n",
      "loss: 1.8926364863135219 acc: 0.68\n",
      "loss: 1.9041726670822265 acc: 0.7\n",
      "loss: 1.8443665968095464 acc: 0.76\n",
      "loss: 1.9012563945511147 acc: 0.69\n",
      "loss: 1.851584818717405 acc: 0.77\n",
      "loss: 1.872870062404389 acc: 0.73\n",
      "loss: 1.874515441718052 acc: 0.7\n",
      "loss: 1.9606923019202744 acc: 0.58\n",
      "loss: 1.8868969084993872 acc: 0.7\n",
      "loss: 1.8687820169687188 acc: 0.73\n",
      "loss: 1.9042574725451482 acc: 0.66\n",
      "loss: 1.8536422223777516 acc: 0.75\n",
      "loss: 1.8852882110093572 acc: 0.7\n",
      "loss: 1.882584256414259 acc: 0.69\n",
      "loss: 1.8630431699870311 acc: 0.73\n",
      "loss: 1.8602435224105247 acc: 0.76\n",
      "loss: 1.8776618899370905 acc: 0.7\n",
      "loss: 1.8647671749729073 acc: 0.74\n",
      "loss: 1.878648600850604 acc: 0.77\n",
      "loss: 1.8914692179809753 acc: 0.73\n",
      "loss: 1.838484177354351 acc: 0.73\n",
      "loss: 1.8736304155056434 acc: 0.72\n",
      "loss: 1.878354064701219 acc: 0.77\n",
      "loss: 1.9033894403666058 acc: 0.79\n",
      "loss: 1.8863818140062856 acc: 0.76\n",
      "loss: 1.8217391095169477 acc: 0.81\n",
      "loss: 1.84259292614276 acc: 0.79\n",
      "loss: 1.8803142067727479 acc: 0.7\n",
      "loss: 1.9023858780067335 acc: 0.71\n",
      "loss: 1.8729794716620296 acc: 0.73\n",
      "loss: 1.893680470708409 acc: 0.72\n",
      "loss: 1.885250393506081 acc: 0.73\n",
      "loss: 1.8964385081890314 acc: 0.73\n",
      "loss: 1.8854279178040683 acc: 0.66\n",
      "loss: 1.8364725140112066 acc: 0.79\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 1.8365\t Accuracy 0.7900\n",
      "loss: 1.8827526289706358 acc: 0.72\n",
      "loss: 1.8553023709413439 acc: 0.75\n",
      "loss: 1.8720918199732954 acc: 0.78\n",
      "loss: 1.863385202692017 acc: 0.74\n",
      "loss: 1.911623236046632 acc: 0.7\n",
      "loss: 1.9098416625584977 acc: 0.69\n",
      "loss: 1.9121522733168355 acc: 0.66\n",
      "loss: 1.865607159517646 acc: 0.74\n",
      "loss: 1.8927739908212131 acc: 0.73\n",
      "loss: 1.8740334083984806 acc: 0.74\n",
      "loss: 1.8842736459429164 acc: 0.72\n",
      "loss: 1.8860683678940928 acc: 0.71\n",
      "loss: 1.895348580126859 acc: 0.76\n",
      "loss: 1.8495853385752892 acc: 0.76\n",
      "loss: 1.865346248495111 acc: 0.71\n",
      "loss: 1.8842375986732796 acc: 0.71\n",
      "loss: 1.8747070105096777 acc: 0.76\n",
      "loss: 1.8811989788526424 acc: 0.72\n",
      "loss: 1.835872293839879 acc: 0.77\n",
      "loss: 1.8903054896092046 acc: 0.74\n",
      "loss: 1.865104725598574 acc: 0.73\n",
      "loss: 1.8707828665851516 acc: 0.73\n",
      "loss: 1.8562481559303896 acc: 0.71\n",
      "loss: 1.8918409513561192 acc: 0.68\n",
      "loss: 1.8788767798780355 acc: 0.7\n",
      "loss: 1.8559615018351705 acc: 0.81\n",
      "loss: 1.8798060558342686 acc: 0.75\n",
      "loss: 1.892279607524772 acc: 0.74\n",
      "loss: 1.8573582179006474 acc: 0.78\n",
      "loss: 1.8535415662329726 acc: 0.78\n",
      "loss: 1.919546669634417 acc: 0.65\n",
      "loss: 1.8709818561767173 acc: 0.77\n",
      "loss: 1.8374189532962426 acc: 0.73\n",
      "loss: 1.8702032169027982 acc: 0.74\n",
      "loss: 1.8651839839413398 acc: 0.68\n",
      "loss: 1.8576712194639577 acc: 0.8\n",
      "loss: 1.8941033776733425 acc: 0.75\n",
      "loss: 1.9174196404653125 acc: 0.74\n",
      "loss: 1.861582198553418 acc: 0.71\n",
      "loss: 1.885058767559144 acc: 0.76\n",
      "loss: 1.8790038421210626 acc: 0.67\n",
      "loss: 1.8980840264080299 acc: 0.73\n",
      "loss: 1.8553500570672694 acc: 0.73\n",
      "loss: 1.896241924444077 acc: 0.77\n",
      "loss: 1.85309108136451 acc: 0.76\n",
      "loss: 1.8543292821242567 acc: 0.74\n",
      "loss: 1.8506897745825248 acc: 0.76\n",
      "loss: 1.8617974173503031 acc: 0.74\n",
      "loss: 1.8932773596420953 acc: 0.67\n",
      "loss: 1.8870403041082142 acc: 0.7\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 1.8870\t Accuracy 0.7000\n",
      "loss: 1.91837644890768 acc: 0.68\n",
      "loss: 1.8827765198561486 acc: 0.68\n",
      "loss: 1.8816170861009096 acc: 0.7\n",
      "loss: 1.8724451335078494 acc: 0.72\n",
      "loss: 1.9254146481231686 acc: 0.7\n",
      "loss: 1.88389028040502 acc: 0.73\n",
      "loss: 1.8746179347592495 acc: 0.75\n",
      "loss: 1.853090655994045 acc: 0.74\n",
      "loss: 1.9150147908012243 acc: 0.65\n",
      "loss: 1.8983037400170917 acc: 0.68\n",
      "loss: 1.8433926111317458 acc: 0.76\n",
      "loss: 1.8846401502309462 acc: 0.72\n",
      "loss: 1.883107377390929 acc: 0.72\n",
      "loss: 1.8499884350302196 acc: 0.74\n",
      "loss: 1.833488501187687 acc: 0.78\n",
      "loss: 1.824910823599766 acc: 0.82\n",
      "loss: 1.8833262359400431 acc: 0.74\n",
      "loss: 1.8934729238886894 acc: 0.72\n",
      "loss: 1.8748628345571041 acc: 0.75\n",
      "loss: 1.842301858855813 acc: 0.85\n",
      "loss: 1.8721154508226876 acc: 0.75\n",
      "loss: 1.8792424992718595 acc: 0.77\n",
      "loss: 1.9303712866050173 acc: 0.71\n",
      "loss: 1.839572660019016 acc: 0.79\n",
      "loss: 1.8954983655827753 acc: 0.71\n",
      "loss: 1.8851478312664482 acc: 0.77\n",
      "loss: 1.8106181007902171 acc: 0.84\n",
      "loss: 1.9028593353081027 acc: 0.72\n",
      "loss: 1.9329497257230497 acc: 0.67\n",
      "loss: 1.9111991138377493 acc: 0.68\n",
      "loss: 1.8761821970723924 acc: 0.72\n",
      "loss: 1.8552850091834006 acc: 0.81\n",
      "loss: 1.8528708022124363 acc: 0.8\n",
      "loss: 1.8654319861464546 acc: 0.76\n",
      "loss: 1.88941294718357 acc: 0.75\n",
      "loss: 1.8580756668619407 acc: 0.79\n",
      "loss: 1.8794665551638656 acc: 0.75\n",
      "loss: 1.8941344946290348 acc: 0.72\n",
      "loss: 1.8487003154886517 acc: 0.69\n",
      "loss: 1.8707916977138541 acc: 0.77\n",
      "loss: 1.8442203872483243 acc: 0.79\n",
      "loss: 1.8803816714638015 acc: 0.77\n",
      "loss: 1.9065180368775037 acc: 0.66\n",
      "loss: 1.8701935785799828 acc: 0.79\n",
      "loss: 1.8107572278690858 acc: 0.79\n",
      "loss: 1.8940007846770788 acc: 0.72\n",
      "loss: 1.9181615442218796 acc: 0.68\n",
      "loss: 1.8711976674197257 acc: 0.77\n",
      "loss: 1.9056675704162789 acc: 0.66\n",
      "loss: 1.8505819282022729 acc: 0.77\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 1.8506\t Accuracy 0.7700\n",
      "loss: 1.8752273820544274 acc: 0.7\n",
      "loss: 1.9109159407560898 acc: 0.7\n",
      "loss: 1.8948854980212835 acc: 0.72\n",
      "loss: 1.8518557216343277 acc: 0.78\n",
      "loss: 1.871059635817216 acc: 0.76\n",
      "loss: 1.8769323242761933 acc: 0.75\n",
      "loss: 1.8338277614098846 acc: 0.81\n",
      "loss: 1.9209494076493023 acc: 0.73\n",
      "loss: 1.8632342008265637 acc: 0.71\n",
      "loss: 1.8588350034343646 acc: 0.74\n",
      "loss: 1.8930091859930323 acc: 0.77\n",
      "loss: 1.845427521240087 acc: 0.77\n",
      "loss: 1.8625458892402937 acc: 0.74\n",
      "loss: 1.9149721471822467 acc: 0.69\n",
      "loss: 1.8541688658828446 acc: 0.71\n",
      "loss: 1.8888807349521926 acc: 0.71\n",
      "loss: 1.8995601467703644 acc: 0.67\n",
      "loss: 1.8988416536979074 acc: 0.7\n",
      "loss: 1.8814189185725547 acc: 0.76\n",
      "loss: 1.9035159938314477 acc: 0.69\n",
      "loss: 1.875913776082754 acc: 0.76\n",
      "loss: 1.8589509430930715 acc: 0.82\n",
      "loss: 1.8517764729910613 acc: 0.74\n",
      "loss: 1.85611211042239 acc: 0.75\n",
      "loss: 1.887426642900345 acc: 0.7\n",
      "loss: 1.835121248889431 acc: 0.77\n",
      "loss: 1.8660719942222663 acc: 0.77\n",
      "loss: 1.899702451027083 acc: 0.71\n",
      "loss: 1.917087693178139 acc: 0.7\n",
      "loss: 1.8704706525184003 acc: 0.72\n",
      "loss: 1.8839879362187801 acc: 0.7\n",
      "loss: 1.8741478618881766 acc: 0.78\n",
      "loss: 1.8827134684851778 acc: 0.7\n",
      "loss: 1.877776205094921 acc: 0.69\n",
      "loss: 1.8962841467099312 acc: 0.72\n",
      "loss: 1.8672769873721062 acc: 0.8\n",
      "loss: 1.8723866624188694 acc: 0.79\n",
      "loss: 1.8588638267406639 acc: 0.77\n",
      "loss: 1.8125036768259362 acc: 0.84\n",
      "loss: 1.8482110825845077 acc: 0.78\n",
      "loss: 1.8646805691675739 acc: 0.75\n",
      "loss: 1.845599774267231 acc: 0.8\n",
      "loss: 1.8577837710242155 acc: 0.77\n",
      "loss: 1.91689242867676 acc: 0.68\n",
      "loss: 1.8919258717205014 acc: 0.69\n",
      "loss: 1.8870508582624785 acc: 0.77\n",
      "loss: 1.8795576592444647 acc: 0.75\n",
      "loss: 1.868173071314073 acc: 0.73\n",
      "loss: 1.8595363136759355 acc: 0.72\n",
      "loss: 1.8732628458785792 acc: 0.69\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 1.8733\t Accuracy 0.6900\n",
      "loss: 1.8206337479745336 acc: 0.85\n",
      "loss: 1.8547045080463622 acc: 0.81\n",
      "loss: 1.911398943211144 acc: 0.71\n",
      "loss: 1.8789384033741903 acc: 0.71\n",
      "loss: 1.905575276646775 acc: 0.75\n",
      "loss: 1.8773140626796865 acc: 0.77\n",
      "loss: 1.8738548187229165 acc: 0.74\n",
      "loss: 1.8603629632589465 acc: 0.75\n",
      "loss: 1.9018632340845625 acc: 0.71\n",
      "loss: 1.8937646495130394 acc: 0.71\n",
      "loss: 1.9132884418904825 acc: 0.73\n",
      "loss: 1.882881112952874 acc: 0.74\n",
      "loss: 1.8750987115738136 acc: 0.72\n",
      "loss: 1.9118767363868412 acc: 0.66\n",
      "loss: 1.896177571220971 acc: 0.75\n",
      "loss: 1.8555663668666396 acc: 0.75\n",
      "loss: 1.8239831806034255 acc: 0.83\n",
      "loss: 1.8501286629938292 acc: 0.75\n",
      "loss: 1.8341936242575598 acc: 0.82\n",
      "loss: 1.891497439614294 acc: 0.71\n",
      "loss: 1.888506217657777 acc: 0.72\n",
      "loss: 1.8568862362660206 acc: 0.8\n",
      "loss: 1.8658218170161427 acc: 0.73\n",
      "loss: 1.8845637989699133 acc: 0.71\n",
      "loss: 1.8707309467793627 acc: 0.79\n",
      "loss: 1.8736474487797816 acc: 0.79\n",
      "loss: 1.8672441495474217 acc: 0.76\n",
      "loss: 1.881072644216612 acc: 0.65\n",
      "loss: 1.8730596175136034 acc: 0.75\n",
      "loss: 1.8767222721859278 acc: 0.73\n",
      "loss: 1.9147406748196802 acc: 0.73\n",
      "loss: 1.8904370299546591 acc: 0.72\n",
      "loss: 1.7981277005406235 acc: 0.89\n",
      "loss: 1.871080491728223 acc: 0.76\n",
      "loss: 1.8554532282226015 acc: 0.77\n",
      "loss: 1.8798486538119545 acc: 0.75\n",
      "loss: 1.850722695970125 acc: 0.77\n",
      "loss: 1.8445745113408338 acc: 0.72\n",
      "loss: 1.8885994137127482 acc: 0.76\n",
      "loss: 1.8730401019371945 acc: 0.71\n",
      "loss: 1.889401535877129 acc: 0.73\n",
      "loss: 1.8538275366798898 acc: 0.77\n",
      "loss: 1.8519512950383958 acc: 0.79\n",
      "loss: 1.8530624583758415 acc: 0.82\n",
      "loss: 1.864017818012976 acc: 0.74\n",
      "loss: 1.8874185840908333 acc: 0.79\n",
      "loss: 1.9047181915065416 acc: 0.71\n",
      "loss: 1.8794739430346579 acc: 0.71\n",
      "loss: 1.8441445843188367 acc: 0.78\n",
      "loss: 1.840990106050517 acc: 0.8\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 1.8410\t Accuracy 0.8000\n",
      "loss: 1.836765099140914 acc: 0.79\n",
      "loss: 1.8628255349007432 acc: 0.74\n",
      "loss: 1.829576269782766 acc: 0.81\n",
      "loss: 1.864759387033471 acc: 0.74\n",
      "loss: 1.8491756287906198 acc: 0.85\n",
      "loss: 1.8661390813339358 acc: 0.76\n",
      "loss: 1.832499965370342 acc: 0.8\n",
      "loss: 1.843462683511409 acc: 0.76\n",
      "loss: 1.8726818197585846 acc: 0.72\n",
      "loss: 1.8584980694643536 acc: 0.77\n",
      "loss: 1.873915154277846 acc: 0.76\n",
      "loss: 1.8168790902528782 acc: 0.8\n",
      "loss: 1.8349194083577063 acc: 0.76\n",
      "loss: 1.8143735579755131 acc: 0.84\n",
      "loss: 1.8528518690496236 acc: 0.75\n",
      "loss: 1.8931247075820852 acc: 0.71\n",
      "loss: 1.8695948583889013 acc: 0.72\n",
      "loss: 1.8901102821334148 acc: 0.69\n",
      "loss: 1.8544039580241038 acc: 0.82\n",
      "loss: 1.8599660413359016 acc: 0.78\n",
      "loss: 1.8731109182819725 acc: 0.7\n",
      "loss: 1.9258848779937898 acc: 0.67\n",
      "loss: 1.850941841820716 acc: 0.75\n",
      "loss: 1.8806267560718632 acc: 0.73\n",
      "loss: 1.8870108596103035 acc: 0.74\n",
      "loss: 1.872174688210672 acc: 0.75\n",
      "loss: 1.8572605326437466 acc: 0.83\n",
      "loss: 1.8529509309318737 acc: 0.71\n",
      "loss: 1.8258218481482025 acc: 0.8\n",
      "loss: 1.9106373164149755 acc: 0.72\n",
      "loss: 1.8643165930416805 acc: 0.74\n",
      "loss: 1.9073558407875482 acc: 0.62\n",
      "loss: 1.8418964416413917 acc: 0.76\n",
      "loss: 1.8676723571993312 acc: 0.75\n",
      "loss: 1.9023496099091974 acc: 0.67\n",
      "loss: 1.8695522807174811 acc: 0.81\n",
      "loss: 1.8495480681191803 acc: 0.73\n",
      "loss: 1.890485305433852 acc: 0.72\n",
      "loss: 1.9051595560040853 acc: 0.71\n",
      "loss: 1.8615071898911335 acc: 0.73\n",
      "loss: 1.887415438129668 acc: 0.76\n",
      "loss: 1.8782623281606041 acc: 0.73\n",
      "loss: 1.8466427249388864 acc: 0.74\n",
      "loss: 1.855316893839468 acc: 0.74\n",
      "loss: 1.867309879701654 acc: 0.77\n",
      "loss: 1.8382506199118325 acc: 0.78\n",
      "loss: 1.8407861591783177 acc: 0.81\n",
      "loss: 1.8880579688889276 acc: 0.74\n",
      "loss: 1.8346435224266315 acc: 0.72\n",
      "loss: 1.865354658573997 acc: 0.75\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 1.8654\t Accuracy 0.7500\n",
      "loss: 1.8772173252958695 acc: 0.79\n",
      "loss: 1.8848612441679489 acc: 0.77\n",
      "loss: 1.8658236592345838 acc: 0.76\n",
      "loss: 1.8481586344083398 acc: 0.82\n",
      "loss: 1.9005338482456913 acc: 0.74\n",
      "loss: 1.8903640495263074 acc: 0.71\n",
      "loss: 1.8338909119981435 acc: 0.85\n",
      "loss: 1.8975306942375654 acc: 0.72\n",
      "loss: 1.8628817242473903 acc: 0.77\n",
      "loss: 1.8892117673692548 acc: 0.74\n",
      "loss: 1.8356052725392078 acc: 0.76\n",
      "loss: 1.8712568567097037 acc: 0.75\n",
      "loss: 1.8687969322235753 acc: 0.75\n",
      "loss: 1.845894301714124 acc: 0.78\n",
      "loss: 1.8596722272496624 acc: 0.84\n",
      "loss: 1.855275810742254 acc: 0.77\n",
      "loss: 1.8680913357749267 acc: 0.73\n",
      "loss: 1.8664464301834758 acc: 0.71\n",
      "loss: 1.8991862959319166 acc: 0.73\n",
      "loss: 1.8959810530204246 acc: 0.75\n",
      "loss: 1.8697985224564062 acc: 0.75\n",
      "loss: 1.9056434169272578 acc: 0.74\n",
      "loss: 1.8532161589593839 acc: 0.73\n",
      "loss: 1.8809549820664488 acc: 0.78\n",
      "loss: 1.8996414770539392 acc: 0.76\n",
      "loss: 1.9233704742914788 acc: 0.65\n",
      "loss: 1.8881446635118417 acc: 0.7\n",
      "loss: 1.8797320791124101 acc: 0.75\n",
      "loss: 1.866006650334832 acc: 0.74\n",
      "loss: 1.8858647966742654 acc: 0.73\n",
      "loss: 1.8275621651363354 acc: 0.84\n",
      "loss: 1.908251918231125 acc: 0.7\n",
      "loss: 1.8996962077522128 acc: 0.73\n",
      "loss: 1.9229118217243768 acc: 0.67\n",
      "loss: 1.8573187948976544 acc: 0.79\n",
      "loss: 1.8514978471906973 acc: 0.81\n",
      "loss: 1.8892467448274213 acc: 0.75\n",
      "loss: 1.8467215840534918 acc: 0.78\n",
      "loss: 1.8593730313939998 acc: 0.71\n",
      "loss: 1.8909355894012787 acc: 0.72\n",
      "loss: 1.8637545117379724 acc: 0.81\n",
      "loss: 1.8658182086650033 acc: 0.77\n",
      "loss: 1.8958927694047085 acc: 0.74\n",
      "loss: 1.8752225492466874 acc: 0.72\n",
      "loss: 1.8558870277142037 acc: 0.82\n",
      "loss: 1.9014342536040458 acc: 0.63\n",
      "loss: 1.8723289248948225 acc: 0.73\n",
      "loss: 1.835064162580752 acc: 0.81\n",
      "loss: 1.9022241400464888 acc: 0.75\n",
      "loss: 1.8328094797680112 acc: 0.83\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 1.8328\t Accuracy 0.8300\n",
      "loss: 1.8409898576580432 acc: 0.74\n",
      "loss: 1.8403173958935657 acc: 0.82\n",
      "loss: 1.8502826669547858 acc: 0.74\n",
      "loss: 1.8659583729919909 acc: 0.79\n",
      "loss: 1.8560139362803676 acc: 0.78\n",
      "loss: 1.870759799301939 acc: 0.75\n",
      "loss: 1.873009085733364 acc: 0.75\n",
      "loss: 1.8490728678204575 acc: 0.75\n",
      "loss: 1.848883689532134 acc: 0.76\n",
      "loss: 1.849154668899381 acc: 0.81\n",
      "loss: 1.8017087670205254 acc: 0.83\n",
      "loss: 1.8796788222429894 acc: 0.66\n",
      "loss: 1.8514428602414097 acc: 0.77\n",
      "loss: 1.8416936319789081 acc: 0.77\n",
      "loss: 1.8711143701793347 acc: 0.73\n",
      "loss: 1.831958171045065 acc: 0.85\n",
      "loss: 1.8680090223555823 acc: 0.76\n",
      "loss: 1.8657239545544315 acc: 0.77\n",
      "loss: 1.8773495118671624 acc: 0.7\n",
      "loss: 1.87455793612319 acc: 0.75\n",
      "loss: 1.8220081708055609 acc: 0.81\n",
      "loss: 1.8694427288866655 acc: 0.78\n",
      "loss: 1.8605657859146558 acc: 0.82\n",
      "loss: 1.8801755318490363 acc: 0.76\n",
      "loss: 1.8841295277452834 acc: 0.73\n",
      "loss: 1.838645622434419 acc: 0.84\n",
      "loss: 1.8765978688129856 acc: 0.7\n",
      "loss: 1.859568720754445 acc: 0.78\n",
      "loss: 1.8401205971547672 acc: 0.79\n",
      "loss: 1.855014210512273 acc: 0.76\n",
      "loss: 1.8273055888708236 acc: 0.8\n",
      "loss: 1.8337681138366297 acc: 0.82\n",
      "loss: 1.8653844991757882 acc: 0.79\n",
      "loss: 1.8752321435454709 acc: 0.72\n",
      "loss: 1.8710009230738016 acc: 0.74\n",
      "loss: 1.846135336793663 acc: 0.8\n",
      "loss: 1.860468461630351 acc: 0.78\n",
      "loss: 1.8811384611931627 acc: 0.73\n",
      "loss: 1.8787111016820575 acc: 0.76\n",
      "loss: 1.8974992249148925 acc: 0.74\n",
      "loss: 1.908160859710174 acc: 0.67\n",
      "loss: 1.8487174027196012 acc: 0.79\n",
      "loss: 1.883308746353135 acc: 0.76\n",
      "loss: 1.8286911462368307 acc: 0.87\n",
      "loss: 1.8629151416538408 acc: 0.77\n",
      "loss: 1.890220396246204 acc: 0.74\n",
      "loss: 1.8852885721341266 acc: 0.75\n",
      "loss: 1.8505868877608074 acc: 0.81\n",
      "loss: 1.8704731782717892 acc: 0.7\n",
      "loss: 1.8773911780014598 acc: 0.69\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 1.8774\t Accuracy 0.6900\n",
      "loss: 1.8428528639391084 acc: 0.82\n",
      "loss: 1.839735283750906 acc: 0.82\n",
      "loss: 1.845735195167262 acc: 0.77\n",
      "loss: 1.8499682396535126 acc: 0.82\n",
      "loss: 1.8803253009937335 acc: 0.76\n",
      "loss: 1.850775422648052 acc: 0.76\n",
      "loss: 1.8614772477494008 acc: 0.74\n",
      "loss: 1.8698598439705487 acc: 0.75\n",
      "loss: 1.837113502757337 acc: 0.83\n",
      "loss: 1.84649604223966 acc: 0.78\n",
      "loss: 1.8372157889330643 acc: 0.81\n",
      "loss: 1.8547094195175509 acc: 0.79\n",
      "loss: 1.836933482565758 acc: 0.72\n",
      "loss: 1.9121087150296412 acc: 0.65\n",
      "loss: 1.8659888852378654 acc: 0.75\n",
      "loss: 1.8649047171274737 acc: 0.77\n",
      "loss: 1.8489350955880326 acc: 0.77\n",
      "loss: 1.8528503866433976 acc: 0.78\n",
      "loss: 1.8653673821635133 acc: 0.67\n",
      "loss: 1.8245218928984897 acc: 0.85\n",
      "loss: 1.8777939469658935 acc: 0.79\n",
      "loss: 1.8578998431010183 acc: 0.79\n",
      "loss: 1.8808727674211279 acc: 0.73\n",
      "loss: 1.8342264145752143 acc: 0.83\n",
      "loss: 1.8336065852141332 acc: 0.81\n",
      "loss: 1.8662610256553804 acc: 0.76\n",
      "loss: 1.8972006983162697 acc: 0.71\n",
      "loss: 1.894689053168248 acc: 0.68\n",
      "loss: 1.9060278057165276 acc: 0.72\n",
      "loss: 1.8486857083489934 acc: 0.79\n",
      "loss: 1.8895483140255882 acc: 0.77\n",
      "loss: 1.841548895135562 acc: 0.78\n",
      "loss: 1.8613693912132478 acc: 0.75\n",
      "loss: 1.8715713169950519 acc: 0.77\n",
      "loss: 1.8726409807836086 acc: 0.79\n",
      "loss: 1.8542401456444852 acc: 0.78\n",
      "loss: 1.8557545818910497 acc: 0.78\n",
      "loss: 1.8743441075567657 acc: 0.74\n",
      "loss: 1.8147058143580181 acc: 0.84\n",
      "loss: 1.849611409558134 acc: 0.8\n",
      "loss: 1.842886697372281 acc: 0.78\n",
      "loss: 1.885427138998258 acc: 0.77\n",
      "loss: 1.890183835738917 acc: 0.71\n",
      "loss: 1.8469278309455521 acc: 0.79\n",
      "loss: 1.8724275012250677 acc: 0.73\n",
      "loss: 1.8570333954085587 acc: 0.76\n",
      "loss: 1.860235141597055 acc: 0.81\n",
      "loss: 1.9113553915443446 acc: 0.75\n",
      "loss: 1.8615775949164317 acc: 0.8\n",
      "loss: 1.861621639196516 acc: 0.74\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 1.8616\t Accuracy 0.7400\n",
      "loss: 1.8234739265289885 acc: 0.82\n",
      "loss: 1.8561023729795747 acc: 0.76\n",
      "loss: 1.8520539364310256 acc: 0.82\n",
      "loss: 1.8401566724326273 acc: 0.79\n",
      "loss: 1.8758985296354183 acc: 0.77\n",
      "loss: 1.879574173128294 acc: 0.78\n",
      "loss: 1.8461579682215532 acc: 0.83\n",
      "loss: 1.89653958375293 acc: 0.67\n",
      "loss: 1.8914650131326076 acc: 0.7\n",
      "loss: 1.852850386842368 acc: 0.79\n",
      "loss: 1.8828019443331667 acc: 0.71\n",
      "loss: 1.845471426510111 acc: 0.83\n",
      "loss: 1.8727119717926934 acc: 0.78\n",
      "loss: 1.867160086221499 acc: 0.78\n",
      "loss: 1.8557678454072066 acc: 0.8\n",
      "loss: 1.8445533112543568 acc: 0.74\n",
      "loss: 1.8876836135302444 acc: 0.71\n",
      "loss: 1.886410118071817 acc: 0.74\n",
      "loss: 1.8857256439478498 acc: 0.79\n",
      "loss: 1.8650195669743477 acc: 0.79\n",
      "loss: 1.8480505275369694 acc: 0.83\n",
      "loss: 1.8617814448264058 acc: 0.74\n",
      "loss: 1.8554839437871617 acc: 0.81\n",
      "loss: 1.8563918730457336 acc: 0.77\n",
      "loss: 1.8404924465774488 acc: 0.83\n",
      "loss: 1.8638736958479276 acc: 0.82\n",
      "loss: 1.8154606611119315 acc: 0.83\n",
      "loss: 1.8808491017780669 acc: 0.77\n",
      "loss: 1.838264242883822 acc: 0.83\n",
      "loss: 1.8738168680341822 acc: 0.75\n",
      "loss: 1.8868496835873492 acc: 0.78\n",
      "loss: 1.845056967367452 acc: 0.76\n",
      "loss: 1.8284089262848593 acc: 0.82\n",
      "loss: 1.8580288405420646 acc: 0.81\n",
      "loss: 1.8168419872362032 acc: 0.8\n",
      "loss: 1.8584853985032197 acc: 0.81\n",
      "loss: 1.8835971310691955 acc: 0.74\n",
      "loss: 1.8664220355719825 acc: 0.71\n",
      "loss: 1.8307019077555244 acc: 0.87\n",
      "loss: 1.9017371662741172 acc: 0.75\n",
      "loss: 1.8875554340340386 acc: 0.72\n",
      "loss: 1.9079357874714264 acc: 0.71\n",
      "loss: 1.8678618641975058 acc: 0.78\n",
      "loss: 1.8420924557978808 acc: 0.79\n",
      "loss: 1.8661915077191196 acc: 0.77\n",
      "loss: 1.8955110618734452 acc: 0.71\n",
      "loss: 1.8255993512350526 acc: 0.81\n",
      "loss: 1.87190245086586 acc: 0.81\n",
      "loss: 1.873467561465753 acc: 0.78\n",
      "loss: 1.8523550670479159 acc: 0.8\n",
      "loss: 1.8267328240984986 acc: 0.82\n",
      "loss: 1.828506215952075 acc: 0.83\n",
      "loss: 1.8503374358767148 acc: 0.82\n",
      "loss: 1.871924661673221 acc: 0.76\n",
      "loss: 1.8533919013511762 acc: 0.82\n",
      "loss: 1.7949807073765234 acc: 0.84\n",
      "loss: 1.7985780391398405 acc: 0.82\n",
      "loss: 1.8427963141954868 acc: 0.8\n",
      "loss: 1.7889416873731496 acc: 0.9\n",
      "loss: 1.8368676219027464 acc: 0.83\n",
      "loss: 1.8107894002571698 acc: 0.82\n",
      "loss: 1.8664021717359112 acc: 0.79\n",
      "loss: 1.8885400624732571 acc: 0.73\n",
      "loss: 1.8888947251243686 acc: 0.76\n",
      "loss: 1.8358201653801123 acc: 0.83\n",
      "loss: 1.8390826404523881 acc: 0.78\n",
      "loss: 1.795920560292727 acc: 0.83\n",
      "loss: 1.8312711649832796 acc: 0.82\n",
      "loss: 1.8053401129105615 acc: 0.85\n",
      "loss: 1.9192055748468837 acc: 0.73\n",
      "loss: 1.8609113077179935 acc: 0.83\n",
      "loss: 1.8584610200484963 acc: 0.72\n",
      "loss: 1.8638419369907593 acc: 0.8\n",
      "loss: 1.8652259632268484 acc: 0.8\n",
      "loss: 1.8884722730008305 acc: 0.73\n",
      "loss: 1.8871691426550807 acc: 0.75\n",
      "loss: 1.8969987704366558 acc: 0.78\n",
      "loss: 1.8765307654693562 acc: 0.82\n",
      "loss: 1.8143300566411233 acc: 0.85\n",
      "loss: 1.8469462037314284 acc: 0.85\n",
      "loss: 1.8134868104848294 acc: 0.86\n",
      "loss: 1.787178173550081 acc: 0.88\n",
      "loss: 1.8067454055855003 acc: 0.87\n",
      "loss: 1.833952167565365 acc: 0.82\n",
      "loss: 1.8448289951322208 acc: 0.83\n",
      "loss: 1.8775541371602058 acc: 0.85\n",
      "loss: 1.892559593612969 acc: 0.8\n",
      "loss: 1.8235535442916944 acc: 0.82\n",
      "loss: 1.7126450285050634 acc: 0.95\n",
      "loss: 1.762151585860928 acc: 0.9\n",
      "loss: 1.7896937962703179 acc: 0.9\n",
      "loss: 1.833323493367479 acc: 0.84\n",
      "loss: 1.8456644051976514 acc: 0.79\n",
      "loss: 1.762736099799954 acc: 0.77\n",
      "loss: 1.8454603994350374 acc: 0.83\n",
      "loss: 1.8324120657310707 acc: 0.85\n",
      "loss: 1.9211301172641149 acc: 0.71\n",
      "loss: 1.7240239641952122 acc: 0.93\n",
      "loss: 1.9389702359645316 acc: 0.71\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8703\t Average training accuracy 0.7481\n",
      "Epoch [1]\t Average validation loss 1.8387\t Average validation accuracy 0.8164\n",
      "\n",
      "loss: 1.8888703373465106 acc: 0.74\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 1.8889\t Accuracy 0.7400\n",
      "loss: 1.8687188420445535 acc: 0.73\n",
      "loss: 1.8368070696664478 acc: 0.81\n",
      "loss: 1.8304931049179627 acc: 0.81\n",
      "loss: 1.877201385366869 acc: 0.71\n",
      "loss: 1.8615564206242845 acc: 0.81\n",
      "loss: 1.8677468297044075 acc: 0.77\n",
      "loss: 1.8987274866938686 acc: 0.7\n",
      "loss: 1.8473276168081645 acc: 0.81\n",
      "loss: 1.890668063916967 acc: 0.74\n",
      "loss: 1.8680591529767352 acc: 0.78\n",
      "loss: 1.8360477399266728 acc: 0.82\n",
      "loss: 1.8598706632647912 acc: 0.79\n",
      "loss: 1.8324813832648106 acc: 0.77\n",
      "loss: 1.8861454813271434 acc: 0.73\n",
      "loss: 1.8345600430963485 acc: 0.82\n",
      "loss: 1.8643544446272862 acc: 0.79\n",
      "loss: 1.871875676599129 acc: 0.81\n",
      "loss: 1.80901160926075 acc: 0.86\n",
      "loss: 1.8709447865291091 acc: 0.79\n",
      "loss: 1.8389291399225645 acc: 0.87\n",
      "loss: 1.8553848620236113 acc: 0.76\n",
      "loss: 1.887054782809655 acc: 0.78\n",
      "loss: 1.875483259987744 acc: 0.83\n",
      "loss: 1.8718309612556099 acc: 0.75\n",
      "loss: 1.852278366303064 acc: 0.78\n",
      "loss: 1.8449216686719283 acc: 0.8\n",
      "loss: 1.878054970972863 acc: 0.78\n",
      "loss: 1.836123005866706 acc: 0.77\n",
      "loss: 1.8597124326243308 acc: 0.78\n",
      "loss: 1.814375181153957 acc: 0.81\n",
      "loss: 1.8462243807938907 acc: 0.8\n",
      "loss: 1.8652370993308054 acc: 0.72\n",
      "loss: 1.8458918561619853 acc: 0.85\n",
      "loss: 1.8724266539745538 acc: 0.74\n",
      "loss: 1.834912299674541 acc: 0.8\n",
      "loss: 1.91420025273939 acc: 0.71\n",
      "loss: 1.8279532667435703 acc: 0.8\n",
      "loss: 1.8678623232934786 acc: 0.79\n",
      "loss: 1.849037601977844 acc: 0.74\n",
      "loss: 1.8803682008699125 acc: 0.8\n",
      "loss: 1.8281303204913955 acc: 0.79\n",
      "loss: 1.8565546308435747 acc: 0.8\n",
      "loss: 1.8612134083906315 acc: 0.77\n",
      "loss: 1.8366295155185322 acc: 0.84\n",
      "loss: 1.8387508130969534 acc: 0.84\n",
      "loss: 1.821514847107759 acc: 0.77\n",
      "loss: 1.8398089667679587 acc: 0.82\n",
      "loss: 1.8341894308370712 acc: 0.78\n",
      "loss: 1.7946226688179476 acc: 0.85\n",
      "loss: 1.848029319186782 acc: 0.78\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 1.8480\t Accuracy 0.7800\n",
      "loss: 1.8729117997264138 acc: 0.78\n",
      "loss: 1.8361477070137042 acc: 0.79\n",
      "loss: 1.8792165488376902 acc: 0.76\n",
      "loss: 1.8290420342522768 acc: 0.85\n",
      "loss: 1.8557552103786699 acc: 0.8\n",
      "loss: 1.8684308147683197 acc: 0.75\n",
      "loss: 1.8433863019300631 acc: 0.82\n",
      "loss: 1.8229381171631758 acc: 0.79\n",
      "loss: 1.8768938348125601 acc: 0.78\n",
      "loss: 1.8357034692658873 acc: 0.81\n",
      "loss: 1.8692569220897983 acc: 0.76\n",
      "loss: 1.890232333472305 acc: 0.66\n",
      "loss: 1.8592034029486504 acc: 0.79\n",
      "loss: 1.87018629453638 acc: 0.77\n",
      "loss: 1.8534022633061327 acc: 0.82\n",
      "loss: 1.829170858024193 acc: 0.79\n",
      "loss: 1.8684479504595481 acc: 0.76\n",
      "loss: 1.8771702918984357 acc: 0.75\n",
      "loss: 1.8770589895682843 acc: 0.7\n",
      "loss: 1.8754947807002893 acc: 0.76\n",
      "loss: 1.8721075719068654 acc: 0.74\n",
      "loss: 1.8805397978744312 acc: 0.77\n",
      "loss: 1.8337349939036636 acc: 0.82\n",
      "loss: 1.9063645346523759 acc: 0.71\n",
      "loss: 1.8466067084134234 acc: 0.8\n",
      "loss: 1.9040247480270507 acc: 0.72\n",
      "loss: 1.8957936570843317 acc: 0.69\n",
      "loss: 1.9138722073712962 acc: 0.69\n",
      "loss: 1.8536762637792967 acc: 0.83\n",
      "loss: 1.8965079623998407 acc: 0.72\n",
      "loss: 1.8843860099339307 acc: 0.74\n",
      "loss: 1.827874674078108 acc: 0.83\n",
      "loss: 1.8893437923734375 acc: 0.78\n",
      "loss: 1.8541886938656487 acc: 0.81\n",
      "loss: 1.8798035756642304 acc: 0.71\n",
      "loss: 1.8508245311864864 acc: 0.75\n",
      "loss: 1.8450760324994746 acc: 0.83\n",
      "loss: 1.8492006530150686 acc: 0.8\n",
      "loss: 1.8888707907121942 acc: 0.75\n",
      "loss: 1.8712944990392086 acc: 0.79\n",
      "loss: 1.832508837561503 acc: 0.82\n",
      "loss: 1.8549421787655882 acc: 0.77\n",
      "loss: 1.839469044058182 acc: 0.77\n",
      "loss: 1.8160470411585112 acc: 0.83\n",
      "loss: 1.8592856286596626 acc: 0.83\n",
      "loss: 1.8278085711250074 acc: 0.83\n",
      "loss: 1.863039339616834 acc: 0.78\n",
      "loss: 1.8539575149292407 acc: 0.8\n",
      "loss: 1.9019547788777456 acc: 0.69\n",
      "loss: 1.85007463046936 acc: 0.8\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 1.8501\t Accuracy 0.8000\n",
      "loss: 1.8435647566546418 acc: 0.79\n",
      "loss: 1.8445608407642298 acc: 0.83\n",
      "loss: 1.879473452861741 acc: 0.78\n",
      "loss: 1.8502680076566895 acc: 0.79\n",
      "loss: 1.8943545894739802 acc: 0.78\n",
      "loss: 1.85753932924283 acc: 0.72\n",
      "loss: 1.8386884231893368 acc: 0.78\n",
      "loss: 1.85761021763103 acc: 0.76\n",
      "loss: 1.8767636439861537 acc: 0.73\n",
      "loss: 1.8374528577448654 acc: 0.79\n",
      "loss: 1.857794240992814 acc: 0.78\n",
      "loss: 1.838681328781885 acc: 0.83\n",
      "loss: 1.8821638739423605 acc: 0.72\n",
      "loss: 1.8515973160671055 acc: 0.77\n",
      "loss: 1.8505631380284988 acc: 0.79\n",
      "loss: 1.810919261344229 acc: 0.83\n",
      "loss: 1.8443032729698448 acc: 0.79\n",
      "loss: 1.8777622643365866 acc: 0.76\n",
      "loss: 1.882683354645471 acc: 0.69\n",
      "loss: 1.8625326570577985 acc: 0.83\n",
      "loss: 1.8782022699258456 acc: 0.77\n",
      "loss: 1.8749724357990292 acc: 0.78\n",
      "loss: 1.8814273690375733 acc: 0.73\n",
      "loss: 1.8380694432925688 acc: 0.78\n",
      "loss: 1.8773758409319523 acc: 0.81\n",
      "loss: 1.8518958754770594 acc: 0.78\n",
      "loss: 1.872166689186711 acc: 0.78\n",
      "loss: 1.8675914317681437 acc: 0.73\n",
      "loss: 1.874683898128398 acc: 0.75\n",
      "loss: 1.8760468813394005 acc: 0.8\n",
      "loss: 1.883400934098451 acc: 0.78\n",
      "loss: 1.8815994393313904 acc: 0.72\n",
      "loss: 1.876567380263814 acc: 0.76\n",
      "loss: 1.8395236492852427 acc: 0.82\n",
      "loss: 1.8280361484165852 acc: 0.8\n",
      "loss: 1.8439741444572402 acc: 0.79\n",
      "loss: 1.8473972882069793 acc: 0.79\n",
      "loss: 1.8756085848848683 acc: 0.75\n",
      "loss: 1.839711634790648 acc: 0.75\n",
      "loss: 1.8999629666762035 acc: 0.76\n",
      "loss: 1.8633738720247652 acc: 0.79\n",
      "loss: 1.8450765799287125 acc: 0.79\n",
      "loss: 1.8683637447539119 acc: 0.74\n",
      "loss: 1.8657419224872467 acc: 0.78\n",
      "loss: 1.800785943255181 acc: 0.8\n",
      "loss: 1.8622725317811837 acc: 0.77\n",
      "loss: 1.8600429329242294 acc: 0.78\n",
      "loss: 1.8146190617515812 acc: 0.81\n",
      "loss: 1.8853049227222376 acc: 0.74\n",
      "loss: 1.8505273160617992 acc: 0.79\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 1.8505\t Accuracy 0.7900\n",
      "loss: 1.7926153131665352 acc: 0.85\n",
      "loss: 1.8998950371683485 acc: 0.7\n",
      "loss: 1.8588934207764074 acc: 0.8\n",
      "loss: 1.8744487324896812 acc: 0.75\n",
      "loss: 1.8568911738833274 acc: 0.81\n",
      "loss: 1.878993272029187 acc: 0.71\n",
      "loss: 1.899206030323492 acc: 0.74\n",
      "loss: 1.8637030124099925 acc: 0.75\n",
      "loss: 1.854780829939634 acc: 0.86\n",
      "loss: 1.84839867712038 acc: 0.82\n",
      "loss: 1.887088157591423 acc: 0.75\n",
      "loss: 1.8732347206929671 acc: 0.73\n",
      "loss: 1.876372846567161 acc: 0.77\n",
      "loss: 1.8482889553590405 acc: 0.79\n",
      "loss: 1.8402555625567265 acc: 0.79\n",
      "loss: 1.7912173785026493 acc: 0.89\n",
      "loss: 1.8821886617679686 acc: 0.73\n",
      "loss: 1.859130769440512 acc: 0.78\n",
      "loss: 1.8694729376151227 acc: 0.76\n",
      "loss: 1.8087789708666149 acc: 0.79\n",
      "loss: 1.8341817352990344 acc: 0.83\n",
      "loss: 1.9043081272375384 acc: 0.72\n",
      "loss: 1.8616111274553884 acc: 0.76\n",
      "loss: 1.828328866635179 acc: 0.8\n",
      "loss: 1.8625118225843127 acc: 0.79\n",
      "loss: 1.833808849447838 acc: 0.81\n",
      "loss: 1.8504094728770386 acc: 0.83\n",
      "loss: 1.8747530863462227 acc: 0.73\n",
      "loss: 1.8570821553030594 acc: 0.78\n",
      "loss: 1.8638343232415608 acc: 0.81\n",
      "loss: 1.871093390178353 acc: 0.79\n",
      "loss: 1.8547051322773322 acc: 0.81\n",
      "loss: 1.8250240289003437 acc: 0.82\n",
      "loss: 1.8695474483719081 acc: 0.75\n",
      "loss: 1.8550537565374983 acc: 0.76\n",
      "loss: 1.8034053325537636 acc: 0.85\n",
      "loss: 1.8863320441238867 acc: 0.75\n",
      "loss: 1.850173974068175 acc: 0.72\n",
      "loss: 1.8160180645404451 acc: 0.77\n",
      "loss: 1.83995344325039 acc: 0.76\n",
      "loss: 1.88179390791638 acc: 0.77\n",
      "loss: 1.8574290745779891 acc: 0.82\n",
      "loss: 1.8797582547133538 acc: 0.69\n",
      "loss: 1.814730743032468 acc: 0.81\n",
      "loss: 1.863355827297381 acc: 0.78\n",
      "loss: 1.8411859070019159 acc: 0.76\n",
      "loss: 1.827738515581912 acc: 0.83\n",
      "loss: 1.9092943215158524 acc: 0.73\n",
      "loss: 1.879407494633892 acc: 0.75\n",
      "loss: 1.8521714936136255 acc: 0.84\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 1.8522\t Accuracy 0.8400\n",
      "loss: 1.904168412304096 acc: 0.73\n",
      "loss: 1.8351275881833917 acc: 0.82\n",
      "loss: 1.8637237775892015 acc: 0.8\n",
      "loss: 1.8609438248638142 acc: 0.78\n",
      "loss: 1.87443824403408 acc: 0.74\n",
      "loss: 1.8651880096162698 acc: 0.78\n",
      "loss: 1.8742194831930985 acc: 0.75\n",
      "loss: 1.8817127033196817 acc: 0.74\n",
      "loss: 1.8559621849151702 acc: 0.78\n",
      "loss: 1.8121545795888927 acc: 0.8\n",
      "loss: 1.8411895529442963 acc: 0.8\n",
      "loss: 1.8452842552963924 acc: 0.8\n",
      "loss: 1.8912751669205081 acc: 0.8\n",
      "loss: 1.8870209118093804 acc: 0.8\n",
      "loss: 1.87473521408239 acc: 0.75\n",
      "loss: 1.8618069142603715 acc: 0.81\n",
      "loss: 1.8604465630752438 acc: 0.77\n",
      "loss: 1.81281950165677 acc: 0.83\n",
      "loss: 1.8843513712442606 acc: 0.76\n",
      "loss: 1.8558579041309968 acc: 0.76\n",
      "loss: 1.8337988077168195 acc: 0.82\n",
      "loss: 1.8586115454794594 acc: 0.76\n",
      "loss: 1.858819190006753 acc: 0.79\n",
      "loss: 1.8799950863446133 acc: 0.78\n",
      "loss: 1.8904395876827236 acc: 0.7\n",
      "loss: 1.832537634828598 acc: 0.8\n",
      "loss: 1.8528627325472165 acc: 0.78\n",
      "loss: 1.827845054525887 acc: 0.86\n",
      "loss: 1.8277579552497392 acc: 0.82\n",
      "loss: 1.8228538675765922 acc: 0.86\n",
      "loss: 1.908218295728331 acc: 0.64\n",
      "loss: 1.8292156517503582 acc: 0.81\n",
      "loss: 1.8193194963621213 acc: 0.81\n",
      "loss: 1.887913259032334 acc: 0.72\n",
      "loss: 1.8504649944282323 acc: 0.78\n",
      "loss: 1.8276343419767562 acc: 0.77\n",
      "loss: 1.8722503779010709 acc: 0.74\n",
      "loss: 1.858951772247243 acc: 0.78\n",
      "loss: 1.877385911952704 acc: 0.76\n",
      "loss: 1.897131476349273 acc: 0.76\n",
      "loss: 1.8387189199953873 acc: 0.8\n",
      "loss: 1.8928130758151978 acc: 0.77\n",
      "loss: 1.83082033902124 acc: 0.81\n",
      "loss: 1.8706419535637102 acc: 0.72\n",
      "loss: 1.8440189443195143 acc: 0.71\n",
      "loss: 1.8702151612793692 acc: 0.77\n",
      "loss: 1.8490339508613294 acc: 0.78\n",
      "loss: 1.8406895156830358 acc: 0.79\n",
      "loss: 1.8679094305383845 acc: 0.81\n",
      "loss: 1.86604811041535 acc: 0.71\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 1.8660\t Accuracy 0.7100\n",
      "loss: 1.8642723949033746 acc: 0.76\n",
      "loss: 1.8905529434006891 acc: 0.72\n",
      "loss: 1.916148273651367 acc: 0.71\n",
      "loss: 1.9145159120768538 acc: 0.72\n",
      "loss: 1.8329443304844781 acc: 0.81\n",
      "loss: 1.8162658112230006 acc: 0.82\n",
      "loss: 1.8583195364346767 acc: 0.79\n",
      "loss: 1.883727430984504 acc: 0.76\n",
      "loss: 1.8702583464419407 acc: 0.84\n",
      "loss: 1.8409331774374618 acc: 0.8\n",
      "loss: 1.857685664802317 acc: 0.78\n",
      "loss: 1.8773624962908206 acc: 0.75\n",
      "loss: 1.867535338439379 acc: 0.8\n",
      "loss: 1.8540535828339262 acc: 0.79\n",
      "loss: 1.8810767838355538 acc: 0.76\n",
      "loss: 1.8467426852578626 acc: 0.77\n",
      "loss: 1.8997112981400546 acc: 0.72\n",
      "loss: 1.8034509099953973 acc: 0.86\n",
      "loss: 1.8725309528947671 acc: 0.76\n",
      "loss: 1.845251025952178 acc: 0.81\n",
      "loss: 1.8386858732116353 acc: 0.83\n",
      "loss: 1.8526193373740452 acc: 0.82\n",
      "loss: 1.8753534681646926 acc: 0.72\n",
      "loss: 1.8349299429384962 acc: 0.81\n",
      "loss: 1.8025878797161285 acc: 0.87\n",
      "loss: 1.8482749041932478 acc: 0.8\n",
      "loss: 1.8599199028291227 acc: 0.78\n",
      "loss: 1.828662843548467 acc: 0.84\n",
      "loss: 1.8397369006306399 acc: 0.82\n",
      "loss: 1.8387642369075956 acc: 0.84\n",
      "loss: 1.8302343389710152 acc: 0.84\n",
      "loss: 1.871472590549457 acc: 0.78\n",
      "loss: 1.8351961538137713 acc: 0.87\n",
      "loss: 1.842062255209395 acc: 0.76\n",
      "loss: 1.8483089698185895 acc: 0.82\n",
      "loss: 1.8190778599375335 acc: 0.83\n",
      "loss: 1.8622299928051218 acc: 0.72\n",
      "loss: 1.864782526229571 acc: 0.77\n",
      "loss: 1.8287763135266852 acc: 0.86\n",
      "loss: 1.8380359775300519 acc: 0.8\n",
      "loss: 1.8357966816646851 acc: 0.81\n",
      "loss: 1.8421378956975871 acc: 0.82\n",
      "loss: 1.8677551056176886 acc: 0.8\n",
      "loss: 1.856765794620665 acc: 0.77\n",
      "loss: 1.856722953528743 acc: 0.78\n",
      "loss: 1.8550466000208226 acc: 0.76\n",
      "loss: 1.9307555397251628 acc: 0.74\n",
      "loss: 1.8478857681431375 acc: 0.81\n",
      "loss: 1.8318682089221427 acc: 0.84\n",
      "loss: 1.8151278689140256 acc: 0.84\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 1.8151\t Accuracy 0.8400\n",
      "loss: 1.901279128487885 acc: 0.73\n",
      "loss: 1.8882491011691322 acc: 0.78\n",
      "loss: 1.9043592800075944 acc: 0.74\n",
      "loss: 1.8769504347677495 acc: 0.78\n",
      "loss: 1.8642451683242924 acc: 0.79\n",
      "loss: 1.8401265671390012 acc: 0.81\n",
      "loss: 1.8443548993664984 acc: 0.79\n",
      "loss: 1.8028830374081897 acc: 0.88\n",
      "loss: 1.8146557665193435 acc: 0.86\n",
      "loss: 1.8352986789172214 acc: 0.8\n",
      "loss: 1.8754403744551673 acc: 0.8\n",
      "loss: 1.8334670920415908 acc: 0.82\n",
      "loss: 1.8684971545206417 acc: 0.78\n",
      "loss: 1.838928957990294 acc: 0.79\n",
      "loss: 1.846371271703441 acc: 0.79\n",
      "loss: 1.8397313322108746 acc: 0.79\n",
      "loss: 1.8684702468577326 acc: 0.75\n",
      "loss: 1.8604114961262994 acc: 0.79\n",
      "loss: 1.8250057556965615 acc: 0.83\n",
      "loss: 1.8743934126299673 acc: 0.75\n",
      "loss: 1.8268967679310688 acc: 0.81\n",
      "loss: 1.8590991418634923 acc: 0.71\n",
      "loss: 1.8255888245274365 acc: 0.85\n",
      "loss: 1.8374071786461852 acc: 0.89\n",
      "loss: 1.89927709652181 acc: 0.71\n",
      "loss: 1.8513741964821238 acc: 0.78\n",
      "loss: 1.8213695408962147 acc: 0.84\n",
      "loss: 1.872324881240465 acc: 0.83\n",
      "loss: 1.8322024631216665 acc: 0.81\n",
      "loss: 1.8802854874933836 acc: 0.81\n",
      "loss: 1.8439294191941002 acc: 0.79\n",
      "loss: 1.8186927436974072 acc: 0.8\n",
      "loss: 1.8778144278439886 acc: 0.76\n",
      "loss: 1.8208886874993147 acc: 0.84\n",
      "loss: 1.8273672121202835 acc: 0.82\n",
      "loss: 1.8569464288870199 acc: 0.78\n",
      "loss: 1.8373658405854119 acc: 0.8\n",
      "loss: 1.832492098261734 acc: 0.83\n",
      "loss: 1.8184210583979177 acc: 0.84\n",
      "loss: 1.8632657023562778 acc: 0.8\n",
      "loss: 1.8284516485136937 acc: 0.79\n",
      "loss: 1.814453103200816 acc: 0.82\n",
      "loss: 1.8353129423114019 acc: 0.81\n",
      "loss: 1.800053963378956 acc: 0.81\n",
      "loss: 1.8501615634467088 acc: 0.8\n",
      "loss: 1.8212495088644949 acc: 0.83\n",
      "loss: 1.8731839855213308 acc: 0.77\n",
      "loss: 1.854607847411339 acc: 0.76\n",
      "loss: 1.8703042654660107 acc: 0.78\n",
      "loss: 1.8992244492296697 acc: 0.81\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 1.8992\t Accuracy 0.8100\n",
      "loss: 1.8719688560635754 acc: 0.79\n",
      "loss: 1.818291137640423 acc: 0.8\n",
      "loss: 1.8368760975781009 acc: 0.84\n",
      "loss: 1.854141075968738 acc: 0.82\n",
      "loss: 1.8283614364051737 acc: 0.77\n",
      "loss: 1.8080933172215896 acc: 0.88\n",
      "loss: 1.8146306526431923 acc: 0.78\n",
      "loss: 1.9249775594607221 acc: 0.76\n",
      "loss: 1.8475656284133073 acc: 0.81\n",
      "loss: 1.8381287488581648 acc: 0.84\n",
      "loss: 1.8834542754918018 acc: 0.73\n",
      "loss: 1.835565717674889 acc: 0.87\n",
      "loss: 1.8589314239768258 acc: 0.76\n",
      "loss: 1.8690035554781443 acc: 0.76\n",
      "loss: 1.8089805663853276 acc: 0.83\n",
      "loss: 1.8540356311972548 acc: 0.84\n",
      "loss: 1.8536902787764709 acc: 0.75\n",
      "loss: 1.862534758422464 acc: 0.81\n",
      "loss: 1.8674342526558376 acc: 0.79\n",
      "loss: 1.8826424827083417 acc: 0.8\n",
      "loss: 1.876424496483335 acc: 0.8\n",
      "loss: 1.8594483302192757 acc: 0.77\n",
      "loss: 1.8662027276235262 acc: 0.79\n",
      "loss: 1.840826532704754 acc: 0.82\n",
      "loss: 1.8514697190906277 acc: 0.74\n",
      "loss: 1.856711312926257 acc: 0.76\n",
      "loss: 1.8674729104744427 acc: 0.81\n",
      "loss: 1.828031617918019 acc: 0.82\n",
      "loss: 1.862236388701558 acc: 0.81\n",
      "loss: 1.8270217092255259 acc: 0.8\n",
      "loss: 1.8252749682737432 acc: 0.84\n",
      "loss: 1.8832863659101293 acc: 0.75\n",
      "loss: 1.8583215056058442 acc: 0.76\n",
      "loss: 1.8468484165672012 acc: 0.8\n",
      "loss: 1.8485412574340865 acc: 0.82\n",
      "loss: 1.8274491764098428 acc: 0.81\n",
      "loss: 1.9079885514467392 acc: 0.72\n",
      "loss: 1.8172577008166366 acc: 0.82\n",
      "loss: 1.8436011034760613 acc: 0.79\n",
      "loss: 1.8677956503051987 acc: 0.73\n",
      "loss: 1.8469431314593177 acc: 0.81\n",
      "loss: 1.8701386109296758 acc: 0.8\n",
      "loss: 1.8353063654499755 acc: 0.84\n",
      "loss: 1.8632238705118425 acc: 0.78\n",
      "loss: 1.8456664810455001 acc: 0.83\n",
      "loss: 1.8931643137731304 acc: 0.74\n",
      "loss: 1.8765249530050712 acc: 0.76\n",
      "loss: 1.83015338568027 acc: 0.76\n",
      "loss: 1.8825922351855382 acc: 0.73\n",
      "loss: 1.8734103845082266 acc: 0.73\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 1.8734\t Accuracy 0.7300\n",
      "loss: 1.8721575488364317 acc: 0.73\n",
      "loss: 1.817358897090353 acc: 0.87\n",
      "loss: 1.8671053822063877 acc: 0.79\n",
      "loss: 1.8830201668646103 acc: 0.79\n",
      "loss: 1.8615076113003588 acc: 0.8\n",
      "loss: 1.8353478846678994 acc: 0.79\n",
      "loss: 1.8810720682695419 acc: 0.74\n",
      "loss: 1.9182556586835287 acc: 0.72\n",
      "loss: 1.8761692886595918 acc: 0.77\n",
      "loss: 1.8442352033373905 acc: 0.75\n",
      "loss: 1.8339536878942775 acc: 0.79\n",
      "loss: 1.844473838243723 acc: 0.77\n",
      "loss: 1.8812495774520668 acc: 0.76\n",
      "loss: 1.8825236731640669 acc: 0.77\n",
      "loss: 1.8624066230180616 acc: 0.77\n",
      "loss: 1.8399796519678582 acc: 0.81\n",
      "loss: 1.8844448797582531 acc: 0.67\n",
      "loss: 1.8371244750986349 acc: 0.88\n",
      "loss: 1.8572729350653316 acc: 0.78\n",
      "loss: 1.8909260579613443 acc: 0.73\n",
      "loss: 1.877165412265035 acc: 0.78\n",
      "loss: 1.864495640174256 acc: 0.85\n",
      "loss: 1.8579766765680108 acc: 0.83\n",
      "loss: 1.873449367037114 acc: 0.81\n",
      "loss: 1.863293134476512 acc: 0.76\n",
      "loss: 1.85599755880944 acc: 0.78\n",
      "loss: 1.8786240263090312 acc: 0.76\n",
      "loss: 1.8587220602401353 acc: 0.81\n",
      "loss: 1.815871308090089 acc: 0.86\n",
      "loss: 1.8267044571858342 acc: 0.81\n",
      "loss: 1.8625553416888077 acc: 0.76\n",
      "loss: 1.857106972136963 acc: 0.81\n",
      "loss: 1.848049079841259 acc: 0.82\n",
      "loss: 1.8939756440735294 acc: 0.75\n",
      "loss: 1.8675751182780642 acc: 0.8\n",
      "loss: 1.8752066718299643 acc: 0.76\n",
      "loss: 1.7603739927860158 acc: 0.88\n",
      "loss: 1.8395157681181171 acc: 0.77\n",
      "loss: 1.8249578649873859 acc: 0.82\n",
      "loss: 1.8791492122774438 acc: 0.76\n",
      "loss: 1.871400999859373 acc: 0.76\n",
      "loss: 1.8714482338212954 acc: 0.82\n",
      "loss: 1.8244982993744443 acc: 0.78\n",
      "loss: 1.8718687643054515 acc: 0.75\n",
      "loss: 1.8583276890560672 acc: 0.74\n",
      "loss: 1.8718060166927915 acc: 0.77\n",
      "loss: 1.860431650291604 acc: 0.77\n",
      "loss: 1.8583439374736639 acc: 0.8\n",
      "loss: 1.8073001211732747 acc: 0.81\n",
      "loss: 1.859161695483489 acc: 0.76\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 1.8592\t Accuracy 0.7600\n",
      "loss: 1.8271380032976805 acc: 0.83\n",
      "loss: 1.8380244902174903 acc: 0.82\n",
      "loss: 1.8501187886985457 acc: 0.78\n",
      "loss: 1.8226327245167162 acc: 0.78\n",
      "loss: 1.8364764787598573 acc: 0.76\n",
      "loss: 1.844391527565653 acc: 0.83\n",
      "loss: 1.8680306641195379 acc: 0.69\n",
      "loss: 1.8407515001403123 acc: 0.84\n",
      "loss: 1.82579734889517 acc: 0.84\n",
      "loss: 1.8310466578285027 acc: 0.8\n",
      "loss: 1.8390705164269687 acc: 0.82\n",
      "loss: 1.8365618974054656 acc: 0.82\n",
      "loss: 1.8218943423351925 acc: 0.79\n",
      "loss: 1.8411033655420201 acc: 0.78\n",
      "loss: 1.878431014945108 acc: 0.79\n",
      "loss: 1.8877968772125615 acc: 0.79\n",
      "loss: 1.8626055217945299 acc: 0.76\n",
      "loss: 1.8307876190603423 acc: 0.81\n",
      "loss: 1.7867114276760783 acc: 0.8\n",
      "loss: 1.844933609735989 acc: 0.78\n",
      "loss: 1.8524730641593732 acc: 0.8\n",
      "loss: 1.8457788118355531 acc: 0.76\n",
      "loss: 1.8004474276240614 acc: 0.85\n",
      "loss: 1.856425312238671 acc: 0.78\n",
      "loss: 1.862285612023577 acc: 0.79\n",
      "loss: 1.8604322378023201 acc: 0.81\n",
      "loss: 1.85015889491405 acc: 0.79\n",
      "loss: 1.8785425827432871 acc: 0.75\n",
      "loss: 1.8483912049345201 acc: 0.84\n",
      "loss: 1.8465413306770713 acc: 0.83\n",
      "loss: 1.864802872561973 acc: 0.78\n",
      "loss: 1.889225760185316 acc: 0.73\n",
      "loss: 1.9075074711508224 acc: 0.68\n",
      "loss: 1.8545822264437704 acc: 0.79\n",
      "loss: 1.8457321449083108 acc: 0.78\n",
      "loss: 1.9130890543478416 acc: 0.71\n",
      "loss: 1.846590160015358 acc: 0.76\n",
      "loss: 1.8584408081584585 acc: 0.86\n",
      "loss: 1.8438693960664658 acc: 0.8\n",
      "loss: 1.8374214900328965 acc: 0.77\n",
      "loss: 1.850392436876628 acc: 0.78\n",
      "loss: 1.8447383130259762 acc: 0.77\n",
      "loss: 1.8144324586289786 acc: 0.85\n",
      "loss: 1.8608326962568795 acc: 0.77\n",
      "loss: 1.9105600381643064 acc: 0.73\n",
      "loss: 1.8361243823042626 acc: 0.83\n",
      "loss: 1.8690443894943447 acc: 0.77\n",
      "loss: 1.8345266699554004 acc: 0.84\n",
      "loss: 1.8838088101208206 acc: 0.82\n",
      "loss: 1.8761706402661178 acc: 0.78\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 1.8762\t Accuracy 0.7800\n",
      "loss: 1.8148582899808952 acc: 0.81\n",
      "loss: 1.8649727148161355 acc: 0.82\n",
      "loss: 1.8409926667144711 acc: 0.84\n",
      "loss: 1.8443288049634026 acc: 0.79\n",
      "loss: 1.8418628906307373 acc: 0.82\n",
      "loss: 1.8914287509142622 acc: 0.76\n",
      "loss: 1.8586611953888796 acc: 0.78\n",
      "loss: 1.8137995971102867 acc: 0.87\n",
      "loss: 1.90165019947862 acc: 0.8\n",
      "loss: 1.853719886327912 acc: 0.73\n",
      "loss: 1.8640719998191875 acc: 0.77\n",
      "loss: 1.8116015766312652 acc: 0.79\n",
      "loss: 1.8475311521268913 acc: 0.79\n",
      "loss: 1.8887029833442117 acc: 0.79\n",
      "loss: 1.8343604701930898 acc: 0.81\n",
      "loss: 1.855769573791763 acc: 0.77\n",
      "loss: 1.8437859529811258 acc: 0.76\n",
      "loss: 1.8346588739846277 acc: 0.85\n",
      "loss: 1.8366222683274764 acc: 0.8\n",
      "loss: 1.8818064834558406 acc: 0.69\n",
      "loss: 1.8644124616795463 acc: 0.76\n",
      "loss: 1.8398255027666455 acc: 0.82\n",
      "loss: 1.8397091287544596 acc: 0.78\n",
      "loss: 1.8257445536273638 acc: 0.81\n",
      "loss: 1.8521115249030593 acc: 0.8\n",
      "loss: 1.902078500114905 acc: 0.73\n",
      "loss: 1.8251944218567515 acc: 0.86\n",
      "loss: 1.8208082136046053 acc: 0.86\n",
      "loss: 1.8572247903128372 acc: 0.82\n",
      "loss: 1.8158464052926921 acc: 0.81\n",
      "loss: 1.8803565500047068 acc: 0.81\n",
      "loss: 1.866219093022701 acc: 0.79\n",
      "loss: 1.9039435780776701 acc: 0.79\n",
      "loss: 1.8767521805090481 acc: 0.73\n",
      "loss: 1.8156149480533186 acc: 0.84\n",
      "loss: 1.8367667380749853 acc: 0.84\n",
      "loss: 1.8838237359961483 acc: 0.77\n",
      "loss: 1.8544986807061183 acc: 0.81\n",
      "loss: 1.9304354555572667 acc: 0.69\n",
      "loss: 1.8144199427009244 acc: 0.85\n",
      "loss: 1.8663239319995417 acc: 0.75\n",
      "loss: 1.8333151420889913 acc: 0.84\n",
      "loss: 1.8418580018484412 acc: 0.8\n",
      "loss: 1.8097153175074394 acc: 0.83\n",
      "loss: 1.8770438443759148 acc: 0.72\n",
      "loss: 1.8020266518688528 acc: 0.83\n",
      "loss: 1.8637419267734219 acc: 0.84\n",
      "loss: 1.8590913385676684 acc: 0.76\n",
      "loss: 1.8713197386773388 acc: 0.79\n",
      "loss: 1.841101430036138 acc: 0.81\n",
      "loss: 1.8192266724359656 acc: 0.85\n",
      "loss: 1.827743131167593 acc: 0.86\n",
      "loss: 1.8474953919949906 acc: 0.83\n",
      "loss: 1.8611646824643224 acc: 0.79\n",
      "loss: 1.8483362582787626 acc: 0.82\n",
      "loss: 1.78368337602541 acc: 0.83\n",
      "loss: 1.790750333391683 acc: 0.85\n",
      "loss: 1.837256290791568 acc: 0.83\n",
      "loss: 1.7839288786765835 acc: 0.91\n",
      "loss: 1.8290361533537034 acc: 0.85\n",
      "loss: 1.8092563456096369 acc: 0.83\n",
      "loss: 1.8618782918494037 acc: 0.8\n",
      "loss: 1.8869255352863346 acc: 0.77\n",
      "loss: 1.884746137804471 acc: 0.8\n",
      "loss: 1.8292220750514274 acc: 0.85\n",
      "loss: 1.834274599889022 acc: 0.8\n",
      "loss: 1.7914934812389591 acc: 0.85\n",
      "loss: 1.826971592623701 acc: 0.81\n",
      "loss: 1.8027837770832347 acc: 0.86\n",
      "loss: 1.9109956596823452 acc: 0.78\n",
      "loss: 1.8545520848226191 acc: 0.82\n",
      "loss: 1.85174418463313 acc: 0.78\n",
      "loss: 1.860808784515794 acc: 0.84\n",
      "loss: 1.8598943444490286 acc: 0.82\n",
      "loss: 1.8873065002454317 acc: 0.77\n",
      "loss: 1.879983675838307 acc: 0.76\n",
      "loss: 1.895300082819089 acc: 0.8\n",
      "loss: 1.8686093219993476 acc: 0.85\n",
      "loss: 1.8095939419580018 acc: 0.88\n",
      "loss: 1.839287486748606 acc: 0.86\n",
      "loss: 1.8111471436672237 acc: 0.87\n",
      "loss: 1.77822020718231 acc: 0.9\n",
      "loss: 1.8061575686530913 acc: 0.9\n",
      "loss: 1.8279965361807928 acc: 0.82\n",
      "loss: 1.8391511480898797 acc: 0.85\n",
      "loss: 1.8728083497689747 acc: 0.86\n",
      "loss: 1.8868366299110897 acc: 0.81\n",
      "loss: 1.8061013808660824 acc: 0.83\n",
      "loss: 1.7090336246562294 acc: 0.95\n",
      "loss: 1.754615121957791 acc: 0.92\n",
      "loss: 1.7824052618396582 acc: 0.92\n",
      "loss: 1.8254455228892945 acc: 0.83\n",
      "loss: 1.836646268917012 acc: 0.78\n",
      "loss: 1.757335428617669 acc: 0.79\n",
      "loss: 1.8365750721863252 acc: 0.86\n",
      "loss: 1.8282382620433142 acc: 0.87\n",
      "loss: 1.9183565030772132 acc: 0.75\n",
      "loss: 1.7211708593841966 acc: 0.92\n",
      "loss: 1.9216706373931713 acc: 0.76\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8554\t Average training accuracy 0.7859\n",
      "Epoch [2]\t Average validation loss 1.8327\t Average validation accuracy 0.8350\n",
      "\n",
      "loss: 1.8331137336640468 acc: 0.82\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 1.8331\t Accuracy 0.8200\n",
      "loss: 1.8543960756963755 acc: 0.82\n",
      "loss: 1.8316532419375948 acc: 0.87\n",
      "loss: 1.8424786647743716 acc: 0.81\n",
      "loss: 1.8679492660973052 acc: 0.74\n",
      "loss: 1.7969540951575262 acc: 0.84\n",
      "loss: 1.831776320309416 acc: 0.82\n",
      "loss: 1.8332622429043732 acc: 0.81\n",
      "loss: 1.8662518760976374 acc: 0.82\n",
      "loss: 1.8578625198307386 acc: 0.83\n",
      "loss: 1.8758146880020883 acc: 0.75\n",
      "loss: 1.898941010660863 acc: 0.79\n",
      "loss: 1.8021473453125187 acc: 0.87\n",
      "loss: 1.8117851503469538 acc: 0.84\n",
      "loss: 1.8766394590155107 acc: 0.75\n",
      "loss: 1.8281896957036758 acc: 0.86\n",
      "loss: 1.8473610083923537 acc: 0.8\n",
      "loss: 1.8556156077570138 acc: 0.74\n",
      "loss: 1.8546289051387168 acc: 0.81\n",
      "loss: 1.8610609887646254 acc: 0.84\n",
      "loss: 1.869003963578154 acc: 0.79\n",
      "loss: 1.8522705095098848 acc: 0.8\n",
      "loss: 1.8320090130625015 acc: 0.86\n",
      "loss: 1.8170261489751371 acc: 0.82\n",
      "loss: 1.8147272828488092 acc: 0.86\n",
      "loss: 1.8499335951636697 acc: 0.8\n",
      "loss: 1.8595281696220716 acc: 0.81\n",
      "loss: 1.8486036514968884 acc: 0.83\n",
      "loss: 1.8781417034426886 acc: 0.78\n",
      "loss: 1.8495386388300101 acc: 0.79\n",
      "loss: 1.8712761447290276 acc: 0.78\n",
      "loss: 1.8570105128621708 acc: 0.84\n",
      "loss: 1.857141500617471 acc: 0.77\n",
      "loss: 1.869516400486293 acc: 0.78\n",
      "loss: 1.8651343097107846 acc: 0.79\n",
      "loss: 1.855269630310283 acc: 0.76\n",
      "loss: 1.8115216643776655 acc: 0.84\n",
      "loss: 1.8331036079677399 acc: 0.81\n",
      "loss: 1.8465439379855895 acc: 0.8\n",
      "loss: 1.8511031853306965 acc: 0.79\n",
      "loss: 1.8430696397817063 acc: 0.8\n",
      "loss: 1.8588906431403402 acc: 0.82\n",
      "loss: 1.8088135395886622 acc: 0.87\n",
      "loss: 1.8672415277165668 acc: 0.71\n",
      "loss: 1.8384682343893841 acc: 0.77\n",
      "loss: 1.834202155486486 acc: 0.83\n",
      "loss: 1.8693650717680312 acc: 0.79\n",
      "loss: 1.8622789831627093 acc: 0.8\n",
      "loss: 1.902074897118738 acc: 0.76\n",
      "loss: 1.833988122174014 acc: 0.78\n",
      "loss: 1.8587639030209238 acc: 0.81\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 1.8588\t Accuracy 0.8100\n",
      "loss: 1.8533128594130068 acc: 0.77\n",
      "loss: 1.8336456913950732 acc: 0.86\n",
      "loss: 1.870215859556196 acc: 0.81\n",
      "loss: 1.8733157787296986 acc: 0.8\n",
      "loss: 1.855241943297895 acc: 0.79\n",
      "loss: 1.8515225449132295 acc: 0.77\n",
      "loss: 1.887855363403177 acc: 0.8\n",
      "loss: 1.8183851944064333 acc: 0.86\n",
      "loss: 1.8491525634202781 acc: 0.74\n",
      "loss: 1.8624213004267014 acc: 0.81\n",
      "loss: 1.8663876632585337 acc: 0.75\n",
      "loss: 1.845037333655147 acc: 0.85\n",
      "loss: 1.8428016622437209 acc: 0.78\n",
      "loss: 1.8358899324316675 acc: 0.78\n",
      "loss: 1.8490264889852002 acc: 0.81\n",
      "loss: 1.8546590734370825 acc: 0.76\n",
      "loss: 1.8431611970600879 acc: 0.88\n",
      "loss: 1.8524492525267753 acc: 0.84\n",
      "loss: 1.869507563406327 acc: 0.82\n",
      "loss: 1.8380895960388017 acc: 0.85\n",
      "loss: 1.888651552584344 acc: 0.7\n",
      "loss: 1.863693046942578 acc: 0.78\n",
      "loss: 1.8420180561690949 acc: 0.81\n",
      "loss: 1.8326158026555492 acc: 0.79\n",
      "loss: 1.8237678221704519 acc: 0.82\n",
      "loss: 1.81904919352495 acc: 0.87\n",
      "loss: 1.873141527908803 acc: 0.73\n",
      "loss: 1.835399474723282 acc: 0.82\n",
      "loss: 1.850520930003918 acc: 0.78\n",
      "loss: 1.8143002581280785 acc: 0.82\n",
      "loss: 1.8594060141733553 acc: 0.81\n",
      "loss: 1.8486474324535234 acc: 0.81\n",
      "loss: 1.8143563474132753 acc: 0.83\n",
      "loss: 1.8212209700769824 acc: 0.8\n",
      "loss: 1.8448804707498316 acc: 0.8\n",
      "loss: 1.838115509872904 acc: 0.8\n",
      "loss: 1.852784981304626 acc: 0.76\n",
      "loss: 1.8452315115873317 acc: 0.82\n",
      "loss: 1.8857360933018348 acc: 0.8\n",
      "loss: 1.8586055389471985 acc: 0.8\n",
      "loss: 1.872744637181784 acc: 0.73\n",
      "loss: 1.828351779697725 acc: 0.82\n",
      "loss: 1.8379349198821455 acc: 0.79\n",
      "loss: 1.8557078238428997 acc: 0.75\n",
      "loss: 1.8288717333614288 acc: 0.77\n",
      "loss: 1.8626196610940935 acc: 0.85\n",
      "loss: 1.797645058573839 acc: 0.87\n",
      "loss: 1.872106644709516 acc: 0.77\n",
      "loss: 1.8444228185819547 acc: 0.79\n",
      "loss: 1.8670509244919558 acc: 0.75\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 1.8671\t Accuracy 0.7500\n",
      "loss: 1.8432601329698632 acc: 0.83\n",
      "loss: 1.8306688211739723 acc: 0.83\n",
      "loss: 1.8816704307644565 acc: 0.83\n",
      "loss: 1.8323559747245235 acc: 0.81\n",
      "loss: 1.8730094602388525 acc: 0.78\n",
      "loss: 1.8458468667502836 acc: 0.77\n",
      "loss: 1.8821647019504453 acc: 0.76\n",
      "loss: 1.8475937737153771 acc: 0.78\n",
      "loss: 1.8890435282909446 acc: 0.78\n",
      "loss: 1.8812671206313751 acc: 0.75\n",
      "loss: 1.860939806321734 acc: 0.82\n",
      "loss: 1.857543756793625 acc: 0.76\n",
      "loss: 1.8059595775245865 acc: 0.85\n",
      "loss: 1.8641261034552434 acc: 0.8\n",
      "loss: 1.85815722842364 acc: 0.76\n",
      "loss: 1.8491790415163964 acc: 0.79\n",
      "loss: 1.888020859486858 acc: 0.77\n",
      "loss: 1.9162594534557227 acc: 0.73\n",
      "loss: 1.8316307261927007 acc: 0.82\n",
      "loss: 1.8621968385558116 acc: 0.76\n",
      "loss: 1.8177793348370346 acc: 0.9\n",
      "loss: 1.8450853694249583 acc: 0.8\n",
      "loss: 1.8452953954353908 acc: 0.81\n",
      "loss: 1.833354891183607 acc: 0.79\n",
      "loss: 1.9011480157173253 acc: 0.71\n",
      "loss: 1.8783317736362872 acc: 0.76\n",
      "loss: 1.8780266984164822 acc: 0.78\n",
      "loss: 1.9257215279406865 acc: 0.71\n",
      "loss: 1.841043725151721 acc: 0.81\n",
      "loss: 1.8031270049084893 acc: 0.81\n",
      "loss: 1.8172977967657586 acc: 0.84\n",
      "loss: 1.8820236269130148 acc: 0.75\n",
      "loss: 1.8498792395043544 acc: 0.78\n",
      "loss: 1.8545475897996981 acc: 0.81\n",
      "loss: 1.81192029538886 acc: 0.88\n",
      "loss: 1.8718166395528448 acc: 0.75\n",
      "loss: 1.8095649551753834 acc: 0.85\n",
      "loss: 1.8419518163571516 acc: 0.86\n",
      "loss: 1.8468189796704368 acc: 0.79\n",
      "loss: 1.8566837265097234 acc: 0.82\n",
      "loss: 1.880029710444475 acc: 0.74\n",
      "loss: 1.867320745992678 acc: 0.78\n",
      "loss: 1.8195899293577624 acc: 0.87\n",
      "loss: 1.8813208951799993 acc: 0.74\n",
      "loss: 1.8232376385375557 acc: 0.85\n",
      "loss: 1.830419534411886 acc: 0.75\n",
      "loss: 1.849882870238184 acc: 0.81\n",
      "loss: 1.8799753817436191 acc: 0.79\n",
      "loss: 1.9031410665441866 acc: 0.72\n",
      "loss: 1.85887985259037 acc: 0.82\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 1.8589\t Accuracy 0.8200\n",
      "loss: 1.9127786752802571 acc: 0.71\n",
      "loss: 1.8733211865466068 acc: 0.76\n",
      "loss: 1.8684343619891126 acc: 0.78\n",
      "loss: 1.8632221099553796 acc: 0.77\n",
      "loss: 1.8693020635989552 acc: 0.75\n",
      "loss: 1.8303831568169397 acc: 0.82\n",
      "loss: 1.8589812886770587 acc: 0.77\n",
      "loss: 1.888610413371613 acc: 0.75\n",
      "loss: 1.838517623869647 acc: 0.8\n",
      "loss: 1.8462844493524044 acc: 0.84\n",
      "loss: 1.8567975030059163 acc: 0.84\n",
      "loss: 1.8159891167502622 acc: 0.88\n",
      "loss: 1.8884682419336973 acc: 0.79\n",
      "loss: 1.8422781162052864 acc: 0.78\n",
      "loss: 1.8293610987087459 acc: 0.79\n",
      "loss: 1.8641081997544466 acc: 0.84\n",
      "loss: 1.881319827097318 acc: 0.74\n",
      "loss: 1.8702435945730989 acc: 0.77\n",
      "loss: 1.8211461429294695 acc: 0.87\n",
      "loss: 1.8404805202916936 acc: 0.86\n",
      "loss: 1.8903773158010586 acc: 0.84\n",
      "loss: 1.8666325405890232 acc: 0.82\n",
      "loss: 1.8466575872959015 acc: 0.85\n",
      "loss: 1.8460902479665253 acc: 0.79\n",
      "loss: 1.883669913827048 acc: 0.77\n",
      "loss: 1.889657012507598 acc: 0.69\n",
      "loss: 1.8801483102733056 acc: 0.75\n",
      "loss: 1.8627435237480032 acc: 0.81\n",
      "loss: 1.8309185353641075 acc: 0.9\n",
      "loss: 1.914427467885254 acc: 0.77\n",
      "loss: 1.847983347054577 acc: 0.82\n",
      "loss: 1.8326042461762422 acc: 0.88\n",
      "loss: 1.8414221843885157 acc: 0.77\n",
      "loss: 1.879155189272208 acc: 0.8\n",
      "loss: 1.853571417343947 acc: 0.78\n",
      "loss: 1.8295912580631062 acc: 0.79\n",
      "loss: 1.8604780715635048 acc: 0.87\n",
      "loss: 1.9002660280626846 acc: 0.74\n",
      "loss: 1.8496916529813021 acc: 0.82\n",
      "loss: 1.8435743227220533 acc: 0.79\n",
      "loss: 1.849285437210188 acc: 0.83\n",
      "loss: 1.8172771263448104 acc: 0.84\n",
      "loss: 1.8503401337225904 acc: 0.87\n",
      "loss: 1.8221720031725286 acc: 0.87\n",
      "loss: 1.8611869976936866 acc: 0.84\n",
      "loss: 1.8951751462091047 acc: 0.76\n",
      "loss: 1.8110971843656527 acc: 0.86\n",
      "loss: 1.8482480300731543 acc: 0.82\n",
      "loss: 1.8888801586348318 acc: 0.73\n",
      "loss: 1.8721212558150122 acc: 0.78\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 1.8721\t Accuracy 0.7800\n",
      "loss: 1.8468442093347692 acc: 0.78\n",
      "loss: 1.844771337074925 acc: 0.81\n",
      "loss: 1.82171025776363 acc: 0.8\n",
      "loss: 1.8404897445885076 acc: 0.9\n",
      "loss: 1.8359967182913441 acc: 0.84\n",
      "loss: 1.8399613691751622 acc: 0.82\n",
      "loss: 1.8386734269341327 acc: 0.8\n",
      "loss: 1.8508359992535077 acc: 0.82\n",
      "loss: 1.889655286859787 acc: 0.76\n",
      "loss: 1.8092274556698336 acc: 0.83\n",
      "loss: 1.858370881268027 acc: 0.74\n",
      "loss: 1.8649870431683888 acc: 0.77\n",
      "loss: 1.829504528010232 acc: 0.78\n",
      "loss: 1.8618407669262325 acc: 0.73\n",
      "loss: 1.856541101053127 acc: 0.81\n",
      "loss: 1.8704376072655204 acc: 0.75\n",
      "loss: 1.8781561844570964 acc: 0.82\n",
      "loss: 1.8688272981621024 acc: 0.8\n",
      "loss: 1.8175418960366811 acc: 0.83\n",
      "loss: 1.8524038583611573 acc: 0.8\n",
      "loss: 1.8420144065699589 acc: 0.77\n",
      "loss: 1.842380984707966 acc: 0.87\n",
      "loss: 1.848083323349702 acc: 0.81\n",
      "loss: 1.879587825540089 acc: 0.79\n",
      "loss: 1.8456259397199446 acc: 0.77\n",
      "loss: 1.8548258497570713 acc: 0.8\n",
      "loss: 1.9063471925903972 acc: 0.75\n",
      "loss: 1.850710514463395 acc: 0.8\n",
      "loss: 1.8608444498143255 acc: 0.83\n",
      "loss: 1.7997364784003027 acc: 0.82\n",
      "loss: 1.8452776568919331 acc: 0.84\n",
      "loss: 1.8400147327038936 acc: 0.82\n",
      "loss: 1.8713712042936248 acc: 0.8\n",
      "loss: 1.862485566566637 acc: 0.78\n",
      "loss: 1.8228662350116887 acc: 0.83\n",
      "loss: 1.8153189370190466 acc: 0.78\n",
      "loss: 1.838496816328018 acc: 0.85\n",
      "loss: 1.8444456509116787 acc: 0.81\n",
      "loss: 1.8347487688484327 acc: 0.79\n",
      "loss: 1.88094339289811 acc: 0.82\n",
      "loss: 1.8502929022638637 acc: 0.81\n",
      "loss: 1.843062357401977 acc: 0.84\n",
      "loss: 1.8681992910162988 acc: 0.78\n",
      "loss: 1.799741710605617 acc: 0.84\n",
      "loss: 1.8529740421326404 acc: 0.79\n",
      "loss: 1.8089943349642434 acc: 0.86\n",
      "loss: 1.8616236837252487 acc: 0.8\n",
      "loss: 1.8132023953234921 acc: 0.83\n",
      "loss: 1.834401733719461 acc: 0.81\n",
      "loss: 1.8642141962355914 acc: 0.71\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 1.8642\t Accuracy 0.7100\n",
      "loss: 1.8626124900329577 acc: 0.84\n",
      "loss: 1.8585449045848264 acc: 0.85\n",
      "loss: 1.866379594674992 acc: 0.7\n",
      "loss: 1.8776996003619746 acc: 0.75\n",
      "loss: 1.851644498410251 acc: 0.87\n",
      "loss: 1.893335354619228 acc: 0.79\n",
      "loss: 1.8226098073033032 acc: 0.79\n",
      "loss: 1.847039828775214 acc: 0.76\n",
      "loss: 1.8640180597034857 acc: 0.78\n",
      "loss: 1.8664446519117914 acc: 0.79\n",
      "loss: 1.8215237375860114 acc: 0.84\n",
      "loss: 1.8257011712949387 acc: 0.84\n",
      "loss: 1.824245796814786 acc: 0.85\n",
      "loss: 1.8351309446365491 acc: 0.82\n",
      "loss: 1.8660818453637598 acc: 0.72\n",
      "loss: 1.8653478372579568 acc: 0.8\n",
      "loss: 1.8524403817183295 acc: 0.72\n",
      "loss: 1.8363347771852403 acc: 0.85\n",
      "loss: 1.8156516775272333 acc: 0.82\n",
      "loss: 1.8453102230797607 acc: 0.83\n",
      "loss: 1.853253018125354 acc: 0.79\n",
      "loss: 1.8430406246056092 acc: 0.81\n",
      "loss: 1.8458791934576035 acc: 0.79\n",
      "loss: 1.8403958768954936 acc: 0.82\n",
      "loss: 1.8550091536372992 acc: 0.75\n",
      "loss: 1.8556921446493724 acc: 0.8\n",
      "loss: 1.8349874987932213 acc: 0.78\n",
      "loss: 1.8718537780031703 acc: 0.79\n",
      "loss: 1.84955037839585 acc: 0.82\n",
      "loss: 1.8563094342201445 acc: 0.8\n",
      "loss: 1.8537069170453955 acc: 0.8\n",
      "loss: 1.8357247081348689 acc: 0.82\n",
      "loss: 1.8423398712453776 acc: 0.82\n",
      "loss: 1.8643800901323908 acc: 0.8\n",
      "loss: 1.8882142494715763 acc: 0.77\n",
      "loss: 1.8644005030364494 acc: 0.74\n",
      "loss: 1.816561517588846 acc: 0.87\n",
      "loss: 1.8338319858841612 acc: 0.79\n",
      "loss: 1.8476987383775016 acc: 0.84\n",
      "loss: 1.825051437187283 acc: 0.82\n",
      "loss: 1.8337484949537461 acc: 0.78\n",
      "loss: 1.8718175006097255 acc: 0.75\n",
      "loss: 1.8417745556410923 acc: 0.85\n",
      "loss: 1.816326599025504 acc: 0.89\n",
      "loss: 1.832831302425964 acc: 0.87\n",
      "loss: 1.8528607989470758 acc: 0.77\n",
      "loss: 1.8352652475107052 acc: 0.8\n",
      "loss: 1.8723729913847094 acc: 0.8\n",
      "loss: 1.8697846723671971 acc: 0.76\n",
      "loss: 1.8482501692201165 acc: 0.82\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 1.8483\t Accuracy 0.8200\n",
      "loss: 1.8574026495246792 acc: 0.77\n",
      "loss: 1.8420306332442595 acc: 0.81\n",
      "loss: 1.8425325200605311 acc: 0.8\n",
      "loss: 1.8305108334626732 acc: 0.84\n",
      "loss: 1.8397439338134267 acc: 0.81\n",
      "loss: 1.851220676845958 acc: 0.78\n",
      "loss: 1.842023315770952 acc: 0.83\n",
      "loss: 1.8323148827860698 acc: 0.82\n",
      "loss: 1.8499099795875833 acc: 0.82\n",
      "loss: 1.8577328408632627 acc: 0.81\n",
      "loss: 1.8391765683969077 acc: 0.81\n",
      "loss: 1.8738210487657614 acc: 0.74\n",
      "loss: 1.8430317710209325 acc: 0.8\n",
      "loss: 1.8634412568037784 acc: 0.82\n",
      "loss: 1.8456761694615236 acc: 0.79\n",
      "loss: 1.8278232577834839 acc: 0.82\n",
      "loss: 1.8696303606687095 acc: 0.84\n",
      "loss: 1.8277326164350562 acc: 0.84\n",
      "loss: 1.8261733738099764 acc: 0.83\n",
      "loss: 1.8669217544291015 acc: 0.78\n",
      "loss: 1.8529526126927829 acc: 0.85\n",
      "loss: 1.8501984147827475 acc: 0.81\n",
      "loss: 1.8368898812634378 acc: 0.78\n",
      "loss: 1.8303636807288926 acc: 0.84\n",
      "loss: 1.86135395260936 acc: 0.81\n",
      "loss: 1.8468073013672097 acc: 0.82\n",
      "loss: 1.8098889309617006 acc: 0.86\n",
      "loss: 1.8249930857235714 acc: 0.81\n",
      "loss: 1.8363299870887564 acc: 0.8\n",
      "loss: 1.8180072679564205 acc: 0.87\n",
      "loss: 1.8530211755044363 acc: 0.82\n",
      "loss: 1.8951293507194975 acc: 0.79\n",
      "loss: 1.8184631301907948 acc: 0.85\n",
      "loss: 1.8743930758899756 acc: 0.76\n",
      "loss: 1.7938092000074135 acc: 0.82\n",
      "loss: 1.8481910171629705 acc: 0.75\n",
      "loss: 1.8568441022964546 acc: 0.76\n",
      "loss: 1.8579908139254555 acc: 0.82\n",
      "loss: 1.8779368898075066 acc: 0.82\n",
      "loss: 1.85005390043189 acc: 0.86\n",
      "loss: 1.8821670020191832 acc: 0.75\n",
      "loss: 1.8404287409593545 acc: 0.81\n",
      "loss: 1.820781224770608 acc: 0.85\n",
      "loss: 1.806975325416979 acc: 0.84\n",
      "loss: 1.806596731426698 acc: 0.86\n",
      "loss: 1.8248866619339372 acc: 0.79\n",
      "loss: 1.807255480019698 acc: 0.85\n",
      "loss: 1.8502721919190186 acc: 0.79\n",
      "loss: 1.8376140254405064 acc: 0.83\n",
      "loss: 1.8439540239796763 acc: 0.81\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 1.8440\t Accuracy 0.8100\n",
      "loss: 1.8418315986699039 acc: 0.79\n",
      "loss: 1.8164864531305094 acc: 0.91\n",
      "loss: 1.828954189289063 acc: 0.82\n",
      "loss: 1.849281891698258 acc: 0.77\n",
      "loss: 1.796042390743858 acc: 0.85\n",
      "loss: 1.820729611119045 acc: 0.87\n",
      "loss: 1.8368248706653565 acc: 0.83\n",
      "loss: 1.8089819637668052 acc: 0.84\n",
      "loss: 1.8371049373077406 acc: 0.81\n",
      "loss: 1.8373088897672587 acc: 0.77\n",
      "loss: 1.809732439116652 acc: 0.82\n",
      "loss: 1.824465537889495 acc: 0.82\n",
      "loss: 1.866313670574518 acc: 0.72\n",
      "loss: 1.9189651652110506 acc: 0.73\n",
      "loss: 1.8555625097770763 acc: 0.78\n",
      "loss: 1.83560998063557 acc: 0.78\n",
      "loss: 1.8751183853563893 acc: 0.84\n",
      "loss: 1.8278448984154383 acc: 0.83\n",
      "loss: 1.8374046787311074 acc: 0.76\n",
      "loss: 1.7934035992263357 acc: 0.86\n",
      "loss: 1.8309949434093216 acc: 0.84\n",
      "loss: 1.8613809116915883 acc: 0.81\n",
      "loss: 1.883521137210526 acc: 0.77\n",
      "loss: 1.8414952672092835 acc: 0.78\n",
      "loss: 1.8245819211207135 acc: 0.77\n",
      "loss: 1.8583433227108526 acc: 0.8\n",
      "loss: 1.8726044715990955 acc: 0.76\n",
      "loss: 1.8494491386839678 acc: 0.74\n",
      "loss: 1.8358185924285315 acc: 0.8\n",
      "loss: 1.826720642324968 acc: 0.83\n",
      "loss: 1.9054914297637016 acc: 0.73\n",
      "loss: 1.8425194836160605 acc: 0.78\n",
      "loss: 1.817213485763191 acc: 0.88\n",
      "loss: 1.8603845243917865 acc: 0.77\n",
      "loss: 1.8208090481350632 acc: 0.82\n",
      "loss: 1.8297947489807322 acc: 0.81\n",
      "loss: 1.8509862696737098 acc: 0.83\n",
      "loss: 1.8553694727954144 acc: 0.8\n",
      "loss: 1.8316916956026568 acc: 0.79\n",
      "loss: 1.8501148044836535 acc: 0.81\n",
      "loss: 1.8245791616087397 acc: 0.83\n",
      "loss: 1.834719333658248 acc: 0.87\n",
      "loss: 1.8327759108362058 acc: 0.83\n",
      "loss: 1.8457506987672352 acc: 0.73\n",
      "loss: 1.8426764346904159 acc: 0.82\n",
      "loss: 1.850346694625962 acc: 0.81\n",
      "loss: 1.828006727438236 acc: 0.83\n",
      "loss: 1.872496208964003 acc: 0.84\n",
      "loss: 1.8412064807823498 acc: 0.8\n",
      "loss: 1.8608899191051917 acc: 0.79\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 1.8609\t Accuracy 0.7900\n",
      "loss: 1.8915843318941254 acc: 0.74\n",
      "loss: 1.8238927127472195 acc: 0.8\n",
      "loss: 1.919003703566289 acc: 0.69\n",
      "loss: 1.8490865218609236 acc: 0.78\n",
      "loss: 1.8647795495541502 acc: 0.86\n",
      "loss: 1.8670762068965008 acc: 0.84\n",
      "loss: 1.885191936472654 acc: 0.72\n",
      "loss: 1.8488574053108102 acc: 0.81\n",
      "loss: 1.854291348849264 acc: 0.8\n",
      "loss: 1.8659391104974175 acc: 0.82\n",
      "loss: 1.8592981882276154 acc: 0.85\n",
      "loss: 1.8879390877450368 acc: 0.71\n",
      "loss: 1.8162059196012266 acc: 0.85\n",
      "loss: 1.8832859609694492 acc: 0.77\n",
      "loss: 1.8967532225184809 acc: 0.75\n",
      "loss: 1.8194067773211982 acc: 0.86\n",
      "loss: 1.8620237068087122 acc: 0.79\n",
      "loss: 1.833784026221571 acc: 0.83\n",
      "loss: 1.8101223478278576 acc: 0.86\n",
      "loss: 1.8416354290212056 acc: 0.86\n",
      "loss: 1.8578420855084627 acc: 0.78\n",
      "loss: 1.8586424922867015 acc: 0.78\n",
      "loss: 1.8703302344353099 acc: 0.77\n",
      "loss: 1.8629832129025694 acc: 0.79\n",
      "loss: 1.9017670556320883 acc: 0.77\n",
      "loss: 1.8439830254578333 acc: 0.78\n",
      "loss: 1.8476313477147293 acc: 0.82\n",
      "loss: 1.8820681471781462 acc: 0.79\n",
      "loss: 1.8655016280389762 acc: 0.81\n",
      "loss: 1.8643917759910449 acc: 0.8\n",
      "loss: 1.8277961292962481 acc: 0.8\n",
      "loss: 1.8184431726480172 acc: 0.85\n",
      "loss: 1.8415206283517265 acc: 0.81\n",
      "loss: 1.8306500691744514 acc: 0.86\n",
      "loss: 1.8569922611965008 acc: 0.74\n",
      "loss: 1.8605033852320707 acc: 0.86\n",
      "loss: 1.8481069445294063 acc: 0.81\n",
      "loss: 1.803789417096128 acc: 0.86\n",
      "loss: 1.8528055498416356 acc: 0.84\n",
      "loss: 1.849956588088074 acc: 0.77\n",
      "loss: 1.8369159032895508 acc: 0.83\n",
      "loss: 1.858838052609004 acc: 0.76\n",
      "loss: 1.832125826126601 acc: 0.81\n",
      "loss: 1.8655311814524813 acc: 0.78\n",
      "loss: 1.822303673505447 acc: 0.8\n",
      "loss: 1.8611313224609967 acc: 0.83\n",
      "loss: 1.8177151229650506 acc: 0.83\n",
      "loss: 1.8251627534541799 acc: 0.87\n",
      "loss: 1.8468693012614623 acc: 0.83\n",
      "loss: 1.8066148964799769 acc: 0.86\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 1.8066\t Accuracy 0.8600\n",
      "loss: 1.8508518418934228 acc: 0.8\n",
      "loss: 1.7937179883583854 acc: 0.88\n",
      "loss: 1.871423492042122 acc: 0.78\n",
      "loss: 1.897796888832162 acc: 0.69\n",
      "loss: 1.8230165210586793 acc: 0.85\n",
      "loss: 1.8477238960367943 acc: 0.8\n",
      "loss: 1.8213621976288967 acc: 0.85\n",
      "loss: 1.8258251184387349 acc: 0.81\n",
      "loss: 1.8404387946957805 acc: 0.86\n",
      "loss: 1.8285084956874595 acc: 0.82\n",
      "loss: 1.83527878060923 acc: 0.81\n",
      "loss: 1.8643402785217174 acc: 0.8\n",
      "loss: 1.8611686893158872 acc: 0.79\n",
      "loss: 1.8245653157386355 acc: 0.83\n",
      "loss: 1.875709862427387 acc: 0.79\n",
      "loss: 1.8734334863070674 acc: 0.81\n",
      "loss: 1.8559349650896277 acc: 0.83\n",
      "loss: 1.8878147202534663 acc: 0.8\n",
      "loss: 1.8548906978808979 acc: 0.83\n",
      "loss: 1.8253090501422162 acc: 0.82\n",
      "loss: 1.8387257464723854 acc: 0.82\n",
      "loss: 1.8193460261219767 acc: 0.82\n",
      "loss: 1.8546176854390453 acc: 0.81\n",
      "loss: 1.8150437349099149 acc: 0.81\n",
      "loss: 1.8375765553515924 acc: 0.79\n",
      "loss: 1.8332893968527761 acc: 0.84\n",
      "loss: 1.8018481398525321 acc: 0.82\n",
      "loss: 1.8338751213347828 acc: 0.87\n",
      "loss: 1.8497798148811473 acc: 0.82\n",
      "loss: 1.8368128863862367 acc: 0.82\n",
      "loss: 1.840203537172201 acc: 0.86\n",
      "loss: 1.831216813164616 acc: 0.79\n",
      "loss: 1.8440739837661697 acc: 0.78\n",
      "loss: 1.8646421622245497 acc: 0.76\n",
      "loss: 1.8206791739394985 acc: 0.76\n",
      "loss: 1.8460962683049456 acc: 0.83\n",
      "loss: 1.8525088559258653 acc: 0.78\n",
      "loss: 1.8456053591780974 acc: 0.77\n",
      "loss: 1.7945858861544475 acc: 0.82\n",
      "loss: 1.8021372362773553 acc: 0.84\n",
      "loss: 1.8720146355337888 acc: 0.74\n",
      "loss: 1.8560717263641826 acc: 0.82\n",
      "loss: 1.878222716646149 acc: 0.76\n",
      "loss: 1.8358630603487691 acc: 0.84\n",
      "loss: 1.8228521244312776 acc: 0.81\n",
      "loss: 1.8336720406837115 acc: 0.81\n",
      "loss: 1.860164142160279 acc: 0.79\n",
      "loss: 1.8340962684072417 acc: 0.83\n",
      "loss: 1.8708201225421104 acc: 0.76\n",
      "loss: 1.8379756823618547 acc: 0.84\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 1.8380\t Accuracy 0.8400\n",
      "loss: 1.836896985867957 acc: 0.81\n",
      "loss: 1.8538134875847552 acc: 0.8\n",
      "loss: 1.8748567170696049 acc: 0.81\n",
      "loss: 1.8535542671395189 acc: 0.8\n",
      "loss: 1.8608337331788958 acc: 0.76\n",
      "loss: 1.8229780256991126 acc: 0.81\n",
      "loss: 1.8304705180490055 acc: 0.81\n",
      "loss: 1.8468335254009856 acc: 0.84\n",
      "loss: 1.8148984844393985 acc: 0.85\n",
      "loss: 1.8532803214131908 acc: 0.84\n",
      "loss: 1.8590551038170984 acc: 0.77\n",
      "loss: 1.832046018913467 acc: 0.9\n",
      "loss: 1.8454417533978178 acc: 0.79\n",
      "loss: 1.8813746500428892 acc: 0.76\n",
      "loss: 1.887949359138311 acc: 0.77\n",
      "loss: 1.856046053675701 acc: 0.84\n",
      "loss: 1.8548709579484342 acc: 0.76\n",
      "loss: 1.8723080995725403 acc: 0.78\n",
      "loss: 1.8556294892025067 acc: 0.81\n",
      "loss: 1.8319279533678035 acc: 0.83\n",
      "loss: 1.838377802757565 acc: 0.85\n",
      "loss: 1.8730549724817596 acc: 0.83\n",
      "loss: 1.849719289303372 acc: 0.82\n",
      "loss: 1.808571181587654 acc: 0.86\n",
      "loss: 1.854735123681088 acc: 0.81\n",
      "loss: 1.8090419375564684 acc: 0.85\n",
      "loss: 1.8326288335338865 acc: 0.79\n",
      "loss: 1.879736629225663 acc: 0.76\n",
      "loss: 1.8727373911450766 acc: 0.74\n",
      "loss: 1.8372223622335273 acc: 0.82\n",
      "loss: 1.8741726529839469 acc: 0.8\n",
      "loss: 1.845829868715043 acc: 0.78\n",
      "loss: 1.8687434957242006 acc: 0.75\n",
      "loss: 1.8232956747636087 acc: 0.83\n",
      "loss: 1.8419982154943242 acc: 0.84\n",
      "loss: 1.8005105765470475 acc: 0.86\n",
      "loss: 1.8204700829922487 acc: 0.81\n",
      "loss: 1.840913071289677 acc: 0.76\n",
      "loss: 1.862080051491169 acc: 0.8\n",
      "loss: 1.8750689778379437 acc: 0.74\n",
      "loss: 1.8472471476639447 acc: 0.81\n",
      "loss: 1.833861548804117 acc: 0.81\n",
      "loss: 1.8297664585598428 acc: 0.86\n",
      "loss: 1.8425720412561493 acc: 0.9\n",
      "loss: 1.8694813078092432 acc: 0.74\n",
      "loss: 1.8412225668624718 acc: 0.86\n",
      "loss: 1.840685957620698 acc: 0.81\n",
      "loss: 1.8603929547335947 acc: 0.8\n",
      "loss: 1.8243596621828397 acc: 0.83\n",
      "loss: 1.8307812593202477 acc: 0.81\n",
      "loss: 1.810487814544485 acc: 0.85\n",
      "loss: 1.8152401025477727 acc: 0.87\n",
      "loss: 1.8420997074899097 acc: 0.83\n",
      "loss: 1.8497844446679685 acc: 0.81\n",
      "loss: 1.837570595816699 acc: 0.85\n",
      "loss: 1.7731288695153538 acc: 0.85\n",
      "loss: 1.7830272441318105 acc: 0.85\n",
      "loss: 1.827950117534722 acc: 0.83\n",
      "loss: 1.7745773404606497 acc: 0.92\n",
      "loss: 1.8206151957015764 acc: 0.86\n",
      "loss: 1.8008272170342918 acc: 0.84\n",
      "loss: 1.8583146209296513 acc: 0.79\n",
      "loss: 1.8782263949758484 acc: 0.79\n",
      "loss: 1.8788916831643316 acc: 0.81\n",
      "loss: 1.82165677964327 acc: 0.87\n",
      "loss: 1.8224166653415272 acc: 0.83\n",
      "loss: 1.782452347203472 acc: 0.86\n",
      "loss: 1.8149788114412746 acc: 0.83\n",
      "loss: 1.7962851210400106 acc: 0.87\n",
      "loss: 1.89832439567464 acc: 0.81\n",
      "loss: 1.8441561081374003 acc: 0.85\n",
      "loss: 1.843646759081542 acc: 0.79\n",
      "loss: 1.8563533172459419 acc: 0.86\n",
      "loss: 1.8539493505777178 acc: 0.83\n",
      "loss: 1.8773009111583303 acc: 0.77\n",
      "loss: 1.8717914986737119 acc: 0.75\n",
      "loss: 1.8907864180584637 acc: 0.81\n",
      "loss: 1.862117425328548 acc: 0.86\n",
      "loss: 1.8084474153972534 acc: 0.89\n",
      "loss: 1.8329801120591256 acc: 0.88\n",
      "loss: 1.8037122334140614 acc: 0.88\n",
      "loss: 1.768396382115352 acc: 0.9\n",
      "loss: 1.7965064867265537 acc: 0.89\n",
      "loss: 1.8188694414628566 acc: 0.86\n",
      "loss: 1.8305760935442879 acc: 0.87\n",
      "loss: 1.866269514536647 acc: 0.87\n",
      "loss: 1.8767116876418144 acc: 0.82\n",
      "loss: 1.7935978275455438 acc: 0.87\n",
      "loss: 1.7024109405134724 acc: 0.95\n",
      "loss: 1.7479714928972556 acc: 0.92\n",
      "loss: 1.7723003397371997 acc: 0.95\n",
      "loss: 1.8172428904245157 acc: 0.84\n",
      "loss: 1.8312184685045738 acc: 0.78\n",
      "loss: 1.7481201098357988 acc: 0.81\n",
      "loss: 1.8266255282307358 acc: 0.88\n",
      "loss: 1.8205384352820306 acc: 0.89\n",
      "loss: 1.9113301246740775 acc: 0.75\n",
      "loss: 1.7139925205452948 acc: 0.93\n",
      "loss: 1.9049678169027882 acc: 0.8\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8487\t Average training accuracy 0.8045\n",
      "Epoch [3]\t Average validation loss 1.8242\t Average validation accuracy 0.8476\n",
      "\n",
      "loss: 1.7990130582691952 acc: 0.89\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 1.7990\t Accuracy 0.8900\n",
      "loss: 1.8188941810905634 acc: 0.87\n",
      "loss: 1.8638702527798694 acc: 0.76\n",
      "loss: 1.8588518076583975 acc: 0.74\n",
      "loss: 1.8441485409916376 acc: 0.81\n",
      "loss: 1.8926685961125793 acc: 0.71\n",
      "loss: 1.8455750334690249 acc: 0.82\n",
      "loss: 1.8408321507837742 acc: 0.85\n",
      "loss: 1.8464007041877983 acc: 0.81\n",
      "loss: 1.824442308719118 acc: 0.8\n",
      "loss: 1.8856102974947269 acc: 0.8\n",
      "loss: 1.8575638639048764 acc: 0.85\n",
      "loss: 1.8268778491663997 acc: 0.83\n",
      "loss: 1.8573160975930187 acc: 0.8\n",
      "loss: 1.8650411896681414 acc: 0.76\n",
      "loss: 1.8518707652114574 acc: 0.82\n",
      "loss: 1.8178107974212079 acc: 0.87\n",
      "loss: 1.8946206165993678 acc: 0.76\n",
      "loss: 1.8610348306159181 acc: 0.77\n",
      "loss: 1.838544052243035 acc: 0.8\n",
      "loss: 1.8335056594811565 acc: 0.82\n",
      "loss: 1.8329817062425848 acc: 0.77\n",
      "loss: 1.8802964541226297 acc: 0.77\n",
      "loss: 1.873326691238812 acc: 0.8\n",
      "loss: 1.8736748438740651 acc: 0.79\n",
      "loss: 1.8389388809436111 acc: 0.84\n",
      "loss: 1.8492025067169957 acc: 0.75\n",
      "loss: 1.813347733450249 acc: 0.82\n",
      "loss: 1.8434922134741398 acc: 0.75\n",
      "loss: 1.8138831753452633 acc: 0.83\n",
      "loss: 1.8494097877397209 acc: 0.79\n",
      "loss: 1.8251622483944288 acc: 0.89\n",
      "loss: 1.885857839764439 acc: 0.78\n",
      "loss: 1.8577500761694754 acc: 0.83\n",
      "loss: 1.806560385691948 acc: 0.85\n",
      "loss: 1.8582493723508768 acc: 0.82\n",
      "loss: 1.8048928954740933 acc: 0.84\n",
      "loss: 1.8612970105663373 acc: 0.78\n",
      "loss: 1.8872089673879193 acc: 0.77\n",
      "loss: 1.856898146325111 acc: 0.8\n",
      "loss: 1.8802980871438286 acc: 0.83\n",
      "loss: 1.87035646367946 acc: 0.79\n",
      "loss: 1.8310554807786021 acc: 0.8\n",
      "loss: 1.8407960720086438 acc: 0.84\n",
      "loss: 1.7925565894166693 acc: 0.83\n",
      "loss: 1.8490621445524322 acc: 0.77\n",
      "loss: 1.902555959083541 acc: 0.75\n",
      "loss: 1.8399585343111227 acc: 0.8\n",
      "loss: 1.8776731244879585 acc: 0.8\n",
      "loss: 1.836912955768062 acc: 0.83\n",
      "loss: 1.8523908957442423 acc: 0.77\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 1.8524\t Accuracy 0.7700\n",
      "loss: 1.8942882961964416 acc: 0.73\n",
      "loss: 1.8033373209336747 acc: 0.86\n",
      "loss: 1.8251191270310094 acc: 0.83\n",
      "loss: 1.8459377898388158 acc: 0.81\n",
      "loss: 1.8488365314504456 acc: 0.78\n",
      "loss: 1.8930827814107243 acc: 0.76\n",
      "loss: 1.8570863334211412 acc: 0.82\n",
      "loss: 1.83036391198814 acc: 0.83\n",
      "loss: 1.8670319383059768 acc: 0.79\n",
      "loss: 1.8301406636042392 acc: 0.87\n",
      "loss: 1.8684962193152652 acc: 0.81\n",
      "loss: 1.8661115079071064 acc: 0.83\n",
      "loss: 1.8246993010164116 acc: 0.87\n",
      "loss: 1.8636546633369446 acc: 0.82\n",
      "loss: 1.8639218866483316 acc: 0.85\n",
      "loss: 1.83701719864358 acc: 0.86\n",
      "loss: 1.894610827846219 acc: 0.73\n",
      "loss: 1.8428829776755906 acc: 0.81\n",
      "loss: 1.8165945892852764 acc: 0.86\n",
      "loss: 1.8278272637636837 acc: 0.86\n",
      "loss: 1.8889908299892377 acc: 0.76\n",
      "loss: 1.842284166033779 acc: 0.79\n",
      "loss: 1.8537235985465423 acc: 0.78\n",
      "loss: 1.846576378867358 acc: 0.8\n",
      "loss: 1.856224745182941 acc: 0.82\n",
      "loss: 1.831722921803536 acc: 0.86\n",
      "loss: 1.8444236201426272 acc: 0.79\n",
      "loss: 1.8263120294642754 acc: 0.81\n",
      "loss: 1.8753548602053411 acc: 0.76\n",
      "loss: 1.8688676686787937 acc: 0.81\n",
      "loss: 1.855216284427793 acc: 0.81\n",
      "loss: 1.8185537692045302 acc: 0.82\n",
      "loss: 1.8342689941049455 acc: 0.84\n",
      "loss: 1.843293743242298 acc: 0.8\n",
      "loss: 1.8191264475250515 acc: 0.85\n",
      "loss: 1.8714399540106503 acc: 0.83\n",
      "loss: 1.799514306806048 acc: 0.87\n",
      "loss: 1.852889344199654 acc: 0.79\n",
      "loss: 1.8755377725432905 acc: 0.78\n",
      "loss: 1.8468725557298473 acc: 0.82\n",
      "loss: 1.8587276965461308 acc: 0.76\n",
      "loss: 1.8465703631002024 acc: 0.8\n",
      "loss: 1.8692781696266316 acc: 0.73\n",
      "loss: 1.8340058016217122 acc: 0.82\n",
      "loss: 1.8286052972827118 acc: 0.81\n",
      "loss: 1.841931569955909 acc: 0.79\n",
      "loss: 1.832201886608089 acc: 0.81\n",
      "loss: 1.825523886175368 acc: 0.84\n",
      "loss: 1.8460702618518718 acc: 0.78\n",
      "loss: 1.8612819409639119 acc: 0.76\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 1.8613\t Accuracy 0.7600\n",
      "loss: 1.8527649025701152 acc: 0.78\n",
      "loss: 1.8084045887207367 acc: 0.84\n",
      "loss: 1.839023683714548 acc: 0.83\n",
      "loss: 1.839839900490484 acc: 0.77\n",
      "loss: 1.820976755638223 acc: 0.85\n",
      "loss: 1.8193894182741936 acc: 0.85\n",
      "loss: 1.8451627080624178 acc: 0.8\n",
      "loss: 1.8388464119487151 acc: 0.79\n",
      "loss: 1.8509245382641344 acc: 0.81\n",
      "loss: 1.8426664753581432 acc: 0.82\n",
      "loss: 1.8423620329049868 acc: 0.81\n",
      "loss: 1.8695547244158544 acc: 0.77\n",
      "loss: 1.8497201924286901 acc: 0.83\n",
      "loss: 1.8397213152552458 acc: 0.8\n",
      "loss: 1.7844135968627333 acc: 0.91\n",
      "loss: 1.8056578585020084 acc: 0.89\n",
      "loss: 1.848963884290623 acc: 0.83\n",
      "loss: 1.819914833256883 acc: 0.84\n",
      "loss: 1.8846353275150363 acc: 0.76\n",
      "loss: 1.8371278764605512 acc: 0.82\n",
      "loss: 1.8312857376076874 acc: 0.86\n",
      "loss: 1.801287250997689 acc: 0.88\n",
      "loss: 1.8648817316794717 acc: 0.75\n",
      "loss: 1.8540605637865162 acc: 0.8\n",
      "loss: 1.8274188799621642 acc: 0.78\n",
      "loss: 1.8133577294188419 acc: 0.87\n",
      "loss: 1.8497557679224 acc: 0.8\n",
      "loss: 1.8663714652784957 acc: 0.76\n",
      "loss: 1.8614429741691612 acc: 0.81\n",
      "loss: 1.841868100940803 acc: 0.85\n",
      "loss: 1.8556798132566146 acc: 0.8\n",
      "loss: 1.8185399092762813 acc: 0.82\n",
      "loss: 1.851232803705108 acc: 0.77\n",
      "loss: 1.856086578448779 acc: 0.75\n",
      "loss: 1.835675852988871 acc: 0.87\n",
      "loss: 1.867282964212407 acc: 0.75\n",
      "loss: 1.7964424384741775 acc: 0.83\n",
      "loss: 1.842013534286864 acc: 0.84\n",
      "loss: 1.8451549486234073 acc: 0.77\n",
      "loss: 1.852844557104752 acc: 0.85\n",
      "loss: 1.839782543183148 acc: 0.8\n",
      "loss: 1.8560074763246928 acc: 0.79\n",
      "loss: 1.8169646298222881 acc: 0.84\n",
      "loss: 1.836843946555632 acc: 0.85\n",
      "loss: 1.8489659852710896 acc: 0.84\n",
      "loss: 1.8511671303024182 acc: 0.81\n",
      "loss: 1.845143724181377 acc: 0.82\n",
      "loss: 1.8436220447749005 acc: 0.83\n",
      "loss: 1.855227639123618 acc: 0.81\n",
      "loss: 1.857265169188668 acc: 0.77\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 1.8573\t Accuracy 0.7700\n",
      "loss: 1.8129448774959827 acc: 0.85\n",
      "loss: 1.803621458621036 acc: 0.85\n",
      "loss: 1.8549530853944205 acc: 0.82\n",
      "loss: 1.8168341398554053 acc: 0.82\n",
      "loss: 1.8461825618190735 acc: 0.81\n",
      "loss: 1.879184454804347 acc: 0.81\n",
      "loss: 1.866084831778979 acc: 0.81\n",
      "loss: 1.838462953674699 acc: 0.86\n",
      "loss: 1.8083012695244574 acc: 0.83\n",
      "loss: 1.785725563331939 acc: 0.86\n",
      "loss: 1.8281918405165851 acc: 0.8\n",
      "loss: 1.836182193572136 acc: 0.82\n",
      "loss: 1.8364069685314166 acc: 0.82\n",
      "loss: 1.8845346859372183 acc: 0.78\n",
      "loss: 1.8565904850632293 acc: 0.8\n",
      "loss: 1.8367061532803697 acc: 0.84\n",
      "loss: 1.8355255686492729 acc: 0.83\n",
      "loss: 1.831282233941231 acc: 0.86\n",
      "loss: 1.896751971492177 acc: 0.79\n",
      "loss: 1.8292656530813411 acc: 0.81\n",
      "loss: 1.8208176576481583 acc: 0.85\n",
      "loss: 1.815955063290146 acc: 0.85\n",
      "loss: 1.8231177514377261 acc: 0.82\n",
      "loss: 1.796116550085692 acc: 0.89\n",
      "loss: 1.8289722524392464 acc: 0.83\n",
      "loss: 1.8796230522206414 acc: 0.8\n",
      "loss: 1.8643948719587693 acc: 0.78\n",
      "loss: 1.8159881605611405 acc: 0.85\n",
      "loss: 1.828406439039874 acc: 0.82\n",
      "loss: 1.840537310597157 acc: 0.79\n",
      "loss: 1.7891950687687117 acc: 0.89\n",
      "loss: 1.8590773231704418 acc: 0.84\n",
      "loss: 1.8436122235515784 acc: 0.79\n",
      "loss: 1.8199778289842092 acc: 0.8\n",
      "loss: 1.8270075353381767 acc: 0.87\n",
      "loss: 1.876404082783937 acc: 0.76\n",
      "loss: 1.8548141840758396 acc: 0.8\n",
      "loss: 1.8616533409851235 acc: 0.78\n",
      "loss: 1.8474990510143152 acc: 0.8\n",
      "loss: 1.8152441723303385 acc: 0.85\n",
      "loss: 1.8267346176250658 acc: 0.79\n",
      "loss: 1.8031829113368332 acc: 0.86\n",
      "loss: 1.8397859762811948 acc: 0.85\n",
      "loss: 1.8278274887754853 acc: 0.8\n",
      "loss: 1.894044869881955 acc: 0.77\n",
      "loss: 1.8199414307468755 acc: 0.87\n",
      "loss: 1.8721988086486328 acc: 0.79\n",
      "loss: 1.8106644164359023 acc: 0.82\n",
      "loss: 1.8389002479304803 acc: 0.85\n",
      "loss: 1.8531106249549678 acc: 0.84\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 1.8531\t Accuracy 0.8400\n",
      "loss: 1.8868437021256232 acc: 0.74\n",
      "loss: 1.8515309383738825 acc: 0.8\n",
      "loss: 1.8540131687983004 acc: 0.82\n",
      "loss: 1.852459761529481 acc: 0.83\n",
      "loss: 1.8531344818374447 acc: 0.81\n",
      "loss: 1.8176185196493513 acc: 0.88\n",
      "loss: 1.8200607803944613 acc: 0.78\n",
      "loss: 1.8013949297813334 acc: 0.84\n",
      "loss: 1.8519341067939599 acc: 0.81\n",
      "loss: 1.8647867508777765 acc: 0.76\n",
      "loss: 1.8259145346299477 acc: 0.87\n",
      "loss: 1.8634586503308583 acc: 0.78\n",
      "loss: 1.8355347272246392 acc: 0.85\n",
      "loss: 1.819594233467146 acc: 0.82\n",
      "loss: 1.82475707859232 acc: 0.84\n",
      "loss: 1.8362903722426955 acc: 0.83\n",
      "loss: 1.823085841624275 acc: 0.84\n",
      "loss: 1.819477527912662 acc: 0.85\n",
      "loss: 1.8514946992794818 acc: 0.81\n",
      "loss: 1.8811349264263582 acc: 0.76\n",
      "loss: 1.8541998184673605 acc: 0.78\n",
      "loss: 1.8295581314311113 acc: 0.78\n",
      "loss: 1.8520397214611233 acc: 0.74\n",
      "loss: 1.8149364559665295 acc: 0.8\n",
      "loss: 1.8326937172498388 acc: 0.85\n",
      "loss: 1.836367340366012 acc: 0.84\n",
      "loss: 1.863010446226474 acc: 0.78\n",
      "loss: 1.8708054712513025 acc: 0.8\n",
      "loss: 1.8388352432941864 acc: 0.84\n",
      "loss: 1.8260951922110906 acc: 0.84\n",
      "loss: 1.8150154239961689 acc: 0.83\n",
      "loss: 1.873106743012729 acc: 0.78\n",
      "loss: 1.8841176231029941 acc: 0.74\n",
      "loss: 1.8384249850476169 acc: 0.84\n",
      "loss: 1.8243370093923523 acc: 0.84\n",
      "loss: 1.8270218434880663 acc: 0.82\n",
      "loss: 1.8251185629357087 acc: 0.85\n",
      "loss: 1.8063283666642107 acc: 0.87\n",
      "loss: 1.8122557408835058 acc: 0.84\n",
      "loss: 1.8414547346646 acc: 0.82\n",
      "loss: 1.8550241086416017 acc: 0.81\n",
      "loss: 1.8561418774714005 acc: 0.76\n",
      "loss: 1.8358028441038061 acc: 0.87\n",
      "loss: 1.8512029951757984 acc: 0.79\n",
      "loss: 1.856147241056494 acc: 0.79\n",
      "loss: 1.8516402161920105 acc: 0.81\n",
      "loss: 1.8477479940396753 acc: 0.8\n",
      "loss: 1.8569975954514713 acc: 0.76\n",
      "loss: 1.8928701832610468 acc: 0.74\n",
      "loss: 1.8480450335328578 acc: 0.83\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 1.8480\t Accuracy 0.8300\n",
      "loss: 1.8837976479111742 acc: 0.76\n",
      "loss: 1.8647284517161526 acc: 0.78\n",
      "loss: 1.8695374767224973 acc: 0.79\n",
      "loss: 1.8223931408530682 acc: 0.79\n",
      "loss: 1.82971042033065 acc: 0.88\n",
      "loss: 1.8770600622676594 acc: 0.78\n",
      "loss: 1.8761061494903666 acc: 0.79\n",
      "loss: 1.8153045588589112 acc: 0.84\n",
      "loss: 1.8232214256075736 acc: 0.88\n",
      "loss: 1.8480409435293346 acc: 0.78\n",
      "loss: 1.8836265046050606 acc: 0.75\n",
      "loss: 1.8223150087088937 acc: 0.8\n",
      "loss: 1.817312602320343 acc: 0.84\n",
      "loss: 1.859078275400483 acc: 0.8\n",
      "loss: 1.8638699094649192 acc: 0.76\n",
      "loss: 1.8575625370789453 acc: 0.84\n",
      "loss: 1.850781959348072 acc: 0.78\n",
      "loss: 1.8657163114749462 acc: 0.84\n",
      "loss: 1.8529572033498776 acc: 0.73\n",
      "loss: 1.824475470627256 acc: 0.84\n",
      "loss: 1.8692044573256064 acc: 0.79\n",
      "loss: 1.8614754088034713 acc: 0.81\n",
      "loss: 1.8043046208450377 acc: 0.87\n",
      "loss: 1.8461238507773299 acc: 0.83\n",
      "loss: 1.8267112114090585 acc: 0.87\n",
      "loss: 1.859459415026384 acc: 0.78\n",
      "loss: 1.862411280834056 acc: 0.8\n",
      "loss: 1.8542715309840865 acc: 0.82\n",
      "loss: 1.8365390425091919 acc: 0.8\n",
      "loss: 1.8870063228497926 acc: 0.82\n",
      "loss: 1.8629250067933318 acc: 0.84\n",
      "loss: 1.8681578344346854 acc: 0.76\n",
      "loss: 1.824275810733792 acc: 0.85\n",
      "loss: 1.839212443615746 acc: 0.83\n",
      "loss: 1.8183842974047215 acc: 0.87\n",
      "loss: 1.813689649543681 acc: 0.85\n",
      "loss: 1.8718094436037298 acc: 0.75\n",
      "loss: 1.8395879257268022 acc: 0.85\n",
      "loss: 1.8856820709086484 acc: 0.75\n",
      "loss: 1.8271460905478492 acc: 0.86\n",
      "loss: 1.8271931727801887 acc: 0.8\n",
      "loss: 1.8611050918415606 acc: 0.76\n",
      "loss: 1.8316660701630634 acc: 0.84\n",
      "loss: 1.8360543517298484 acc: 0.84\n",
      "loss: 1.8640196801246909 acc: 0.8\n",
      "loss: 1.8582081284053933 acc: 0.79\n",
      "loss: 1.8497822302285007 acc: 0.83\n",
      "loss: 1.8451488788100425 acc: 0.88\n",
      "loss: 1.832151536432219 acc: 0.82\n",
      "loss: 1.8585290686342342 acc: 0.86\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 1.8585\t Accuracy 0.8600\n",
      "loss: 1.85968232433093 acc: 0.81\n",
      "loss: 1.8391831473316684 acc: 0.87\n",
      "loss: 1.8825463192358833 acc: 0.76\n",
      "loss: 1.861486314019237 acc: 0.83\n",
      "loss: 1.8110836000430062 acc: 0.82\n",
      "loss: 1.821609908129438 acc: 0.89\n",
      "loss: 1.8561098211622433 acc: 0.79\n",
      "loss: 1.841728352977251 acc: 0.79\n",
      "loss: 1.8224117382466978 acc: 0.85\n",
      "loss: 1.868489243433173 acc: 0.8\n",
      "loss: 1.815084003994408 acc: 0.8\n",
      "loss: 1.824431423656523 acc: 0.85\n",
      "loss: 1.8875051025678986 acc: 0.76\n",
      "loss: 1.823848223821845 acc: 0.87\n",
      "loss: 1.8387836638920831 acc: 0.81\n",
      "loss: 1.8637020994919515 acc: 0.81\n",
      "loss: 1.814774730850865 acc: 0.82\n",
      "loss: 1.8302919474618398 acc: 0.85\n",
      "loss: 1.8632106335519643 acc: 0.77\n",
      "loss: 1.834086413739658 acc: 0.85\n",
      "loss: 1.8543590736389135 acc: 0.82\n",
      "loss: 1.8792378213428862 acc: 0.71\n",
      "loss: 1.841956734965864 acc: 0.83\n",
      "loss: 1.8271875102670299 acc: 0.82\n",
      "loss: 1.833314651994437 acc: 0.84\n",
      "loss: 1.8640814854479033 acc: 0.79\n",
      "loss: 1.8621432844940153 acc: 0.78\n",
      "loss: 1.8075616834416601 acc: 0.81\n",
      "loss: 1.88407811570127 acc: 0.72\n",
      "loss: 1.8259592887814304 acc: 0.85\n",
      "loss: 1.8448928079155353 acc: 0.8\n",
      "loss: 1.8118165643143742 acc: 0.86\n",
      "loss: 1.8363494717585587 acc: 0.86\n",
      "loss: 1.8374136114147057 acc: 0.8\n",
      "loss: 1.8217473979625216 acc: 0.84\n",
      "loss: 1.8058578128893514 acc: 0.87\n",
      "loss: 1.8182179484979284 acc: 0.86\n",
      "loss: 1.8622982486178643 acc: 0.81\n",
      "loss: 1.8581270906035607 acc: 0.83\n",
      "loss: 1.8279668063012517 acc: 0.84\n",
      "loss: 1.8286059609454945 acc: 0.85\n",
      "loss: 1.8149955608280643 acc: 0.86\n",
      "loss: 1.8660219955195583 acc: 0.75\n",
      "loss: 1.8257583806983337 acc: 0.83\n",
      "loss: 1.8589274597635213 acc: 0.72\n",
      "loss: 1.8339327398418 acc: 0.83\n",
      "loss: 1.8650023716349684 acc: 0.79\n",
      "loss: 1.8631310212061936 acc: 0.83\n",
      "loss: 1.8546305300952113 acc: 0.81\n",
      "loss: 1.886732473018406 acc: 0.77\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 1.8867\t Accuracy 0.7700\n",
      "loss: 1.836560833020423 acc: 0.81\n",
      "loss: 1.8704765529788627 acc: 0.83\n",
      "loss: 1.8527081786823998 acc: 0.8\n",
      "loss: 1.8551318958668208 acc: 0.77\n",
      "loss: 1.8589542763889984 acc: 0.71\n",
      "loss: 1.892603702433686 acc: 0.76\n",
      "loss: 1.805845885201397 acc: 0.83\n",
      "loss: 1.809872126435506 acc: 0.87\n",
      "loss: 1.8919813761156852 acc: 0.79\n",
      "loss: 1.8362030703340668 acc: 0.82\n",
      "loss: 1.8226733408726432 acc: 0.83\n",
      "loss: 1.8482434794768554 acc: 0.83\n",
      "loss: 1.8260367598485068 acc: 0.83\n",
      "loss: 1.8298250856168505 acc: 0.85\n",
      "loss: 1.8440151251721257 acc: 0.82\n",
      "loss: 1.8337807945576958 acc: 0.82\n",
      "loss: 1.8778327373419759 acc: 0.79\n",
      "loss: 1.8114221099228296 acc: 0.86\n",
      "loss: 1.8685119152674625 acc: 0.79\n",
      "loss: 1.8379907009846193 acc: 0.87\n",
      "loss: 1.841327696619438 acc: 0.85\n",
      "loss: 1.8206234574533255 acc: 0.88\n",
      "loss: 1.8455117131451566 acc: 0.84\n",
      "loss: 1.8450661258436776 acc: 0.82\n",
      "loss: 1.850540016130052 acc: 0.78\n",
      "loss: 1.8157159583988263 acc: 0.84\n",
      "loss: 1.860829373420074 acc: 0.78\n",
      "loss: 1.8466193945885498 acc: 0.81\n",
      "loss: 1.8401896836006255 acc: 0.78\n",
      "loss: 1.8368243983548282 acc: 0.81\n",
      "loss: 1.8433558067934905 acc: 0.81\n",
      "loss: 1.8475466824500748 acc: 0.8\n",
      "loss: 1.8675137464960625 acc: 0.81\n",
      "loss: 1.8486754648066952 acc: 0.79\n",
      "loss: 1.8623724418963707 acc: 0.8\n",
      "loss: 1.8575410961892882 acc: 0.82\n",
      "loss: 1.8492672762275455 acc: 0.81\n",
      "loss: 1.883199666162169 acc: 0.77\n",
      "loss: 1.8312377603908134 acc: 0.79\n",
      "loss: 1.7933015228047389 acc: 0.9\n",
      "loss: 1.8484009904953334 acc: 0.85\n",
      "loss: 1.8347358325059404 acc: 0.83\n",
      "loss: 1.8402417701416942 acc: 0.79\n",
      "loss: 1.7967301063650034 acc: 0.87\n",
      "loss: 1.8205682099053857 acc: 0.79\n",
      "loss: 1.8396658558796692 acc: 0.81\n",
      "loss: 1.8478040603723938 acc: 0.82\n",
      "loss: 1.853250264867284 acc: 0.81\n",
      "loss: 1.8352658536155866 acc: 0.82\n",
      "loss: 1.8331738553309447 acc: 0.81\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 1.8332\t Accuracy 0.8100\n",
      "loss: 1.8607841873328266 acc: 0.79\n",
      "loss: 1.8285076947738694 acc: 0.83\n",
      "loss: 1.816912606881941 acc: 0.85\n",
      "loss: 1.8445055756990758 acc: 0.83\n",
      "loss: 1.8695545272500098 acc: 0.75\n",
      "loss: 1.8496508167531147 acc: 0.83\n",
      "loss: 1.8872374429620493 acc: 0.77\n",
      "loss: 1.8012279448834883 acc: 0.89\n",
      "loss: 1.8198404305405393 acc: 0.85\n",
      "loss: 1.8187269770909733 acc: 0.84\n",
      "loss: 1.8567198274649643 acc: 0.74\n",
      "loss: 1.8147357875326195 acc: 0.84\n",
      "loss: 1.832961376815677 acc: 0.84\n",
      "loss: 1.819561016350334 acc: 0.82\n",
      "loss: 1.8357988844492779 acc: 0.86\n",
      "loss: 1.8433868731550307 acc: 0.83\n",
      "loss: 1.8228832396124375 acc: 0.82\n",
      "loss: 1.8360825260228675 acc: 0.83\n",
      "loss: 1.8465992187915246 acc: 0.75\n",
      "loss: 1.8417319664525693 acc: 0.82\n",
      "loss: 1.8559302842299885 acc: 0.88\n",
      "loss: 1.821630565750841 acc: 0.84\n",
      "loss: 1.8167789301386907 acc: 0.86\n",
      "loss: 1.774650099748639 acc: 0.87\n",
      "loss: 1.831035079009134 acc: 0.82\n",
      "loss: 1.8144229871209783 acc: 0.82\n",
      "loss: 1.826887987226212 acc: 0.85\n",
      "loss: 1.8376367618034857 acc: 0.81\n",
      "loss: 1.8136233215200581 acc: 0.84\n",
      "loss: 1.8372876257730615 acc: 0.88\n",
      "loss: 1.816190084426834 acc: 0.85\n",
      "loss: 1.8662645691942032 acc: 0.78\n",
      "loss: 1.8216330907890066 acc: 0.81\n",
      "loss: 1.8136338457776353 acc: 0.82\n",
      "loss: 1.8270523744869251 acc: 0.85\n",
      "loss: 1.8721737009540766 acc: 0.79\n",
      "loss: 1.8216230064732464 acc: 0.84\n",
      "loss: 1.8308265421467393 acc: 0.84\n",
      "loss: 1.855568992419485 acc: 0.71\n",
      "loss: 1.8359690618802853 acc: 0.81\n",
      "loss: 1.8156756199229998 acc: 0.84\n",
      "loss: 1.8484025010948566 acc: 0.87\n",
      "loss: 1.8394460368665184 acc: 0.84\n",
      "loss: 1.8172303012539277 acc: 0.87\n",
      "loss: 1.83246706568524 acc: 0.86\n",
      "loss: 1.8918731073480344 acc: 0.77\n",
      "loss: 1.853251631166753 acc: 0.81\n",
      "loss: 1.834216304035902 acc: 0.87\n",
      "loss: 1.8243272965923227 acc: 0.8\n",
      "loss: 1.8600768452939005 acc: 0.8\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 1.8601\t Accuracy 0.8000\n",
      "loss: 1.8635253986849734 acc: 0.82\n",
      "loss: 1.8603315436627474 acc: 0.81\n",
      "loss: 1.85112327280205 acc: 0.81\n",
      "loss: 1.849406546188481 acc: 0.78\n",
      "loss: 1.8463393265897061 acc: 0.83\n",
      "loss: 1.822645937534877 acc: 0.85\n",
      "loss: 1.8237367504387771 acc: 0.81\n",
      "loss: 1.8426836352094775 acc: 0.84\n",
      "loss: 1.8107371152847622 acc: 0.85\n",
      "loss: 1.8243550679645169 acc: 0.85\n",
      "loss: 1.829615295238013 acc: 0.84\n",
      "loss: 1.8562416433729894 acc: 0.79\n",
      "loss: 1.8701139307186219 acc: 0.8\n",
      "loss: 1.785668468981238 acc: 0.87\n",
      "loss: 1.8108921839637175 acc: 0.79\n",
      "loss: 1.866625697470085 acc: 0.78\n",
      "loss: 1.8573581204115488 acc: 0.82\n",
      "loss: 1.8556613264754611 acc: 0.73\n",
      "loss: 1.8027474601175897 acc: 0.85\n",
      "loss: 1.8810962237128521 acc: 0.75\n",
      "loss: 1.7998107970871595 acc: 0.89\n",
      "loss: 1.846357916736335 acc: 0.8\n",
      "loss: 1.8363186546341816 acc: 0.84\n",
      "loss: 1.8478457320107118 acc: 0.75\n",
      "loss: 1.8264692009670527 acc: 0.82\n",
      "loss: 1.8935180394267386 acc: 0.69\n",
      "loss: 1.9056672715078384 acc: 0.83\n",
      "loss: 1.846222995492822 acc: 0.83\n",
      "loss: 1.8679652387808003 acc: 0.84\n",
      "loss: 1.8092687520383532 acc: 0.85\n",
      "loss: 1.8531188598937036 acc: 0.81\n",
      "loss: 1.8991923401930169 acc: 0.72\n",
      "loss: 1.8115784513773745 acc: 0.8\n",
      "loss: 1.8586186817936303 acc: 0.82\n",
      "loss: 1.8071198791031569 acc: 0.83\n",
      "loss: 1.8639516591140872 acc: 0.84\n",
      "loss: 1.8453675782195642 acc: 0.78\n",
      "loss: 1.867341619330709 acc: 0.84\n",
      "loss: 1.8469196740417824 acc: 0.79\n",
      "loss: 1.8357762482830124 acc: 0.81\n",
      "loss: 1.8880111017764145 acc: 0.74\n",
      "loss: 1.8458034082057646 acc: 0.87\n",
      "loss: 1.812396679055858 acc: 0.82\n",
      "loss: 1.840744683384369 acc: 0.81\n",
      "loss: 1.8732596872492955 acc: 0.75\n",
      "loss: 1.866429069065945 acc: 0.76\n",
      "loss: 1.8418143871824475 acc: 0.81\n",
      "loss: 1.8459453740068756 acc: 0.82\n",
      "loss: 1.8363945399523363 acc: 0.83\n",
      "loss: 1.832969677086921 acc: 0.92\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 1.8330\t Accuracy 0.9200\n",
      "loss: 1.8568025262613512 acc: 0.85\n",
      "loss: 1.8439465477195476 acc: 0.85\n",
      "loss: 1.841192884451439 acc: 0.86\n",
      "loss: 1.8506914470058131 acc: 0.81\n",
      "loss: 1.8757897724854578 acc: 0.84\n",
      "loss: 1.8374876815448686 acc: 0.82\n",
      "loss: 1.8427816557708787 acc: 0.82\n",
      "loss: 1.8163111634657423 acc: 0.84\n",
      "loss: 1.8276621196521958 acc: 0.83\n",
      "loss: 1.8397119255382028 acc: 0.83\n",
      "loss: 1.8433596337616125 acc: 0.79\n",
      "loss: 1.8040925486426007 acc: 0.9\n",
      "loss: 1.8547615581230739 acc: 0.81\n",
      "loss: 1.8588396348572318 acc: 0.79\n",
      "loss: 1.8683879471256386 acc: 0.75\n",
      "loss: 1.8848019360759596 acc: 0.77\n",
      "loss: 1.8289879868339565 acc: 0.88\n",
      "loss: 1.8374480968966937 acc: 0.87\n",
      "loss: 1.8763511597279057 acc: 0.71\n",
      "loss: 1.840927079051104 acc: 0.84\n",
      "loss: 1.8608992332054946 acc: 0.79\n",
      "loss: 1.8506609113910302 acc: 0.82\n",
      "loss: 1.8023663481928518 acc: 0.88\n",
      "loss: 1.8629130760052914 acc: 0.78\n",
      "loss: 1.8429466420541345 acc: 0.8\n",
      "loss: 1.8268688857654152 acc: 0.85\n",
      "loss: 1.8776742999548637 acc: 0.74\n",
      "loss: 1.8661331410174973 acc: 0.78\n",
      "loss: 1.863645832812382 acc: 0.81\n",
      "loss: 1.8258070077039215 acc: 0.75\n",
      "loss: 1.8384762030927566 acc: 0.85\n",
      "loss: 1.8793858618228723 acc: 0.84\n",
      "loss: 1.8826925751178707 acc: 0.78\n",
      "loss: 1.8467284749582624 acc: 0.83\n",
      "loss: 1.8263046926354642 acc: 0.83\n",
      "loss: 1.8301765943561992 acc: 0.85\n",
      "loss: 1.8385070218643307 acc: 0.86\n",
      "loss: 1.868518752536233 acc: 0.81\n",
      "loss: 1.8245543161359137 acc: 0.85\n",
      "loss: 1.8717502662199192 acc: 0.79\n",
      "loss: 1.8434169458767948 acc: 0.81\n",
      "loss: 1.857125403660655 acc: 0.78\n",
      "loss: 1.8599396910069788 acc: 0.78\n",
      "loss: 1.8481494991839722 acc: 0.79\n",
      "loss: 1.8509206447300681 acc: 0.78\n",
      "loss: 1.8071838778883011 acc: 0.83\n",
      "loss: 1.8514777661551267 acc: 0.84\n",
      "loss: 1.8312256999372254 acc: 0.89\n",
      "loss: 1.8465286509376542 acc: 0.8\n",
      "loss: 1.8240858793905752 acc: 0.81\n",
      "loss: 1.807740309691818 acc: 0.85\n",
      "loss: 1.8120717064265108 acc: 0.88\n",
      "loss: 1.836738906062163 acc: 0.83\n",
      "loss: 1.8449013075572502 acc: 0.85\n",
      "loss: 1.8326784142773958 acc: 0.86\n",
      "loss: 1.7700169522808176 acc: 0.84\n",
      "loss: 1.7774221219738078 acc: 0.85\n",
      "loss: 1.8254571451916377 acc: 0.84\n",
      "loss: 1.7683727491495782 acc: 0.9\n",
      "loss: 1.8147805856395458 acc: 0.86\n",
      "loss: 1.8003225475682811 acc: 0.84\n",
      "loss: 1.8512604658689253 acc: 0.78\n",
      "loss: 1.8728755486588926 acc: 0.82\n",
      "loss: 1.8734713216137604 acc: 0.82\n",
      "loss: 1.814914441882611 acc: 0.88\n",
      "loss: 1.8181851707798315 acc: 0.84\n",
      "loss: 1.7753442275092233 acc: 0.9\n",
      "loss: 1.8123404418146611 acc: 0.85\n",
      "loss: 1.7924466809999355 acc: 0.87\n",
      "loss: 1.8965551009623243 acc: 0.83\n",
      "loss: 1.8390772132923094 acc: 0.87\n",
      "loss: 1.8394050771081316 acc: 0.78\n",
      "loss: 1.8553171867225553 acc: 0.86\n",
      "loss: 1.8501585185942921 acc: 0.84\n",
      "loss: 1.8773009676436598 acc: 0.77\n",
      "loss: 1.866865597466365 acc: 0.74\n",
      "loss: 1.888165164528572 acc: 0.82\n",
      "loss: 1.8550835743410112 acc: 0.86\n",
      "loss: 1.8032866721063638 acc: 0.9\n",
      "loss: 1.8276354347764445 acc: 0.89\n",
      "loss: 1.7997023269535097 acc: 0.89\n",
      "loss: 1.7623793713135951 acc: 0.92\n",
      "loss: 1.791078051625756 acc: 0.9\n",
      "loss: 1.8098822628816114 acc: 0.87\n",
      "loss: 1.8285105262824004 acc: 0.87\n",
      "loss: 1.8611061171153278 acc: 0.87\n",
      "loss: 1.8692475918451081 acc: 0.83\n",
      "loss: 1.7805583684369415 acc: 0.91\n",
      "loss: 1.7001851979888383 acc: 0.96\n",
      "loss: 1.744700223064384 acc: 0.92\n",
      "loss: 1.7671274128547634 acc: 0.95\n",
      "loss: 1.811374252868254 acc: 0.84\n",
      "loss: 1.8231156747436703 acc: 0.79\n",
      "loss: 1.7408451632567234 acc: 0.84\n",
      "loss: 1.8199401689630137 acc: 0.87\n",
      "loss: 1.816249712254959 acc: 0.9\n",
      "loss: 1.9077498234656758 acc: 0.78\n",
      "loss: 1.7079972876242178 acc: 0.93\n",
      "loss: 1.8975260025726326 acc: 0.81\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8437\t Average training accuracy 0.8148\n",
      "Epoch [4]\t Average validation loss 1.8193\t Average validation accuracy 0.8556\n",
      "\n",
      "loss: 1.8698202729489208 acc: 0.78\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 1.8698\t Accuracy 0.7800\n",
      "loss: 1.8230597872886671 acc: 0.83\n",
      "loss: 1.8661734770372291 acc: 0.79\n",
      "loss: 1.8223675815994593 acc: 0.84\n",
      "loss: 1.8326619432386189 acc: 0.78\n",
      "loss: 1.8573775377496295 acc: 0.83\n",
      "loss: 1.8546003430068148 acc: 0.78\n",
      "loss: 1.8527605372077185 acc: 0.82\n",
      "loss: 1.8353908260199026 acc: 0.84\n",
      "loss: 1.8298607412484407 acc: 0.8\n",
      "loss: 1.8567489504259087 acc: 0.83\n",
      "loss: 1.86542775388644 acc: 0.8\n",
      "loss: 1.8490016016061785 acc: 0.79\n",
      "loss: 1.838467303336275 acc: 0.84\n",
      "loss: 1.8317720094496894 acc: 0.87\n",
      "loss: 1.862767410191312 acc: 0.76\n",
      "loss: 1.861156523449231 acc: 0.8\n",
      "loss: 1.8250842765587876 acc: 0.86\n",
      "loss: 1.8063778600841596 acc: 0.86\n",
      "loss: 1.810140118268861 acc: 0.85\n",
      "loss: 1.846200201583911 acc: 0.78\n",
      "loss: 1.83322766409954 acc: 0.85\n",
      "loss: 1.854445539920258 acc: 0.82\n",
      "loss: 1.8217046576890363 acc: 0.87\n",
      "loss: 1.8491892066383335 acc: 0.83\n",
      "loss: 1.7879236543418204 acc: 0.83\n",
      "loss: 1.8720110499067117 acc: 0.82\n",
      "loss: 1.8870863271849359 acc: 0.79\n",
      "loss: 1.859117865101908 acc: 0.84\n",
      "loss: 1.8576985412019673 acc: 0.82\n",
      "loss: 1.8425528589474935 acc: 0.76\n",
      "loss: 1.8400156801823362 acc: 0.84\n",
      "loss: 1.8426127597113637 acc: 0.81\n",
      "loss: 1.802627385676077 acc: 0.91\n",
      "loss: 1.8646436464565423 acc: 0.79\n",
      "loss: 1.836483195454344 acc: 0.86\n",
      "loss: 1.8568485743389171 acc: 0.82\n",
      "loss: 1.8418812413214167 acc: 0.87\n",
      "loss: 1.8532772663341819 acc: 0.81\n",
      "loss: 1.8097487130936296 acc: 0.84\n",
      "loss: 1.835749653992074 acc: 0.81\n",
      "loss: 1.797939489038263 acc: 0.89\n",
      "loss: 1.8116526365083678 acc: 0.84\n",
      "loss: 1.850049918810425 acc: 0.77\n",
      "loss: 1.8696782012122417 acc: 0.81\n",
      "loss: 1.8004079006917186 acc: 0.88\n",
      "loss: 1.838257714031799 acc: 0.8\n",
      "loss: 1.8101519064731622 acc: 0.82\n",
      "loss: 1.8587797081645 acc: 0.77\n",
      "loss: 1.8330588881830332 acc: 0.78\n",
      "loss: 1.8444256192457642 acc: 0.83\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 1.8444\t Accuracy 0.8300\n",
      "loss: 1.801048717221255 acc: 0.85\n",
      "loss: 1.8102881054947937 acc: 0.87\n",
      "loss: 1.8921295362907369 acc: 0.76\n",
      "loss: 1.8152505252500608 acc: 0.84\n",
      "loss: 1.8259100421393542 acc: 0.82\n",
      "loss: 1.8046750246984806 acc: 0.87\n",
      "loss: 1.8638808433871115 acc: 0.82\n",
      "loss: 1.8535277639969931 acc: 0.87\n",
      "loss: 1.8476514365010168 acc: 0.85\n",
      "loss: 1.8156848949989168 acc: 0.88\n",
      "loss: 1.839063715875952 acc: 0.83\n",
      "loss: 1.8436672629771635 acc: 0.82\n",
      "loss: 1.8245631282746795 acc: 0.85\n",
      "loss: 1.8240882270989556 acc: 0.85\n",
      "loss: 1.8755742435093925 acc: 0.8\n",
      "loss: 1.8098162001891318 acc: 0.85\n",
      "loss: 1.8609808918952473 acc: 0.8\n",
      "loss: 1.8328283076805016 acc: 0.8\n",
      "loss: 1.817980712534996 acc: 0.81\n",
      "loss: 1.8106170125731844 acc: 0.88\n",
      "loss: 1.8243086492801752 acc: 0.81\n",
      "loss: 1.8849457339259326 acc: 0.69\n",
      "loss: 1.8347883417786854 acc: 0.84\n",
      "loss: 1.8759537990495867 acc: 0.8\n",
      "loss: 1.8228161174640796 acc: 0.83\n",
      "loss: 1.839261738184071 acc: 0.83\n",
      "loss: 1.8348497715636365 acc: 0.85\n",
      "loss: 1.856078643890159 acc: 0.8\n",
      "loss: 1.8465047225912117 acc: 0.85\n",
      "loss: 1.8585673272568821 acc: 0.8\n",
      "loss: 1.7946003202074805 acc: 0.92\n",
      "loss: 1.8242516466960739 acc: 0.84\n",
      "loss: 1.83812730475872 acc: 0.82\n",
      "loss: 1.8420925841150781 acc: 0.81\n",
      "loss: 1.8491605997761482 acc: 0.89\n",
      "loss: 1.8280168126674303 acc: 0.82\n",
      "loss: 1.817573542108758 acc: 0.9\n",
      "loss: 1.8336435320206648 acc: 0.82\n",
      "loss: 1.791029415230841 acc: 0.88\n",
      "loss: 1.855196194437236 acc: 0.82\n",
      "loss: 1.84302533204371 acc: 0.83\n",
      "loss: 1.8555038366039245 acc: 0.79\n",
      "loss: 1.8187214976535389 acc: 0.83\n",
      "loss: 1.847096708661504 acc: 0.82\n",
      "loss: 1.8382053232243925 acc: 0.82\n",
      "loss: 1.8415173973887449 acc: 0.87\n",
      "loss: 1.8334566556140748 acc: 0.87\n",
      "loss: 1.810175545237106 acc: 0.85\n",
      "loss: 1.8491073214004123 acc: 0.8\n",
      "loss: 1.870340275077989 acc: 0.8\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 1.8703\t Accuracy 0.8000\n",
      "loss: 1.8618755002540928 acc: 0.84\n",
      "loss: 1.815771734917286 acc: 0.89\n",
      "loss: 1.8102073521139268 acc: 0.84\n",
      "loss: 1.8456085303026049 acc: 0.82\n",
      "loss: 1.8584122054796615 acc: 0.76\n",
      "loss: 1.8576025271545828 acc: 0.8\n",
      "loss: 1.8356551027525843 acc: 0.82\n",
      "loss: 1.8150227556677674 acc: 0.78\n",
      "loss: 1.8112228547843876 acc: 0.85\n",
      "loss: 1.8341060555604007 acc: 0.83\n",
      "loss: 1.8185844460626066 acc: 0.83\n",
      "loss: 1.8801856661993614 acc: 0.82\n",
      "loss: 1.8877793031282841 acc: 0.76\n",
      "loss: 1.85295106924589 acc: 0.74\n",
      "loss: 1.8099653373988465 acc: 0.86\n",
      "loss: 1.8904205879141227 acc: 0.8\n",
      "loss: 1.8254322638400442 acc: 0.87\n",
      "loss: 1.8254991766525348 acc: 0.86\n",
      "loss: 1.8095452663542764 acc: 0.85\n",
      "loss: 1.8236887830626223 acc: 0.89\n",
      "loss: 1.8347923597634819 acc: 0.79\n",
      "loss: 1.825125929462727 acc: 0.85\n",
      "loss: 1.8258658123387606 acc: 0.79\n",
      "loss: 1.7990814299568714 acc: 0.88\n",
      "loss: 1.8375500777356037 acc: 0.81\n",
      "loss: 1.8426913394844504 acc: 0.8\n",
      "loss: 1.8542216867786192 acc: 0.85\n",
      "loss: 1.8202741070741808 acc: 0.84\n",
      "loss: 1.838767857165032 acc: 0.83\n",
      "loss: 1.8221605467083535 acc: 0.85\n",
      "loss: 1.8435493981944162 acc: 0.82\n",
      "loss: 1.8306038031415914 acc: 0.84\n",
      "loss: 1.7810201749682582 acc: 0.89\n",
      "loss: 1.8096853205028147 acc: 0.88\n",
      "loss: 1.850122499137941 acc: 0.83\n",
      "loss: 1.865516075308505 acc: 0.81\n",
      "loss: 1.842696996310824 acc: 0.82\n",
      "loss: 1.8211313565807759 acc: 0.87\n",
      "loss: 1.8742905292225078 acc: 0.8\n",
      "loss: 1.832900845469459 acc: 0.84\n",
      "loss: 1.8216864673952713 acc: 0.8\n",
      "loss: 1.8562341165978216 acc: 0.82\n",
      "loss: 1.8305051905934102 acc: 0.79\n",
      "loss: 1.821683206787527 acc: 0.87\n",
      "loss: 1.8496917829270871 acc: 0.82\n",
      "loss: 1.8355344791785089 acc: 0.82\n",
      "loss: 1.813232587721113 acc: 0.88\n",
      "loss: 1.8706667550507654 acc: 0.77\n",
      "loss: 1.8456516391327318 acc: 0.83\n",
      "loss: 1.7985116442431872 acc: 0.85\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 1.7985\t Accuracy 0.8500\n",
      "loss: 1.841119472624345 acc: 0.8\n",
      "loss: 1.8478265246330312 acc: 0.78\n",
      "loss: 1.805558888563854 acc: 0.89\n",
      "loss: 1.8214698534224778 acc: 0.85\n",
      "loss: 1.8658816894167387 acc: 0.76\n",
      "loss: 1.822989017661462 acc: 0.82\n",
      "loss: 1.8535485547001718 acc: 0.84\n",
      "loss: 1.8586627388235175 acc: 0.78\n",
      "loss: 1.8508950429907234 acc: 0.85\n",
      "loss: 1.8199937680800977 acc: 0.84\n",
      "loss: 1.8485492344684957 acc: 0.77\n",
      "loss: 1.8409392064187784 acc: 0.79\n",
      "loss: 1.8461308609636016 acc: 0.84\n",
      "loss: 1.830159279250092 acc: 0.85\n",
      "loss: 1.8380875782518242 acc: 0.82\n",
      "loss: 1.860752817254397 acc: 0.75\n",
      "loss: 1.8635654452677743 acc: 0.77\n",
      "loss: 1.8344379195310578 acc: 0.84\n",
      "loss: 1.8326275726563983 acc: 0.83\n",
      "loss: 1.8262018503296011 acc: 0.82\n",
      "loss: 1.8513931005676663 acc: 0.83\n",
      "loss: 1.8333554613216811 acc: 0.82\n",
      "loss: 1.8395043036318581 acc: 0.81\n",
      "loss: 1.825364501845186 acc: 0.84\n",
      "loss: 1.8371276090246613 acc: 0.85\n",
      "loss: 1.860652111885137 acc: 0.81\n",
      "loss: 1.7911297059087872 acc: 0.88\n",
      "loss: 1.806275397002029 acc: 0.86\n",
      "loss: 1.8256717189772005 acc: 0.82\n",
      "loss: 1.8127024340623044 acc: 0.88\n",
      "loss: 1.8257085740569383 acc: 0.83\n",
      "loss: 1.9012072725594424 acc: 0.79\n",
      "loss: 1.8810453419924371 acc: 0.84\n",
      "loss: 1.8537764108960113 acc: 0.84\n",
      "loss: 1.8055273849015834 acc: 0.88\n",
      "loss: 1.8046901458754283 acc: 0.8\n",
      "loss: 1.8637686406962501 acc: 0.82\n",
      "loss: 1.8807094279237493 acc: 0.76\n",
      "loss: 1.8791441035530676 acc: 0.79\n",
      "loss: 1.8185201123799353 acc: 0.88\n",
      "loss: 1.844639245024464 acc: 0.8\n",
      "loss: 1.883528049878394 acc: 0.72\n",
      "loss: 1.8612785328508514 acc: 0.79\n",
      "loss: 1.818616161785186 acc: 0.83\n",
      "loss: 1.833513196350944 acc: 0.81\n",
      "loss: 1.8558548574129135 acc: 0.84\n",
      "loss: 1.9013572327065682 acc: 0.75\n",
      "loss: 1.8594548494248317 acc: 0.81\n",
      "loss: 1.8336127267815556 acc: 0.84\n",
      "loss: 1.8477872838845255 acc: 0.81\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 1.8478\t Accuracy 0.8100\n",
      "loss: 1.8244677381210803 acc: 0.84\n",
      "loss: 1.8250009521632191 acc: 0.84\n",
      "loss: 1.8289951465620786 acc: 0.86\n",
      "loss: 1.8223820839228793 acc: 0.89\n",
      "loss: 1.8250479761195246 acc: 0.84\n",
      "loss: 1.8231053548557805 acc: 0.83\n",
      "loss: 1.8125738620058252 acc: 0.88\n",
      "loss: 1.8514064697652066 acc: 0.87\n",
      "loss: 1.8476897391658966 acc: 0.76\n",
      "loss: 1.859595243286937 acc: 0.77\n",
      "loss: 1.862227888736728 acc: 0.74\n",
      "loss: 1.8314221881362778 acc: 0.82\n",
      "loss: 1.8761408945336917 acc: 0.73\n",
      "loss: 1.8693740203629983 acc: 0.83\n",
      "loss: 1.8144210631525284 acc: 0.83\n",
      "loss: 1.8796259624542484 acc: 0.77\n",
      "loss: 1.8399761869774534 acc: 0.83\n",
      "loss: 1.8329234661143226 acc: 0.87\n",
      "loss: 1.8383357999109875 acc: 0.82\n",
      "loss: 1.7678779063940733 acc: 0.91\n",
      "loss: 1.8328558728650355 acc: 0.83\n",
      "loss: 1.8809734477968971 acc: 0.79\n",
      "loss: 1.866711134504819 acc: 0.83\n",
      "loss: 1.8388799414527872 acc: 0.86\n",
      "loss: 1.8082529060321786 acc: 0.82\n",
      "loss: 1.8494083726313741 acc: 0.85\n",
      "loss: 1.8463644292476196 acc: 0.84\n",
      "loss: 1.8556545580945933 acc: 0.83\n",
      "loss: 1.8326487351077467 acc: 0.81\n",
      "loss: 1.8674830941300198 acc: 0.81\n",
      "loss: 1.845420051572666 acc: 0.84\n",
      "loss: 1.843638976197127 acc: 0.83\n",
      "loss: 1.8220933717703125 acc: 0.82\n",
      "loss: 1.8319816227691808 acc: 0.87\n",
      "loss: 1.833105923549451 acc: 0.8\n",
      "loss: 1.7868904718790752 acc: 0.84\n",
      "loss: 1.8578552372663766 acc: 0.83\n",
      "loss: 1.8342838178465533 acc: 0.83\n",
      "loss: 1.8360392861820065 acc: 0.79\n",
      "loss: 1.8083970315593485 acc: 0.86\n",
      "loss: 1.8236493995648688 acc: 0.83\n",
      "loss: 1.8703413390525485 acc: 0.81\n",
      "loss: 1.8285812591953141 acc: 0.84\n",
      "loss: 1.8306517136963611 acc: 0.8\n",
      "loss: 1.85092384858683 acc: 0.85\n",
      "loss: 1.8654762736273347 acc: 0.79\n",
      "loss: 1.8747654542810681 acc: 0.78\n",
      "loss: 1.8666596302274856 acc: 0.82\n",
      "loss: 1.8200679014500138 acc: 0.78\n",
      "loss: 1.8697386360670452 acc: 0.85\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 1.8697\t Accuracy 0.8500\n",
      "loss: 1.8720945626171996 acc: 0.72\n",
      "loss: 1.826053871705168 acc: 0.84\n",
      "loss: 1.8279456128781986 acc: 0.84\n",
      "loss: 1.8652729152300949 acc: 0.78\n",
      "loss: 1.8433121701601523 acc: 0.83\n",
      "loss: 1.8436813628613726 acc: 0.85\n",
      "loss: 1.8606646051948852 acc: 0.79\n",
      "loss: 1.8599769094903689 acc: 0.82\n",
      "loss: 1.8424955095678146 acc: 0.81\n",
      "loss: 1.842503809843128 acc: 0.79\n",
      "loss: 1.8396973870670492 acc: 0.81\n",
      "loss: 1.8547366679338437 acc: 0.8\n",
      "loss: 1.8563896350879352 acc: 0.77\n",
      "loss: 1.834426131382239 acc: 0.85\n",
      "loss: 1.8495276978281487 acc: 0.78\n",
      "loss: 1.8478130872473049 acc: 0.78\n",
      "loss: 1.8393145913429898 acc: 0.79\n",
      "loss: 1.7815911676294613 acc: 0.89\n",
      "loss: 1.846989029017703 acc: 0.83\n",
      "loss: 1.8192179645116964 acc: 0.86\n",
      "loss: 1.8426320517303014 acc: 0.83\n",
      "loss: 1.823432672854696 acc: 0.83\n",
      "loss: 1.8424526488044348 acc: 0.85\n",
      "loss: 1.821907576968642 acc: 0.77\n",
      "loss: 1.8676980767263998 acc: 0.83\n",
      "loss: 1.8512286413549226 acc: 0.79\n",
      "loss: 1.8391812348490904 acc: 0.81\n",
      "loss: 1.8422928865849502 acc: 0.86\n",
      "loss: 1.8611789497789235 acc: 0.81\n",
      "loss: 1.8299800559793715 acc: 0.86\n",
      "loss: 1.7874721508681832 acc: 0.88\n",
      "loss: 1.8725338990712663 acc: 0.74\n",
      "loss: 1.8090737385905122 acc: 0.86\n",
      "loss: 1.8187989435092982 acc: 0.85\n",
      "loss: 1.8314361676819206 acc: 0.82\n",
      "loss: 1.8421275304938727 acc: 0.81\n",
      "loss: 1.849118808340032 acc: 0.8\n",
      "loss: 1.8281587399552899 acc: 0.85\n",
      "loss: 1.7892714020239995 acc: 0.85\n",
      "loss: 1.8563540235871647 acc: 0.78\n",
      "loss: 1.836215639883988 acc: 0.82\n",
      "loss: 1.8527716581737643 acc: 0.83\n",
      "loss: 1.8604147446833208 acc: 0.78\n",
      "loss: 1.8568414001793019 acc: 0.78\n",
      "loss: 1.8904609810150999 acc: 0.79\n",
      "loss: 1.8122068179256283 acc: 0.88\n",
      "loss: 1.8604437276083383 acc: 0.77\n",
      "loss: 1.8463230915653506 acc: 0.82\n",
      "loss: 1.8099664139950986 acc: 0.9\n",
      "loss: 1.8462428113527922 acc: 0.82\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 1.8462\t Accuracy 0.8200\n",
      "loss: 1.838776039488974 acc: 0.82\n",
      "loss: 1.8585542411642377 acc: 0.81\n",
      "loss: 1.8575184445602257 acc: 0.76\n",
      "loss: 1.876998427471239 acc: 0.77\n",
      "loss: 1.822388336996837 acc: 0.83\n",
      "loss: 1.8407133942859266 acc: 0.82\n",
      "loss: 1.818047874592416 acc: 0.78\n",
      "loss: 1.878560397192287 acc: 0.75\n",
      "loss: 1.8218704131792613 acc: 0.76\n",
      "loss: 1.8418436117122485 acc: 0.85\n",
      "loss: 1.8278551080383985 acc: 0.88\n",
      "loss: 1.8480597293909877 acc: 0.81\n",
      "loss: 1.7920417325625329 acc: 0.93\n",
      "loss: 1.88339252990167 acc: 0.78\n",
      "loss: 1.8398290050763055 acc: 0.79\n",
      "loss: 1.827566074651539 acc: 0.81\n",
      "loss: 1.8801938983670161 acc: 0.77\n",
      "loss: 1.8404300365988828 acc: 0.83\n",
      "loss: 1.84158044465631 acc: 0.82\n",
      "loss: 1.8021991966833013 acc: 0.86\n",
      "loss: 1.8226974601290933 acc: 0.8\n",
      "loss: 1.8452560312484372 acc: 0.8\n",
      "loss: 1.833280261814684 acc: 0.82\n",
      "loss: 1.8156405326140108 acc: 0.83\n",
      "loss: 1.8427857693969285 acc: 0.82\n",
      "loss: 1.8764089776683204 acc: 0.78\n",
      "loss: 1.8552237417665953 acc: 0.84\n",
      "loss: 1.87058903054917 acc: 0.78\n",
      "loss: 1.8776227801735357 acc: 0.78\n",
      "loss: 1.8634384653538811 acc: 0.79\n",
      "loss: 1.8351066698147014 acc: 0.86\n",
      "loss: 1.8666683933987398 acc: 0.79\n",
      "loss: 1.8806403982875062 acc: 0.78\n",
      "loss: 1.8353729179973843 acc: 0.78\n",
      "loss: 1.7976967762032976 acc: 0.93\n",
      "loss: 1.8226544878770432 acc: 0.86\n",
      "loss: 1.825673224480098 acc: 0.83\n",
      "loss: 1.8366322384850438 acc: 0.84\n",
      "loss: 1.856816742619196 acc: 0.79\n",
      "loss: 1.8324168890348107 acc: 0.81\n",
      "loss: 1.8582843674182827 acc: 0.8\n",
      "loss: 1.7907287600455266 acc: 0.84\n",
      "loss: 1.7852840662820606 acc: 0.84\n",
      "loss: 1.8422346032715373 acc: 0.79\n",
      "loss: 1.8346395887770848 acc: 0.85\n",
      "loss: 1.8510887269065728 acc: 0.79\n",
      "loss: 1.8199725209688975 acc: 0.8\n",
      "loss: 1.8810639459757001 acc: 0.81\n",
      "loss: 1.85570359162187 acc: 0.8\n",
      "loss: 1.7817317736647027 acc: 0.87\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 1.7817\t Accuracy 0.8700\n",
      "loss: 1.8603456763622175 acc: 0.79\n",
      "loss: 1.8494248701946663 acc: 0.78\n",
      "loss: 1.8320445756343429 acc: 0.86\n",
      "loss: 1.836500822546595 acc: 0.8\n",
      "loss: 1.8255384405768285 acc: 0.85\n",
      "loss: 1.843253161381146 acc: 0.85\n",
      "loss: 1.8322762861775705 acc: 0.86\n",
      "loss: 1.8296526027573596 acc: 0.9\n",
      "loss: 1.8174899127709825 acc: 0.82\n",
      "loss: 1.8410367002600545 acc: 0.78\n",
      "loss: 1.8471214111186112 acc: 0.77\n",
      "loss: 1.8215895723940583 acc: 0.94\n",
      "loss: 1.8423677127923037 acc: 0.84\n",
      "loss: 1.8479844238757983 acc: 0.86\n",
      "loss: 1.8490616575673935 acc: 0.8\n",
      "loss: 1.8387115515391868 acc: 0.83\n",
      "loss: 1.8397333036908208 acc: 0.82\n",
      "loss: 1.8562182739967366 acc: 0.79\n",
      "loss: 1.8486489126173833 acc: 0.82\n",
      "loss: 1.8553148293740016 acc: 0.74\n",
      "loss: 1.819002308510942 acc: 0.85\n",
      "loss: 1.8304323987100768 acc: 0.82\n",
      "loss: 1.80460489467454 acc: 0.85\n",
      "loss: 1.8226624445265314 acc: 0.87\n",
      "loss: 1.8579231215050944 acc: 0.84\n",
      "loss: 1.8361284220981346 acc: 0.85\n",
      "loss: 1.8157012650460889 acc: 0.86\n",
      "loss: 1.8489915178432839 acc: 0.79\n",
      "loss: 1.803319716186805 acc: 0.88\n",
      "loss: 1.8345766105565888 acc: 0.84\n",
      "loss: 1.8299564372227701 acc: 0.83\n",
      "loss: 1.8352585143912408 acc: 0.77\n",
      "loss: 1.8868530480723444 acc: 0.79\n",
      "loss: 1.8402424776344042 acc: 0.85\n",
      "loss: 1.8337088935429744 acc: 0.84\n",
      "loss: 1.8289178600920857 acc: 0.83\n",
      "loss: 1.8679451489550107 acc: 0.82\n",
      "loss: 1.8454687393680178 acc: 0.85\n",
      "loss: 1.8134946119585493 acc: 0.84\n",
      "loss: 1.8247410579276389 acc: 0.85\n",
      "loss: 1.8499580577858257 acc: 0.82\n",
      "loss: 1.8780079594202714 acc: 0.81\n",
      "loss: 1.8285583741268367 acc: 0.85\n",
      "loss: 1.8391184592977634 acc: 0.84\n",
      "loss: 1.8495871330174731 acc: 0.8\n",
      "loss: 1.7989029534139374 acc: 0.87\n",
      "loss: 1.813119323787322 acc: 0.83\n",
      "loss: 1.83512751231673 acc: 0.76\n",
      "loss: 1.8339451236106432 acc: 0.81\n",
      "loss: 1.8366174598026996 acc: 0.86\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 1.8366\t Accuracy 0.8600\n",
      "loss: 1.826691365971041 acc: 0.82\n",
      "loss: 1.8536384431488009 acc: 0.82\n",
      "loss: 1.8456574836490542 acc: 0.82\n",
      "loss: 1.8171698237339833 acc: 0.84\n",
      "loss: 1.821804993115689 acc: 0.86\n",
      "loss: 1.8336514816340912 acc: 0.83\n",
      "loss: 1.7982220803836428 acc: 0.87\n",
      "loss: 1.874910709445522 acc: 0.72\n",
      "loss: 1.882636407407031 acc: 0.76\n",
      "loss: 1.8364436885173439 acc: 0.81\n",
      "loss: 1.8522609107947867 acc: 0.78\n",
      "loss: 1.8511146332337283 acc: 0.8\n",
      "loss: 1.862418694472215 acc: 0.76\n",
      "loss: 1.839374089399115 acc: 0.84\n",
      "loss: 1.8814268596448793 acc: 0.78\n",
      "loss: 1.883356491244899 acc: 0.82\n",
      "loss: 1.8350991002739945 acc: 0.85\n",
      "loss: 1.8450259729824965 acc: 0.81\n",
      "loss: 1.8178385547817983 acc: 0.86\n",
      "loss: 1.8001168315476255 acc: 0.85\n",
      "loss: 1.8186464854361954 acc: 0.84\n",
      "loss: 1.910233932510865 acc: 0.7\n",
      "loss: 1.842422965874514 acc: 0.82\n",
      "loss: 1.864822072677416 acc: 0.79\n",
      "loss: 1.8730295371940238 acc: 0.73\n",
      "loss: 1.8564896629573602 acc: 0.75\n",
      "loss: 1.8320200290877557 acc: 0.86\n",
      "loss: 1.867816300585051 acc: 0.78\n",
      "loss: 1.8384006353841542 acc: 0.84\n",
      "loss: 1.8394564402388425 acc: 0.77\n",
      "loss: 1.8838767965708376 acc: 0.77\n",
      "loss: 1.8713191235330653 acc: 0.88\n",
      "loss: 1.8195420502702147 acc: 0.87\n",
      "loss: 1.8494567955445902 acc: 0.8\n",
      "loss: 1.8392178378597936 acc: 0.86\n",
      "loss: 1.8575619542102144 acc: 0.82\n",
      "loss: 1.8464820290152582 acc: 0.82\n",
      "loss: 1.8546985116571608 acc: 0.8\n",
      "loss: 1.8187428337887872 acc: 0.82\n",
      "loss: 1.868183799253445 acc: 0.78\n",
      "loss: 1.792302030112683 acc: 0.92\n",
      "loss: 1.8812426508432145 acc: 0.79\n",
      "loss: 1.834327300918076 acc: 0.82\n",
      "loss: 1.8183856532931233 acc: 0.83\n",
      "loss: 1.860308487343012 acc: 0.83\n",
      "loss: 1.877557548913803 acc: 0.75\n",
      "loss: 1.8445593447472481 acc: 0.81\n",
      "loss: 1.8514392945993645 acc: 0.87\n",
      "loss: 1.8943174085035346 acc: 0.76\n",
      "loss: 1.8328158091115636 acc: 0.84\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 1.8328\t Accuracy 0.8400\n",
      "loss: 1.8655114809041464 acc: 0.84\n",
      "loss: 1.8384938047444113 acc: 0.84\n",
      "loss: 1.8575970385232556 acc: 0.82\n",
      "loss: 1.8856710412442705 acc: 0.78\n",
      "loss: 1.816931397991131 acc: 0.85\n",
      "loss: 1.8670726650116671 acc: 0.79\n",
      "loss: 1.8110542393816464 acc: 0.83\n",
      "loss: 1.831744189820382 acc: 0.87\n",
      "loss: 1.8118015045965783 acc: 0.85\n",
      "loss: 1.8327417415739635 acc: 0.79\n",
      "loss: 1.7980802998618781 acc: 0.87\n",
      "loss: 1.8132193887368462 acc: 0.86\n",
      "loss: 1.8186681144056704 acc: 0.86\n",
      "loss: 1.8355402059753287 acc: 0.83\n",
      "loss: 1.8575395420033274 acc: 0.81\n",
      "loss: 1.8378127223616767 acc: 0.84\n",
      "loss: 1.8203633702946322 acc: 0.81\n",
      "loss: 1.8443984203448531 acc: 0.84\n",
      "loss: 1.8509018750867967 acc: 0.76\n",
      "loss: 1.834032656492685 acc: 0.86\n",
      "loss: 1.8242887414833175 acc: 0.88\n",
      "loss: 1.8397868733621507 acc: 0.83\n",
      "loss: 1.8545727415825537 acc: 0.85\n",
      "loss: 1.8544215187784072 acc: 0.83\n",
      "loss: 1.8262182714496433 acc: 0.84\n",
      "loss: 1.8471058674426224 acc: 0.83\n",
      "loss: 1.8872320339159887 acc: 0.78\n",
      "loss: 1.8379955238861774 acc: 0.83\n",
      "loss: 1.8765542465004057 acc: 0.76\n",
      "loss: 1.8461202126207488 acc: 0.83\n",
      "loss: 1.8115179824732468 acc: 0.85\n",
      "loss: 1.7815985477678062 acc: 0.9\n",
      "loss: 1.8360016006816693 acc: 0.87\n",
      "loss: 1.863601733732053 acc: 0.84\n",
      "loss: 1.8381388397356269 acc: 0.84\n",
      "loss: 1.8530193659714596 acc: 0.79\n",
      "loss: 1.8332774015338373 acc: 0.79\n",
      "loss: 1.848929747220678 acc: 0.73\n",
      "loss: 1.8441146183924426 acc: 0.82\n",
      "loss: 1.8620293479390633 acc: 0.75\n",
      "loss: 1.854094733069379 acc: 0.82\n",
      "loss: 1.8560469111161755 acc: 0.83\n",
      "loss: 1.820841959719082 acc: 0.86\n",
      "loss: 1.8380807724527077 acc: 0.85\n",
      "loss: 1.8724889190580576 acc: 0.79\n",
      "loss: 1.8667311341179593 acc: 0.79\n",
      "loss: 1.8530548815886108 acc: 0.78\n",
      "loss: 1.858673889594818 acc: 0.74\n",
      "loss: 1.847424961135083 acc: 0.86\n",
      "loss: 1.8681115090453735 acc: 0.77\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 1.8681\t Accuracy 0.7700\n",
      "loss: 1.7930297158053445 acc: 0.88\n",
      "loss: 1.8686149968900088 acc: 0.86\n",
      "loss: 1.8080507219759425 acc: 0.84\n",
      "loss: 1.8241626602105567 acc: 0.82\n",
      "loss: 1.8191008384341114 acc: 0.84\n",
      "loss: 1.8323759764648309 acc: 0.85\n",
      "loss: 1.8786431060318194 acc: 0.72\n",
      "loss: 1.835372895813177 acc: 0.86\n",
      "loss: 1.8023907047065748 acc: 0.83\n",
      "loss: 1.864927723648147 acc: 0.82\n",
      "loss: 1.836681447916769 acc: 0.88\n",
      "loss: 1.8832712082541136 acc: 0.71\n",
      "loss: 1.8244997356274815 acc: 0.85\n",
      "loss: 1.8358563235869705 acc: 0.83\n",
      "loss: 1.8407935487611773 acc: 0.8\n",
      "loss: 1.8789822604166924 acc: 0.79\n",
      "loss: 1.8248977936491197 acc: 0.84\n",
      "loss: 1.8171099106979063 acc: 0.87\n",
      "loss: 1.8291591239897251 acc: 0.9\n",
      "loss: 1.8499615166769598 acc: 0.8\n",
      "loss: 1.829723082065177 acc: 0.84\n",
      "loss: 1.818836531660191 acc: 0.88\n",
      "loss: 1.8185277051197375 acc: 0.88\n",
      "loss: 1.855068229914732 acc: 0.78\n",
      "loss: 1.8226698932861325 acc: 0.82\n",
      "loss: 1.8221881112503027 acc: 0.84\n",
      "loss: 1.8429060280636314 acc: 0.81\n",
      "loss: 1.8026269952565923 acc: 0.86\n",
      "loss: 1.8468736829262824 acc: 0.77\n",
      "loss: 1.842554475251448 acc: 0.82\n",
      "loss: 1.8320848217782508 acc: 0.82\n",
      "loss: 1.8335661402255445 acc: 0.83\n",
      "loss: 1.8093798626261692 acc: 0.87\n",
      "loss: 1.8367368583644228 acc: 0.84\n",
      "loss: 1.8594129511619615 acc: 0.79\n",
      "loss: 1.8305923182550037 acc: 0.81\n",
      "loss: 1.8153497864495522 acc: 0.79\n",
      "loss: 1.857856518074981 acc: 0.79\n",
      "loss: 1.848474498987749 acc: 0.83\n",
      "loss: 1.851244162211988 acc: 0.8\n",
      "loss: 1.8091092283513013 acc: 0.84\n",
      "loss: 1.849203078057077 acc: 0.79\n",
      "loss: 1.872007384264551 acc: 0.81\n",
      "loss: 1.8032448487570911 acc: 0.86\n",
      "loss: 1.8277159496409363 acc: 0.86\n",
      "loss: 1.867838373475884 acc: 0.81\n",
      "loss: 1.8355798645791859 acc: 0.86\n",
      "loss: 1.8450836960257921 acc: 0.89\n",
      "loss: 1.831332348196259 acc: 0.78\n",
      "loss: 1.8203301067422677 acc: 0.83\n",
      "loss: 1.8038721976906955 acc: 0.84\n",
      "loss: 1.8122703164941374 acc: 0.85\n",
      "loss: 1.835738647722393 acc: 0.83\n",
      "loss: 1.8425692488147778 acc: 0.84\n",
      "loss: 1.8270909443398613 acc: 0.88\n",
      "loss: 1.7623549684231283 acc: 0.88\n",
      "loss: 1.7744973745166037 acc: 0.87\n",
      "loss: 1.8262215353244553 acc: 0.84\n",
      "loss: 1.769063557439468 acc: 0.92\n",
      "loss: 1.8119543967037577 acc: 0.86\n",
      "loss: 1.8003843187026682 acc: 0.84\n",
      "loss: 1.853745534892939 acc: 0.8\n",
      "loss: 1.872050070619421 acc: 0.81\n",
      "loss: 1.871561988455459 acc: 0.83\n",
      "loss: 1.811803453081192 acc: 0.9\n",
      "loss: 1.8171820047603386 acc: 0.82\n",
      "loss: 1.7743943415207954 acc: 0.89\n",
      "loss: 1.8073031493651905 acc: 0.82\n",
      "loss: 1.792153237890278 acc: 0.87\n",
      "loss: 1.8915622193821242 acc: 0.82\n",
      "loss: 1.8381374787924918 acc: 0.89\n",
      "loss: 1.8358456219750021 acc: 0.77\n",
      "loss: 1.8539972041310862 acc: 0.87\n",
      "loss: 1.8481383747548998 acc: 0.85\n",
      "loss: 1.8754158263834537 acc: 0.77\n",
      "loss: 1.867109899128712 acc: 0.75\n",
      "loss: 1.8867835202128942 acc: 0.8\n",
      "loss: 1.8536272219763301 acc: 0.86\n",
      "loss: 1.8039423047966907 acc: 0.88\n",
      "loss: 1.8260053454930334 acc: 0.88\n",
      "loss: 1.8006752689227792 acc: 0.88\n",
      "loss: 1.7600469941134926 acc: 0.91\n",
      "loss: 1.791468105050088 acc: 0.89\n",
      "loss: 1.8065505096602308 acc: 0.86\n",
      "loss: 1.8257440795208049 acc: 0.87\n",
      "loss: 1.8600789669535507 acc: 0.87\n",
      "loss: 1.8668328975061796 acc: 0.82\n",
      "loss: 1.7766304085242302 acc: 0.9\n",
      "loss: 1.6985636448673018 acc: 0.98\n",
      "loss: 1.7410728429309623 acc: 0.92\n",
      "loss: 1.7628764204119025 acc: 0.96\n",
      "loss: 1.8079184131744515 acc: 0.86\n",
      "loss: 1.8204438452908167 acc: 0.79\n",
      "loss: 1.7390865591578464 acc: 0.82\n",
      "loss: 1.8217459370655513 acc: 0.87\n",
      "loss: 1.8150535989337597 acc: 0.92\n",
      "loss: 1.9033336236661647 acc: 0.77\n",
      "loss: 1.7083936064487748 acc: 0.94\n",
      "loss: 1.891633840009871 acc: 0.84\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8401\t Average training accuracy 0.8229\n",
      "Epoch [5]\t Average validation loss 1.8173\t Average validation accuracy 0.8566\n",
      "\n",
      "loss: 1.860311961646007 acc: 0.83\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 1.8603\t Accuracy 0.8300\n",
      "loss: 1.8514936238149176 acc: 0.81\n",
      "loss: 1.8422477355589348 acc: 0.76\n",
      "loss: 1.8393775543846531 acc: 0.8\n",
      "loss: 1.8569608945648775 acc: 0.82\n",
      "loss: 1.8538405826578697 acc: 0.8\n",
      "loss: 1.8054714603703461 acc: 0.85\n",
      "loss: 1.8270023767563328 acc: 0.82\n",
      "loss: 1.8055271430287905 acc: 0.82\n",
      "loss: 1.8543163052863332 acc: 0.82\n",
      "loss: 1.797155876704217 acc: 0.91\n",
      "loss: 1.859598170947191 acc: 0.75\n",
      "loss: 1.862074807804546 acc: 0.86\n",
      "loss: 1.8746846814822056 acc: 0.75\n",
      "loss: 1.8597292548150657 acc: 0.83\n",
      "loss: 1.8476454760598038 acc: 0.82\n",
      "loss: 1.8092709313033342 acc: 0.82\n",
      "loss: 1.8530544392843453 acc: 0.76\n",
      "loss: 1.81198161791662 acc: 0.87\n",
      "loss: 1.8512914946525396 acc: 0.81\n",
      "loss: 1.8116714737231414 acc: 0.87\n",
      "loss: 1.8310434509161124 acc: 0.85\n",
      "loss: 1.842904357199819 acc: 0.85\n",
      "loss: 1.8270969882624026 acc: 0.82\n",
      "loss: 1.8616247346893704 acc: 0.78\n",
      "loss: 1.8106390976384028 acc: 0.86\n",
      "loss: 1.855704270857064 acc: 0.83\n",
      "loss: 1.8151928557340398 acc: 0.82\n",
      "loss: 1.8189504174716 acc: 0.81\n",
      "loss: 1.8458079061924328 acc: 0.84\n",
      "loss: 1.8392584583496123 acc: 0.8\n",
      "loss: 1.8480380193620634 acc: 0.83\n",
      "loss: 1.8314316918488507 acc: 0.85\n",
      "loss: 1.8148856928347126 acc: 0.79\n",
      "loss: 1.8466027643759906 acc: 0.78\n",
      "loss: 1.786788486437757 acc: 0.84\n",
      "loss: 1.8283671003347342 acc: 0.83\n",
      "loss: 1.8484355256163807 acc: 0.82\n",
      "loss: 1.8137903795246024 acc: 0.88\n",
      "loss: 1.821098801126307 acc: 0.87\n",
      "loss: 1.8320304638163782 acc: 0.78\n",
      "loss: 1.8588836567490086 acc: 0.76\n",
      "loss: 1.8441459318564803 acc: 0.84\n",
      "loss: 1.8829094058063227 acc: 0.79\n",
      "loss: 1.8666754235289453 acc: 0.79\n",
      "loss: 1.84620751584536 acc: 0.8\n",
      "loss: 1.8554359242671412 acc: 0.82\n",
      "loss: 1.8739075053398662 acc: 0.77\n",
      "loss: 1.8328183724979943 acc: 0.83\n",
      "loss: 1.802502809473958 acc: 0.86\n",
      "loss: 1.837901857337702 acc: 0.87\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 1.8379\t Accuracy 0.8700\n",
      "loss: 1.79802763587421 acc: 0.79\n",
      "loss: 1.8374729682598143 acc: 0.78\n",
      "loss: 1.815654889226972 acc: 0.86\n",
      "loss: 1.8172909187452024 acc: 0.84\n",
      "loss: 1.8272142305275478 acc: 0.84\n",
      "loss: 1.7954838933500519 acc: 0.89\n",
      "loss: 1.829510335227396 acc: 0.83\n",
      "loss: 1.813526371746662 acc: 0.81\n",
      "loss: 1.8125860941534144 acc: 0.79\n",
      "loss: 1.8158224851120435 acc: 0.86\n",
      "loss: 1.807265082714001 acc: 0.88\n",
      "loss: 1.8549522925761694 acc: 0.81\n",
      "loss: 1.8598346878879701 acc: 0.78\n",
      "loss: 1.8815633991504697 acc: 0.77\n",
      "loss: 1.8247030865089584 acc: 0.9\n",
      "loss: 1.8069824122658498 acc: 0.83\n",
      "loss: 1.802418023181208 acc: 0.86\n",
      "loss: 1.8217454725051903 acc: 0.82\n",
      "loss: 1.815397303305401 acc: 0.79\n",
      "loss: 1.8195132440247184 acc: 0.84\n",
      "loss: 1.8391123315293112 acc: 0.82\n",
      "loss: 1.8458936461758746 acc: 0.78\n",
      "loss: 1.8253739470050392 acc: 0.82\n",
      "loss: 1.853428536776664 acc: 0.76\n",
      "loss: 1.819662316754505 acc: 0.87\n",
      "loss: 1.8651484656741937 acc: 0.81\n",
      "loss: 1.8356200544863552 acc: 0.81\n",
      "loss: 1.8140538296261024 acc: 0.86\n",
      "loss: 1.8371734813006106 acc: 0.82\n",
      "loss: 1.8408604412932208 acc: 0.81\n",
      "loss: 1.8336127538019071 acc: 0.85\n",
      "loss: 1.8306589447561479 acc: 0.85\n",
      "loss: 1.8544843341037134 acc: 0.86\n",
      "loss: 1.847475823084341 acc: 0.84\n",
      "loss: 1.8169655187763813 acc: 0.88\n",
      "loss: 1.8847427334600046 acc: 0.78\n",
      "loss: 1.8138460908739884 acc: 0.89\n",
      "loss: 1.8462949222265386 acc: 0.8\n",
      "loss: 1.8254048057550234 acc: 0.88\n",
      "loss: 1.817197105097825 acc: 0.84\n",
      "loss: 1.8351825757960802 acc: 0.82\n",
      "loss: 1.8790731565978154 acc: 0.9\n",
      "loss: 1.8329990483457161 acc: 0.82\n",
      "loss: 1.8712215038975435 acc: 0.76\n",
      "loss: 1.8632525722225217 acc: 0.83\n",
      "loss: 1.8575187724387439 acc: 0.79\n",
      "loss: 1.8423628719710956 acc: 0.78\n",
      "loss: 1.8706757728082737 acc: 0.81\n",
      "loss: 1.8585712608244491 acc: 0.8\n",
      "loss: 1.8441785688925356 acc: 0.84\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 1.8442\t Accuracy 0.8400\n",
      "loss: 1.8242048513764177 acc: 0.81\n",
      "loss: 1.8078449091294215 acc: 0.8\n",
      "loss: 1.8283050688645823 acc: 0.81\n",
      "loss: 1.8856195355077046 acc: 0.77\n",
      "loss: 1.8378134549464844 acc: 0.84\n",
      "loss: 1.867143479423238 acc: 0.82\n",
      "loss: 1.8392754853431605 acc: 0.81\n",
      "loss: 1.853252847856519 acc: 0.83\n",
      "loss: 1.834609525482978 acc: 0.83\n",
      "loss: 1.8420328745971484 acc: 0.8\n",
      "loss: 1.853908654945933 acc: 0.86\n",
      "loss: 1.8710713775054237 acc: 0.77\n",
      "loss: 1.867905087298764 acc: 0.78\n",
      "loss: 1.844827245279501 acc: 0.8\n",
      "loss: 1.85811982760023 acc: 0.82\n",
      "loss: 1.8886801482924915 acc: 0.84\n",
      "loss: 1.8224442020605764 acc: 0.85\n",
      "loss: 1.8701497687048798 acc: 0.79\n",
      "loss: 1.8188778284049965 acc: 0.85\n",
      "loss: 1.8399579901129401 acc: 0.85\n",
      "loss: 1.850980762850075 acc: 0.81\n",
      "loss: 1.8332830735858343 acc: 0.82\n",
      "loss: 1.8207681435881415 acc: 0.86\n",
      "loss: 1.862808987341599 acc: 0.82\n",
      "loss: 1.7986896654837832 acc: 0.86\n",
      "loss: 1.841879501611904 acc: 0.8\n",
      "loss: 1.8813974401008406 acc: 0.81\n",
      "loss: 1.8541340324595772 acc: 0.76\n",
      "loss: 1.789435796682419 acc: 0.85\n",
      "loss: 1.8020624550075768 acc: 0.82\n",
      "loss: 1.8394234323824974 acc: 0.83\n",
      "loss: 1.8521716740443868 acc: 0.86\n",
      "loss: 1.8265579893336565 acc: 0.84\n",
      "loss: 1.787035287774105 acc: 0.88\n",
      "loss: 1.8657831374230298 acc: 0.81\n",
      "loss: 1.8473303597542972 acc: 0.84\n",
      "loss: 1.8391042616465887 acc: 0.8\n",
      "loss: 1.8440322227957544 acc: 0.84\n",
      "loss: 1.848683925677556 acc: 0.8\n",
      "loss: 1.8042281643803812 acc: 0.84\n",
      "loss: 1.8309929693670142 acc: 0.85\n",
      "loss: 1.824232907626548 acc: 0.87\n",
      "loss: 1.8437244445710015 acc: 0.86\n",
      "loss: 1.828635330236784 acc: 0.85\n",
      "loss: 1.8550553284591964 acc: 0.81\n",
      "loss: 1.873318226082203 acc: 0.78\n",
      "loss: 1.857825122466095 acc: 0.84\n",
      "loss: 1.8430884124199947 acc: 0.79\n",
      "loss: 1.8361901036924433 acc: 0.85\n",
      "loss: 1.8791406084120974 acc: 0.7\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 1.8791\t Accuracy 0.7000\n",
      "loss: 1.8517623280580167 acc: 0.85\n",
      "loss: 1.8056016463727076 acc: 0.87\n",
      "loss: 1.8575257062581738 acc: 0.79\n",
      "loss: 1.8309629404466532 acc: 0.87\n",
      "loss: 1.838324364340551 acc: 0.84\n",
      "loss: 1.84419611724904 acc: 0.8\n",
      "loss: 1.8337960184508972 acc: 0.83\n",
      "loss: 1.8182305446875628 acc: 0.88\n",
      "loss: 1.8546316487023475 acc: 0.79\n",
      "loss: 1.854999832448703 acc: 0.8\n",
      "loss: 1.868835451599795 acc: 0.79\n",
      "loss: 1.8189748554178635 acc: 0.87\n",
      "loss: 1.859959793564961 acc: 0.79\n",
      "loss: 1.8617258985870464 acc: 0.83\n",
      "loss: 1.8190949172387767 acc: 0.91\n",
      "loss: 1.8371452540925004 acc: 0.83\n",
      "loss: 1.8616477916637941 acc: 0.84\n",
      "loss: 1.862022899749326 acc: 0.81\n",
      "loss: 1.889528319044341 acc: 0.76\n",
      "loss: 1.8481079950894967 acc: 0.77\n",
      "loss: 1.803868818808744 acc: 0.86\n",
      "loss: 1.8060805638746336 acc: 0.85\n",
      "loss: 1.820283819156508 acc: 0.83\n",
      "loss: 1.901250043394451 acc: 0.75\n",
      "loss: 1.8319001666115813 acc: 0.84\n",
      "loss: 1.8229866689555507 acc: 0.84\n",
      "loss: 1.8623094392968458 acc: 0.83\n",
      "loss: 1.8220965380689769 acc: 0.86\n",
      "loss: 1.86706674726208 acc: 0.77\n",
      "loss: 1.8102506432699514 acc: 0.84\n",
      "loss: 1.8524968620269595 acc: 0.81\n",
      "loss: 1.8459983162473992 acc: 0.81\n",
      "loss: 1.8368666378548806 acc: 0.83\n",
      "loss: 1.8264232286424533 acc: 0.87\n",
      "loss: 1.8247479006698724 acc: 0.84\n",
      "loss: 1.8284715552702568 acc: 0.84\n",
      "loss: 1.8437679446921476 acc: 0.85\n",
      "loss: 1.8199599396041153 acc: 0.84\n",
      "loss: 1.848458192481308 acc: 0.83\n",
      "loss: 1.8370634788927673 acc: 0.85\n",
      "loss: 1.844871689403255 acc: 0.84\n",
      "loss: 1.8369813059424343 acc: 0.88\n",
      "loss: 1.8719296718479612 acc: 0.76\n",
      "loss: 1.8007656611911682 acc: 0.84\n",
      "loss: 1.8379943818920201 acc: 0.8\n",
      "loss: 1.846511083625521 acc: 0.82\n",
      "loss: 1.866367149778252 acc: 0.79\n",
      "loss: 1.8737041810820272 acc: 0.8\n",
      "loss: 1.8520301871961595 acc: 0.85\n",
      "loss: 1.868165470462632 acc: 0.81\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 1.8682\t Accuracy 0.8100\n",
      "loss: 1.8098884165391542 acc: 0.88\n",
      "loss: 1.8546471234996336 acc: 0.8\n",
      "loss: 1.8393581249624773 acc: 0.83\n",
      "loss: 1.856299495857718 acc: 0.77\n",
      "loss: 1.8212377924556 acc: 0.82\n",
      "loss: 1.835783564073094 acc: 0.8\n",
      "loss: 1.8516376679184066 acc: 0.8\n",
      "loss: 1.8675719306780951 acc: 0.79\n",
      "loss: 1.8786348689820285 acc: 0.76\n",
      "loss: 1.7896957167569727 acc: 0.84\n",
      "loss: 1.8120614399176893 acc: 0.85\n",
      "loss: 1.8343462092099359 acc: 0.84\n",
      "loss: 1.7977148161427658 acc: 0.85\n",
      "loss: 1.8475323153408865 acc: 0.81\n",
      "loss: 1.8094796511901665 acc: 0.88\n",
      "loss: 1.8388124639113195 acc: 0.77\n",
      "loss: 1.873488994682849 acc: 0.81\n",
      "loss: 1.8529855271811488 acc: 0.79\n",
      "loss: 1.8342175695738616 acc: 0.82\n",
      "loss: 1.8362833651320742 acc: 0.82\n",
      "loss: 1.8468575657741766 acc: 0.84\n",
      "loss: 1.8045646546091094 acc: 0.86\n",
      "loss: 1.814751664572996 acc: 0.85\n",
      "loss: 1.8313079121662221 acc: 0.84\n",
      "loss: 1.8296790638859253 acc: 0.82\n",
      "loss: 1.8405538692962218 acc: 0.84\n",
      "loss: 1.8517405665606192 acc: 0.79\n",
      "loss: 1.8808938450036934 acc: 0.74\n",
      "loss: 1.802041609050821 acc: 0.9\n",
      "loss: 1.8350986095538468 acc: 0.81\n",
      "loss: 1.859592208307931 acc: 0.79\n",
      "loss: 1.8388911203318496 acc: 0.79\n",
      "loss: 1.8887285941351737 acc: 0.74\n",
      "loss: 1.8256381741223167 acc: 0.89\n",
      "loss: 1.8317133323551136 acc: 0.81\n",
      "loss: 1.8502777883126609 acc: 0.82\n",
      "loss: 1.8668433961913415 acc: 0.85\n",
      "loss: 1.8204721438249334 acc: 0.88\n",
      "loss: 1.8453932954131338 acc: 0.86\n",
      "loss: 1.7859536428785088 acc: 0.87\n",
      "loss: 1.826736629426102 acc: 0.83\n",
      "loss: 1.81523624496327 acc: 0.85\n",
      "loss: 1.8553495325463658 acc: 0.82\n",
      "loss: 1.8498769210589214 acc: 0.84\n",
      "loss: 1.8078924111734345 acc: 0.89\n",
      "loss: 1.831947127479923 acc: 0.84\n",
      "loss: 1.8362105146960197 acc: 0.82\n",
      "loss: 1.8679321606376207 acc: 0.81\n",
      "loss: 1.8360869300494491 acc: 0.85\n",
      "loss: 1.825976150986266 acc: 0.86\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 1.8260\t Accuracy 0.8600\n",
      "loss: 1.875548912459565 acc: 0.78\n",
      "loss: 1.7972186061760071 acc: 0.87\n",
      "loss: 1.8012084528550028 acc: 0.85\n",
      "loss: 1.8533292553483571 acc: 0.83\n",
      "loss: 1.8433314501273748 acc: 0.84\n",
      "loss: 1.8375947817526352 acc: 0.82\n",
      "loss: 1.7954533079854962 acc: 0.89\n",
      "loss: 1.8699287010351762 acc: 0.8\n",
      "loss: 1.8307019410761671 acc: 0.88\n",
      "loss: 1.8699595113585368 acc: 0.74\n",
      "loss: 1.8260489608520332 acc: 0.84\n",
      "loss: 1.8827213664854596 acc: 0.76\n",
      "loss: 1.8863845696097814 acc: 0.79\n",
      "loss: 1.8228065541968883 acc: 0.86\n",
      "loss: 1.8244802021712945 acc: 0.8\n",
      "loss: 1.8453238021960068 acc: 0.87\n",
      "loss: 1.842284492221376 acc: 0.83\n",
      "loss: 1.8804392292923557 acc: 0.77\n",
      "loss: 1.8480970441804778 acc: 0.83\n",
      "loss: 1.8496255476743775 acc: 0.82\n",
      "loss: 1.8369053849249846 acc: 0.84\n",
      "loss: 1.8300940627911573 acc: 0.79\n",
      "loss: 1.8217266299850536 acc: 0.85\n",
      "loss: 1.795022533024334 acc: 0.89\n",
      "loss: 1.8276327563985495 acc: 0.81\n",
      "loss: 1.8466510438295516 acc: 0.79\n",
      "loss: 1.8482111963021515 acc: 0.79\n",
      "loss: 1.8518984012063848 acc: 0.78\n",
      "loss: 1.8120632271321144 acc: 0.86\n",
      "loss: 1.8218776226925584 acc: 0.81\n",
      "loss: 1.8679558445804767 acc: 0.8\n",
      "loss: 1.8130543500885117 acc: 0.84\n",
      "loss: 1.8222957806098687 acc: 0.81\n",
      "loss: 1.8352513900723122 acc: 0.79\n",
      "loss: 1.8456584440801966 acc: 0.83\n",
      "loss: 1.858543377619157 acc: 0.81\n",
      "loss: 1.8920193010337627 acc: 0.73\n",
      "loss: 1.8223883594906802 acc: 0.82\n",
      "loss: 1.826377309122042 acc: 0.85\n",
      "loss: 1.872207373225792 acc: 0.75\n",
      "loss: 1.8296368627185635 acc: 0.83\n",
      "loss: 1.8264893424663122 acc: 0.85\n",
      "loss: 1.822315236651204 acc: 0.78\n",
      "loss: 1.8042946914969027 acc: 0.86\n",
      "loss: 1.88573370372582 acc: 0.81\n",
      "loss: 1.8773524489000655 acc: 0.8\n",
      "loss: 1.8460552886304475 acc: 0.87\n",
      "loss: 1.8156679735026449 acc: 0.87\n",
      "loss: 1.8740565577304258 acc: 0.77\n",
      "loss: 1.8528541169160997 acc: 0.84\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 1.8529\t Accuracy 0.8400\n",
      "loss: 1.833377681292261 acc: 0.8\n",
      "loss: 1.8349778231978227 acc: 0.81\n",
      "loss: 1.8590028439914295 acc: 0.77\n",
      "loss: 1.8621531733637073 acc: 0.82\n",
      "loss: 1.8534448767824268 acc: 0.9\n",
      "loss: 1.8545348872590197 acc: 0.85\n",
      "loss: 1.8455003129458174 acc: 0.77\n",
      "loss: 1.8318289177910534 acc: 0.83\n",
      "loss: 1.8467583818976425 acc: 0.84\n",
      "loss: 1.8261590630330704 acc: 0.83\n",
      "loss: 1.8128895946590802 acc: 0.89\n",
      "loss: 1.8017716187924977 acc: 0.82\n",
      "loss: 1.8790346683509835 acc: 0.83\n",
      "loss: 1.824603108064247 acc: 0.86\n",
      "loss: 1.8212121507174532 acc: 0.79\n",
      "loss: 1.8221888538159101 acc: 0.84\n",
      "loss: 1.8199076038033308 acc: 0.86\n",
      "loss: 1.853495970470049 acc: 0.84\n",
      "loss: 1.8601873083546385 acc: 0.81\n",
      "loss: 1.8601430374689911 acc: 0.8\n",
      "loss: 1.808797358418744 acc: 0.83\n",
      "loss: 1.8029098482925086 acc: 0.88\n",
      "loss: 1.8518300232129143 acc: 0.81\n",
      "loss: 1.8305845389043163 acc: 0.83\n",
      "loss: 1.8662507446837622 acc: 0.82\n",
      "loss: 1.8454630676314223 acc: 0.79\n",
      "loss: 1.8214227399420082 acc: 0.87\n",
      "loss: 1.8347962033986824 acc: 0.83\n",
      "loss: 1.8046954676374045 acc: 0.9\n",
      "loss: 1.8393421361564348 acc: 0.81\n",
      "loss: 1.8705641703657279 acc: 0.86\n",
      "loss: 1.827291236661237 acc: 0.82\n",
      "loss: 1.7836720770635606 acc: 0.87\n",
      "loss: 1.8228784998899954 acc: 0.85\n",
      "loss: 1.8615588197615522 acc: 0.75\n",
      "loss: 1.8583694262106183 acc: 0.8\n",
      "loss: 1.8333237618601033 acc: 0.83\n",
      "loss: 1.8378164965558101 acc: 0.86\n",
      "loss: 1.8597067751359762 acc: 0.78\n",
      "loss: 1.844055277760214 acc: 0.83\n",
      "loss: 1.7825539675891593 acc: 0.84\n",
      "loss: 1.8072140807034236 acc: 0.85\n",
      "loss: 1.833326230903564 acc: 0.83\n",
      "loss: 1.8424875350593675 acc: 0.84\n",
      "loss: 1.8322557379187503 acc: 0.82\n",
      "loss: 1.8497052801181013 acc: 0.87\n",
      "loss: 1.8196749829009797 acc: 0.87\n",
      "loss: 1.8493965821416822 acc: 0.86\n",
      "loss: 1.8364880424052814 acc: 0.85\n",
      "loss: 1.7990192572126193 acc: 0.84\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 1.7990\t Accuracy 0.8400\n",
      "loss: 1.850460133178101 acc: 0.75\n",
      "loss: 1.8552945458410222 acc: 0.84\n",
      "loss: 1.8493064907738759 acc: 0.81\n",
      "loss: 1.8055862845520039 acc: 0.82\n",
      "loss: 1.8453438395487272 acc: 0.85\n",
      "loss: 1.832273115773749 acc: 0.86\n",
      "loss: 1.845638777144395 acc: 0.86\n",
      "loss: 1.8083421492615444 acc: 0.88\n",
      "loss: 1.8738240893792324 acc: 0.75\n",
      "loss: 1.8036729553260846 acc: 0.88\n",
      "loss: 1.8319809258770432 acc: 0.84\n",
      "loss: 1.8053906296404807 acc: 0.85\n",
      "loss: 1.8272760131220522 acc: 0.85\n",
      "loss: 1.8367834217813879 acc: 0.83\n",
      "loss: 1.8339407713406846 acc: 0.84\n",
      "loss: 1.8550873810864976 acc: 0.77\n",
      "loss: 1.8390919446722593 acc: 0.82\n",
      "loss: 1.8362136246397254 acc: 0.85\n",
      "loss: 1.8612434918753704 acc: 0.79\n",
      "loss: 1.8063607473635563 acc: 0.9\n",
      "loss: 1.8530463211670836 acc: 0.79\n",
      "loss: 1.8232252478339952 acc: 0.84\n",
      "loss: 1.8290209873644527 acc: 0.8\n",
      "loss: 1.7950409271664538 acc: 0.87\n",
      "loss: 1.8166857981154774 acc: 0.86\n",
      "loss: 1.8383708596127253 acc: 0.83\n",
      "loss: 1.8276709438531844 acc: 0.85\n",
      "loss: 1.848591102521478 acc: 0.77\n",
      "loss: 1.814845890981105 acc: 0.8\n",
      "loss: 1.8586864472140494 acc: 0.83\n",
      "loss: 1.8560148718777638 acc: 0.88\n",
      "loss: 1.8246107486032241 acc: 0.91\n",
      "loss: 1.8612466004226582 acc: 0.85\n",
      "loss: 1.8357290282779106 acc: 0.87\n",
      "loss: 1.8408889160299204 acc: 0.76\n",
      "loss: 1.7874681343862222 acc: 0.92\n",
      "loss: 1.833821705030829 acc: 0.85\n",
      "loss: 1.8516053105241135 acc: 0.83\n",
      "loss: 1.8289498569385523 acc: 0.77\n",
      "loss: 1.8435505019895857 acc: 0.78\n",
      "loss: 1.8429317926415114 acc: 0.8\n",
      "loss: 1.8337247346686154 acc: 0.81\n",
      "loss: 1.808769340766855 acc: 0.93\n",
      "loss: 1.8573421326408226 acc: 0.81\n",
      "loss: 1.877754072478816 acc: 0.8\n",
      "loss: 1.796492177629499 acc: 0.94\n",
      "loss: 1.837931586182073 acc: 0.89\n",
      "loss: 1.8314919572618633 acc: 0.85\n",
      "loss: 1.7849503504242565 acc: 0.92\n",
      "loss: 1.8655164145066028 acc: 0.81\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 1.8655\t Accuracy 0.8100\n",
      "loss: 1.8106271967665348 acc: 0.8\n",
      "loss: 1.8589077920490056 acc: 0.82\n",
      "loss: 1.7789430294992088 acc: 0.86\n",
      "loss: 1.8483639268037084 acc: 0.84\n",
      "loss: 1.8618800686709929 acc: 0.82\n",
      "loss: 1.7788776914392375 acc: 0.88\n",
      "loss: 1.7957486070399933 acc: 0.87\n",
      "loss: 1.826583841063067 acc: 0.84\n",
      "loss: 1.8505195795846037 acc: 0.86\n",
      "loss: 1.8691265350424597 acc: 0.76\n",
      "loss: 1.8656782238811649 acc: 0.83\n",
      "loss: 1.8650831138814299 acc: 0.78\n",
      "loss: 1.8408935898589167 acc: 0.85\n",
      "loss: 1.829542064351697 acc: 0.82\n",
      "loss: 1.836667724480697 acc: 0.85\n",
      "loss: 1.8096973963484537 acc: 0.86\n",
      "loss: 1.8261542926818448 acc: 0.86\n",
      "loss: 1.8442184298788462 acc: 0.81\n",
      "loss: 1.818747030980079 acc: 0.88\n",
      "loss: 1.881668582617242 acc: 0.77\n",
      "loss: 1.824192574726503 acc: 0.87\n",
      "loss: 1.790551591704796 acc: 0.9\n",
      "loss: 1.8459798234019933 acc: 0.85\n",
      "loss: 1.8054214480285886 acc: 0.85\n",
      "loss: 1.8837716241188922 acc: 0.81\n",
      "loss: 1.8311070043772477 acc: 0.83\n",
      "loss: 1.8333753428096076 acc: 0.86\n",
      "loss: 1.8335215407722492 acc: 0.84\n",
      "loss: 1.8235934791038784 acc: 0.86\n",
      "loss: 1.8172901203277092 acc: 0.87\n",
      "loss: 1.8124499391677673 acc: 0.88\n",
      "loss: 1.854929811134532 acc: 0.76\n",
      "loss: 1.890839388627575 acc: 0.77\n",
      "loss: 1.861214539324068 acc: 0.79\n",
      "loss: 1.8177007889276016 acc: 0.83\n",
      "loss: 1.8424270477569396 acc: 0.81\n",
      "loss: 1.8465201917625778 acc: 0.8\n",
      "loss: 1.8404702219362716 acc: 0.81\n",
      "loss: 1.7715042461950066 acc: 0.93\n",
      "loss: 1.8975543069544647 acc: 0.75\n",
      "loss: 1.835968801157216 acc: 0.86\n",
      "loss: 1.813521006834169 acc: 0.85\n",
      "loss: 1.8282778350070519 acc: 0.82\n",
      "loss: 1.844777099935837 acc: 0.84\n",
      "loss: 1.824133370249105 acc: 0.86\n",
      "loss: 1.8325635174365684 acc: 0.82\n",
      "loss: 1.8315350691770356 acc: 0.82\n",
      "loss: 1.8080831199314165 acc: 0.84\n",
      "loss: 1.8295770568301912 acc: 0.8\n",
      "loss: 1.8522411265273533 acc: 0.82\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 1.8522\t Accuracy 0.8200\n",
      "loss: 1.8280442151688256 acc: 0.85\n",
      "loss: 1.8295716354964202 acc: 0.8\n",
      "loss: 1.8235178838884585 acc: 0.83\n",
      "loss: 1.8281875356852015 acc: 0.83\n",
      "loss: 1.8638555210284866 acc: 0.81\n",
      "loss: 1.8011707289755006 acc: 0.86\n",
      "loss: 1.8359731765773173 acc: 0.84\n",
      "loss: 1.8557445653623426 acc: 0.82\n",
      "loss: 1.8363203249375808 acc: 0.83\n",
      "loss: 1.8540454013815377 acc: 0.87\n",
      "loss: 1.833144924853523 acc: 0.85\n",
      "loss: 1.8251105902983011 acc: 0.83\n",
      "loss: 1.8027384531925017 acc: 0.83\n",
      "loss: 1.912511902174123 acc: 0.69\n",
      "loss: 1.7876609340400607 acc: 0.87\n",
      "loss: 1.8755546186495562 acc: 0.78\n",
      "loss: 1.8043609310989235 acc: 0.86\n",
      "loss: 1.8471849076868998 acc: 0.86\n",
      "loss: 1.823744418530232 acc: 0.85\n",
      "loss: 1.8683564860390762 acc: 0.78\n",
      "loss: 1.856565269754671 acc: 0.8\n",
      "loss: 1.834594471981657 acc: 0.77\n",
      "loss: 1.8262862882493105 acc: 0.87\n",
      "loss: 1.8163052019443675 acc: 0.86\n",
      "loss: 1.8088074325252805 acc: 0.91\n",
      "loss: 1.8241166122499535 acc: 0.78\n",
      "loss: 1.85279274088493 acc: 0.81\n",
      "loss: 1.8276753574221107 acc: 0.83\n",
      "loss: 1.8482420801360024 acc: 0.89\n",
      "loss: 1.8346839089550269 acc: 0.87\n",
      "loss: 1.8400719298120642 acc: 0.85\n",
      "loss: 1.844357966203651 acc: 0.81\n",
      "loss: 1.794864528719113 acc: 0.84\n",
      "loss: 1.8569587340322509 acc: 0.8\n",
      "loss: 1.8614518912447693 acc: 0.77\n",
      "loss: 1.8583906663004859 acc: 0.76\n",
      "loss: 1.8469214498539583 acc: 0.84\n",
      "loss: 1.8245068610517166 acc: 0.87\n",
      "loss: 1.8247548049916948 acc: 0.87\n",
      "loss: 1.7920091796855586 acc: 0.91\n",
      "loss: 1.8357031073695955 acc: 0.88\n",
      "loss: 1.8574710016358735 acc: 0.8\n",
      "loss: 1.8474207748167462 acc: 0.73\n",
      "loss: 1.8329820381732846 acc: 0.87\n",
      "loss: 1.8297317317611927 acc: 0.82\n",
      "loss: 1.8319974189820616 acc: 0.82\n",
      "loss: 1.8415819989635422 acc: 0.8\n",
      "loss: 1.8142747245594704 acc: 0.81\n",
      "loss: 1.8120731105627599 acc: 0.88\n",
      "loss: 1.8388713094823688 acc: 0.85\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 1.8389\t Accuracy 0.8500\n",
      "loss: 1.8056484382617553 acc: 0.88\n",
      "loss: 1.8337130792963339 acc: 0.75\n",
      "loss: 1.8153611132582268 acc: 0.84\n",
      "loss: 1.8197763476448219 acc: 0.86\n",
      "loss: 1.831932741916473 acc: 0.77\n",
      "loss: 1.831409053801397 acc: 0.86\n",
      "loss: 1.816441983448759 acc: 0.84\n",
      "loss: 1.8170506468883856 acc: 0.8\n",
      "loss: 1.796595289934633 acc: 0.9\n",
      "loss: 1.8790468531000886 acc: 0.77\n",
      "loss: 1.8281874656407544 acc: 0.82\n",
      "loss: 1.849854937952395 acc: 0.8\n",
      "loss: 1.8397616251264872 acc: 0.78\n",
      "loss: 1.8462531672833626 acc: 0.81\n",
      "loss: 1.8703556386973943 acc: 0.79\n",
      "loss: 1.8350364952604903 acc: 0.78\n",
      "loss: 1.847344164469604 acc: 0.88\n",
      "loss: 1.8206374073329792 acc: 0.86\n",
      "loss: 1.8256722969312913 acc: 0.83\n",
      "loss: 1.8326530897009994 acc: 0.83\n",
      "loss: 1.8291663280167865 acc: 0.83\n",
      "loss: 1.8339599990837798 acc: 0.84\n",
      "loss: 1.824005264359071 acc: 0.83\n",
      "loss: 1.8045579108497745 acc: 0.87\n",
      "loss: 1.8345929009095798 acc: 0.83\n",
      "loss: 1.8619150043799946 acc: 0.82\n",
      "loss: 1.8243345155139594 acc: 0.87\n",
      "loss: 1.8028948027595704 acc: 0.87\n",
      "loss: 1.849748300743539 acc: 0.83\n",
      "loss: 1.8435657163711414 acc: 0.84\n",
      "loss: 1.829788894990749 acc: 0.82\n",
      "loss: 1.8313525254114977 acc: 0.87\n",
      "loss: 1.8468486784227023 acc: 0.82\n",
      "loss: 1.8658529135100241 acc: 0.77\n",
      "loss: 1.831317470194568 acc: 0.84\n",
      "loss: 1.8400537496961307 acc: 0.83\n",
      "loss: 1.8274821213295425 acc: 0.82\n",
      "loss: 1.8523598561260797 acc: 0.79\n",
      "loss: 1.8832990867780053 acc: 0.77\n",
      "loss: 1.889260568328005 acc: 0.75\n",
      "loss: 1.8425199062990851 acc: 0.78\n",
      "loss: 1.8436468959011356 acc: 0.77\n",
      "loss: 1.8334359324544696 acc: 0.84\n",
      "loss: 1.8135872372998418 acc: 0.81\n",
      "loss: 1.8523900143845227 acc: 0.8\n",
      "loss: 1.8508596234500305 acc: 0.83\n",
      "loss: 1.8068474599149063 acc: 0.84\n",
      "loss: 1.846445453485561 acc: 0.78\n",
      "loss: 1.8368869321194499 acc: 0.78\n",
      "loss: 1.818205807050396 acc: 0.83\n",
      "loss: 1.8021200506452908 acc: 0.83\n",
      "loss: 1.809348983770414 acc: 0.86\n",
      "loss: 1.8359152116950628 acc: 0.84\n",
      "loss: 1.8364261945072666 acc: 0.87\n",
      "loss: 1.8273482694151995 acc: 0.86\n",
      "loss: 1.7638970058385977 acc: 0.86\n",
      "loss: 1.7697690906910077 acc: 0.86\n",
      "loss: 1.8247518532259743 acc: 0.85\n",
      "loss: 1.7670421391216826 acc: 0.92\n",
      "loss: 1.8089867809621965 acc: 0.86\n",
      "loss: 1.8012573021642209 acc: 0.84\n",
      "loss: 1.8520290960409937 acc: 0.8\n",
      "loss: 1.8728731950314192 acc: 0.81\n",
      "loss: 1.8737599181334883 acc: 0.84\n",
      "loss: 1.8093023958286856 acc: 0.91\n",
      "loss: 1.8160448897595978 acc: 0.85\n",
      "loss: 1.774382441470126 acc: 0.89\n",
      "loss: 1.8074193515656591 acc: 0.83\n",
      "loss: 1.7918417684685062 acc: 0.88\n",
      "loss: 1.8913277453827726 acc: 0.83\n",
      "loss: 1.835939989441448 acc: 0.89\n",
      "loss: 1.8376993375927133 acc: 0.77\n",
      "loss: 1.8538177066046546 acc: 0.88\n",
      "loss: 1.8473207958124194 acc: 0.85\n",
      "loss: 1.8724190888067667 acc: 0.78\n",
      "loss: 1.8653746572536918 acc: 0.75\n",
      "loss: 1.8891612423344595 acc: 0.8\n",
      "loss: 1.8498181695826796 acc: 0.86\n",
      "loss: 1.7994818991753894 acc: 0.89\n",
      "loss: 1.8233825970542628 acc: 0.89\n",
      "loss: 1.8019191803695385 acc: 0.89\n",
      "loss: 1.762704685198417 acc: 0.91\n",
      "loss: 1.791953102064329 acc: 0.89\n",
      "loss: 1.8043744526088183 acc: 0.87\n",
      "loss: 1.8246707002281268 acc: 0.87\n",
      "loss: 1.8586722380297487 acc: 0.87\n",
      "loss: 1.8651408416621575 acc: 0.83\n",
      "loss: 1.7724550978014348 acc: 0.91\n",
      "loss: 1.700669999268004 acc: 0.96\n",
      "loss: 1.7416191808052601 acc: 0.94\n",
      "loss: 1.7624344534116705 acc: 0.96\n",
      "loss: 1.8086334095787944 acc: 0.85\n",
      "loss: 1.811594946000057 acc: 0.8\n",
      "loss: 1.7364286540019291 acc: 0.86\n",
      "loss: 1.8202842379172464 acc: 0.88\n",
      "loss: 1.8121445989787233 acc: 0.92\n",
      "loss: 1.903315224406832 acc: 0.77\n",
      "loss: 1.7088539305032302 acc: 0.95\n",
      "loss: 1.8875539837051736 acc: 0.84\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8374\t Average training accuracy 0.8269\n",
      "Epoch [6]\t Average validation loss 1.8161\t Average validation accuracy 0.8610\n",
      "\n",
      "loss: 1.8520031271658783 acc: 0.8\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 1.8520\t Accuracy 0.8000\n",
      "loss: 1.806991831236664 acc: 0.91\n",
      "loss: 1.7968889133452945 acc: 0.81\n",
      "loss: 1.879839347968954 acc: 0.79\n",
      "loss: 1.8592820080783488 acc: 0.8\n",
      "loss: 1.8961291689546473 acc: 0.81\n",
      "loss: 1.864678241014368 acc: 0.8\n",
      "loss: 1.8206623333824994 acc: 0.86\n",
      "loss: 1.8304151408685903 acc: 0.86\n",
      "loss: 1.8260724002956819 acc: 0.87\n",
      "loss: 1.817763489903346 acc: 0.84\n",
      "loss: 1.838814385461763 acc: 0.85\n",
      "loss: 1.8282909451765434 acc: 0.85\n",
      "loss: 1.8657670968313724 acc: 0.81\n",
      "loss: 1.853188443156185 acc: 0.78\n",
      "loss: 1.8130673669977144 acc: 0.84\n",
      "loss: 1.832821773158707 acc: 0.85\n",
      "loss: 1.8646430926013104 acc: 0.8\n",
      "loss: 1.8037939329623134 acc: 0.86\n",
      "loss: 1.8263835642357011 acc: 0.89\n",
      "loss: 1.8370383131434627 acc: 0.86\n",
      "loss: 1.8643293179614904 acc: 0.79\n",
      "loss: 1.785687995939779 acc: 0.9\n",
      "loss: 1.8625772000342733 acc: 0.79\n",
      "loss: 1.8112594945322018 acc: 0.87\n",
      "loss: 1.8137155189403698 acc: 0.84\n",
      "loss: 1.8077229947482467 acc: 0.83\n",
      "loss: 1.7891390512900265 acc: 0.91\n",
      "loss: 1.799791652646197 acc: 0.9\n",
      "loss: 1.8505612507418814 acc: 0.81\n",
      "loss: 1.8194475824207184 acc: 0.83\n",
      "loss: 1.8424394817314727 acc: 0.79\n",
      "loss: 1.808165923936285 acc: 0.84\n",
      "loss: 1.8363262959358666 acc: 0.89\n",
      "loss: 1.8012303622397143 acc: 0.89\n",
      "loss: 1.7774379313588489 acc: 0.89\n",
      "loss: 1.8412405993243655 acc: 0.85\n",
      "loss: 1.825932910773431 acc: 0.83\n",
      "loss: 1.8345445730291197 acc: 0.85\n",
      "loss: 1.8251416298129086 acc: 0.85\n",
      "loss: 1.7923355650878494 acc: 0.86\n",
      "loss: 1.8086988237858426 acc: 0.86\n",
      "loss: 1.832898931068053 acc: 0.83\n",
      "loss: 1.8447286764871074 acc: 0.86\n",
      "loss: 1.8402224153094542 acc: 0.79\n",
      "loss: 1.836796113465565 acc: 0.84\n",
      "loss: 1.825992427759752 acc: 0.83\n",
      "loss: 1.8701029362595798 acc: 0.8\n",
      "loss: 1.8194399358859965 acc: 0.85\n",
      "loss: 1.8333078993277716 acc: 0.88\n",
      "loss: 1.8169004178026917 acc: 0.84\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 1.8169\t Accuracy 0.8400\n",
      "loss: 1.8136606472254633 acc: 0.9\n",
      "loss: 1.8608173292828285 acc: 0.83\n",
      "loss: 1.833214148113157 acc: 0.86\n",
      "loss: 1.8158056627249466 acc: 0.87\n",
      "loss: 1.8421260365087635 acc: 0.79\n",
      "loss: 1.8475049738610596 acc: 0.83\n",
      "loss: 1.831309645215072 acc: 0.88\n",
      "loss: 1.8126655347970655 acc: 0.88\n",
      "loss: 1.850424537484781 acc: 0.83\n",
      "loss: 1.80039599148883 acc: 0.92\n",
      "loss: 1.8178314566075864 acc: 0.83\n",
      "loss: 1.8361333545494656 acc: 0.86\n",
      "loss: 1.8606030541969063 acc: 0.77\n",
      "loss: 1.8711203970852777 acc: 0.79\n",
      "loss: 1.8586877453757416 acc: 0.86\n",
      "loss: 1.82412364217744 acc: 0.82\n",
      "loss: 1.8184950820431351 acc: 0.84\n",
      "loss: 1.8376565147544637 acc: 0.85\n",
      "loss: 1.833778286761069 acc: 0.84\n",
      "loss: 1.8606752564439644 acc: 0.83\n",
      "loss: 1.8477141527778713 acc: 0.82\n",
      "loss: 1.8300244748762886 acc: 0.81\n",
      "loss: 1.8668067071834977 acc: 0.86\n",
      "loss: 1.8535166640628216 acc: 0.77\n",
      "loss: 1.890586355204926 acc: 0.79\n",
      "loss: 1.8258396010431353 acc: 0.83\n",
      "loss: 1.8483096408777728 acc: 0.8\n",
      "loss: 1.8739425780291503 acc: 0.77\n",
      "loss: 1.8262600671509681 acc: 0.82\n",
      "loss: 1.8372010250518798 acc: 0.84\n",
      "loss: 1.8570764161943054 acc: 0.86\n",
      "loss: 1.8284477194109754 acc: 0.83\n",
      "loss: 1.8532096187637015 acc: 0.8\n",
      "loss: 1.820826887445241 acc: 0.87\n",
      "loss: 1.836377911908338 acc: 0.83\n",
      "loss: 1.8672850565321917 acc: 0.76\n",
      "loss: 1.8226005154762133 acc: 0.81\n",
      "loss: 1.7933018508184022 acc: 0.87\n",
      "loss: 1.8556805717530889 acc: 0.85\n",
      "loss: 1.8182236154365972 acc: 0.85\n",
      "loss: 1.8357588536033205 acc: 0.79\n",
      "loss: 1.8304613974587463 acc: 0.84\n",
      "loss: 1.8239689664984422 acc: 0.84\n",
      "loss: 1.862969825636347 acc: 0.83\n",
      "loss: 1.8250102667685026 acc: 0.87\n",
      "loss: 1.842759190394386 acc: 0.85\n",
      "loss: 1.8433115246053757 acc: 0.84\n",
      "loss: 1.8440031243457242 acc: 0.84\n",
      "loss: 1.8209746668059485 acc: 0.79\n",
      "loss: 1.8268830010426362 acc: 0.84\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 1.8269\t Accuracy 0.8400\n",
      "loss: 1.8470700566708564 acc: 0.81\n",
      "loss: 1.8577336278982397 acc: 0.79\n",
      "loss: 1.831883088529003 acc: 0.8\n",
      "loss: 1.8734123209952585 acc: 0.79\n",
      "loss: 1.8081427369331828 acc: 0.82\n",
      "loss: 1.7807778621446273 acc: 0.89\n",
      "loss: 1.8665775181277164 acc: 0.8\n",
      "loss: 1.8094054074428476 acc: 0.86\n",
      "loss: 1.8660714459630097 acc: 0.8\n",
      "loss: 1.8177282928642327 acc: 0.85\n",
      "loss: 1.8367305167376529 acc: 0.84\n",
      "loss: 1.8007250726201482 acc: 0.85\n",
      "loss: 1.8178237507978487 acc: 0.88\n",
      "loss: 1.8667076114229153 acc: 0.8\n",
      "loss: 1.8501832374973362 acc: 0.89\n",
      "loss: 1.8388238542814508 acc: 0.81\n",
      "loss: 1.8522869839152338 acc: 0.82\n",
      "loss: 1.855236656590756 acc: 0.82\n",
      "loss: 1.8538555752970196 acc: 0.82\n",
      "loss: 1.8393939169361078 acc: 0.82\n",
      "loss: 1.8315922329941077 acc: 0.85\n",
      "loss: 1.8319557713639634 acc: 0.86\n",
      "loss: 1.8071084530411756 acc: 0.83\n",
      "loss: 1.843092868042327 acc: 0.88\n",
      "loss: 1.8122448986384032 acc: 0.84\n",
      "loss: 1.887111285299166 acc: 0.75\n",
      "loss: 1.8203953221023823 acc: 0.87\n",
      "loss: 1.8068140120722769 acc: 0.88\n",
      "loss: 1.8382493884069653 acc: 0.83\n",
      "loss: 1.8405272299556679 acc: 0.85\n",
      "loss: 1.8407453524043993 acc: 0.83\n",
      "loss: 1.8081752117655538 acc: 0.87\n",
      "loss: 1.8287383651878886 acc: 0.88\n",
      "loss: 1.8262237062934252 acc: 0.84\n",
      "loss: 1.8622557370670578 acc: 0.82\n",
      "loss: 1.8238679597148268 acc: 0.81\n",
      "loss: 1.8663266570267034 acc: 0.73\n",
      "loss: 1.802932375829294 acc: 0.86\n",
      "loss: 1.8424122257562856 acc: 0.83\n",
      "loss: 1.853812859867157 acc: 0.8\n",
      "loss: 1.8060200537123572 acc: 0.86\n",
      "loss: 1.8356553104673798 acc: 0.84\n",
      "loss: 1.8550860820610735 acc: 0.81\n",
      "loss: 1.8583615238469071 acc: 0.79\n",
      "loss: 1.8620909803846812 acc: 0.78\n",
      "loss: 1.8522872612621994 acc: 0.81\n",
      "loss: 1.794392958538927 acc: 0.85\n",
      "loss: 1.8451252723238516 acc: 0.8\n",
      "loss: 1.8283760535146536 acc: 0.83\n",
      "loss: 1.8450529250084389 acc: 0.81\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 1.8451\t Accuracy 0.8100\n",
      "loss: 1.8581470093629002 acc: 0.84\n",
      "loss: 1.8453903975192074 acc: 0.83\n",
      "loss: 1.8770059592654547 acc: 0.8\n",
      "loss: 1.8319931566049916 acc: 0.81\n",
      "loss: 1.8715694081128647 acc: 0.81\n",
      "loss: 1.828372325188935 acc: 0.84\n",
      "loss: 1.8851584175277907 acc: 0.78\n",
      "loss: 1.7947810355607783 acc: 0.87\n",
      "loss: 1.8392944610191677 acc: 0.85\n",
      "loss: 1.8197068349302163 acc: 0.86\n",
      "loss: 1.8423278494232267 acc: 0.78\n",
      "loss: 1.8803023523295392 acc: 0.79\n",
      "loss: 1.798427132205503 acc: 0.83\n",
      "loss: 1.8484328636131782 acc: 0.81\n",
      "loss: 1.815610915014851 acc: 0.86\n",
      "loss: 1.8196564752784619 acc: 0.87\n",
      "loss: 1.8043459313735501 acc: 0.85\n",
      "loss: 1.8522001431355541 acc: 0.85\n",
      "loss: 1.7876702756510061 acc: 0.87\n",
      "loss: 1.8279991478227817 acc: 0.82\n",
      "loss: 1.8105889899739176 acc: 0.83\n",
      "loss: 1.8216554132983063 acc: 0.83\n",
      "loss: 1.8454241643737461 acc: 0.82\n",
      "loss: 1.8492020220351648 acc: 0.88\n",
      "loss: 1.8625254584829805 acc: 0.84\n",
      "loss: 1.862508491804745 acc: 0.81\n",
      "loss: 1.8496823592399823 acc: 0.77\n",
      "loss: 1.8628417389560963 acc: 0.79\n",
      "loss: 1.8276040815940748 acc: 0.83\n",
      "loss: 1.814721787124791 acc: 0.87\n",
      "loss: 1.833663914524434 acc: 0.82\n",
      "loss: 1.8054649266772964 acc: 0.89\n",
      "loss: 1.845672596847637 acc: 0.81\n",
      "loss: 1.85561129223194 acc: 0.86\n",
      "loss: 1.8622073594156938 acc: 0.77\n",
      "loss: 1.842593295647503 acc: 0.8\n",
      "loss: 1.8273450907761253 acc: 0.84\n",
      "loss: 1.8055273757560724 acc: 0.86\n",
      "loss: 1.8287294425940264 acc: 0.85\n",
      "loss: 1.8279800278890375 acc: 0.86\n",
      "loss: 1.8766200033755494 acc: 0.76\n",
      "loss: 1.8307604762343246 acc: 0.88\n",
      "loss: 1.7913754787530374 acc: 0.83\n",
      "loss: 1.83193803918934 acc: 0.82\n",
      "loss: 1.8149204683959417 acc: 0.88\n",
      "loss: 1.8352229978931447 acc: 0.77\n",
      "loss: 1.8400778930173858 acc: 0.81\n",
      "loss: 1.846744566387744 acc: 0.86\n",
      "loss: 1.8318208494350854 acc: 0.82\n",
      "loss: 1.8288850007573865 acc: 0.83\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 1.8289\t Accuracy 0.8300\n",
      "loss: 1.862930246844234 acc: 0.81\n",
      "loss: 1.8132565954854902 acc: 0.87\n",
      "loss: 1.8119114840536648 acc: 0.85\n",
      "loss: 1.8269933452676954 acc: 0.89\n",
      "loss: 1.8241919909267856 acc: 0.81\n",
      "loss: 1.8609607155409373 acc: 0.79\n",
      "loss: 1.860824944970561 acc: 0.83\n",
      "loss: 1.8421292351328555 acc: 0.82\n",
      "loss: 1.8156800066604328 acc: 0.84\n",
      "loss: 1.845232303701774 acc: 0.8\n",
      "loss: 1.8445717235035444 acc: 0.83\n",
      "loss: 1.8002233244003394 acc: 0.86\n",
      "loss: 1.853269916995252 acc: 0.81\n",
      "loss: 1.8431031004554694 acc: 0.82\n",
      "loss: 1.8371320118594332 acc: 0.82\n",
      "loss: 1.8557869484198792 acc: 0.78\n",
      "loss: 1.8163958823494217 acc: 0.84\n",
      "loss: 1.8610753993184466 acc: 0.84\n",
      "loss: 1.8308570679839786 acc: 0.86\n",
      "loss: 1.8553608895873084 acc: 0.84\n",
      "loss: 1.8414773851856518 acc: 0.82\n",
      "loss: 1.8428447894985254 acc: 0.87\n",
      "loss: 1.8393546332582391 acc: 0.86\n",
      "loss: 1.8003988697394682 acc: 0.82\n",
      "loss: 1.8360574951181223 acc: 0.87\n",
      "loss: 1.8221882368573776 acc: 0.79\n",
      "loss: 1.7875942669832419 acc: 0.89\n",
      "loss: 1.8515972357219248 acc: 0.81\n",
      "loss: 1.8365522752602084 acc: 0.83\n",
      "loss: 1.856992430470491 acc: 0.81\n",
      "loss: 1.8720397398405972 acc: 0.78\n",
      "loss: 1.8476896011028137 acc: 0.87\n",
      "loss: 1.8147314834238661 acc: 0.85\n",
      "loss: 1.874690158542661 acc: 0.78\n",
      "loss: 1.8160597456554257 acc: 0.86\n",
      "loss: 1.8545080957192552 acc: 0.82\n",
      "loss: 1.8745657773715385 acc: 0.79\n",
      "loss: 1.8262954628055763 acc: 0.82\n",
      "loss: 1.8225202651526942 acc: 0.85\n",
      "loss: 1.8322955774023644 acc: 0.84\n",
      "loss: 1.87314611569638 acc: 0.8\n",
      "loss: 1.8430738083562914 acc: 0.83\n",
      "loss: 1.832878837836042 acc: 0.91\n",
      "loss: 1.8220755681405252 acc: 0.86\n",
      "loss: 1.8343728108042026 acc: 0.85\n",
      "loss: 1.8391841701694034 acc: 0.8\n",
      "loss: 1.8086829421653285 acc: 0.86\n",
      "loss: 1.8418651637764882 acc: 0.84\n",
      "loss: 1.8415067439625077 acc: 0.8\n",
      "loss: 1.829544825886801 acc: 0.84\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 1.8295\t Accuracy 0.8400\n",
      "loss: 1.8718101066776571 acc: 0.82\n",
      "loss: 1.8721699809948422 acc: 0.77\n",
      "loss: 1.8340810382838075 acc: 0.9\n",
      "loss: 1.8432190782895828 acc: 0.85\n",
      "loss: 1.836548648757164 acc: 0.83\n",
      "loss: 1.8261239202266843 acc: 0.83\n",
      "loss: 1.8546497658703263 acc: 0.77\n",
      "loss: 1.8149701371663678 acc: 0.85\n",
      "loss: 1.820916500156195 acc: 0.86\n",
      "loss: 1.8305313875881852 acc: 0.86\n",
      "loss: 1.861995495466256 acc: 0.8\n",
      "loss: 1.8069889589552985 acc: 0.89\n",
      "loss: 1.855646270087895 acc: 0.8\n",
      "loss: 1.828134821639974 acc: 0.83\n",
      "loss: 1.863922647266377 acc: 0.81\n",
      "loss: 1.8197580969349572 acc: 0.85\n",
      "loss: 1.811082091784482 acc: 0.85\n",
      "loss: 1.7934988519271 acc: 0.89\n",
      "loss: 1.8344625328858013 acc: 0.87\n",
      "loss: 1.8622481074739126 acc: 0.71\n",
      "loss: 1.8419440565026661 acc: 0.81\n",
      "loss: 1.8437242775240008 acc: 0.83\n",
      "loss: 1.8002051503018877 acc: 0.86\n",
      "loss: 1.8812491719906594 acc: 0.75\n",
      "loss: 1.8246729550463388 acc: 0.78\n",
      "loss: 1.8411298588321605 acc: 0.79\n",
      "loss: 1.840317808626647 acc: 0.84\n",
      "loss: 1.8714213238102855 acc: 0.82\n",
      "loss: 1.884992232840149 acc: 0.78\n",
      "loss: 1.794324231088526 acc: 0.86\n",
      "loss: 1.8455263716260342 acc: 0.79\n",
      "loss: 1.8415510412765337 acc: 0.8\n",
      "loss: 1.8200273566953253 acc: 0.86\n",
      "loss: 1.8181173373510113 acc: 0.87\n",
      "loss: 1.8079421776242057 acc: 0.83\n",
      "loss: 1.8691899189705747 acc: 0.72\n",
      "loss: 1.8312795593125026 acc: 0.77\n",
      "loss: 1.8196200257495247 acc: 0.84\n",
      "loss: 1.851454793528329 acc: 0.83\n",
      "loss: 1.8514274455109507 acc: 0.81\n",
      "loss: 1.7969619472559806 acc: 0.87\n",
      "loss: 1.806469826314499 acc: 0.83\n",
      "loss: 1.8072287137547378 acc: 0.84\n",
      "loss: 1.8234840549323061 acc: 0.83\n",
      "loss: 1.8582581007735433 acc: 0.8\n",
      "loss: 1.8157810664087415 acc: 0.84\n",
      "loss: 1.90073363817279 acc: 0.71\n",
      "loss: 1.8587358661954392 acc: 0.8\n",
      "loss: 1.8319456927807298 acc: 0.83\n",
      "loss: 1.8578849403842284 acc: 0.87\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 1.8579\t Accuracy 0.8700\n",
      "loss: 1.8467869474540384 acc: 0.82\n",
      "loss: 1.838574890180908 acc: 0.82\n",
      "loss: 1.8523819127648122 acc: 0.79\n",
      "loss: 1.8010260339578779 acc: 0.86\n",
      "loss: 1.827192956425108 acc: 0.87\n",
      "loss: 1.8426332596433952 acc: 0.8\n",
      "loss: 1.853420469172349 acc: 0.78\n",
      "loss: 1.8514347902110586 acc: 0.82\n",
      "loss: 1.8110578253409753 acc: 0.85\n",
      "loss: 1.8610312321741143 acc: 0.84\n",
      "loss: 1.863522793583971 acc: 0.79\n",
      "loss: 1.7953498433205943 acc: 0.85\n",
      "loss: 1.8415314226251622 acc: 0.8\n",
      "loss: 1.823263354213578 acc: 0.86\n",
      "loss: 1.867774928628694 acc: 0.8\n",
      "loss: 1.8316230820604955 acc: 0.91\n",
      "loss: 1.8314939898721732 acc: 0.82\n",
      "loss: 1.833217941369385 acc: 0.8\n",
      "loss: 1.844193027923263 acc: 0.84\n",
      "loss: 1.8260259331969606 acc: 0.86\n",
      "loss: 1.8402423712144165 acc: 0.77\n",
      "loss: 1.816638337115288 acc: 0.85\n",
      "loss: 1.8006521519612482 acc: 0.84\n",
      "loss: 1.8585555036512187 acc: 0.84\n",
      "loss: 1.8226031491468415 acc: 0.83\n",
      "loss: 1.8473176665241038 acc: 0.82\n",
      "loss: 1.8126602058077714 acc: 0.86\n",
      "loss: 1.8339074098448893 acc: 0.83\n",
      "loss: 1.8574497745618859 acc: 0.81\n",
      "loss: 1.7881749450293165 acc: 0.89\n",
      "loss: 1.8335896201370883 acc: 0.82\n",
      "loss: 1.8670076145830916 acc: 0.8\n",
      "loss: 1.8333252429886893 acc: 0.83\n",
      "loss: 1.8049853654847996 acc: 0.88\n",
      "loss: 1.856509355045481 acc: 0.8\n",
      "loss: 1.8429472974205847 acc: 0.79\n",
      "loss: 1.8167265497930307 acc: 0.86\n",
      "loss: 1.8848942557947488 acc: 0.79\n",
      "loss: 1.8484086814053697 acc: 0.8\n",
      "loss: 1.8512258187522896 acc: 0.78\n",
      "loss: 1.800688898706454 acc: 0.88\n",
      "loss: 1.8072366795405284 acc: 0.88\n",
      "loss: 1.8142137879905444 acc: 0.86\n",
      "loss: 1.833128600719404 acc: 0.75\n",
      "loss: 1.8474362298863505 acc: 0.78\n",
      "loss: 1.8380179900606635 acc: 0.87\n",
      "loss: 1.8118596184994848 acc: 0.89\n",
      "loss: 1.8754377527738348 acc: 0.81\n",
      "loss: 1.8338805255464317 acc: 0.87\n",
      "loss: 1.837810683499273 acc: 0.83\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 1.8378\t Accuracy 0.8300\n",
      "loss: 1.8297989355779567 acc: 0.86\n",
      "loss: 1.8331585741339742 acc: 0.85\n",
      "loss: 1.8465256923619 acc: 0.78\n",
      "loss: 1.8499597989499321 acc: 0.76\n",
      "loss: 1.8271299976257935 acc: 0.86\n",
      "loss: 1.8857676136179446 acc: 0.76\n",
      "loss: 1.8107536989389654 acc: 0.84\n",
      "loss: 1.8671098935364157 acc: 0.83\n",
      "loss: 1.8578264781681384 acc: 0.82\n",
      "loss: 1.856225981852859 acc: 0.81\n",
      "loss: 1.8258211842711436 acc: 0.83\n",
      "loss: 1.8720464947383646 acc: 0.7\n",
      "loss: 1.8418922661623012 acc: 0.83\n",
      "loss: 1.8564162115346854 acc: 0.83\n",
      "loss: 1.806050786417599 acc: 0.8\n",
      "loss: 1.843893317087313 acc: 0.76\n",
      "loss: 1.9010628639327078 acc: 0.74\n",
      "loss: 1.8391352127997278 acc: 0.84\n",
      "loss: 1.886638237078972 acc: 0.8\n",
      "loss: 1.8578018098693363 acc: 0.79\n",
      "loss: 1.8563250383434544 acc: 0.82\n",
      "loss: 1.818714284879801 acc: 0.86\n",
      "loss: 1.8152214173300691 acc: 0.88\n",
      "loss: 1.8185568816346591 acc: 0.88\n",
      "loss: 1.870768828322158 acc: 0.78\n",
      "loss: 1.8092703353803625 acc: 0.9\n",
      "loss: 1.81459459131307 acc: 0.89\n",
      "loss: 1.8549259585582554 acc: 0.82\n",
      "loss: 1.8431080473581978 acc: 0.84\n",
      "loss: 1.8355061706937044 acc: 0.83\n",
      "loss: 1.8293686688448592 acc: 0.81\n",
      "loss: 1.8504281677915577 acc: 0.78\n",
      "loss: 1.8308480936816556 acc: 0.85\n",
      "loss: 1.8138441187791299 acc: 0.85\n",
      "loss: 1.8318301214042767 acc: 0.86\n",
      "loss: 1.8324841926002478 acc: 0.83\n",
      "loss: 1.776928370062715 acc: 0.84\n",
      "loss: 1.7810175547031621 acc: 0.9\n",
      "loss: 1.8646948586336691 acc: 0.78\n",
      "loss: 1.855264506199959 acc: 0.79\n",
      "loss: 1.7927232730611533 acc: 0.88\n",
      "loss: 1.8743478301278882 acc: 0.82\n",
      "loss: 1.8436716791247902 acc: 0.81\n",
      "loss: 1.8303806850230353 acc: 0.82\n",
      "loss: 1.8459190189435901 acc: 0.86\n",
      "loss: 1.8441616985492482 acc: 0.79\n",
      "loss: 1.8444115430650034 acc: 0.86\n",
      "loss: 1.8334567241468411 acc: 0.86\n",
      "loss: 1.8224495143689763 acc: 0.84\n",
      "loss: 1.8745351840131972 acc: 0.76\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 1.8745\t Accuracy 0.7600\n",
      "loss: 1.829450740540813 acc: 0.84\n",
      "loss: 1.8141150252663534 acc: 0.86\n",
      "loss: 1.8289220966695465 acc: 0.85\n",
      "loss: 1.8209549134142133 acc: 0.85\n",
      "loss: 1.7975237194762241 acc: 0.87\n",
      "loss: 1.8256028072297699 acc: 0.84\n",
      "loss: 1.8286056901651102 acc: 0.83\n",
      "loss: 1.8030354164484743 acc: 0.88\n",
      "loss: 1.8123539342011912 acc: 0.85\n",
      "loss: 1.8117297821625737 acc: 0.86\n",
      "loss: 1.7871923327328312 acc: 0.93\n",
      "loss: 1.8253132518758044 acc: 0.9\n",
      "loss: 1.8453063748505525 acc: 0.86\n",
      "loss: 1.8580280996458836 acc: 0.86\n",
      "loss: 1.7870935844365143 acc: 0.93\n",
      "loss: 1.786046274584127 acc: 0.86\n",
      "loss: 1.847521762615577 acc: 0.81\n",
      "loss: 1.850801492357342 acc: 0.84\n",
      "loss: 1.8163853938890682 acc: 0.82\n",
      "loss: 1.8190002674182033 acc: 0.84\n",
      "loss: 1.8012110694502816 acc: 0.86\n",
      "loss: 1.8406631077327014 acc: 0.85\n",
      "loss: 1.848830639724966 acc: 0.82\n",
      "loss: 1.8128700529130168 acc: 0.81\n",
      "loss: 1.8109385441753587 acc: 0.85\n",
      "loss: 1.8170239306615432 acc: 0.85\n",
      "loss: 1.7994105267059226 acc: 0.86\n",
      "loss: 1.841184991404486 acc: 0.83\n",
      "loss: 1.80600364961869 acc: 0.82\n",
      "loss: 1.8014678189089 acc: 0.82\n",
      "loss: 1.8468925723179734 acc: 0.77\n",
      "loss: 1.8339729151392867 acc: 0.85\n",
      "loss: 1.8569951709483583 acc: 0.78\n",
      "loss: 1.8273775299198434 acc: 0.83\n",
      "loss: 1.859905739663927 acc: 0.79\n",
      "loss: 1.8354910301644958 acc: 0.83\n",
      "loss: 1.8571775300173825 acc: 0.84\n",
      "loss: 1.8418814674233612 acc: 0.78\n",
      "loss: 1.8629481265925991 acc: 0.83\n",
      "loss: 1.8604656144684335 acc: 0.78\n",
      "loss: 1.851357464226329 acc: 0.86\n",
      "loss: 1.8283008984093312 acc: 0.82\n",
      "loss: 1.8744678649078264 acc: 0.79\n",
      "loss: 1.8476697352447078 acc: 0.87\n",
      "loss: 1.8407817172891747 acc: 0.83\n",
      "loss: 1.8206843785802658 acc: 0.88\n",
      "loss: 1.8140079209200577 acc: 0.88\n",
      "loss: 1.8372891204194932 acc: 0.82\n",
      "loss: 1.85813713594179 acc: 0.85\n",
      "loss: 1.811974184003435 acc: 0.82\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 1.8120\t Accuracy 0.8200\n",
      "loss: 1.858374720731377 acc: 0.83\n",
      "loss: 1.8773120717366252 acc: 0.79\n",
      "loss: 1.8676449666304054 acc: 0.81\n",
      "loss: 1.8388889945005207 acc: 0.81\n",
      "loss: 1.8273446064966503 acc: 0.82\n",
      "loss: 1.8498637614747566 acc: 0.82\n",
      "loss: 1.8181119948030187 acc: 0.88\n",
      "loss: 1.7915204861486964 acc: 0.82\n",
      "loss: 1.852468743004649 acc: 0.82\n",
      "loss: 1.8481972025434061 acc: 0.77\n",
      "loss: 1.8473136707336733 acc: 0.78\n",
      "loss: 1.81691585404108 acc: 0.83\n",
      "loss: 1.8652306349970151 acc: 0.84\n",
      "loss: 1.8381535570271132 acc: 0.85\n",
      "loss: 1.7983646414459589 acc: 0.85\n",
      "loss: 1.8504954432128335 acc: 0.82\n",
      "loss: 1.7934655070003784 acc: 0.87\n",
      "loss: 1.831400425432621 acc: 0.82\n",
      "loss: 1.850261895647115 acc: 0.86\n",
      "loss: 1.8349338963093813 acc: 0.78\n",
      "loss: 1.8248591338502325 acc: 0.81\n",
      "loss: 1.8367783891197411 acc: 0.78\n",
      "loss: 1.8514818882933466 acc: 0.79\n",
      "loss: 1.8607351083119312 acc: 0.8\n",
      "loss: 1.8097232311559333 acc: 0.85\n",
      "loss: 1.8079627881560483 acc: 0.84\n",
      "loss: 1.8126388377945204 acc: 0.84\n",
      "loss: 1.8285316920027663 acc: 0.86\n",
      "loss: 1.8625230132817294 acc: 0.79\n",
      "loss: 1.8654922741689008 acc: 0.78\n",
      "loss: 1.7883737994996494 acc: 0.89\n",
      "loss: 1.8364754829424677 acc: 0.84\n",
      "loss: 1.8519048195192378 acc: 0.8\n",
      "loss: 1.8312323999433338 acc: 0.8\n",
      "loss: 1.8236763446677986 acc: 0.88\n",
      "loss: 1.8230136225352895 acc: 0.85\n",
      "loss: 1.8136414819193627 acc: 0.85\n",
      "loss: 1.8475358729266318 acc: 0.79\n",
      "loss: 1.8051683884451402 acc: 0.86\n",
      "loss: 1.8519400228908756 acc: 0.86\n",
      "loss: 1.8837560166917564 acc: 0.76\n",
      "loss: 1.8336645863099228 acc: 0.82\n",
      "loss: 1.8266666860234941 acc: 0.86\n",
      "loss: 1.8379384089176332 acc: 0.85\n",
      "loss: 1.8379333919764307 acc: 0.79\n",
      "loss: 1.8445234915230255 acc: 0.85\n",
      "loss: 1.8453042154907562 acc: 0.76\n",
      "loss: 1.8476828028410548 acc: 0.82\n",
      "loss: 1.8525156697438723 acc: 0.84\n",
      "loss: 1.8000220461017906 acc: 0.82\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 1.8000\t Accuracy 0.8200\n",
      "loss: 1.820186961025197 acc: 0.83\n",
      "loss: 1.8168929674238448 acc: 0.83\n",
      "loss: 1.8123325758954965 acc: 0.88\n",
      "loss: 1.8731292977388612 acc: 0.79\n",
      "loss: 1.86512928599449 acc: 0.79\n",
      "loss: 1.8010735091447296 acc: 0.84\n",
      "loss: 1.8701313884294537 acc: 0.81\n",
      "loss: 1.8473598893771663 acc: 0.84\n",
      "loss: 1.8271616010118148 acc: 0.86\n",
      "loss: 1.8277897372719443 acc: 0.8\n",
      "loss: 1.8293175987462678 acc: 0.85\n",
      "loss: 1.876559374483708 acc: 0.76\n",
      "loss: 1.799137546110229 acc: 0.86\n",
      "loss: 1.8597690499440915 acc: 0.8\n",
      "loss: 1.859450919406256 acc: 0.79\n",
      "loss: 1.8152525519919638 acc: 0.86\n",
      "loss: 1.8429467603598568 acc: 0.81\n",
      "loss: 1.808151265676289 acc: 0.81\n",
      "loss: 1.8582763670651274 acc: 0.76\n",
      "loss: 1.858941076096332 acc: 0.83\n",
      "loss: 1.8390093060556172 acc: 0.84\n",
      "loss: 1.870279390970073 acc: 0.83\n",
      "loss: 1.7973210808688294 acc: 0.9\n",
      "loss: 1.859297772649904 acc: 0.81\n",
      "loss: 1.8331427441009125 acc: 0.87\n",
      "loss: 1.8339169798899042 acc: 0.88\n",
      "loss: 1.8402184540240227 acc: 0.85\n",
      "loss: 1.8256316906361991 acc: 0.85\n",
      "loss: 1.8483030455278802 acc: 0.8\n",
      "loss: 1.823998931057517 acc: 0.84\n",
      "loss: 1.807029189635698 acc: 0.86\n",
      "loss: 1.8476507263517639 acc: 0.83\n",
      "loss: 1.8322479580173765 acc: 0.88\n",
      "loss: 1.8543647265511383 acc: 0.82\n",
      "loss: 1.792955013526711 acc: 0.86\n",
      "loss: 1.8009106262685008 acc: 0.89\n",
      "loss: 1.8335810613270467 acc: 0.88\n",
      "loss: 1.8207244597507082 acc: 0.84\n",
      "loss: 1.8303902512691281 acc: 0.83\n",
      "loss: 1.8288532429755104 acc: 0.89\n",
      "loss: 1.8344356007529177 acc: 0.83\n",
      "loss: 1.8325010314500856 acc: 0.85\n",
      "loss: 1.8591141403668172 acc: 0.76\n",
      "loss: 1.8482498022296812 acc: 0.82\n",
      "loss: 1.8396239038421875 acc: 0.78\n",
      "loss: 1.877510140574194 acc: 0.76\n",
      "loss: 1.8013868680690845 acc: 0.84\n",
      "loss: 1.8408919755085176 acc: 0.8\n",
      "loss: 1.8239938483792364 acc: 0.86\n",
      "loss: 1.8149633600808783 acc: 0.83\n",
      "loss: 1.8008002002246173 acc: 0.84\n",
      "loss: 1.8117222905559853 acc: 0.87\n",
      "loss: 1.833421147824128 acc: 0.84\n",
      "loss: 1.8351945377132923 acc: 0.9\n",
      "loss: 1.825230961259116 acc: 0.87\n",
      "loss: 1.761214584698418 acc: 0.87\n",
      "loss: 1.76864439637733 acc: 0.86\n",
      "loss: 1.821611648922438 acc: 0.87\n",
      "loss: 1.766341615320891 acc: 0.91\n",
      "loss: 1.8078400315381689 acc: 0.88\n",
      "loss: 1.8012615138560144 acc: 0.85\n",
      "loss: 1.8485101121562226 acc: 0.82\n",
      "loss: 1.8690034703424772 acc: 0.84\n",
      "loss: 1.8728363107716444 acc: 0.85\n",
      "loss: 1.806332471809266 acc: 0.91\n",
      "loss: 1.8150960925053607 acc: 0.83\n",
      "loss: 1.7740952091367206 acc: 0.9\n",
      "loss: 1.8029059655719037 acc: 0.85\n",
      "loss: 1.7930503640058324 acc: 0.88\n",
      "loss: 1.8931149474850135 acc: 0.84\n",
      "loss: 1.8322914348251735 acc: 0.92\n",
      "loss: 1.8349143519877438 acc: 0.77\n",
      "loss: 1.8520014423651532 acc: 0.88\n",
      "loss: 1.8479835766949515 acc: 0.83\n",
      "loss: 1.869113654736381 acc: 0.77\n",
      "loss: 1.8661840130052827 acc: 0.74\n",
      "loss: 1.8890594877509284 acc: 0.81\n",
      "loss: 1.847393291674283 acc: 0.87\n",
      "loss: 1.798396472053986 acc: 0.91\n",
      "loss: 1.823894914391178 acc: 0.89\n",
      "loss: 1.798755556905534 acc: 0.91\n",
      "loss: 1.7612341804588072 acc: 0.91\n",
      "loss: 1.7898363871092429 acc: 0.89\n",
      "loss: 1.8009163174004348 acc: 0.86\n",
      "loss: 1.8231858947640907 acc: 0.86\n",
      "loss: 1.8586518088983528 acc: 0.87\n",
      "loss: 1.8610934128275514 acc: 0.85\n",
      "loss: 1.7690633237934223 acc: 0.92\n",
      "loss: 1.6998046382916854 acc: 0.98\n",
      "loss: 1.740231047688069 acc: 0.93\n",
      "loss: 1.759270733184301 acc: 0.96\n",
      "loss: 1.8059604897204702 acc: 0.86\n",
      "loss: 1.8119036099550252 acc: 0.8\n",
      "loss: 1.7343492729976515 acc: 0.86\n",
      "loss: 1.8172766426861935 acc: 0.88\n",
      "loss: 1.8120589568085494 acc: 0.92\n",
      "loss: 1.900663829734807 acc: 0.77\n",
      "loss: 1.7060709612293201 acc: 0.95\n",
      "loss: 1.884428524871432 acc: 0.85\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8357\t Average training accuracy 0.8308\n",
      "Epoch [7]\t Average validation loss 1.8144\t Average validation accuracy 0.8666\n",
      "\n",
      "loss: 1.8730731657521504 acc: 0.88\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 1.8731\t Accuracy 0.8800\n",
      "loss: 1.8025520858154827 acc: 0.85\n",
      "loss: 1.8111398886178236 acc: 0.8\n",
      "loss: 1.8619495846784713 acc: 0.85\n",
      "loss: 1.8385475111737721 acc: 0.82\n",
      "loss: 1.8410043963613547 acc: 0.78\n",
      "loss: 1.8628144918876066 acc: 0.78\n",
      "loss: 1.8458811809114968 acc: 0.84\n",
      "loss: 1.8205049335913788 acc: 0.85\n",
      "loss: 1.8422071676792229 acc: 0.85\n",
      "loss: 1.835000462563738 acc: 0.84\n",
      "loss: 1.8325372610883772 acc: 0.84\n",
      "loss: 1.8491616256836598 acc: 0.77\n",
      "loss: 1.8485430751836136 acc: 0.78\n",
      "loss: 1.8482381165543438 acc: 0.83\n",
      "loss: 1.8230443141158412 acc: 0.85\n",
      "loss: 1.795516108716542 acc: 0.85\n",
      "loss: 1.8323962281800912 acc: 0.82\n",
      "loss: 1.8018519027079973 acc: 0.89\n",
      "loss: 1.873422291165801 acc: 0.81\n",
      "loss: 1.8653098629357305 acc: 0.83\n",
      "loss: 1.813807912152227 acc: 0.87\n",
      "loss: 1.8524536898317678 acc: 0.8\n",
      "loss: 1.8173899977602757 acc: 0.86\n",
      "loss: 1.821599123074245 acc: 0.8\n",
      "loss: 1.7977825354891042 acc: 0.88\n",
      "loss: 1.831580842488262 acc: 0.81\n",
      "loss: 1.8267760282454375 acc: 0.85\n",
      "loss: 1.8205317266615029 acc: 0.82\n",
      "loss: 1.8165326938324562 acc: 0.84\n",
      "loss: 1.821502334354824 acc: 0.85\n",
      "loss: 1.8063967645796593 acc: 0.91\n",
      "loss: 1.8092197681775983 acc: 0.83\n",
      "loss: 1.83543067057121 acc: 0.83\n",
      "loss: 1.8118150665010453 acc: 0.82\n",
      "loss: 1.8043910423549827 acc: 0.84\n",
      "loss: 1.8527375245152229 acc: 0.86\n",
      "loss: 1.855372205485541 acc: 0.86\n",
      "loss: 1.804465827465985 acc: 0.81\n",
      "loss: 1.8163741094046417 acc: 0.84\n",
      "loss: 1.839381311440404 acc: 0.81\n",
      "loss: 1.8580361426882641 acc: 0.78\n",
      "loss: 1.8329237441877149 acc: 0.82\n",
      "loss: 1.8313100431131792 acc: 0.85\n",
      "loss: 1.8046663639182592 acc: 0.9\n",
      "loss: 1.8687427340706668 acc: 0.75\n",
      "loss: 1.818463825747545 acc: 0.86\n",
      "loss: 1.8878525474170784 acc: 0.84\n",
      "loss: 1.8575162887977361 acc: 0.79\n",
      "loss: 1.8121100223220967 acc: 0.84\n",
      "loss: 1.8252086093896387 acc: 0.8\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 1.8252\t Accuracy 0.8000\n",
      "loss: 1.8238863086277144 acc: 0.83\n",
      "loss: 1.7895949403108902 acc: 0.88\n",
      "loss: 1.8178810345380916 acc: 0.85\n",
      "loss: 1.826799024482911 acc: 0.86\n",
      "loss: 1.8645225952614366 acc: 0.8\n",
      "loss: 1.8490061309997403 acc: 0.78\n",
      "loss: 1.8196083738281092 acc: 0.84\n",
      "loss: 1.8753724644871286 acc: 0.8\n",
      "loss: 1.8417323538092936 acc: 0.78\n",
      "loss: 1.8204539397528534 acc: 0.86\n",
      "loss: 1.818136287777272 acc: 0.81\n",
      "loss: 1.882870779742178 acc: 0.85\n",
      "loss: 1.8490510822972004 acc: 0.81\n",
      "loss: 1.7919124426454582 acc: 0.85\n",
      "loss: 1.8449683327320312 acc: 0.8\n",
      "loss: 1.839713578553038 acc: 0.84\n",
      "loss: 1.8235121682546762 acc: 0.87\n",
      "loss: 1.867671610406488 acc: 0.77\n",
      "loss: 1.8541724149472023 acc: 0.82\n",
      "loss: 1.8341584271865323 acc: 0.83\n",
      "loss: 1.784380586836955 acc: 0.89\n",
      "loss: 1.8304971434999064 acc: 0.84\n",
      "loss: 1.878283920963604 acc: 0.8\n",
      "loss: 1.8340978224053046 acc: 0.85\n",
      "loss: 1.806850696868121 acc: 0.87\n",
      "loss: 1.82210524966902 acc: 0.86\n",
      "loss: 1.8520324826670462 acc: 0.84\n",
      "loss: 1.8020859167351946 acc: 0.88\n",
      "loss: 1.8180680892372383 acc: 0.82\n",
      "loss: 1.8510104170863784 acc: 0.79\n",
      "loss: 1.832802416394365 acc: 0.86\n",
      "loss: 1.8589980482273278 acc: 0.76\n",
      "loss: 1.8117250766991089 acc: 0.89\n",
      "loss: 1.8086418869584315 acc: 0.88\n",
      "loss: 1.8528395338566164 acc: 0.82\n",
      "loss: 1.825835798218314 acc: 0.84\n",
      "loss: 1.76998460861681 acc: 0.92\n",
      "loss: 1.8516031468935616 acc: 0.78\n",
      "loss: 1.7968478582876828 acc: 0.87\n",
      "loss: 1.8337343456391748 acc: 0.81\n",
      "loss: 1.8176603746473692 acc: 0.89\n",
      "loss: 1.8027809538400092 acc: 0.85\n",
      "loss: 1.8008730708416854 acc: 0.82\n",
      "loss: 1.801306621741146 acc: 0.86\n",
      "loss: 1.861707005072145 acc: 0.78\n",
      "loss: 1.8233170147184643 acc: 0.77\n",
      "loss: 1.7925378657549422 acc: 0.87\n",
      "loss: 1.8187793498006335 acc: 0.84\n",
      "loss: 1.7936483433639108 acc: 0.84\n",
      "loss: 1.856973444295471 acc: 0.84\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 1.8570\t Accuracy 0.8400\n",
      "loss: 1.8427909199444383 acc: 0.83\n",
      "loss: 1.8115658506762926 acc: 0.82\n",
      "loss: 1.845838273946683 acc: 0.82\n",
      "loss: 1.83448551925868 acc: 0.85\n",
      "loss: 1.8366138327859338 acc: 0.82\n",
      "loss: 1.8547193969589157 acc: 0.85\n",
      "loss: 1.8082987001028528 acc: 0.81\n",
      "loss: 1.8682565748720257 acc: 0.81\n",
      "loss: 1.8339410908058347 acc: 0.86\n",
      "loss: 1.8523035679136703 acc: 0.85\n",
      "loss: 1.8410153090099701 acc: 0.82\n",
      "loss: 1.8967818319835665 acc: 0.71\n",
      "loss: 1.8402312638821028 acc: 0.83\n",
      "loss: 1.8623875913624588 acc: 0.79\n",
      "loss: 1.8408960611673197 acc: 0.86\n",
      "loss: 1.8297683889306797 acc: 0.83\n",
      "loss: 1.866829552811073 acc: 0.77\n",
      "loss: 1.830690214552601 acc: 0.85\n",
      "loss: 1.8300059389839394 acc: 0.82\n",
      "loss: 1.843617494870302 acc: 0.85\n",
      "loss: 1.8662245426172894 acc: 0.8\n",
      "loss: 1.87851328902988 acc: 0.77\n",
      "loss: 1.8294131395891453 acc: 0.83\n",
      "loss: 1.840819141943048 acc: 0.86\n",
      "loss: 1.856505598959163 acc: 0.82\n",
      "loss: 1.8068989686656354 acc: 0.84\n",
      "loss: 1.8226068591049454 acc: 0.84\n",
      "loss: 1.8752116942505979 acc: 0.76\n",
      "loss: 1.8459207145857743 acc: 0.84\n",
      "loss: 1.8482093027112163 acc: 0.8\n",
      "loss: 1.8410303001281014 acc: 0.83\n",
      "loss: 1.8004875381015197 acc: 0.88\n",
      "loss: 1.8409368466684908 acc: 0.82\n",
      "loss: 1.8388156858608773 acc: 0.87\n",
      "loss: 1.8451108816500363 acc: 0.82\n",
      "loss: 1.8377843660020312 acc: 0.86\n",
      "loss: 1.8590332580877886 acc: 0.81\n",
      "loss: 1.8510825309914298 acc: 0.81\n",
      "loss: 1.8404425554879782 acc: 0.85\n",
      "loss: 1.829962712553353 acc: 0.85\n",
      "loss: 1.8274957704226764 acc: 0.8\n",
      "loss: 1.82652858147719 acc: 0.86\n",
      "loss: 1.8433532828953887 acc: 0.85\n",
      "loss: 1.8280029329173941 acc: 0.81\n",
      "loss: 1.830041328662359 acc: 0.83\n",
      "loss: 1.844613526939154 acc: 0.83\n",
      "loss: 1.8521626053529703 acc: 0.85\n",
      "loss: 1.82826014198561 acc: 0.84\n",
      "loss: 1.854108094719996 acc: 0.76\n",
      "loss: 1.795679843326827 acc: 0.89\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 1.7957\t Accuracy 0.8900\n",
      "loss: 1.8311895871130368 acc: 0.9\n",
      "loss: 1.8351249757100725 acc: 0.86\n",
      "loss: 1.8271362381455185 acc: 0.83\n",
      "loss: 1.8203747468931055 acc: 0.88\n",
      "loss: 1.8426751919386262 acc: 0.88\n",
      "loss: 1.8564352233154995 acc: 0.79\n",
      "loss: 1.8652885543978202 acc: 0.8\n",
      "loss: 1.8442592504274182 acc: 0.79\n",
      "loss: 1.868335207946953 acc: 0.8\n",
      "loss: 1.849656965119603 acc: 0.8\n",
      "loss: 1.855311508269685 acc: 0.83\n",
      "loss: 1.8277559904348601 acc: 0.9\n",
      "loss: 1.8083385779519876 acc: 0.89\n",
      "loss: 1.8767721419716523 acc: 0.79\n",
      "loss: 1.8351406042035534 acc: 0.85\n",
      "loss: 1.8872770506162677 acc: 0.77\n",
      "loss: 1.843828397195977 acc: 0.82\n",
      "loss: 1.8241792916982171 acc: 0.86\n",
      "loss: 1.846688484567739 acc: 0.79\n",
      "loss: 1.8072665546673914 acc: 0.89\n",
      "loss: 1.8778304711402998 acc: 0.79\n",
      "loss: 1.8528822802530884 acc: 0.84\n",
      "loss: 1.8688478816969079 acc: 0.81\n",
      "loss: 1.8396566697791963 acc: 0.78\n",
      "loss: 1.8032048225224144 acc: 0.89\n",
      "loss: 1.8079101538306201 acc: 0.84\n",
      "loss: 1.8240856836216748 acc: 0.9\n",
      "loss: 1.8423712820611107 acc: 0.84\n",
      "loss: 1.8402491372768572 acc: 0.83\n",
      "loss: 1.8561536975153763 acc: 0.84\n",
      "loss: 1.8434182379931912 acc: 0.83\n",
      "loss: 1.8740316768122327 acc: 0.8\n",
      "loss: 1.8152148685979197 acc: 0.88\n",
      "loss: 1.829254712151709 acc: 0.83\n",
      "loss: 1.8326307225341478 acc: 0.88\n",
      "loss: 1.8309716190872074 acc: 0.85\n",
      "loss: 1.8152007222943911 acc: 0.87\n",
      "loss: 1.8281536252207757 acc: 0.84\n",
      "loss: 1.8557135380308056 acc: 0.75\n",
      "loss: 1.8060099146531534 acc: 0.84\n",
      "loss: 1.8426436328173899 acc: 0.82\n",
      "loss: 1.857669913982245 acc: 0.82\n",
      "loss: 1.8146851703669291 acc: 0.92\n",
      "loss: 1.8580307437762924 acc: 0.85\n",
      "loss: 1.8464794713664756 acc: 0.81\n",
      "loss: 1.8273748036735282 acc: 0.84\n",
      "loss: 1.8341600586642903 acc: 0.88\n",
      "loss: 1.802151356144472 acc: 0.84\n",
      "loss: 1.8248549823045832 acc: 0.88\n",
      "loss: 1.8534695248113173 acc: 0.81\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 1.8535\t Accuracy 0.8100\n",
      "loss: 1.8214381163799032 acc: 0.86\n",
      "loss: 1.7985035893834849 acc: 0.88\n",
      "loss: 1.795085738831595 acc: 0.92\n",
      "loss: 1.8474147395307399 acc: 0.83\n",
      "loss: 1.8597703566979218 acc: 0.84\n",
      "loss: 1.8615379308986564 acc: 0.79\n",
      "loss: 1.8285083371611366 acc: 0.85\n",
      "loss: 1.7956844564493424 acc: 0.84\n",
      "loss: 1.8110051143856538 acc: 0.78\n",
      "loss: 1.8291383604278195 acc: 0.81\n",
      "loss: 1.8242177414026162 acc: 0.8\n",
      "loss: 1.8155726639169685 acc: 0.87\n",
      "loss: 1.839837243114107 acc: 0.82\n",
      "loss: 1.8539765033239886 acc: 0.76\n",
      "loss: 1.8235155538811634 acc: 0.83\n",
      "loss: 1.824129494614496 acc: 0.87\n",
      "loss: 1.8297459829241414 acc: 0.84\n",
      "loss: 1.832023303638091 acc: 0.83\n",
      "loss: 1.8484187371069176 acc: 0.78\n",
      "loss: 1.8274816833676226 acc: 0.86\n",
      "loss: 1.8393965556146663 acc: 0.86\n",
      "loss: 1.83770172859231 acc: 0.85\n",
      "loss: 1.841813010538237 acc: 0.82\n",
      "loss: 1.8259962020337683 acc: 0.88\n",
      "loss: 1.8586088334166242 acc: 0.81\n",
      "loss: 1.8532489528922897 acc: 0.79\n",
      "loss: 1.8237623382122197 acc: 0.86\n",
      "loss: 1.8294427153470338 acc: 0.83\n",
      "loss: 1.829013710733012 acc: 0.86\n",
      "loss: 1.8482463556130282 acc: 0.84\n",
      "loss: 1.7913295499880035 acc: 0.88\n",
      "loss: 1.8181622389172565 acc: 0.87\n",
      "loss: 1.812456902054585 acc: 0.83\n",
      "loss: 1.8370215098298808 acc: 0.82\n",
      "loss: 1.8577440183822238 acc: 0.8\n",
      "loss: 1.8491853319130005 acc: 0.84\n",
      "loss: 1.812708362886648 acc: 0.88\n",
      "loss: 1.8768225736641921 acc: 0.81\n",
      "loss: 1.8420707535261795 acc: 0.88\n",
      "loss: 1.8047585867208875 acc: 0.87\n",
      "loss: 1.8182535594629845 acc: 0.86\n",
      "loss: 1.9064613681957943 acc: 0.67\n",
      "loss: 1.8175999188589167 acc: 0.84\n",
      "loss: 1.8004616629408048 acc: 0.86\n",
      "loss: 1.8605816153856471 acc: 0.79\n",
      "loss: 1.8138495571319155 acc: 0.89\n",
      "loss: 1.8095434617826416 acc: 0.87\n",
      "loss: 1.8343067758025635 acc: 0.85\n",
      "loss: 1.830111640452362 acc: 0.85\n",
      "loss: 1.840388884108955 acc: 0.87\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 1.8404\t Accuracy 0.8700\n",
      "loss: 1.8734384259939616 acc: 0.81\n",
      "loss: 1.8552795431838844 acc: 0.82\n",
      "loss: 1.808505891935533 acc: 0.84\n",
      "loss: 1.8268089452206018 acc: 0.8\n",
      "loss: 1.8272422782090338 acc: 0.87\n",
      "loss: 1.7921679098652084 acc: 0.88\n",
      "loss: 1.8632584787815016 acc: 0.8\n",
      "loss: 1.783242302894804 acc: 0.88\n",
      "loss: 1.8263838351822026 acc: 0.84\n",
      "loss: 1.8506592627686922 acc: 0.86\n",
      "loss: 1.8111036064660875 acc: 0.86\n",
      "loss: 1.8888361233111612 acc: 0.76\n",
      "loss: 1.8238113794069506 acc: 0.8\n",
      "loss: 1.847816317235245 acc: 0.82\n",
      "loss: 1.8555538493061052 acc: 0.79\n",
      "loss: 1.8278616025470742 acc: 0.8\n",
      "loss: 1.8410762797070965 acc: 0.82\n",
      "loss: 1.8240605927562639 acc: 0.83\n",
      "loss: 1.8510352436597204 acc: 0.77\n",
      "loss: 1.830588972729004 acc: 0.74\n",
      "loss: 1.8089149205748591 acc: 0.86\n",
      "loss: 1.8338672239355998 acc: 0.84\n",
      "loss: 1.856320695621495 acc: 0.81\n",
      "loss: 1.8601189494847459 acc: 0.79\n",
      "loss: 1.8375651203604997 acc: 0.76\n",
      "loss: 1.8087412027596899 acc: 0.86\n",
      "loss: 1.8145527941673376 acc: 0.87\n",
      "loss: 1.8556521731774007 acc: 0.82\n",
      "loss: 1.8417357960462664 acc: 0.82\n",
      "loss: 1.8333627540772084 acc: 0.87\n",
      "loss: 1.8364445809976353 acc: 0.88\n",
      "loss: 1.841573934283938 acc: 0.82\n",
      "loss: 1.829138361870341 acc: 0.86\n",
      "loss: 1.799088348087132 acc: 0.93\n",
      "loss: 1.8451556499636828 acc: 0.81\n",
      "loss: 1.7970765350443587 acc: 0.86\n",
      "loss: 1.8510709797133476 acc: 0.76\n",
      "loss: 1.8393971665000564 acc: 0.88\n",
      "loss: 1.8038885040855672 acc: 0.88\n",
      "loss: 1.8237315638208056 acc: 0.87\n",
      "loss: 1.855395147744008 acc: 0.82\n",
      "loss: 1.7756962418325763 acc: 0.89\n",
      "loss: 1.8188196916488917 acc: 0.88\n",
      "loss: 1.811699386923756 acc: 0.89\n",
      "loss: 1.8491883172991073 acc: 0.78\n",
      "loss: 1.8781107991251351 acc: 0.8\n",
      "loss: 1.828685561612797 acc: 0.86\n",
      "loss: 1.8237652324201397 acc: 0.82\n",
      "loss: 1.8234282904758006 acc: 0.85\n",
      "loss: 1.8467246172578757 acc: 0.8\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 1.8467\t Accuracy 0.8000\n",
      "loss: 1.8363164311197326 acc: 0.76\n",
      "loss: 1.845515567686902 acc: 0.83\n",
      "loss: 1.8646819782120108 acc: 0.81\n",
      "loss: 1.8564962240206748 acc: 0.77\n",
      "loss: 1.8323160844672386 acc: 0.83\n",
      "loss: 1.855021503898768 acc: 0.76\n",
      "loss: 1.8544222809048097 acc: 0.77\n",
      "loss: 1.8239109732618497 acc: 0.87\n",
      "loss: 1.8306207571949467 acc: 0.86\n",
      "loss: 1.861008829803352 acc: 0.82\n",
      "loss: 1.7825112579982167 acc: 0.9\n",
      "loss: 1.8732939550531962 acc: 0.77\n",
      "loss: 1.862777705514137 acc: 0.85\n",
      "loss: 1.832226917700142 acc: 0.85\n",
      "loss: 1.8730880018553089 acc: 0.79\n",
      "loss: 1.8296785763102221 acc: 0.81\n",
      "loss: 1.8088294701548606 acc: 0.85\n",
      "loss: 1.809672692485755 acc: 0.86\n",
      "loss: 1.8573317747838012 acc: 0.82\n",
      "loss: 1.8081647311958016 acc: 0.85\n",
      "loss: 1.8159010008968668 acc: 0.88\n",
      "loss: 1.8860146892781824 acc: 0.77\n",
      "loss: 1.8280809834975447 acc: 0.86\n",
      "loss: 1.8196434870917786 acc: 0.85\n",
      "loss: 1.8220593442991047 acc: 0.82\n",
      "loss: 1.8255096328388047 acc: 0.9\n",
      "loss: 1.8001532542507208 acc: 0.86\n",
      "loss: 1.8284125072074717 acc: 0.82\n",
      "loss: 1.8040088022275944 acc: 0.91\n",
      "loss: 1.820083899467756 acc: 0.84\n",
      "loss: 1.8663507681210516 acc: 0.77\n",
      "loss: 1.8012975310481023 acc: 0.83\n",
      "loss: 1.7899238472433143 acc: 0.81\n",
      "loss: 1.839652723650874 acc: 0.87\n",
      "loss: 1.845473080714758 acc: 0.83\n",
      "loss: 1.8189651940510942 acc: 0.89\n",
      "loss: 1.837218086789865 acc: 0.81\n",
      "loss: 1.8161353844344281 acc: 0.86\n",
      "loss: 1.8266077281662099 acc: 0.82\n",
      "loss: 1.8548223045805985 acc: 0.77\n",
      "loss: 1.8557578115112088 acc: 0.77\n",
      "loss: 1.8655814030599263 acc: 0.89\n",
      "loss: 1.8386631827084627 acc: 0.87\n",
      "loss: 1.847279367098297 acc: 0.83\n",
      "loss: 1.8562043969398703 acc: 0.74\n",
      "loss: 1.8784451308170826 acc: 0.8\n",
      "loss: 1.8048367714710434 acc: 0.85\n",
      "loss: 1.8158816358570364 acc: 0.87\n",
      "loss: 1.8086942770287078 acc: 0.83\n",
      "loss: 1.8728263624463406 acc: 0.81\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 1.8728\t Accuracy 0.8100\n",
      "loss: 1.7935399100057596 acc: 0.91\n",
      "loss: 1.808357273548587 acc: 0.86\n",
      "loss: 1.7982638132563995 acc: 0.87\n",
      "loss: 1.8732740824525695 acc: 0.8\n",
      "loss: 1.829982516550265 acc: 0.82\n",
      "loss: 1.8341453099793936 acc: 0.83\n",
      "loss: 1.8049647058349434 acc: 0.87\n",
      "loss: 1.8280417101143214 acc: 0.83\n",
      "loss: 1.8353237016003874 acc: 0.86\n",
      "loss: 1.831071009010549 acc: 0.83\n",
      "loss: 1.842729287831687 acc: 0.79\n",
      "loss: 1.7827418503848482 acc: 0.92\n",
      "loss: 1.862515614930661 acc: 0.78\n",
      "loss: 1.8463843263855793 acc: 0.85\n",
      "loss: 1.824583837358281 acc: 0.85\n",
      "loss: 1.814740304329413 acc: 0.82\n",
      "loss: 1.8240105730160037 acc: 0.83\n",
      "loss: 1.8786593204571118 acc: 0.79\n",
      "loss: 1.8498975245683968 acc: 0.82\n",
      "loss: 1.80701626388754 acc: 0.84\n",
      "loss: 1.8105731291696616 acc: 0.83\n",
      "loss: 1.8403485468719674 acc: 0.8\n",
      "loss: 1.77254007946952 acc: 0.92\n",
      "loss: 1.8351801250971076 acc: 0.84\n",
      "loss: 1.8122361037399353 acc: 0.87\n",
      "loss: 1.8378397921720877 acc: 0.84\n",
      "loss: 1.8183813197809666 acc: 0.87\n",
      "loss: 1.8398758626669187 acc: 0.86\n",
      "loss: 1.8242912809112755 acc: 0.86\n",
      "loss: 1.841316111565939 acc: 0.82\n",
      "loss: 1.8193671173134396 acc: 0.86\n",
      "loss: 1.8191050521352146 acc: 0.83\n",
      "loss: 1.78939854540193 acc: 0.83\n",
      "loss: 1.8333440087560453 acc: 0.75\n",
      "loss: 1.8107146865556942 acc: 0.85\n",
      "loss: 1.8262784584989502 acc: 0.82\n",
      "loss: 1.8458769411466966 acc: 0.8\n",
      "loss: 1.8421920210770453 acc: 0.84\n",
      "loss: 1.8155748363819717 acc: 0.87\n",
      "loss: 1.8242513123180666 acc: 0.84\n",
      "loss: 1.7980646731097538 acc: 0.86\n",
      "loss: 1.8435052161351084 acc: 0.83\n",
      "loss: 1.8319291813326055 acc: 0.83\n",
      "loss: 1.839900866701886 acc: 0.81\n",
      "loss: 1.8312138670717146 acc: 0.87\n",
      "loss: 1.8243815584515501 acc: 0.83\n",
      "loss: 1.8070200742042148 acc: 0.89\n",
      "loss: 1.8083774034917375 acc: 0.87\n",
      "loss: 1.785369825530057 acc: 0.9\n",
      "loss: 1.8287127469708593 acc: 0.85\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 1.8287\t Accuracy 0.8500\n",
      "loss: 1.832832266527248 acc: 0.86\n",
      "loss: 1.8539366954698655 acc: 0.78\n",
      "loss: 1.8304233890781814 acc: 0.84\n",
      "loss: 1.8429467826702677 acc: 0.77\n",
      "loss: 1.799143573413355 acc: 0.82\n",
      "loss: 1.841498311732118 acc: 0.79\n",
      "loss: 1.8144111189091419 acc: 0.82\n",
      "loss: 1.8447649331627374 acc: 0.81\n",
      "loss: 1.8017258287720261 acc: 0.89\n",
      "loss: 1.8476002275935288 acc: 0.81\n",
      "loss: 1.8351914645790859 acc: 0.78\n",
      "loss: 1.8365879104110527 acc: 0.87\n",
      "loss: 1.846585420157793 acc: 0.85\n",
      "loss: 1.822626487000193 acc: 0.85\n",
      "loss: 1.8272046786466414 acc: 0.88\n",
      "loss: 1.8310689314905972 acc: 0.86\n",
      "loss: 1.8346535125446808 acc: 0.79\n",
      "loss: 1.8465936657103694 acc: 0.82\n",
      "loss: 1.8445237431998585 acc: 0.84\n",
      "loss: 1.8178354716315739 acc: 0.86\n",
      "loss: 1.8704368646360476 acc: 0.76\n",
      "loss: 1.8255706487034518 acc: 0.85\n",
      "loss: 1.793267684436943 acc: 0.84\n",
      "loss: 1.8303808233930448 acc: 0.8\n",
      "loss: 1.835479384618898 acc: 0.9\n",
      "loss: 1.8553908042961222 acc: 0.8\n",
      "loss: 1.8400171662367675 acc: 0.83\n",
      "loss: 1.8294511504362734 acc: 0.83\n",
      "loss: 1.8603041790509431 acc: 0.84\n",
      "loss: 1.8197562417944873 acc: 0.88\n",
      "loss: 1.7984064345024158 acc: 0.9\n",
      "loss: 1.852097417677958 acc: 0.83\n",
      "loss: 1.8449058954893933 acc: 0.82\n",
      "loss: 1.8718087923151328 acc: 0.78\n",
      "loss: 1.773670097969744 acc: 0.87\n",
      "loss: 1.7767812002236016 acc: 0.9\n",
      "loss: 1.8116701339792416 acc: 0.82\n",
      "loss: 1.8579392157137082 acc: 0.84\n",
      "loss: 1.8320028978566512 acc: 0.84\n",
      "loss: 1.841396419138399 acc: 0.86\n",
      "loss: 1.8164666412463348 acc: 0.88\n",
      "loss: 1.8166869590220736 acc: 0.82\n",
      "loss: 1.8366568668750582 acc: 0.84\n",
      "loss: 1.826734347606602 acc: 0.86\n",
      "loss: 1.8362131043424723 acc: 0.83\n",
      "loss: 1.8361825682263389 acc: 0.83\n",
      "loss: 1.8398609945082278 acc: 0.83\n",
      "loss: 1.8337790395869271 acc: 0.8\n",
      "loss: 1.8492580035417407 acc: 0.85\n",
      "loss: 1.8400648095761898 acc: 0.8\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 1.8401\t Accuracy 0.8000\n",
      "loss: 1.7882413927379757 acc: 0.85\n",
      "loss: 1.7956231099157318 acc: 0.87\n",
      "loss: 1.8687919412731409 acc: 0.77\n",
      "loss: 1.854719878103758 acc: 0.76\n",
      "loss: 1.8547912492232825 acc: 0.83\n",
      "loss: 1.8283566153946225 acc: 0.87\n",
      "loss: 1.839112857370116 acc: 0.87\n",
      "loss: 1.8276792666468011 acc: 0.82\n",
      "loss: 1.879082645886255 acc: 0.79\n",
      "loss: 1.8435930178451754 acc: 0.82\n",
      "loss: 1.8702145171917308 acc: 0.79\n",
      "loss: 1.800099083955206 acc: 0.85\n",
      "loss: 1.8455969170893698 acc: 0.79\n",
      "loss: 1.8293897048292849 acc: 0.86\n",
      "loss: 1.8500502251807274 acc: 0.82\n",
      "loss: 1.8099869402373827 acc: 0.85\n",
      "loss: 1.832124697703012 acc: 0.8\n",
      "loss: 1.832783582127726 acc: 0.83\n",
      "loss: 1.8402965870550405 acc: 0.81\n",
      "loss: 1.856516112236895 acc: 0.83\n",
      "loss: 1.8412046037099206 acc: 0.85\n",
      "loss: 1.8847143987812642 acc: 0.73\n",
      "loss: 1.8527223178979018 acc: 0.85\n",
      "loss: 1.836995875449503 acc: 0.88\n",
      "loss: 1.8554977953639031 acc: 0.83\n",
      "loss: 1.8095819599109852 acc: 0.9\n",
      "loss: 1.8674048909700098 acc: 0.79\n",
      "loss: 1.8157761581070722 acc: 0.88\n",
      "loss: 1.8510098705537346 acc: 0.78\n",
      "loss: 1.8158728073341945 acc: 0.84\n",
      "loss: 1.8421568593056978 acc: 0.8\n",
      "loss: 1.8604290284749363 acc: 0.82\n",
      "loss: 1.8120843804756979 acc: 0.84\n",
      "loss: 1.8140332810445863 acc: 0.8\n",
      "loss: 1.7993254683685607 acc: 0.85\n",
      "loss: 1.8491339020049105 acc: 0.83\n",
      "loss: 1.8335228365250842 acc: 0.86\n",
      "loss: 1.83242593959426 acc: 0.86\n",
      "loss: 1.8601663185587864 acc: 0.84\n",
      "loss: 1.8487976270506608 acc: 0.82\n",
      "loss: 1.8307026038903238 acc: 0.84\n",
      "loss: 1.844597057287749 acc: 0.82\n",
      "loss: 1.8495005908473283 acc: 0.78\n",
      "loss: 1.8409694398801997 acc: 0.89\n",
      "loss: 1.8486917265369203 acc: 0.87\n",
      "loss: 1.8481014720339397 acc: 0.85\n",
      "loss: 1.8334953728529377 acc: 0.83\n",
      "loss: 1.85875133912891 acc: 0.79\n",
      "loss: 1.848203161766321 acc: 0.83\n",
      "loss: 1.8670928500262882 acc: 0.81\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 1.8671\t Accuracy 0.8100\n",
      "loss: 1.7855583461820743 acc: 0.87\n",
      "loss: 1.8012140232357032 acc: 0.86\n",
      "loss: 1.8581120408827334 acc: 0.84\n",
      "loss: 1.8607406921397966 acc: 0.81\n",
      "loss: 1.8352541068025507 acc: 0.81\n",
      "loss: 1.8611240818711576 acc: 0.83\n",
      "loss: 1.8357491768291085 acc: 0.89\n",
      "loss: 1.8164164425951776 acc: 0.81\n",
      "loss: 1.8215451974011752 acc: 0.83\n",
      "loss: 1.8272353860339248 acc: 0.81\n",
      "loss: 1.8455436412391373 acc: 0.84\n",
      "loss: 1.8384416713158058 acc: 0.87\n",
      "loss: 1.8151573525733762 acc: 0.85\n",
      "loss: 1.8437305133462127 acc: 0.88\n",
      "loss: 1.8026161263098173 acc: 0.87\n",
      "loss: 1.8377985357343223 acc: 0.82\n",
      "loss: 1.8262744894453666 acc: 0.82\n",
      "loss: 1.831799072227514 acc: 0.87\n",
      "loss: 1.8199188823187646 acc: 0.86\n",
      "loss: 1.8640677821474612 acc: 0.81\n",
      "loss: 1.8754275886947291 acc: 0.8\n",
      "loss: 1.8821387498240056 acc: 0.83\n",
      "loss: 1.8429116401286239 acc: 0.86\n",
      "loss: 1.8860877694542264 acc: 0.84\n",
      "loss: 1.8217424524499812 acc: 0.78\n",
      "loss: 1.8385771698295803 acc: 0.84\n",
      "loss: 1.8390676617829878 acc: 0.82\n",
      "loss: 1.8138148180023073 acc: 0.89\n",
      "loss: 1.8594027981964647 acc: 0.77\n",
      "loss: 1.8470227953464429 acc: 0.81\n",
      "loss: 1.8409214661503435 acc: 0.8\n",
      "loss: 1.8522571319764283 acc: 0.84\n",
      "loss: 1.8075513165213275 acc: 0.84\n",
      "loss: 1.8447815688930496 acc: 0.82\n",
      "loss: 1.8325179109499246 acc: 0.79\n",
      "loss: 1.847939697269926 acc: 0.79\n",
      "loss: 1.8589735316691232 acc: 0.81\n",
      "loss: 1.846679225790805 acc: 0.86\n",
      "loss: 1.8314748639483769 acc: 0.88\n",
      "loss: 1.8370429587627646 acc: 0.82\n",
      "loss: 1.795779804322449 acc: 0.88\n",
      "loss: 1.85958797659514 acc: 0.8\n",
      "loss: 1.8312772962601693 acc: 0.85\n",
      "loss: 1.794463398406978 acc: 0.87\n",
      "loss: 1.8047005543743178 acc: 0.83\n",
      "loss: 1.8033381821519345 acc: 0.86\n",
      "loss: 1.8442705067325569 acc: 0.76\n",
      "loss: 1.8464881450586301 acc: 0.79\n",
      "loss: 1.8241030596583736 acc: 0.82\n",
      "loss: 1.8129569374967107 acc: 0.83\n",
      "loss: 1.8020857139718498 acc: 0.84\n",
      "loss: 1.8079593558661502 acc: 0.87\n",
      "loss: 1.8339819089826117 acc: 0.83\n",
      "loss: 1.8325330701045741 acc: 0.89\n",
      "loss: 1.821982423408211 acc: 0.87\n",
      "loss: 1.7620956192270671 acc: 0.87\n",
      "loss: 1.7652567849334695 acc: 0.86\n",
      "loss: 1.821687348293903 acc: 0.86\n",
      "loss: 1.7650478733642105 acc: 0.91\n",
      "loss: 1.807584664576686 acc: 0.86\n",
      "loss: 1.7987695790336469 acc: 0.84\n",
      "loss: 1.8488991174755744 acc: 0.79\n",
      "loss: 1.8694974043244132 acc: 0.87\n",
      "loss: 1.8700670789288545 acc: 0.85\n",
      "loss: 1.8039628131286853 acc: 0.9\n",
      "loss: 1.8134079774105798 acc: 0.83\n",
      "loss: 1.768819309135427 acc: 0.89\n",
      "loss: 1.801438980284849 acc: 0.87\n",
      "loss: 1.788819461661875 acc: 0.88\n",
      "loss: 1.8876326388267757 acc: 0.84\n",
      "loss: 1.8337259279182216 acc: 0.9\n",
      "loss: 1.833478732731782 acc: 0.77\n",
      "loss: 1.8541518687711005 acc: 0.87\n",
      "loss: 1.8452622360259041 acc: 0.85\n",
      "loss: 1.8687984404606266 acc: 0.76\n",
      "loss: 1.8665350623648025 acc: 0.75\n",
      "loss: 1.8869113861153677 acc: 0.8\n",
      "loss: 1.8469919388220655 acc: 0.87\n",
      "loss: 1.799942230958965 acc: 0.89\n",
      "loss: 1.8223799682621094 acc: 0.87\n",
      "loss: 1.7993454331958796 acc: 0.91\n",
      "loss: 1.759563707180051 acc: 0.91\n",
      "loss: 1.7888196842213155 acc: 0.89\n",
      "loss: 1.7981496572140763 acc: 0.86\n",
      "loss: 1.8216604941110979 acc: 0.86\n",
      "loss: 1.8560007300953307 acc: 0.88\n",
      "loss: 1.8594906871460481 acc: 0.84\n",
      "loss: 1.7639900377761004 acc: 0.91\n",
      "loss: 1.7012292560548556 acc: 0.97\n",
      "loss: 1.742857499869374 acc: 0.94\n",
      "loss: 1.7596178872071533 acc: 0.96\n",
      "loss: 1.8062081348085648 acc: 0.86\n",
      "loss: 1.810553746606328 acc: 0.8\n",
      "loss: 1.7316039993060754 acc: 0.87\n",
      "loss: 1.818582017891263 acc: 0.88\n",
      "loss: 1.811074766112257 acc: 0.92\n",
      "loss: 1.8972723398119933 acc: 0.77\n",
      "loss: 1.7066299101007505 acc: 0.96\n",
      "loss: 1.8849211510773132 acc: 0.84\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8340\t Average training accuracy 0.8336\n",
      "Epoch [8]\t Average validation loss 1.8132\t Average validation accuracy 0.8642\n",
      "\n",
      "loss: 1.8169347170563328 acc: 0.83\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 1.8169\t Accuracy 0.8300\n",
      "loss: 1.8612554606608405 acc: 0.79\n",
      "loss: 1.8622720251827267 acc: 0.82\n",
      "loss: 1.791148197523295 acc: 0.91\n",
      "loss: 1.794616665345314 acc: 0.88\n",
      "loss: 1.833592529686753 acc: 0.81\n",
      "loss: 1.8494357634900687 acc: 0.83\n",
      "loss: 1.8460683253794778 acc: 0.78\n",
      "loss: 1.8345943610751012 acc: 0.79\n",
      "loss: 1.8527427569618897 acc: 0.78\n",
      "loss: 1.84786293408407 acc: 0.83\n",
      "loss: 1.8400140437272343 acc: 0.87\n",
      "loss: 1.8222582790922675 acc: 0.83\n",
      "loss: 1.7992476394557293 acc: 0.85\n",
      "loss: 1.8561426022677394 acc: 0.84\n",
      "loss: 1.828340692475312 acc: 0.83\n",
      "loss: 1.7951761820250653 acc: 0.9\n",
      "loss: 1.8275651787656508 acc: 0.82\n",
      "loss: 1.86363954438076 acc: 0.83\n",
      "loss: 1.8447638269837705 acc: 0.81\n",
      "loss: 1.7937241148207859 acc: 0.87\n",
      "loss: 1.8686443925992742 acc: 0.78\n",
      "loss: 1.8168910683396362 acc: 0.87\n",
      "loss: 1.8549999262626904 acc: 0.84\n",
      "loss: 1.8208864763257904 acc: 0.82\n",
      "loss: 1.863163162430222 acc: 0.82\n",
      "loss: 1.8183920054340876 acc: 0.87\n",
      "loss: 1.8146085812481325 acc: 0.84\n",
      "loss: 1.8518553482030216 acc: 0.82\n",
      "loss: 1.8314483663442598 acc: 0.85\n",
      "loss: 1.851827638177768 acc: 0.82\n",
      "loss: 1.8325721515456173 acc: 0.85\n",
      "loss: 1.8787731319436884 acc: 0.78\n",
      "loss: 1.8188741590825677 acc: 0.86\n",
      "loss: 1.7692098337616462 acc: 0.9\n",
      "loss: 1.825395040571334 acc: 0.88\n",
      "loss: 1.8553995352439268 acc: 0.84\n",
      "loss: 1.8323079991953513 acc: 0.86\n",
      "loss: 1.795200337448683 acc: 0.89\n",
      "loss: 1.8445503937550434 acc: 0.86\n",
      "loss: 1.8504027629356044 acc: 0.83\n",
      "loss: 1.8069291424104987 acc: 0.85\n",
      "loss: 1.7884928691467439 acc: 0.85\n",
      "loss: 1.8787283026084052 acc: 0.75\n",
      "loss: 1.780853668487423 acc: 0.91\n",
      "loss: 1.8294105858248282 acc: 0.79\n",
      "loss: 1.8578266696173207 acc: 0.82\n",
      "loss: 1.840781419362163 acc: 0.84\n",
      "loss: 1.8541459664593136 acc: 0.83\n",
      "loss: 1.8661351755318143 acc: 0.79\n",
      "loss: 1.8056557637477297 acc: 0.85\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 1.8057\t Accuracy 0.8500\n",
      "loss: 1.8181499710221098 acc: 0.81\n",
      "loss: 1.8382654367542246 acc: 0.82\n",
      "loss: 1.8283540859936198 acc: 0.86\n",
      "loss: 1.8300389511118436 acc: 0.84\n",
      "loss: 1.797572212460708 acc: 0.84\n",
      "loss: 1.7858961619492209 acc: 0.86\n",
      "loss: 1.8399928498280342 acc: 0.82\n",
      "loss: 1.8420738473644283 acc: 0.83\n",
      "loss: 1.7989898606990615 acc: 0.88\n",
      "loss: 1.851967712079489 acc: 0.8\n",
      "loss: 1.8349538096907343 acc: 0.79\n",
      "loss: 1.788649242663048 acc: 0.88\n",
      "loss: 1.856929118971258 acc: 0.8\n",
      "loss: 1.8675574305970903 acc: 0.79\n",
      "loss: 1.830309353253286 acc: 0.8\n",
      "loss: 1.8683435470573655 acc: 0.82\n",
      "loss: 1.8503161364491894 acc: 0.84\n",
      "loss: 1.85828220183202 acc: 0.83\n",
      "loss: 1.8181364583348663 acc: 0.89\n",
      "loss: 1.812864024334698 acc: 0.9\n",
      "loss: 1.8235558251326902 acc: 0.83\n",
      "loss: 1.8253816670214331 acc: 0.87\n",
      "loss: 1.8388326507386725 acc: 0.83\n",
      "loss: 1.8639544592502852 acc: 0.82\n",
      "loss: 1.8569022069743466 acc: 0.78\n",
      "loss: 1.8636968125843238 acc: 0.83\n",
      "loss: 1.8295518457927138 acc: 0.84\n",
      "loss: 1.8207263994414475 acc: 0.8\n",
      "loss: 1.8086077786396217 acc: 0.9\n",
      "loss: 1.8622604664675044 acc: 0.8\n",
      "loss: 1.815773558829944 acc: 0.82\n",
      "loss: 1.8387529191026468 acc: 0.83\n",
      "loss: 1.8192728897968127 acc: 0.84\n",
      "loss: 1.7807008396768609 acc: 0.88\n",
      "loss: 1.8790240792739719 acc: 0.82\n",
      "loss: 1.8425445680093215 acc: 0.79\n",
      "loss: 1.8210155507794044 acc: 0.84\n",
      "loss: 1.8386522438730015 acc: 0.85\n",
      "loss: 1.8465390573723726 acc: 0.83\n",
      "loss: 1.8128988753382693 acc: 0.88\n",
      "loss: 1.8393358812884475 acc: 0.84\n",
      "loss: 1.8628821767209864 acc: 0.8\n",
      "loss: 1.8100592321963491 acc: 0.87\n",
      "loss: 1.8749429896327066 acc: 0.74\n",
      "loss: 1.8147802255507133 acc: 0.86\n",
      "loss: 1.842540602882165 acc: 0.81\n",
      "loss: 1.808853192334018 acc: 0.81\n",
      "loss: 1.844972197005575 acc: 0.82\n",
      "loss: 1.787593010810801 acc: 0.85\n",
      "loss: 1.8249102087173896 acc: 0.82\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 1.8249\t Accuracy 0.8200\n",
      "loss: 1.8345933463948767 acc: 0.84\n",
      "loss: 1.8525792370686875 acc: 0.81\n",
      "loss: 1.8281740039036862 acc: 0.84\n",
      "loss: 1.8233756884054595 acc: 0.85\n",
      "loss: 1.8015334503166607 acc: 0.91\n",
      "loss: 1.8408991549981217 acc: 0.85\n",
      "loss: 1.8275159659218958 acc: 0.83\n",
      "loss: 1.840866801585917 acc: 0.83\n",
      "loss: 1.8770321895496187 acc: 0.77\n",
      "loss: 1.8284589867552585 acc: 0.81\n",
      "loss: 1.8695110489068727 acc: 0.83\n",
      "loss: 1.8413690120894304 acc: 0.79\n",
      "loss: 1.7991599952908635 acc: 0.86\n",
      "loss: 1.8201703156804654 acc: 0.84\n",
      "loss: 1.8443381081991836 acc: 0.84\n",
      "loss: 1.8533910792452166 acc: 0.84\n",
      "loss: 1.8398965478596188 acc: 0.83\n",
      "loss: 1.8336474416298125 acc: 0.84\n",
      "loss: 1.8000206244951296 acc: 0.89\n",
      "loss: 1.8418007434025836 acc: 0.83\n",
      "loss: 1.8689884256778109 acc: 0.76\n",
      "loss: 1.8587722716553108 acc: 0.82\n",
      "loss: 1.8004764448953494 acc: 0.88\n",
      "loss: 1.8208327916933282 acc: 0.84\n",
      "loss: 1.8483852817694597 acc: 0.81\n",
      "loss: 1.7961845591813042 acc: 0.89\n",
      "loss: 1.8242764246456833 acc: 0.83\n",
      "loss: 1.7809218667522029 acc: 0.9\n",
      "loss: 1.8106668097862644 acc: 0.91\n",
      "loss: 1.7889587401608131 acc: 0.8\n",
      "loss: 1.8339445409338018 acc: 0.8\n",
      "loss: 1.8213126476560675 acc: 0.85\n",
      "loss: 1.8059550685559373 acc: 0.88\n",
      "loss: 1.8112751194730083 acc: 0.88\n",
      "loss: 1.8293128271829429 acc: 0.92\n",
      "loss: 1.861185463391449 acc: 0.77\n",
      "loss: 1.8242109128817783 acc: 0.87\n",
      "loss: 1.760236863731116 acc: 0.91\n",
      "loss: 1.8029541528621096 acc: 0.89\n",
      "loss: 1.8249373707746515 acc: 0.83\n",
      "loss: 1.8730345346560588 acc: 0.81\n",
      "loss: 1.8342022548645363 acc: 0.84\n",
      "loss: 1.7755204241354527 acc: 0.85\n",
      "loss: 1.8299750511519692 acc: 0.85\n",
      "loss: 1.8540505245476175 acc: 0.79\n",
      "loss: 1.8463589869866908 acc: 0.85\n",
      "loss: 1.8789404567006145 acc: 0.81\n",
      "loss: 1.8367938295173636 acc: 0.83\n",
      "loss: 1.7997741885710352 acc: 0.88\n",
      "loss: 1.8635151241687726 acc: 0.75\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 1.8635\t Accuracy 0.7500\n",
      "loss: 1.8425680756322425 acc: 0.86\n",
      "loss: 1.850819474423431 acc: 0.85\n",
      "loss: 1.831389663979055 acc: 0.82\n",
      "loss: 1.8397312157780126 acc: 0.85\n",
      "loss: 1.8230898602237027 acc: 0.84\n",
      "loss: 1.8563450454426242 acc: 0.81\n",
      "loss: 1.8368111169453574 acc: 0.78\n",
      "loss: 1.8458815760668756 acc: 0.89\n",
      "loss: 1.8243023223429273 acc: 0.85\n",
      "loss: 1.833730543248121 acc: 0.81\n",
      "loss: 1.829161594291611 acc: 0.85\n",
      "loss: 1.8428438050118296 acc: 0.82\n",
      "loss: 1.8880902553085246 acc: 0.78\n",
      "loss: 1.8272733544376456 acc: 0.88\n",
      "loss: 1.809770955084353 acc: 0.82\n",
      "loss: 1.799361203317448 acc: 0.86\n",
      "loss: 1.8442811997742599 acc: 0.84\n",
      "loss: 1.8371394375135335 acc: 0.84\n",
      "loss: 1.814821564228609 acc: 0.84\n",
      "loss: 1.8335307467845068 acc: 0.82\n",
      "loss: 1.8481619311738122 acc: 0.82\n",
      "loss: 1.8292782342327012 acc: 0.85\n",
      "loss: 1.824561690498136 acc: 0.84\n",
      "loss: 1.821139919539918 acc: 0.86\n",
      "loss: 1.8093322903016014 acc: 0.86\n",
      "loss: 1.854721410110262 acc: 0.79\n",
      "loss: 1.8504286237040841 acc: 0.85\n",
      "loss: 1.8272934551340752 acc: 0.88\n",
      "loss: 1.8314322191149457 acc: 0.8\n",
      "loss: 1.8668820758087565 acc: 0.83\n",
      "loss: 1.862733633115273 acc: 0.88\n",
      "loss: 1.7824119612434492 acc: 0.89\n",
      "loss: 1.8339923333245958 acc: 0.86\n",
      "loss: 1.8276043941116205 acc: 0.76\n",
      "loss: 1.8331291375583425 acc: 0.8\n",
      "loss: 1.8360080527488085 acc: 0.82\n",
      "loss: 1.848702692180918 acc: 0.83\n",
      "loss: 1.865863322481292 acc: 0.81\n",
      "loss: 1.843001609734595 acc: 0.81\n",
      "loss: 1.8431801085768784 acc: 0.82\n",
      "loss: 1.8535904808842438 acc: 0.79\n",
      "loss: 1.8341020943855997 acc: 0.81\n",
      "loss: 1.8246910576114104 acc: 0.84\n",
      "loss: 1.8077158643672777 acc: 0.85\n",
      "loss: 1.795693191106623 acc: 0.91\n",
      "loss: 1.8420360134005411 acc: 0.84\n",
      "loss: 1.8558009383759038 acc: 0.79\n",
      "loss: 1.8199929526768537 acc: 0.84\n",
      "loss: 1.8232140053137331 acc: 0.86\n",
      "loss: 1.8296836452866225 acc: 0.85\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 1.8297\t Accuracy 0.8500\n",
      "loss: 1.8233779570642443 acc: 0.84\n",
      "loss: 1.848514600970089 acc: 0.83\n",
      "loss: 1.8616115058290024 acc: 0.86\n",
      "loss: 1.827931853254321 acc: 0.83\n",
      "loss: 1.8150690750655312 acc: 0.88\n",
      "loss: 1.8524945150415422 acc: 0.83\n",
      "loss: 1.8238060848083095 acc: 0.82\n",
      "loss: 1.864700375353857 acc: 0.84\n",
      "loss: 1.8197289839052706 acc: 0.88\n",
      "loss: 1.814347854812815 acc: 0.85\n",
      "loss: 1.8229638904222298 acc: 0.8\n",
      "loss: 1.8217441319429393 acc: 0.85\n",
      "loss: 1.8134889867045656 acc: 0.81\n",
      "loss: 1.8523251748760285 acc: 0.78\n",
      "loss: 1.832371981146498 acc: 0.89\n",
      "loss: 1.814052182458983 acc: 0.87\n",
      "loss: 1.8153956361379557 acc: 0.87\n",
      "loss: 1.8206160260777053 acc: 0.8\n",
      "loss: 1.8423564736025382 acc: 0.8\n",
      "loss: 1.8292850279234674 acc: 0.85\n",
      "loss: 1.8443669518241905 acc: 0.86\n",
      "loss: 1.869069470821791 acc: 0.82\n",
      "loss: 1.8425295698625277 acc: 0.89\n",
      "loss: 1.8446040487112043 acc: 0.84\n",
      "loss: 1.8371295900815108 acc: 0.81\n",
      "loss: 1.8267675556147507 acc: 0.82\n",
      "loss: 1.817518895042433 acc: 0.85\n",
      "loss: 1.8355672323204675 acc: 0.86\n",
      "loss: 1.8703002907632267 acc: 0.76\n",
      "loss: 1.8188604153797374 acc: 0.87\n",
      "loss: 1.8591605975821026 acc: 0.82\n",
      "loss: 1.8132110664401473 acc: 0.86\n",
      "loss: 1.8096725517066017 acc: 0.89\n",
      "loss: 1.8341163674820342 acc: 0.83\n",
      "loss: 1.7852894554202337 acc: 0.86\n",
      "loss: 1.8322455934085775 acc: 0.84\n",
      "loss: 1.8239699593696919 acc: 0.87\n",
      "loss: 1.8115947825316139 acc: 0.77\n",
      "loss: 1.8361022848163113 acc: 0.84\n",
      "loss: 1.8589257930156253 acc: 0.8\n",
      "loss: 1.8515354644049467 acc: 0.85\n",
      "loss: 1.8292050266950453 acc: 0.84\n",
      "loss: 1.8261020073672112 acc: 0.79\n",
      "loss: 1.8502151146458712 acc: 0.82\n",
      "loss: 1.8024216014217833 acc: 0.87\n",
      "loss: 1.8055242856030516 acc: 0.86\n",
      "loss: 1.8068192781225014 acc: 0.88\n",
      "loss: 1.8173589433463058 acc: 0.82\n",
      "loss: 1.8038149462335187 acc: 0.87\n",
      "loss: 1.7787411269970357 acc: 0.86\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 1.7787\t Accuracy 0.8600\n",
      "loss: 1.8119025883504427 acc: 0.8\n",
      "loss: 1.8147514092081745 acc: 0.87\n",
      "loss: 1.8342053321427179 acc: 0.83\n",
      "loss: 1.812334125917912 acc: 0.9\n",
      "loss: 1.8019965557853128 acc: 0.83\n",
      "loss: 1.8111974744549155 acc: 0.85\n",
      "loss: 1.8277341843039248 acc: 0.84\n",
      "loss: 1.8900214578521772 acc: 0.76\n",
      "loss: 1.8267797445186968 acc: 0.85\n",
      "loss: 1.8493958626565985 acc: 0.83\n",
      "loss: 1.7772358870930702 acc: 0.9\n",
      "loss: 1.7942061702640333 acc: 0.85\n",
      "loss: 1.8667621389494082 acc: 0.81\n",
      "loss: 1.8033407909126624 acc: 0.91\n",
      "loss: 1.824962576044379 acc: 0.84\n",
      "loss: 1.8338387001146674 acc: 0.79\n",
      "loss: 1.8418130031060067 acc: 0.83\n",
      "loss: 1.828824043650672 acc: 0.85\n",
      "loss: 1.8126757577258754 acc: 0.82\n",
      "loss: 1.818996039578164 acc: 0.82\n",
      "loss: 1.8397429798930345 acc: 0.8\n",
      "loss: 1.8498752118514108 acc: 0.82\n",
      "loss: 1.8138976715124668 acc: 0.84\n",
      "loss: 1.880993476758408 acc: 0.75\n",
      "loss: 1.8385049447366506 acc: 0.82\n",
      "loss: 1.8119076894153914 acc: 0.87\n",
      "loss: 1.8111613922170529 acc: 0.89\n",
      "loss: 1.8092481255714787 acc: 0.88\n",
      "loss: 1.8258598758709197 acc: 0.84\n",
      "loss: 1.8195597990037888 acc: 0.89\n",
      "loss: 1.8561622783009406 acc: 0.78\n",
      "loss: 1.8085760471791392 acc: 0.85\n",
      "loss: 1.8180910487351554 acc: 0.83\n",
      "loss: 1.8355162657604311 acc: 0.84\n",
      "loss: 1.830723250430032 acc: 0.82\n",
      "loss: 1.817851471764969 acc: 0.83\n",
      "loss: 1.8175869939951206 acc: 0.84\n",
      "loss: 1.8291811794640602 acc: 0.84\n",
      "loss: 1.8158738954154092 acc: 0.81\n",
      "loss: 1.8394405222726622 acc: 0.84\n",
      "loss: 1.8758144131791141 acc: 0.85\n",
      "loss: 1.8394343810821383 acc: 0.81\n",
      "loss: 1.8538951621195598 acc: 0.85\n",
      "loss: 1.8158842695049726 acc: 0.84\n",
      "loss: 1.8798955977995506 acc: 0.76\n",
      "loss: 1.8035177810331975 acc: 0.89\n",
      "loss: 1.8438711147345115 acc: 0.82\n",
      "loss: 1.823291746367152 acc: 0.81\n",
      "loss: 1.8594998311626338 acc: 0.78\n",
      "loss: 1.8228633725434635 acc: 0.86\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 1.8229\t Accuracy 0.8600\n",
      "loss: 1.8660727382459854 acc: 0.8\n",
      "loss: 1.867908925489694 acc: 0.82\n",
      "loss: 1.8160968090722545 acc: 0.82\n",
      "loss: 1.8581966065416544 acc: 0.78\n",
      "loss: 1.8540797904330157 acc: 0.84\n",
      "loss: 1.8460496904204233 acc: 0.83\n",
      "loss: 1.8555598964864217 acc: 0.81\n",
      "loss: 1.8233827850984972 acc: 0.82\n",
      "loss: 1.8297859065714386 acc: 0.86\n",
      "loss: 1.8226949844655385 acc: 0.83\n",
      "loss: 1.8323681822973654 acc: 0.77\n",
      "loss: 1.8511714400001509 acc: 0.8\n",
      "loss: 1.8185216603993124 acc: 0.85\n",
      "loss: 1.8701717642566704 acc: 0.76\n",
      "loss: 1.821884658340431 acc: 0.87\n",
      "loss: 1.862815420656769 acc: 0.78\n",
      "loss: 1.8153182340145937 acc: 0.87\n",
      "loss: 1.8668375413987561 acc: 0.76\n",
      "loss: 1.8129214322961884 acc: 0.89\n",
      "loss: 1.824292087497024 acc: 0.86\n",
      "loss: 1.8153877357265145 acc: 0.83\n",
      "loss: 1.8572310777660428 acc: 0.83\n",
      "loss: 1.8337919677552612 acc: 0.83\n",
      "loss: 1.8419848789193056 acc: 0.84\n",
      "loss: 1.8432120853623752 acc: 0.79\n",
      "loss: 1.8125828440237972 acc: 0.87\n",
      "loss: 1.8355861441737054 acc: 0.83\n",
      "loss: 1.8135767942798957 acc: 0.82\n",
      "loss: 1.8111113291887 acc: 0.8\n",
      "loss: 1.8503830497450025 acc: 0.84\n",
      "loss: 1.7922116885817316 acc: 0.9\n",
      "loss: 1.8498778748527491 acc: 0.84\n",
      "loss: 1.8685949557378314 acc: 0.81\n",
      "loss: 1.8663783044885711 acc: 0.76\n",
      "loss: 1.8113269554323777 acc: 0.85\n",
      "loss: 1.8254698225041721 acc: 0.88\n",
      "loss: 1.7961004412327677 acc: 0.87\n",
      "loss: 1.812774140814878 acc: 0.87\n",
      "loss: 1.8437909681686826 acc: 0.83\n",
      "loss: 1.8388117527719994 acc: 0.82\n",
      "loss: 1.804467954312289 acc: 0.83\n",
      "loss: 1.835945766608156 acc: 0.85\n",
      "loss: 1.8502383389680361 acc: 0.89\n",
      "loss: 1.8873024482626557 acc: 0.71\n",
      "loss: 1.8290979508066512 acc: 0.86\n",
      "loss: 1.801730057094149 acc: 0.83\n",
      "loss: 1.8393147599086035 acc: 0.79\n",
      "loss: 1.8189800406628094 acc: 0.86\n",
      "loss: 1.8058144465072319 acc: 0.85\n",
      "loss: 1.8156435760808936 acc: 0.85\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 1.8156\t Accuracy 0.8500\n",
      "loss: 1.8542179794595122 acc: 0.79\n",
      "loss: 1.8364488934759686 acc: 0.91\n",
      "loss: 1.811061931105395 acc: 0.92\n",
      "loss: 1.8647025851696912 acc: 0.79\n",
      "loss: 1.8321801776208924 acc: 0.83\n",
      "loss: 1.8148178328267874 acc: 0.86\n",
      "loss: 1.853744815596266 acc: 0.77\n",
      "loss: 1.8320776006833395 acc: 0.82\n",
      "loss: 1.8130796993640734 acc: 0.84\n",
      "loss: 1.8549612122163268 acc: 0.81\n",
      "loss: 1.8373035541259546 acc: 0.81\n",
      "loss: 1.842726295079511 acc: 0.81\n",
      "loss: 1.824102767246918 acc: 0.84\n",
      "loss: 1.8355498045850405 acc: 0.84\n",
      "loss: 1.801171413554166 acc: 0.89\n",
      "loss: 1.848992575991182 acc: 0.86\n",
      "loss: 1.851468880164541 acc: 0.84\n",
      "loss: 1.8482842147465175 acc: 0.78\n",
      "loss: 1.821567645107458 acc: 0.83\n",
      "loss: 1.8349753171483882 acc: 0.84\n",
      "loss: 1.8269057046029167 acc: 0.89\n",
      "loss: 1.8098455001856453 acc: 0.87\n",
      "loss: 1.8208110076686583 acc: 0.85\n",
      "loss: 1.8614481194401657 acc: 0.84\n",
      "loss: 1.8494212713886518 acc: 0.87\n",
      "loss: 1.8690437900941204 acc: 0.77\n",
      "loss: 1.865818975705188 acc: 0.73\n",
      "loss: 1.8937752200969333 acc: 0.73\n",
      "loss: 1.812401281121068 acc: 0.86\n",
      "loss: 1.7951826761141008 acc: 0.84\n",
      "loss: 1.8639345747956455 acc: 0.82\n",
      "loss: 1.828795765825542 acc: 0.84\n",
      "loss: 1.804440473097401 acc: 0.87\n",
      "loss: 1.8782468119417288 acc: 0.8\n",
      "loss: 1.835310735594267 acc: 0.84\n",
      "loss: 1.7934670353284323 acc: 0.88\n",
      "loss: 1.8734318220742403 acc: 0.84\n",
      "loss: 1.8624766112056068 acc: 0.77\n",
      "loss: 1.803870423536321 acc: 0.89\n",
      "loss: 1.802453841673786 acc: 0.89\n",
      "loss: 1.8187803203206943 acc: 0.81\n",
      "loss: 1.8197040246946725 acc: 0.82\n",
      "loss: 1.825738809865388 acc: 0.87\n",
      "loss: 1.8599515730670197 acc: 0.77\n",
      "loss: 1.8670046320231606 acc: 0.81\n",
      "loss: 1.832511125807118 acc: 0.87\n",
      "loss: 1.801752949523613 acc: 0.88\n",
      "loss: 1.8150522299520366 acc: 0.81\n",
      "loss: 1.8389834354973396 acc: 0.81\n",
      "loss: 1.8648741740975807 acc: 0.8\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 1.8649\t Accuracy 0.8000\n",
      "loss: 1.8435968065252437 acc: 0.81\n",
      "loss: 1.7900267719501823 acc: 0.85\n",
      "loss: 1.8276870373710417 acc: 0.86\n",
      "loss: 1.8025188377537595 acc: 0.85\n",
      "loss: 1.8229872643117455 acc: 0.88\n",
      "loss: 1.8907259437043518 acc: 0.73\n",
      "loss: 1.837673154213331 acc: 0.86\n",
      "loss: 1.8445205592015026 acc: 0.82\n",
      "loss: 1.8562023771873462 acc: 0.78\n",
      "loss: 1.8644558983379111 acc: 0.81\n",
      "loss: 1.825545681894833 acc: 0.86\n",
      "loss: 1.836807932988643 acc: 0.82\n",
      "loss: 1.8157187752365898 acc: 0.9\n",
      "loss: 1.8270348652006922 acc: 0.84\n",
      "loss: 1.813854353827537 acc: 0.82\n",
      "loss: 1.8159197353972245 acc: 0.88\n",
      "loss: 1.8479716570469542 acc: 0.8\n",
      "loss: 1.8452912516079567 acc: 0.84\n",
      "loss: 1.7900131258053866 acc: 0.88\n",
      "loss: 1.8194920518493223 acc: 0.85\n",
      "loss: 1.8000499252095647 acc: 0.89\n",
      "loss: 1.8369636304553336 acc: 0.82\n",
      "loss: 1.849369689345845 acc: 0.81\n",
      "loss: 1.853481626933445 acc: 0.85\n",
      "loss: 1.8579232966291996 acc: 0.8\n",
      "loss: 1.8420994029956679 acc: 0.84\n",
      "loss: 1.859487622506897 acc: 0.83\n",
      "loss: 1.8177538406024076 acc: 0.87\n",
      "loss: 1.8380919101473316 acc: 0.79\n",
      "loss: 1.8439469212188877 acc: 0.78\n",
      "loss: 1.8240077035164193 acc: 0.86\n",
      "loss: 1.8049062616108096 acc: 0.9\n",
      "loss: 1.8108872386461428 acc: 0.91\n",
      "loss: 1.8627247024656404 acc: 0.76\n",
      "loss: 1.8580980419505522 acc: 0.8\n",
      "loss: 1.8500734449663987 acc: 0.83\n",
      "loss: 1.8429224088490912 acc: 0.83\n",
      "loss: 1.8149294642267468 acc: 0.87\n",
      "loss: 1.7790111384535197 acc: 0.9\n",
      "loss: 1.8581358195001838 acc: 0.83\n",
      "loss: 1.8134389761788603 acc: 0.85\n",
      "loss: 1.8235080465404994 acc: 0.89\n",
      "loss: 1.8316153655674687 acc: 0.84\n",
      "loss: 1.8253502261687506 acc: 0.87\n",
      "loss: 1.8192595767900408 acc: 0.89\n",
      "loss: 1.8225827257021314 acc: 0.87\n",
      "loss: 1.8393513551043006 acc: 0.82\n",
      "loss: 1.7996256957584038 acc: 0.85\n",
      "loss: 1.8523843608813633 acc: 0.81\n",
      "loss: 1.8502636098599328 acc: 0.84\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 1.8503\t Accuracy 0.8400\n",
      "loss: 1.8397132942426564 acc: 0.83\n",
      "loss: 1.8493017069842634 acc: 0.79\n",
      "loss: 1.8563275236877532 acc: 0.81\n",
      "loss: 1.8156487776271097 acc: 0.84\n",
      "loss: 1.893243169969372 acc: 0.8\n",
      "loss: 1.8320299400131321 acc: 0.82\n",
      "loss: 1.8345781032953143 acc: 0.81\n",
      "loss: 1.837384451724284 acc: 0.84\n",
      "loss: 1.8432830146587897 acc: 0.8\n",
      "loss: 1.8196188936259035 acc: 0.83\n",
      "loss: 1.828510858293835 acc: 0.88\n",
      "loss: 1.8336132882308407 acc: 0.79\n",
      "loss: 1.8036699553572648 acc: 0.87\n",
      "loss: 1.7998865886219753 acc: 0.87\n",
      "loss: 1.8166467926336987 acc: 0.85\n",
      "loss: 1.859188774823925 acc: 0.82\n",
      "loss: 1.8280942624417507 acc: 0.8\n",
      "loss: 1.8067979247839387 acc: 0.86\n",
      "loss: 1.8606438591505106 acc: 0.79\n",
      "loss: 1.8125987104890118 acc: 0.85\n",
      "loss: 1.812619234984834 acc: 0.88\n",
      "loss: 1.7823198470895898 acc: 0.88\n",
      "loss: 1.820972799145836 acc: 0.84\n",
      "loss: 1.7971130853625588 acc: 0.88\n",
      "loss: 1.8553438004335732 acc: 0.81\n",
      "loss: 1.8561362724760664 acc: 0.86\n",
      "loss: 1.8119373415605025 acc: 0.84\n",
      "loss: 1.8058903998100266 acc: 0.84\n",
      "loss: 1.8657379243840626 acc: 0.8\n",
      "loss: 1.86825642622383 acc: 0.82\n",
      "loss: 1.7869802355006732 acc: 0.88\n",
      "loss: 1.8304045976085754 acc: 0.83\n",
      "loss: 1.8057182770771079 acc: 0.85\n",
      "loss: 1.8151994134736116 acc: 0.86\n",
      "loss: 1.8037312053155292 acc: 0.87\n",
      "loss: 1.7695870628043167 acc: 0.84\n",
      "loss: 1.8390313867504147 acc: 0.83\n",
      "loss: 1.8353474168613944 acc: 0.83\n",
      "loss: 1.8424803532741505 acc: 0.85\n",
      "loss: 1.8142769170527238 acc: 0.91\n",
      "loss: 1.837735282639207 acc: 0.81\n",
      "loss: 1.8579897881926846 acc: 0.84\n",
      "loss: 1.8151324366436707 acc: 0.8\n",
      "loss: 1.8313979111989451 acc: 0.85\n",
      "loss: 1.825457442254791 acc: 0.83\n",
      "loss: 1.7902551101349053 acc: 0.88\n",
      "loss: 1.8177817620683827 acc: 0.91\n",
      "loss: 1.8648576598684605 acc: 0.81\n",
      "loss: 1.78921224880548 acc: 0.89\n",
      "loss: 1.8676308848926682 acc: 0.75\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 1.8676\t Accuracy 0.7500\n",
      "loss: 1.7917579738442917 acc: 0.87\n",
      "loss: 1.8066588707207076 acc: 0.89\n",
      "loss: 1.831856202102185 acc: 0.77\n",
      "loss: 1.8548558676146079 acc: 0.86\n",
      "loss: 1.8584375462543798 acc: 0.8\n",
      "loss: 1.7906185802855765 acc: 0.88\n",
      "loss: 1.857303468550769 acc: 0.85\n",
      "loss: 1.813794047170083 acc: 0.82\n",
      "loss: 1.8402995119690226 acc: 0.81\n",
      "loss: 1.857500461937629 acc: 0.87\n",
      "loss: 1.841682314113272 acc: 0.84\n",
      "loss: 1.8169555029421705 acc: 0.83\n",
      "loss: 1.8467989035144192 acc: 0.8\n",
      "loss: 1.8689066402072256 acc: 0.77\n",
      "loss: 1.8130696625247493 acc: 0.82\n",
      "loss: 1.8705765429964734 acc: 0.78\n",
      "loss: 1.8134134280199512 acc: 0.84\n",
      "loss: 1.824978984865326 acc: 0.82\n",
      "loss: 1.8517570429960162 acc: 0.78\n",
      "loss: 1.8176725115449384 acc: 0.84\n",
      "loss: 1.8442836277503334 acc: 0.83\n",
      "loss: 1.815448714146626 acc: 0.83\n",
      "loss: 1.8310151208325667 acc: 0.83\n",
      "loss: 1.8724795900146936 acc: 0.79\n",
      "loss: 1.8231648082217211 acc: 0.9\n",
      "loss: 1.8299438830177581 acc: 0.91\n",
      "loss: 1.8123884532314987 acc: 0.84\n",
      "loss: 1.8153020674028872 acc: 0.83\n",
      "loss: 1.816246131580953 acc: 0.85\n",
      "loss: 1.9009319439420187 acc: 0.77\n",
      "loss: 1.840841527893631 acc: 0.8\n",
      "loss: 1.8396947700250497 acc: 0.86\n",
      "loss: 1.8495444813242696 acc: 0.84\n",
      "loss: 1.815503326129519 acc: 0.83\n",
      "loss: 1.8520926601371221 acc: 0.82\n",
      "loss: 1.870718917364534 acc: 0.8\n",
      "loss: 1.822989368314825 acc: 0.92\n",
      "loss: 1.833306983680311 acc: 0.87\n",
      "loss: 1.8416473349892108 acc: 0.79\n",
      "loss: 1.8528806361148435 acc: 0.86\n",
      "loss: 1.8893439236103418 acc: 0.83\n",
      "loss: 1.8394641109938703 acc: 0.82\n",
      "loss: 1.8377819846346197 acc: 0.87\n",
      "loss: 1.85426213868542 acc: 0.81\n",
      "loss: 1.8531527056010975 acc: 0.79\n",
      "loss: 1.882542889476298 acc: 0.78\n",
      "loss: 1.8237358903504686 acc: 0.83\n",
      "loss: 1.8436761976906368 acc: 0.82\n",
      "loss: 1.7886256171326633 acc: 0.89\n",
      "loss: 1.8102412974485866 acc: 0.83\n",
      "loss: 1.8020482518851426 acc: 0.85\n",
      "loss: 1.8094177097179422 acc: 0.87\n",
      "loss: 1.8336746830644852 acc: 0.84\n",
      "loss: 1.8305485360624154 acc: 0.9\n",
      "loss: 1.8209639708187018 acc: 0.87\n",
      "loss: 1.759513839373026 acc: 0.87\n",
      "loss: 1.765430101602706 acc: 0.85\n",
      "loss: 1.8205998127562881 acc: 0.88\n",
      "loss: 1.764568630483467 acc: 0.93\n",
      "loss: 1.8075927222076302 acc: 0.87\n",
      "loss: 1.7985307212838506 acc: 0.84\n",
      "loss: 1.8489376814634908 acc: 0.82\n",
      "loss: 1.8672032633360152 acc: 0.87\n",
      "loss: 1.870472097281098 acc: 0.86\n",
      "loss: 1.8043106721191764 acc: 0.9\n",
      "loss: 1.814019766225739 acc: 0.82\n",
      "loss: 1.7674185623761962 acc: 0.91\n",
      "loss: 1.7991704883303832 acc: 0.85\n",
      "loss: 1.7911101551077824 acc: 0.89\n",
      "loss: 1.888360656625617 acc: 0.84\n",
      "loss: 1.8322550350512703 acc: 0.9\n",
      "loss: 1.8342322526976198 acc: 0.77\n",
      "loss: 1.8531822108779858 acc: 0.87\n",
      "loss: 1.8448155793220276 acc: 0.85\n",
      "loss: 1.8678977289813654 acc: 0.77\n",
      "loss: 1.8679861188640745 acc: 0.73\n",
      "loss: 1.8870017213862138 acc: 0.79\n",
      "loss: 1.8440341643154832 acc: 0.87\n",
      "loss: 1.8011333844027408 acc: 0.89\n",
      "loss: 1.8222282372019987 acc: 0.88\n",
      "loss: 1.797751602363621 acc: 0.92\n",
      "loss: 1.7579539456680118 acc: 0.92\n",
      "loss: 1.7885778993244101 acc: 0.89\n",
      "loss: 1.7957904073591984 acc: 0.86\n",
      "loss: 1.8208753692549988 acc: 0.87\n",
      "loss: 1.85426059101378 acc: 0.88\n",
      "loss: 1.8584633945326068 acc: 0.84\n",
      "loss: 1.762049030345337 acc: 0.91\n",
      "loss: 1.7012801764116388 acc: 0.99\n",
      "loss: 1.7423851624428843 acc: 0.94\n",
      "loss: 1.758291709194876 acc: 0.96\n",
      "loss: 1.8063759281865577 acc: 0.87\n",
      "loss: 1.8080875806985892 acc: 0.81\n",
      "loss: 1.7325925656114032 acc: 0.87\n",
      "loss: 1.8184048101542336 acc: 0.88\n",
      "loss: 1.8119199051612296 acc: 0.92\n",
      "loss: 1.8981962280564466 acc: 0.77\n",
      "loss: 1.707979607341786 acc: 0.96\n",
      "loss: 1.880356189369755 acc: 0.83\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8324\t Average training accuracy 0.8355\n",
      "Epoch [9]\t Average validation loss 1.8126\t Average validation accuracy 0.8674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train without momentum\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "loss1, acc1 = runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.821175811222923 acc: 0.89\n",
      "loss: 1.8409216776089192 acc: 0.89\n",
      "loss: 1.8570370859401177 acc: 0.82\n",
      "loss: 1.8733359523200688 acc: 0.8\n",
      "loss: 1.868861590862012 acc: 0.8\n",
      "loss: 1.8878927685201032 acc: 0.82\n",
      "loss: 1.869577554884128 acc: 0.78\n",
      "loss: 1.8235916823877276 acc: 0.86\n",
      "loss: 1.8489469441047044 acc: 0.86\n",
      "loss: 1.8785815477697394 acc: 0.81\n",
      "loss: 1.8836692636406749 acc: 0.8\n",
      "loss: 1.8764224667156566 acc: 0.78\n",
      "loss: 1.948533246865725 acc: 0.72\n",
      "loss: 1.859407488773027 acc: 0.78\n",
      "loss: 1.877882529705249 acc: 0.73\n",
      "loss: 1.905212079044963 acc: 0.84\n",
      "loss: 1.8666971767290648 acc: 0.83\n",
      "loss: 1.898454476598354 acc: 0.79\n",
      "loss: 1.8678160062850733 acc: 0.79\n",
      "loss: 1.8876425279984104 acc: 0.78\n",
      "loss: 1.9031372744156985 acc: 0.79\n",
      "loss: 1.8998381541778442 acc: 0.76\n",
      "loss: 1.8914342936559414 acc: 0.82\n",
      "loss: 1.8550666112980303 acc: 0.79\n",
      "loss: 1.8811556270736736 acc: 0.76\n",
      "loss: 1.8904019877325353 acc: 0.77\n",
      "loss: 1.8930341277691949 acc: 0.79\n",
      "loss: 1.8513586329616862 acc: 0.85\n",
      "loss: 1.849803786662074 acc: 0.85\n",
      "loss: 1.8842017041756145 acc: 0.84\n",
      "loss: 1.8417368277805641 acc: 0.9\n",
      "loss: 1.8837225699080986 acc: 0.8\n",
      "loss: 1.8379165555944263 acc: 0.82\n",
      "loss: 1.828673353338485 acc: 0.83\n",
      "loss: 1.8494990445737072 acc: 0.85\n",
      "loss: 1.8806284466155787 acc: 0.77\n",
      "loss: 1.8147461586431672 acc: 0.81\n",
      "loss: 1.9068068501354623 acc: 0.73\n",
      "loss: 1.900777780384784 acc: 0.77\n",
      "loss: 1.8829397930964804 acc: 0.78\n",
      "loss: 1.8362773286186107 acc: 0.83\n",
      "loss: 1.865511144678959 acc: 0.81\n",
      "loss: 1.8782203742916892 acc: 0.75\n",
      "loss: 1.9105957022441533 acc: 0.77\n",
      "loss: 1.8941916964467 acc: 0.75\n",
      "loss: 1.8700627240946934 acc: 0.83\n",
      "loss: 1.8449977659148038 acc: 0.84\n",
      "loss: 1.8387387947373997 acc: 0.85\n",
      "loss: 1.8970735102121394 acc: 0.75\n",
      "loss: 1.8670556984241082 acc: 0.78\n",
      "loss: 1.7870747773276339 acc: 0.88\n",
      "loss: 1.814419674106621 acc: 0.86\n",
      "loss: 1.7852239615371197 acc: 0.9\n",
      "loss: 1.7556770650526021 acc: 0.96\n",
      "loss: 1.7078380540604834 acc: 0.97\n",
      "loss: 1.7942960245394604 acc: 0.91\n",
      "loss: 1.8039347566353365 acc: 0.81\n",
      "loss: 1.7817096480058208 acc: 0.89\n",
      "loss: 1.8084589529275545 acc: 0.87\n",
      "loss: 1.8457335876901966 acc: 0.81\n",
      "loss: 1.7988152456735933 acc: 0.8\n",
      "loss: 1.6793043530800966 acc: 0.9\n",
      "loss: 1.703484945167994 acc: 0.98\n",
      "loss: 1.7086689082591169 acc: 0.96\n",
      "loss: 1.7866286184186302 acc: 0.92\n",
      "loss: 1.884422008969901 acc: 0.77\n",
      "loss: 1.877476227127066 acc: 0.81\n",
      "loss: 1.7879175513129817 acc: 0.84\n",
      "loss: 1.7904158279656341 acc: 0.89\n",
      "loss: 1.7741862815551992 acc: 0.94\n",
      "loss: 1.7729808861199856 acc: 0.9\n",
      "loss: 1.8052535931691744 acc: 0.93\n",
      "loss: 1.8379070548568783 acc: 0.91\n",
      "loss: 1.7475901707402455 acc: 0.99\n",
      "loss: 1.909487712544738 acc: 0.81\n",
      "loss: 1.8210218078169924 acc: 0.87\n",
      "loss: 1.8009022522506681 acc: 0.91\n",
      "loss: 1.688246952628574 acc: 0.97\n",
      "loss: 1.779358041919275 acc: 0.82\n",
      "loss: 1.7192835666619788 acc: 0.91\n",
      "loss: 1.7462050971414167 acc: 0.89\n",
      "loss: 1.7507151553387272 acc: 0.91\n",
      "loss: 1.8225846874698677 acc: 0.89\n",
      "loss: 1.7878796970926305 acc: 0.87\n",
      "loss: 1.719349317385563 acc: 0.9\n",
      "loss: 1.7442691235347527 acc: 0.95\n",
      "loss: 1.7606105776990935 acc: 0.95\n",
      "loss: 1.6845897886512937 acc: 0.99\n",
      "loss: 1.7001286576436572 acc: 0.98\n",
      "loss: 1.729637355576572 acc: 0.97\n",
      "loss: 1.790577948255148 acc: 0.83\n",
      "loss: 1.722105199785677 acc: 0.9\n",
      "loss: 1.7990819447287631 acc: 0.91\n",
      "loss: 1.7658572490090831 acc: 0.95\n",
      "loss: 1.7509697930402266 acc: 0.94\n",
      "loss: 1.7320106083158426 acc: 0.91\n",
      "loss: 1.8672097982621692 acc: 0.86\n",
      "loss: 1.9216362520591388 acc: 0.74\n",
      "loss: 1.9142843094334432 acc: 0.72\n",
      "loss: 1.8610560473525692 acc: 0.79\n",
      "Final test accuracy 0.8475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = runner.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.658960163641729 acc: 0.08\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.6590\t Accuracy 0.0800\n",
      "loss: 2.459243293817953 acc: 0.1\n",
      "loss: 2.449989370427463 acc: 0.06\n",
      "loss: 2.4417985771001174 acc: 0.03\n",
      "loss: 2.3836437365910585 acc: 0.13\n",
      "loss: 2.361188286867592 acc: 0.11\n",
      "loss: 2.306270215698415 acc: 0.14\n",
      "loss: 2.2114060224411043 acc: 0.14\n",
      "loss: 2.158143143460408 acc: 0.2\n",
      "loss: 2.1073584179571565 acc: 0.3\n",
      "loss: 2.0716445873558427 acc: 0.33\n",
      "loss: 2.024712529903712 acc: 0.29\n",
      "loss: 1.9783474872958353 acc: 0.41\n",
      "loss: 1.9527578218330246 acc: 0.4\n",
      "loss: 1.859041179061892 acc: 0.51\n",
      "loss: 1.8544080800329166 acc: 0.5\n",
      "loss: 1.86048923462536 acc: 0.54\n",
      "loss: 1.9216850340586147 acc: 0.44\n",
      "loss: 1.8876936273692848 acc: 0.47\n",
      "loss: 1.9009675624865365 acc: 0.51\n",
      "loss: 1.8565811781286408 acc: 0.58\n",
      "loss: 1.8632673881170774 acc: 0.57\n",
      "loss: 1.9213123718913065 acc: 0.52\n",
      "loss: 1.9049213529149884 acc: 0.59\n",
      "loss: 1.8131022470419855 acc: 0.61\n",
      "loss: 1.9030453016172266 acc: 0.62\n",
      "loss: 1.8702407912900103 acc: 0.66\n",
      "loss: 1.92725023944229 acc: 0.6\n",
      "loss: 1.933784599232041 acc: 0.62\n",
      "loss: 1.9457218127699876 acc: 0.55\n",
      "loss: 1.995202317520856 acc: 0.54\n",
      "loss: 1.9489221518036184 acc: 0.56\n",
      "loss: 1.9278960749745744 acc: 0.6\n",
      "loss: 1.9414392654545436 acc: 0.61\n",
      "loss: 1.954322034829619 acc: 0.54\n",
      "loss: 1.9603192576997237 acc: 0.52\n",
      "loss: 1.9670255617900059 acc: 0.62\n",
      "loss: 1.9328910709828049 acc: 0.59\n",
      "loss: 1.9406125773272396 acc: 0.65\n",
      "loss: 1.8754713395390812 acc: 0.72\n",
      "loss: 1.919321929705845 acc: 0.57\n",
      "loss: 1.911843765786154 acc: 0.64\n",
      "loss: 1.8688541919361177 acc: 0.72\n",
      "loss: 1.882168798519526 acc: 0.64\n",
      "loss: 1.8730520992482729 acc: 0.66\n",
      "loss: 1.868776749502179 acc: 0.7\n",
      "loss: 1.8556873939417171 acc: 0.7\n",
      "loss: 1.8570770858863943 acc: 0.68\n",
      "loss: 1.8714566328244728 acc: 0.71\n",
      "loss: 1.9154909864405962 acc: 0.67\n",
      "loss: 1.8618046593472173 acc: 0.7\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 1.8618\t Accuracy 0.7000\n",
      "loss: 1.9026845310680616 acc: 0.68\n",
      "loss: 1.8681800531918378 acc: 0.7\n",
      "loss: 1.85532670873167 acc: 0.74\n",
      "loss: 1.8576789298554228 acc: 0.76\n",
      "loss: 1.9136064038934069 acc: 0.71\n",
      "loss: 1.8279521456984076 acc: 0.78\n",
      "loss: 1.8834507716066533 acc: 0.66\n",
      "loss: 1.8410884347844665 acc: 0.74\n",
      "loss: 1.8648746580127975 acc: 0.71\n",
      "loss: 1.8747112927994356 acc: 0.68\n",
      "loss: 1.8763515077747865 acc: 0.72\n",
      "loss: 1.8846940139221948 acc: 0.72\n",
      "loss: 1.8834495004550242 acc: 0.69\n",
      "loss: 1.8950082694367365 acc: 0.73\n",
      "loss: 1.884334414251388 acc: 0.72\n",
      "loss: 1.8637311649031545 acc: 0.76\n",
      "loss: 1.8913800018424753 acc: 0.68\n",
      "loss: 1.8801462616222364 acc: 0.67\n",
      "loss: 1.8403962272638268 acc: 0.78\n",
      "loss: 1.8861555741576612 acc: 0.72\n",
      "loss: 1.8079513966392695 acc: 0.89\n",
      "loss: 1.8729826730572474 acc: 0.68\n",
      "loss: 1.8879333844915698 acc: 0.68\n",
      "loss: 1.8729110190830938 acc: 0.73\n",
      "loss: 1.8528357562339366 acc: 0.77\n",
      "loss: 1.858161838183072 acc: 0.76\n",
      "loss: 1.8863969456662861 acc: 0.69\n",
      "loss: 1.8972982300975128 acc: 0.65\n",
      "loss: 1.8413136853735017 acc: 0.78\n",
      "loss: 1.874223932992977 acc: 0.71\n",
      "loss: 1.8239696532922505 acc: 0.71\n",
      "loss: 1.884731880808906 acc: 0.74\n",
      "loss: 1.8668152050723767 acc: 0.7\n",
      "loss: 1.8867624508765755 acc: 0.71\n",
      "loss: 1.8392799381654268 acc: 0.74\n",
      "loss: 1.8757927873720548 acc: 0.68\n",
      "loss: 1.875814186535117 acc: 0.7\n",
      "loss: 1.8591393703025225 acc: 0.74\n",
      "loss: 1.8412363851936937 acc: 0.79\n",
      "loss: 1.8815668207239806 acc: 0.78\n",
      "loss: 1.8551322595037656 acc: 0.76\n",
      "loss: 1.8676924072287568 acc: 0.73\n",
      "loss: 1.8682803374497619 acc: 0.7\n",
      "loss: 1.8990325911689028 acc: 0.69\n",
      "loss: 1.8469471153773498 acc: 0.78\n",
      "loss: 1.899260267255754 acc: 0.7\n",
      "loss: 1.863828813308666 acc: 0.74\n",
      "loss: 1.904487599919901 acc: 0.66\n",
      "loss: 1.9230014946604197 acc: 0.71\n",
      "loss: 1.903830922498925 acc: 0.73\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 1.9038\t Accuracy 0.7300\n",
      "loss: 1.8617119586081403 acc: 0.79\n",
      "loss: 1.8225407332472678 acc: 0.83\n",
      "loss: 1.8185575583313887 acc: 0.78\n",
      "loss: 1.890718580777474 acc: 0.74\n",
      "loss: 1.8323753878511226 acc: 0.75\n",
      "loss: 1.86676928833633 acc: 0.7\n",
      "loss: 1.8513700102199417 acc: 0.74\n",
      "loss: 1.851035447703748 acc: 0.74\n",
      "loss: 1.832006304435425 acc: 0.79\n",
      "loss: 1.8221276630224577 acc: 0.79\n",
      "loss: 1.863878241341789 acc: 0.68\n",
      "loss: 1.8351140357374771 acc: 0.83\n",
      "loss: 1.8598195154546482 acc: 0.74\n",
      "loss: 1.8464509298089338 acc: 0.72\n",
      "loss: 1.8922120620837994 acc: 0.73\n",
      "loss: 1.818916545630267 acc: 0.84\n",
      "loss: 1.855226380625064 acc: 0.76\n",
      "loss: 1.8679569201365605 acc: 0.75\n",
      "loss: 1.8726418164885612 acc: 0.76\n",
      "loss: 1.8935751923237762 acc: 0.72\n",
      "loss: 1.8578893702181571 acc: 0.74\n",
      "loss: 1.8923307775249092 acc: 0.78\n",
      "loss: 1.8229216128919874 acc: 0.8\n",
      "loss: 1.83150580994758 acc: 0.82\n",
      "loss: 1.8621509493535076 acc: 0.76\n",
      "loss: 1.8660353772064853 acc: 0.74\n",
      "loss: 1.8964928637578553 acc: 0.73\n",
      "loss: 1.8894771732740483 acc: 0.79\n",
      "loss: 1.8853003426951098 acc: 0.73\n",
      "loss: 1.8475877201998296 acc: 0.78\n",
      "loss: 1.8873930024248708 acc: 0.67\n",
      "loss: 1.8546328723907528 acc: 0.81\n",
      "loss: 1.8354241244313716 acc: 0.79\n",
      "loss: 1.9350722058691028 acc: 0.63\n",
      "loss: 1.8754164236394475 acc: 0.74\n",
      "loss: 1.8411874829186834 acc: 0.8\n",
      "loss: 1.890164967814945 acc: 0.75\n",
      "loss: 1.8576017535941596 acc: 0.79\n",
      "loss: 1.8541417128725213 acc: 0.79\n",
      "loss: 1.8513740905854588 acc: 0.74\n",
      "loss: 1.858217316097201 acc: 0.75\n",
      "loss: 1.8830380265337336 acc: 0.68\n",
      "loss: 1.8673354929458474 acc: 0.77\n",
      "loss: 1.8557479450536931 acc: 0.79\n",
      "loss: 1.8711432587007573 acc: 0.7\n",
      "loss: 1.861458807083437 acc: 0.79\n",
      "loss: 1.8654404806225708 acc: 0.76\n",
      "loss: 1.8310756057167001 acc: 0.81\n",
      "loss: 1.8955909451960633 acc: 0.74\n",
      "loss: 1.8224679231419783 acc: 0.81\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 1.8225\t Accuracy 0.8100\n",
      "loss: 1.8552132521391453 acc: 0.78\n",
      "loss: 1.8379499639827923 acc: 0.8\n",
      "loss: 1.889859572139312 acc: 0.75\n",
      "loss: 1.856351073412538 acc: 0.75\n",
      "loss: 1.887762656913647 acc: 0.77\n",
      "loss: 1.853877153204906 acc: 0.8\n",
      "loss: 1.821869768938741 acc: 0.82\n",
      "loss: 1.8492083256209284 acc: 0.79\n",
      "loss: 1.8528169811884967 acc: 0.77\n",
      "loss: 1.8133814279148082 acc: 0.81\n",
      "loss: 1.8655079205759681 acc: 0.77\n",
      "loss: 1.849126359983368 acc: 0.79\n",
      "loss: 1.849317402061308 acc: 0.8\n",
      "loss: 1.8503597788744437 acc: 0.72\n",
      "loss: 1.8502843627349288 acc: 0.77\n",
      "loss: 1.8223011847753456 acc: 0.74\n",
      "loss: 1.8532087218729865 acc: 0.76\n",
      "loss: 1.821867585523034 acc: 0.82\n",
      "loss: 1.8084026664436754 acc: 0.81\n",
      "loss: 1.8325333158763528 acc: 0.79\n",
      "loss: 1.825167213851725 acc: 0.82\n",
      "loss: 1.8173131348077154 acc: 0.77\n",
      "loss: 1.819309272476251 acc: 0.79\n",
      "loss: 1.8499554269095397 acc: 0.8\n",
      "loss: 1.822658900787227 acc: 0.82\n",
      "loss: 1.8712776273160763 acc: 0.74\n",
      "loss: 1.8772712674802674 acc: 0.81\n",
      "loss: 1.8547246759891618 acc: 0.79\n",
      "loss: 1.8465157001283166 acc: 0.73\n",
      "loss: 1.9244111141977618 acc: 0.71\n",
      "loss: 1.8899580799621847 acc: 0.75\n",
      "loss: 1.823216657029503 acc: 0.81\n",
      "loss: 1.8872864035784698 acc: 0.77\n",
      "loss: 1.8798233541157503 acc: 0.78\n",
      "loss: 1.856869577135027 acc: 0.81\n",
      "loss: 1.8388269680471003 acc: 0.79\n",
      "loss: 1.8378042489584032 acc: 0.79\n",
      "loss: 1.8642824222767962 acc: 0.75\n",
      "loss: 1.8792893536925401 acc: 0.77\n",
      "loss: 1.8141140676706042 acc: 0.85\n",
      "loss: 1.845293400823837 acc: 0.78\n",
      "loss: 1.8429179071538966 acc: 0.8\n",
      "loss: 1.8495247191761757 acc: 0.76\n",
      "loss: 1.8392797756355082 acc: 0.83\n",
      "loss: 1.8532448008344613 acc: 0.76\n",
      "loss: 1.8487256556697818 acc: 0.74\n",
      "loss: 1.8350974901068973 acc: 0.81\n",
      "loss: 1.8199913256031501 acc: 0.83\n",
      "loss: 1.79530738862736 acc: 0.91\n",
      "loss: 1.806844432825713 acc: 0.82\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 1.8068\t Accuracy 0.8200\n",
      "loss: 1.8805496645920103 acc: 0.75\n",
      "loss: 1.820313627044663 acc: 0.8\n",
      "loss: 1.8363465419963734 acc: 0.81\n",
      "loss: 1.8813754838476229 acc: 0.77\n",
      "loss: 1.8561032604626857 acc: 0.75\n",
      "loss: 1.8972399035438199 acc: 0.78\n",
      "loss: 1.8302658530771356 acc: 0.82\n",
      "loss: 1.8492911869080877 acc: 0.81\n",
      "loss: 1.8672863934445258 acc: 0.79\n",
      "loss: 1.8560096785300848 acc: 0.84\n",
      "loss: 1.876414292511556 acc: 0.8\n",
      "loss: 1.8021706283893908 acc: 0.84\n",
      "loss: 1.8547639391725579 acc: 0.81\n",
      "loss: 1.8189703199380611 acc: 0.84\n",
      "loss: 1.842541018505229 acc: 0.78\n",
      "loss: 1.8535757394574575 acc: 0.79\n",
      "loss: 1.864070221546225 acc: 0.8\n",
      "loss: 1.8703306990039998 acc: 0.75\n",
      "loss: 1.8348984731406321 acc: 0.8\n",
      "loss: 1.8383010953528909 acc: 0.83\n",
      "loss: 1.8338026552718194 acc: 0.82\n",
      "loss: 1.836841895664702 acc: 0.82\n",
      "loss: 1.804832898633202 acc: 0.85\n",
      "loss: 1.8377093899761854 acc: 0.8\n",
      "loss: 1.8747456401828166 acc: 0.72\n",
      "loss: 1.8431967807410776 acc: 0.77\n",
      "loss: 1.8107019376965305 acc: 0.84\n",
      "loss: 1.8496089621022147 acc: 0.77\n",
      "loss: 1.8671163983696746 acc: 0.74\n",
      "loss: 1.8167441914945486 acc: 0.81\n",
      "loss: 1.8641104793206582 acc: 0.7\n",
      "loss: 1.7797691098527944 acc: 0.82\n",
      "loss: 1.8577946188138963 acc: 0.79\n",
      "loss: 1.8508577237156063 acc: 0.77\n",
      "loss: 1.8643233876543512 acc: 0.77\n",
      "loss: 1.849524061652158 acc: 0.82\n",
      "loss: 1.8309166384673012 acc: 0.77\n",
      "loss: 1.847402206148196 acc: 0.82\n",
      "loss: 1.8226979063649724 acc: 0.88\n",
      "loss: 1.7932852427700194 acc: 0.82\n",
      "loss: 1.8649727106013392 acc: 0.72\n",
      "loss: 1.8383123214822996 acc: 0.83\n",
      "loss: 1.8778817764053952 acc: 0.79\n",
      "loss: 1.8378112929390877 acc: 0.8\n",
      "loss: 1.7991166965416474 acc: 0.83\n",
      "loss: 1.8577205441421785 acc: 0.79\n",
      "loss: 1.8164285475115072 acc: 0.78\n",
      "loss: 1.8698730732932842 acc: 0.83\n",
      "loss: 1.8396906680291067 acc: 0.82\n",
      "loss: 1.8370108701446868 acc: 0.77\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 1.8370\t Accuracy 0.7700\n",
      "loss: 1.8345996257788635 acc: 0.8\n",
      "loss: 1.824772922550149 acc: 0.76\n",
      "loss: 1.8628525949835677 acc: 0.8\n",
      "loss: 1.8512661527716108 acc: 0.76\n",
      "loss: 1.8185266742130672 acc: 0.87\n",
      "loss: 1.7923308597631427 acc: 0.86\n",
      "loss: 1.8734240128139013 acc: 0.83\n",
      "loss: 1.8522638399814315 acc: 0.79\n",
      "loss: 1.864589981034244 acc: 0.81\n",
      "loss: 1.805091935836113 acc: 0.84\n",
      "loss: 1.82885081197895 acc: 0.84\n",
      "loss: 1.8636257524566096 acc: 0.77\n",
      "loss: 1.8744221542623978 acc: 0.8\n",
      "loss: 1.8080987548309593 acc: 0.79\n",
      "loss: 1.8849892492132614 acc: 0.76\n",
      "loss: 1.8521154228517887 acc: 0.79\n",
      "loss: 1.852709447060488 acc: 0.83\n",
      "loss: 1.8741057614474281 acc: 0.8\n",
      "loss: 1.8791523655673104 acc: 0.76\n",
      "loss: 1.8948370147807092 acc: 0.72\n",
      "loss: 1.8716576130467635 acc: 0.77\n",
      "loss: 1.853211558407064 acc: 0.82\n",
      "loss: 1.8682849607158938 acc: 0.75\n",
      "loss: 1.8093088418593894 acc: 0.88\n",
      "loss: 1.8324059816720975 acc: 0.85\n",
      "loss: 1.7987720725368288 acc: 0.86\n",
      "loss: 1.8096690749120803 acc: 0.89\n",
      "loss: 1.8289124557605805 acc: 0.83\n",
      "loss: 1.8613605380866185 acc: 0.75\n",
      "loss: 1.8673633570406059 acc: 0.69\n",
      "loss: 1.8000698170426248 acc: 0.84\n",
      "loss: 1.800401296752307 acc: 0.82\n",
      "loss: 1.8133463359409534 acc: 0.85\n",
      "loss: 1.8494578255170995 acc: 0.81\n",
      "loss: 1.8118524286814996 acc: 0.85\n",
      "loss: 1.8436882413789981 acc: 0.82\n",
      "loss: 1.8358510094192124 acc: 0.8\n",
      "loss: 1.870642756812702 acc: 0.78\n",
      "loss: 1.8303515941141548 acc: 0.81\n",
      "loss: 1.8395731553089067 acc: 0.77\n",
      "loss: 1.8361792123293659 acc: 0.86\n",
      "loss: 1.8576243374389672 acc: 0.78\n",
      "loss: 1.8499790080767196 acc: 0.77\n",
      "loss: 1.9116243520632998 acc: 0.77\n",
      "loss: 1.8088596924755933 acc: 0.82\n",
      "loss: 1.8171679516379975 acc: 0.86\n",
      "loss: 1.8423603505459512 acc: 0.83\n",
      "loss: 1.8184858355272808 acc: 0.85\n",
      "loss: 1.8349304566317546 acc: 0.79\n",
      "loss: 1.835499471247585 acc: 0.85\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 1.8355\t Accuracy 0.8500\n",
      "loss: 1.8699978253934106 acc: 0.74\n",
      "loss: 1.7991177046390414 acc: 0.8\n",
      "loss: 1.8179039067864862 acc: 0.8\n",
      "loss: 1.83075770438577 acc: 0.77\n",
      "loss: 1.862140282835209 acc: 0.76\n",
      "loss: 1.8527640751075183 acc: 0.82\n",
      "loss: 1.826126192611398 acc: 0.79\n",
      "loss: 1.8467739881180136 acc: 0.77\n",
      "loss: 1.8562489436489757 acc: 0.78\n",
      "loss: 1.8701509260709563 acc: 0.81\n",
      "loss: 1.8514103350523454 acc: 0.85\n",
      "loss: 1.7933289317012993 acc: 0.82\n",
      "loss: 1.8560118708584854 acc: 0.79\n",
      "loss: 1.8656318609050075 acc: 0.78\n",
      "loss: 1.7777511684402751 acc: 0.89\n",
      "loss: 1.849070793465863 acc: 0.78\n",
      "loss: 1.840056672374582 acc: 0.87\n",
      "loss: 1.8732176617524428 acc: 0.79\n",
      "loss: 1.859958961602533 acc: 0.81\n",
      "loss: 1.8608227470354899 acc: 0.76\n",
      "loss: 1.850555473569965 acc: 0.83\n",
      "loss: 1.816626356453833 acc: 0.79\n",
      "loss: 1.828664176384501 acc: 0.84\n",
      "loss: 1.8440792707669396 acc: 0.82\n",
      "loss: 1.8430364808893653 acc: 0.81\n",
      "loss: 1.8430829764330106 acc: 0.85\n",
      "loss: 1.861987841037866 acc: 0.76\n",
      "loss: 1.8556823037050665 acc: 0.83\n",
      "loss: 1.8541941085167175 acc: 0.82\n",
      "loss: 1.867183031865248 acc: 0.8\n",
      "loss: 1.873107944003197 acc: 0.82\n",
      "loss: 1.8477048917320955 acc: 0.77\n",
      "loss: 1.844027714799562 acc: 0.8\n",
      "loss: 1.8470120410266326 acc: 0.83\n",
      "loss: 1.8332502066053227 acc: 0.8\n",
      "loss: 1.801348590986936 acc: 0.89\n",
      "loss: 1.8441958416124522 acc: 0.85\n",
      "loss: 1.8165382988271455 acc: 0.88\n",
      "loss: 1.8246331188657225 acc: 0.82\n",
      "loss: 1.8661587509598443 acc: 0.81\n",
      "loss: 1.829752375039883 acc: 0.81\n",
      "loss: 1.8279067089069057 acc: 0.83\n",
      "loss: 1.8528282169304058 acc: 0.78\n",
      "loss: 1.8592647122579506 acc: 0.84\n",
      "loss: 1.8164965587113655 acc: 0.79\n",
      "loss: 1.8301426981191815 acc: 0.77\n",
      "loss: 1.8225826408450216 acc: 0.81\n",
      "loss: 1.8424115878419003 acc: 0.83\n",
      "loss: 1.8098951766759228 acc: 0.85\n",
      "loss: 1.8330875133575113 acc: 0.77\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 1.8331\t Accuracy 0.7700\n",
      "loss: 1.7843125397882544 acc: 0.89\n",
      "loss: 1.829610618354047 acc: 0.79\n",
      "loss: 1.8538292058004262 acc: 0.77\n",
      "loss: 1.8511383262257883 acc: 0.78\n",
      "loss: 1.8124528390152828 acc: 0.87\n",
      "loss: 1.8358963692384245 acc: 0.79\n",
      "loss: 1.8447607836418687 acc: 0.82\n",
      "loss: 1.7965595627688293 acc: 0.87\n",
      "loss: 1.8169567803518742 acc: 0.82\n",
      "loss: 1.8079820382894471 acc: 0.86\n",
      "loss: 1.8623861400536825 acc: 0.75\n",
      "loss: 1.8015886512023287 acc: 0.84\n",
      "loss: 1.8627681826327906 acc: 0.8\n",
      "loss: 1.8236119535401765 acc: 0.91\n",
      "loss: 1.8200477323825461 acc: 0.8\n",
      "loss: 1.806057673509535 acc: 0.86\n",
      "loss: 1.837737153117922 acc: 0.77\n",
      "loss: 1.8733050486894283 acc: 0.77\n",
      "loss: 1.8378748761837582 acc: 0.82\n",
      "loss: 1.8028052201260345 acc: 0.82\n",
      "loss: 1.8437930823500204 acc: 0.81\n",
      "loss: 1.8724630233483972 acc: 0.76\n",
      "loss: 1.7978567126049032 acc: 0.84\n",
      "loss: 1.8429002988744372 acc: 0.86\n",
      "loss: 1.9029472917588464 acc: 0.71\n",
      "loss: 1.8785652496119596 acc: 0.79\n",
      "loss: 1.852178236376234 acc: 0.81\n",
      "loss: 1.8299806890737014 acc: 0.8\n",
      "loss: 1.7924079353814955 acc: 0.86\n",
      "loss: 1.8470442567424823 acc: 0.8\n",
      "loss: 1.8431813106709387 acc: 0.82\n",
      "loss: 1.836355031722314 acc: 0.82\n",
      "loss: 1.8769142828134202 acc: 0.74\n",
      "loss: 1.8177435029932167 acc: 0.83\n",
      "loss: 1.8207814419267172 acc: 0.86\n",
      "loss: 1.8412106334660925 acc: 0.81\n",
      "loss: 1.824673866056252 acc: 0.83\n",
      "loss: 1.8752495095067823 acc: 0.77\n",
      "loss: 1.8594684270925317 acc: 0.8\n",
      "loss: 1.8299611331842556 acc: 0.87\n",
      "loss: 1.83421185171795 acc: 0.77\n",
      "loss: 1.8307993187827467 acc: 0.76\n",
      "loss: 1.8355277622875814 acc: 0.82\n",
      "loss: 1.8397774947132393 acc: 0.82\n",
      "loss: 1.8840314307671935 acc: 0.77\n",
      "loss: 1.8296621760463168 acc: 0.79\n",
      "loss: 1.8188161569381094 acc: 0.84\n",
      "loss: 1.8418849326504647 acc: 0.82\n",
      "loss: 1.8802743009511491 acc: 0.73\n",
      "loss: 1.8518015670127606 acc: 0.78\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 1.8518\t Accuracy 0.7800\n",
      "loss: 1.8472920483221784 acc: 0.82\n",
      "loss: 1.849141134829979 acc: 0.84\n",
      "loss: 1.8442289181401248 acc: 0.79\n",
      "loss: 1.8612370412660062 acc: 0.84\n",
      "loss: 1.8611602263193747 acc: 0.82\n",
      "loss: 1.8541064963822922 acc: 0.81\n",
      "loss: 1.853774415020528 acc: 0.74\n",
      "loss: 1.8650175712363457 acc: 0.79\n",
      "loss: 1.791938868426849 acc: 0.83\n",
      "loss: 1.8529455406237716 acc: 0.83\n",
      "loss: 1.8620395343534468 acc: 0.76\n",
      "loss: 1.8194692503860788 acc: 0.83\n",
      "loss: 1.7864149456819114 acc: 0.86\n",
      "loss: 1.8224200127147083 acc: 0.86\n",
      "loss: 1.8366749203175652 acc: 0.77\n",
      "loss: 1.8521591790001828 acc: 0.79\n",
      "loss: 1.856626222022756 acc: 0.75\n",
      "loss: 1.8734823382460295 acc: 0.79\n",
      "loss: 1.851459258548436 acc: 0.77\n",
      "loss: 1.8590643430202822 acc: 0.79\n",
      "loss: 1.8234283623497185 acc: 0.81\n",
      "loss: 1.8167034101304664 acc: 0.86\n",
      "loss: 1.8266865894162774 acc: 0.83\n",
      "loss: 1.8435295052376788 acc: 0.84\n",
      "loss: 1.828909790637884 acc: 0.82\n",
      "loss: 1.8497678939793403 acc: 0.77\n",
      "loss: 1.8276841606975784 acc: 0.82\n",
      "loss: 1.812315998249196 acc: 0.84\n",
      "loss: 1.8498273359345316 acc: 0.85\n",
      "loss: 1.8324054056351071 acc: 0.81\n",
      "loss: 1.8343181748438355 acc: 0.77\n",
      "loss: 1.8584221041370341 acc: 0.77\n",
      "loss: 1.8231705069321347 acc: 0.82\n",
      "loss: 1.8122325572814517 acc: 0.88\n",
      "loss: 1.8393785680778192 acc: 0.84\n",
      "loss: 1.8897188693298446 acc: 0.81\n",
      "loss: 1.8497793091800352 acc: 0.81\n",
      "loss: 1.8815413446950542 acc: 0.8\n",
      "loss: 1.791489741335614 acc: 0.88\n",
      "loss: 1.8348992735186287 acc: 0.82\n",
      "loss: 1.837671267465201 acc: 0.82\n",
      "loss: 1.8486160974272077 acc: 0.82\n",
      "loss: 1.843743764085574 acc: 0.86\n",
      "loss: 1.8682894372425067 acc: 0.84\n",
      "loss: 1.7716912589842764 acc: 0.91\n",
      "loss: 1.8108787895889449 acc: 0.86\n",
      "loss: 1.8262546175639756 acc: 0.8\n",
      "loss: 1.8475290122255865 acc: 0.76\n",
      "loss: 1.8264620233615125 acc: 0.87\n",
      "loss: 1.863159659276667 acc: 0.76\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 1.8632\t Accuracy 0.7600\n",
      "loss: 1.8461880822656263 acc: 0.85\n",
      "loss: 1.8219955761057005 acc: 0.84\n",
      "loss: 1.8540597758827473 acc: 0.82\n",
      "loss: 1.8136031654170666 acc: 0.81\n",
      "loss: 1.8454661084262631 acc: 0.78\n",
      "loss: 1.8424791183295801 acc: 0.77\n",
      "loss: 1.7962456570220977 acc: 0.88\n",
      "loss: 1.8244694252021785 acc: 0.88\n",
      "loss: 1.8523862732151324 acc: 0.81\n",
      "loss: 1.8140478590103652 acc: 0.79\n",
      "loss: 1.7998002046469181 acc: 0.86\n",
      "loss: 1.85039123027731 acc: 0.79\n",
      "loss: 1.8165163975919967 acc: 0.82\n",
      "loss: 1.8683127963891084 acc: 0.8\n",
      "loss: 1.794250170643078 acc: 0.91\n",
      "loss: 1.8124029493068874 acc: 0.81\n",
      "loss: 1.846327600240992 acc: 0.87\n",
      "loss: 1.7929841039212535 acc: 0.87\n",
      "loss: 1.8686322266779882 acc: 0.76\n",
      "loss: 1.8375615964194723 acc: 0.85\n",
      "loss: 1.8173258607475193 acc: 0.83\n",
      "loss: 1.8140010683933396 acc: 0.85\n",
      "loss: 1.7990924483187154 acc: 0.84\n",
      "loss: 1.820323482205263 acc: 0.82\n",
      "loss: 1.854120908774954 acc: 0.83\n",
      "loss: 1.841826900201506 acc: 0.77\n",
      "loss: 1.8496736974817345 acc: 0.86\n",
      "loss: 1.841780046167579 acc: 0.81\n",
      "loss: 1.8310711329528488 acc: 0.9\n",
      "loss: 1.8376814639585215 acc: 0.84\n",
      "loss: 1.8879424455185048 acc: 0.75\n",
      "loss: 1.8630728408733 acc: 0.76\n",
      "loss: 1.8725653690092532 acc: 0.8\n",
      "loss: 1.851245632594363 acc: 0.82\n",
      "loss: 1.8415811370191781 acc: 0.77\n",
      "loss: 1.852476610382547 acc: 0.83\n",
      "loss: 1.8106657122366232 acc: 0.85\n",
      "loss: 1.8010288712359903 acc: 0.86\n",
      "loss: 1.838078398496366 acc: 0.86\n",
      "loss: 1.8645387296061213 acc: 0.76\n",
      "loss: 1.816933357347256 acc: 0.85\n",
      "loss: 1.812767879748848 acc: 0.88\n",
      "loss: 1.8281949331162597 acc: 0.86\n",
      "loss: 1.8362153273406767 acc: 0.82\n",
      "loss: 1.8245060264732482 acc: 0.84\n",
      "loss: 1.8288431666201383 acc: 0.83\n",
      "loss: 1.8240529070590439 acc: 0.86\n",
      "loss: 1.8712056643644441 acc: 0.79\n",
      "loss: 1.8733860184418858 acc: 0.73\n",
      "loss: 1.8244778380484126 acc: 0.84\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 1.8245\t Accuracy 0.8400\n",
      "loss: 1.8090053509162407 acc: 0.86\n",
      "loss: 1.8195434735225593 acc: 0.81\n",
      "loss: 1.780614288990684 acc: 0.89\n",
      "loss: 1.808020892139941 acc: 0.88\n",
      "loss: 1.802259372323051 acc: 0.84\n",
      "loss: 1.8452383102523149 acc: 0.88\n",
      "loss: 1.8430594289409223 acc: 0.82\n",
      "loss: 1.83231369862003 acc: 0.8\n",
      "loss: 1.84864451955725 acc: 0.82\n",
      "loss: 1.841852742062131 acc: 0.81\n",
      "loss: 1.8695375808244319 acc: 0.82\n",
      "loss: 1.8703205412958992 acc: 0.8\n",
      "loss: 1.8452920657454626 acc: 0.79\n",
      "loss: 1.8618266032241026 acc: 0.86\n",
      "loss: 1.8190873104072802 acc: 0.82\n",
      "loss: 1.8025319907316697 acc: 0.89\n",
      "loss: 1.8257806856906436 acc: 0.83\n",
      "loss: 1.8443234950447869 acc: 0.84\n",
      "loss: 1.8404630618390556 acc: 0.88\n",
      "loss: 1.8453753169199814 acc: 0.87\n",
      "loss: 1.8384087289297213 acc: 0.8\n",
      "loss: 1.8577208582426505 acc: 0.8\n",
      "loss: 1.8246266494177001 acc: 0.83\n",
      "loss: 1.8513646666750359 acc: 0.79\n",
      "loss: 1.8117189319594695 acc: 0.82\n",
      "loss: 1.836537192608157 acc: 0.85\n",
      "loss: 1.8053823426422724 acc: 0.82\n",
      "loss: 1.8597659463409348 acc: 0.8\n",
      "loss: 1.8072252565162148 acc: 0.82\n",
      "loss: 1.7996148380303847 acc: 0.85\n",
      "loss: 1.8195611368750684 acc: 0.78\n",
      "loss: 1.821024397195121 acc: 0.82\n",
      "loss: 1.8239297436194497 acc: 0.83\n",
      "loss: 1.8160930194290532 acc: 0.79\n",
      "loss: 1.8551522337197823 acc: 0.77\n",
      "loss: 1.850746469112388 acc: 0.8\n",
      "loss: 1.8182701684091669 acc: 0.86\n",
      "loss: 1.8456227185157978 acc: 0.82\n",
      "loss: 1.8253987424882139 acc: 0.79\n",
      "loss: 1.8730069492027104 acc: 0.77\n",
      "loss: 1.824083686803737 acc: 0.78\n",
      "loss: 1.8878731747678423 acc: 0.75\n",
      "loss: 1.8384207855516823 acc: 0.8\n",
      "loss: 1.8458471508472079 acc: 0.81\n",
      "loss: 1.8616093092957473 acc: 0.82\n",
      "loss: 1.8279755774629325 acc: 0.84\n",
      "loss: 1.8608887694696654 acc: 0.76\n",
      "loss: 1.8631793032586503 acc: 0.81\n",
      "loss: 1.8070765097177162 acc: 0.86\n",
      "loss: 1.798472802115113 acc: 0.85\n",
      "loss: 1.8010141937798365 acc: 0.87\n",
      "loss: 1.8224971233238458 acc: 0.88\n",
      "loss: 1.8295253421373312 acc: 0.87\n",
      "loss: 1.8343973433751553 acc: 0.89\n",
      "loss: 1.8158766183036632 acc: 0.87\n",
      "loss: 1.7689241871064554 acc: 0.84\n",
      "loss: 1.7736582835504877 acc: 0.84\n",
      "loss: 1.8283231144924992 acc: 0.9\n",
      "loss: 1.7524485309535804 acc: 0.91\n",
      "loss: 1.8057050823554104 acc: 0.89\n",
      "loss: 1.804707162683902 acc: 0.86\n",
      "loss: 1.861222989072262 acc: 0.82\n",
      "loss: 1.87091072601679 acc: 0.84\n",
      "loss: 1.881015463455068 acc: 0.87\n",
      "loss: 1.7966670000245903 acc: 0.94\n",
      "loss: 1.8200891339320497 acc: 0.82\n",
      "loss: 1.7716111511428636 acc: 0.9\n",
      "loss: 1.8001475565821417 acc: 0.86\n",
      "loss: 1.8037600564391183 acc: 0.88\n",
      "loss: 1.8951604091906598 acc: 0.85\n",
      "loss: 1.8225253855549886 acc: 0.86\n",
      "loss: 1.8412302309133886 acc: 0.75\n",
      "loss: 1.8735187540932647 acc: 0.83\n",
      "loss: 1.838527092800546 acc: 0.87\n",
      "loss: 1.8490527604295115 acc: 0.83\n",
      "loss: 1.8654416802850624 acc: 0.75\n",
      "loss: 1.9001068048788263 acc: 0.75\n",
      "loss: 1.8262637269198936 acc: 0.9\n",
      "loss: 1.7841532441662462 acc: 0.9\n",
      "loss: 1.8197634985135351 acc: 0.83\n",
      "loss: 1.7888049502021783 acc: 0.9\n",
      "loss: 1.7559180849611897 acc: 0.92\n",
      "loss: 1.8064633724361305 acc: 0.88\n",
      "loss: 1.7816504449640238 acc: 0.85\n",
      "loss: 1.8165902910912504 acc: 0.89\n",
      "loss: 1.8686026683254848 acc: 0.88\n",
      "loss: 1.8489072043842663 acc: 0.87\n",
      "loss: 1.7506302642280516 acc: 0.9\n",
      "loss: 1.717244822762802 acc: 0.96\n",
      "loss: 1.7460801696300112 acc: 0.93\n",
      "loss: 1.7423868496253692 acc: 0.97\n",
      "loss: 1.8018143108225613 acc: 0.86\n",
      "loss: 1.7889038085048226 acc: 0.83\n",
      "loss: 1.7404041510954198 acc: 0.82\n",
      "loss: 1.8054008100171044 acc: 0.9\n",
      "loss: 1.8120466913385147 acc: 0.92\n",
      "loss: 1.902385978662817 acc: 0.78\n",
      "loss: 1.7047255977150104 acc: 0.94\n",
      "loss: 1.8917799123695032 acc: 0.79\n",
      "\n",
      "Epoch [0]\t Average training loss 1.8595\t Average training accuracy 0.7672\n",
      "Epoch [0]\t Average validation loss 1.8125\t Average validation accuracy 0.8662\n",
      "\n",
      "loss: 1.8542883299567614 acc: 0.85\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 1.8543\t Accuracy 0.8500\n",
      "loss: 1.8467902917993069 acc: 0.84\n",
      "loss: 1.8397452422983809 acc: 0.83\n",
      "loss: 1.8375273809451653 acc: 0.8\n",
      "loss: 1.8087199765653168 acc: 0.88\n",
      "loss: 1.853463341670427 acc: 0.85\n",
      "loss: 1.7846403936124406 acc: 0.88\n",
      "loss: 1.8305800109821682 acc: 0.85\n",
      "loss: 1.8388151234116454 acc: 0.81\n",
      "loss: 1.8456971398527142 acc: 0.77\n",
      "loss: 1.8638166785603396 acc: 0.75\n",
      "loss: 1.8293815662563275 acc: 0.81\n",
      "loss: 1.8024880897469924 acc: 0.92\n",
      "loss: 1.8313418670082486 acc: 0.82\n",
      "loss: 1.7965455439313418 acc: 0.87\n",
      "loss: 1.8228360527652898 acc: 0.81\n",
      "loss: 1.829990022463091 acc: 0.84\n",
      "loss: 1.8634917058370066 acc: 0.8\n",
      "loss: 1.8524275241090786 acc: 0.81\n",
      "loss: 1.8546550217717377 acc: 0.82\n",
      "loss: 1.8158439518778875 acc: 0.87\n",
      "loss: 1.7954385418744685 acc: 0.9\n",
      "loss: 1.864556317153903 acc: 0.79\n",
      "loss: 1.84116182574153 acc: 0.84\n",
      "loss: 1.8730008365813022 acc: 0.77\n",
      "loss: 1.812557953665594 acc: 0.81\n",
      "loss: 1.835673301458442 acc: 0.82\n",
      "loss: 1.8634575939190743 acc: 0.82\n",
      "loss: 1.819091632450826 acc: 0.86\n",
      "loss: 1.832266099689579 acc: 0.83\n",
      "loss: 1.8440039265052233 acc: 0.81\n",
      "loss: 1.8175386167568934 acc: 0.86\n",
      "loss: 1.8733355487546681 acc: 0.82\n",
      "loss: 1.8080292749286797 acc: 0.86\n",
      "loss: 1.791983165182265 acc: 0.85\n",
      "loss: 1.846285739366537 acc: 0.77\n",
      "loss: 1.8004829185712015 acc: 0.86\n",
      "loss: 1.8245323147405847 acc: 0.87\n",
      "loss: 1.7859850217961237 acc: 0.89\n",
      "loss: 1.8428271470525395 acc: 0.83\n",
      "loss: 1.8183692810312748 acc: 0.84\n",
      "loss: 1.7940030979686155 acc: 0.83\n",
      "loss: 1.8001988540662381 acc: 0.79\n",
      "loss: 1.8500959455610937 acc: 0.81\n",
      "loss: 1.8212872213098503 acc: 0.84\n",
      "loss: 1.842690749376471 acc: 0.83\n",
      "loss: 1.8756702566692125 acc: 0.75\n",
      "loss: 1.8360858155319502 acc: 0.78\n",
      "loss: 1.8373993036992087 acc: 0.81\n",
      "loss: 1.8831269342652575 acc: 0.82\n",
      "loss: 1.8416178024214667 acc: 0.83\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 1.8416\t Accuracy 0.8300\n",
      "loss: 1.8236126059310143 acc: 0.82\n",
      "loss: 1.8553081457441665 acc: 0.76\n",
      "loss: 1.8203749987278461 acc: 0.87\n",
      "loss: 1.8292795972700637 acc: 0.86\n",
      "loss: 1.806277075455248 acc: 0.83\n",
      "loss: 1.8175380955876406 acc: 0.8\n",
      "loss: 1.8239445337994855 acc: 0.83\n",
      "loss: 1.8154555059300455 acc: 0.82\n",
      "loss: 1.871942890767655 acc: 0.75\n",
      "loss: 1.8259023328389572 acc: 0.84\n",
      "loss: 1.84088309628313 acc: 0.79\n",
      "loss: 1.8548114329000938 acc: 0.82\n",
      "loss: 1.841126092683638 acc: 0.82\n",
      "loss: 1.8254255847315721 acc: 0.87\n",
      "loss: 1.8475892613197704 acc: 0.81\n",
      "loss: 1.8312577178217992 acc: 0.88\n",
      "loss: 1.8122710302197722 acc: 0.85\n",
      "loss: 1.8274709246057501 acc: 0.83\n",
      "loss: 1.82857484384093 acc: 0.8\n",
      "loss: 1.842873677482447 acc: 0.83\n",
      "loss: 1.8690604156476893 acc: 0.83\n",
      "loss: 1.857790807898612 acc: 0.77\n",
      "loss: 1.8117978932110028 acc: 0.84\n",
      "loss: 1.8287620090600714 acc: 0.87\n",
      "loss: 1.8445990827031722 acc: 0.79\n",
      "loss: 1.8090776132865247 acc: 0.88\n",
      "loss: 1.8263751843346876 acc: 0.84\n",
      "loss: 1.817139747565342 acc: 0.88\n",
      "loss: 1.7913920443920113 acc: 0.92\n",
      "loss: 1.8301961185454183 acc: 0.84\n",
      "loss: 1.833891647967088 acc: 0.84\n",
      "loss: 1.8281544401132666 acc: 0.86\n",
      "loss: 1.8230710381366304 acc: 0.81\n",
      "loss: 1.8019469568004773 acc: 0.83\n",
      "loss: 1.8397411110372113 acc: 0.81\n",
      "loss: 1.8009102140920437 acc: 0.89\n",
      "loss: 1.811321252150242 acc: 0.86\n",
      "loss: 1.8369286882284164 acc: 0.82\n",
      "loss: 1.796507721979852 acc: 0.85\n",
      "loss: 1.8240970766354743 acc: 0.85\n",
      "loss: 1.8427937827175098 acc: 0.8\n",
      "loss: 1.8565869066506158 acc: 0.76\n",
      "loss: 1.833703519963325 acc: 0.84\n",
      "loss: 1.8402303231652757 acc: 0.85\n",
      "loss: 1.810993790631006 acc: 0.87\n",
      "loss: 1.8351739174221515 acc: 0.81\n",
      "loss: 1.8105311426935433 acc: 0.84\n",
      "loss: 1.8278827932009067 acc: 0.79\n",
      "loss: 1.8284116207328316 acc: 0.75\n",
      "loss: 1.8187668264900663 acc: 0.81\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 1.8188\t Accuracy 0.8100\n",
      "loss: 1.7967592192021917 acc: 0.88\n",
      "loss: 1.8016062030171291 acc: 0.86\n",
      "loss: 1.7814733066380297 acc: 0.86\n",
      "loss: 1.9022401992900548 acc: 0.74\n",
      "loss: 1.8499069688882777 acc: 0.84\n",
      "loss: 1.8684338040314026 acc: 0.8\n",
      "loss: 1.8233371844810224 acc: 0.9\n",
      "loss: 1.8353245249725765 acc: 0.85\n",
      "loss: 1.8545926179118744 acc: 0.87\n",
      "loss: 1.8094128051983807 acc: 0.87\n",
      "loss: 1.853122255649269 acc: 0.86\n",
      "loss: 1.813684264807776 acc: 0.85\n",
      "loss: 1.847845796928097 acc: 0.83\n",
      "loss: 1.8366484964135927 acc: 0.82\n",
      "loss: 1.826040564439426 acc: 0.8\n",
      "loss: 1.839067159389201 acc: 0.82\n",
      "loss: 1.8402884432089115 acc: 0.84\n",
      "loss: 1.822115362275659 acc: 0.83\n",
      "loss: 1.821793571655716 acc: 0.83\n",
      "loss: 1.8154611584731741 acc: 0.89\n",
      "loss: 1.8742546460564637 acc: 0.78\n",
      "loss: 1.7993344200559163 acc: 0.85\n",
      "loss: 1.8205887328766182 acc: 0.85\n",
      "loss: 1.8176871404446475 acc: 0.82\n",
      "loss: 1.7797142483022892 acc: 0.86\n",
      "loss: 1.8108177289101015 acc: 0.83\n",
      "loss: 1.8668908944404907 acc: 0.84\n",
      "loss: 1.8556929415526948 acc: 0.76\n",
      "loss: 1.8741959221592905 acc: 0.73\n",
      "loss: 1.8506769649859507 acc: 0.86\n",
      "loss: 1.8642337314541733 acc: 0.82\n",
      "loss: 1.8257192712556165 acc: 0.84\n",
      "loss: 1.8526450828958343 acc: 0.78\n",
      "loss: 1.8389449309060169 acc: 0.78\n",
      "loss: 1.8142534057603614 acc: 0.83\n",
      "loss: 1.843948012296961 acc: 0.8\n",
      "loss: 1.8536792218547422 acc: 0.8\n",
      "loss: 1.852906356594193 acc: 0.78\n",
      "loss: 1.8120057237827887 acc: 0.84\n",
      "loss: 1.8419939618325645 acc: 0.83\n",
      "loss: 1.8123564021025829 acc: 0.82\n",
      "loss: 1.8088390064618087 acc: 0.87\n",
      "loss: 1.7752282856598898 acc: 0.9\n",
      "loss: 1.818643740014556 acc: 0.85\n",
      "loss: 1.846905150786134 acc: 0.85\n",
      "loss: 1.832476824354437 acc: 0.82\n",
      "loss: 1.7834362230054603 acc: 0.91\n",
      "loss: 1.8037572713199614 acc: 0.85\n",
      "loss: 1.8071166992956051 acc: 0.87\n",
      "loss: 1.7880567899897692 acc: 0.87\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 1.7881\t Accuracy 0.8700\n",
      "loss: 1.8083762917624173 acc: 0.84\n",
      "loss: 1.8397789980916084 acc: 0.81\n",
      "loss: 1.8425034566388705 acc: 0.79\n",
      "loss: 1.8378242413409402 acc: 0.87\n",
      "loss: 1.789971333130706 acc: 0.88\n",
      "loss: 1.79494704187241 acc: 0.85\n",
      "loss: 1.826321480736841 acc: 0.83\n",
      "loss: 1.8313976450558218 acc: 0.81\n",
      "loss: 1.8655217739928673 acc: 0.8\n",
      "loss: 1.8221897270644702 acc: 0.83\n",
      "loss: 1.808771909599766 acc: 0.81\n",
      "loss: 1.8225777844477573 acc: 0.85\n",
      "loss: 1.7986857019324827 acc: 0.82\n",
      "loss: 1.876082500448474 acc: 0.75\n",
      "loss: 1.8535420952018151 acc: 0.84\n",
      "loss: 1.8353809773168508 acc: 0.79\n",
      "loss: 1.808120341189457 acc: 0.82\n",
      "loss: 1.8005381461963759 acc: 0.84\n",
      "loss: 1.7847280089043907 acc: 0.89\n",
      "loss: 1.7962185805453863 acc: 0.86\n",
      "loss: 1.8452895541443124 acc: 0.82\n",
      "loss: 1.8499091112533594 acc: 0.78\n",
      "loss: 1.7948006682142335 acc: 0.83\n",
      "loss: 1.8310523417927904 acc: 0.81\n",
      "loss: 1.8255497894811754 acc: 0.82\n",
      "loss: 1.8495612595220448 acc: 0.84\n",
      "loss: 1.854509364062447 acc: 0.82\n",
      "loss: 1.822355285325916 acc: 0.85\n",
      "loss: 1.7737514758281103 acc: 0.9\n",
      "loss: 1.8065565006761473 acc: 0.91\n",
      "loss: 1.846210472882304 acc: 0.77\n",
      "loss: 1.825823325076664 acc: 0.87\n",
      "loss: 1.8169477521808626 acc: 0.83\n",
      "loss: 1.8038475230906215 acc: 0.82\n",
      "loss: 1.8322982805109786 acc: 0.82\n",
      "loss: 1.830313202572561 acc: 0.82\n",
      "loss: 1.8172386803942877 acc: 0.86\n",
      "loss: 1.8182637241757675 acc: 0.85\n",
      "loss: 1.822617885073225 acc: 0.88\n",
      "loss: 1.8087861960026075 acc: 0.84\n",
      "loss: 1.8136075374794935 acc: 0.86\n",
      "loss: 1.851337326697471 acc: 0.76\n",
      "loss: 1.835370168471932 acc: 0.81\n",
      "loss: 1.8279693331598974 acc: 0.86\n",
      "loss: 1.80041234271122 acc: 0.82\n",
      "loss: 1.884810430399388 acc: 0.78\n",
      "loss: 1.832388152412194 acc: 0.85\n",
      "loss: 1.8296193955824223 acc: 0.86\n",
      "loss: 1.8367023599925714 acc: 0.8\n",
      "loss: 1.8669610564889978 acc: 0.81\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 1.8670\t Accuracy 0.8100\n",
      "loss: 1.827463716475699 acc: 0.84\n",
      "loss: 1.8015731809291569 acc: 0.88\n",
      "loss: 1.8163557922275706 acc: 0.87\n",
      "loss: 1.815708168822411 acc: 0.87\n",
      "loss: 1.8262080158579082 acc: 0.82\n",
      "loss: 1.8369496172240485 acc: 0.85\n",
      "loss: 1.8377789087903487 acc: 0.81\n",
      "loss: 1.8295747146836938 acc: 0.87\n",
      "loss: 1.8457396539186532 acc: 0.8\n",
      "loss: 1.8658911087785544 acc: 0.77\n",
      "loss: 1.8693969248722682 acc: 0.82\n",
      "loss: 1.8850089670796695 acc: 0.74\n",
      "loss: 1.7979403376211378 acc: 0.91\n",
      "loss: 1.8436614881757847 acc: 0.84\n",
      "loss: 1.836760196706632 acc: 0.82\n",
      "loss: 1.7916783293544276 acc: 0.84\n",
      "loss: 1.8350755292676117 acc: 0.83\n",
      "loss: 1.8259304246405903 acc: 0.81\n",
      "loss: 1.8248043532887928 acc: 0.8\n",
      "loss: 1.809641764332101 acc: 0.9\n",
      "loss: 1.858261378605027 acc: 0.81\n",
      "loss: 1.8557119024629174 acc: 0.82\n",
      "loss: 1.8173392668572346 acc: 0.82\n",
      "loss: 1.8212089530093132 acc: 0.82\n",
      "loss: 1.8068577272917599 acc: 0.85\n",
      "loss: 1.8220001601698195 acc: 0.87\n",
      "loss: 1.87913048439284 acc: 0.73\n",
      "loss: 1.8932459106223232 acc: 0.76\n",
      "loss: 1.841646461425667 acc: 0.84\n",
      "loss: 1.8448638022705128 acc: 0.82\n",
      "loss: 1.7928611254932778 acc: 0.86\n",
      "loss: 1.8307810422719113 acc: 0.82\n",
      "loss: 1.8455500666829792 acc: 0.82\n",
      "loss: 1.8489618213170198 acc: 0.83\n",
      "loss: 1.8395493062449884 acc: 0.86\n",
      "loss: 1.8343335432773804 acc: 0.83\n",
      "loss: 1.8321430501151317 acc: 0.86\n",
      "loss: 1.9182820971739025 acc: 0.74\n",
      "loss: 1.8320669862233012 acc: 0.9\n",
      "loss: 1.8063971877582425 acc: 0.86\n",
      "loss: 1.8703321892414133 acc: 0.83\n",
      "loss: 1.848985142893081 acc: 0.84\n",
      "loss: 1.8576247834016113 acc: 0.82\n",
      "loss: 1.8223891477883405 acc: 0.9\n",
      "loss: 1.8676434598796998 acc: 0.77\n",
      "loss: 1.7859913226503565 acc: 0.9\n",
      "loss: 1.8008542946443316 acc: 0.9\n",
      "loss: 1.8750136655612186 acc: 0.77\n",
      "loss: 1.8569415016508888 acc: 0.83\n",
      "loss: 1.812824054295979 acc: 0.88\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 1.8128\t Accuracy 0.8800\n",
      "loss: 1.8356276398523659 acc: 0.79\n",
      "loss: 1.788715553985904 acc: 0.86\n",
      "loss: 1.82058111983948 acc: 0.9\n",
      "loss: 1.8005031528093403 acc: 0.82\n",
      "loss: 1.8439796957006755 acc: 0.82\n",
      "loss: 1.8179594458317567 acc: 0.88\n",
      "loss: 1.8483532380345813 acc: 0.87\n",
      "loss: 1.7941375832252893 acc: 0.87\n",
      "loss: 1.8436164396735257 acc: 0.79\n",
      "loss: 1.824869617108003 acc: 0.85\n",
      "loss: 1.8226270481754359 acc: 0.83\n",
      "loss: 1.8164498670889986 acc: 0.85\n",
      "loss: 1.8239656183108455 acc: 0.8\n",
      "loss: 1.7935990030616553 acc: 0.87\n",
      "loss: 1.8184509075540867 acc: 0.86\n",
      "loss: 1.8492989516668021 acc: 0.79\n",
      "loss: 1.7751797973841656 acc: 0.89\n",
      "loss: 1.85453896875579 acc: 0.82\n",
      "loss: 1.8497170419940234 acc: 0.8\n",
      "loss: 1.8962464994812995 acc: 0.72\n",
      "loss: 1.8654997864458647 acc: 0.79\n",
      "loss: 1.8531185435855722 acc: 0.77\n",
      "loss: 1.8268169456272398 acc: 0.84\n",
      "loss: 1.7907531521756763 acc: 0.89\n",
      "loss: 1.8341108302036957 acc: 0.83\n",
      "loss: 1.8266736588942631 acc: 0.84\n",
      "loss: 1.7805583196132466 acc: 0.87\n",
      "loss: 1.8272672541828143 acc: 0.91\n",
      "loss: 1.8194034585265524 acc: 0.88\n",
      "loss: 1.859766968181693 acc: 0.86\n",
      "loss: 1.8268168557317737 acc: 0.81\n",
      "loss: 1.7950358418263945 acc: 0.9\n",
      "loss: 1.8650751811724138 acc: 0.81\n",
      "loss: 1.8735253462944699 acc: 0.74\n",
      "loss: 1.8281622338636327 acc: 0.81\n",
      "loss: 1.8227718273573326 acc: 0.85\n",
      "loss: 1.848615495131011 acc: 0.79\n",
      "loss: 1.8568066581141693 acc: 0.74\n",
      "loss: 1.8506808758624635 acc: 0.78\n",
      "loss: 1.788454895046692 acc: 0.86\n",
      "loss: 1.8552061653896805 acc: 0.79\n",
      "loss: 1.8269911269185426 acc: 0.83\n",
      "loss: 1.819564676413307 acc: 0.83\n",
      "loss: 1.8231944084818008 acc: 0.85\n",
      "loss: 1.8459744404560319 acc: 0.84\n",
      "loss: 1.8596238484957215 acc: 0.77\n",
      "loss: 1.8300535303498615 acc: 0.87\n",
      "loss: 1.8180145709976967 acc: 0.84\n",
      "loss: 1.847193692039804 acc: 0.81\n",
      "loss: 1.8270639315507997 acc: 0.83\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 1.8271\t Accuracy 0.8300\n",
      "loss: 1.8155448811543966 acc: 0.86\n",
      "loss: 1.7974781148168615 acc: 0.86\n",
      "loss: 1.8431948357211987 acc: 0.79\n",
      "loss: 1.8310686905628901 acc: 0.86\n",
      "loss: 1.7983516517547184 acc: 0.87\n",
      "loss: 1.8587999346525361 acc: 0.81\n",
      "loss: 1.8267618217264459 acc: 0.9\n",
      "loss: 1.8502276074480295 acc: 0.81\n",
      "loss: 1.7934657980220694 acc: 0.89\n",
      "loss: 1.8364696884254128 acc: 0.8\n",
      "loss: 1.8478926247982086 acc: 0.88\n",
      "loss: 1.801583610210177 acc: 0.85\n",
      "loss: 1.8240186049836695 acc: 0.81\n",
      "loss: 1.7942830250411683 acc: 0.84\n",
      "loss: 1.8499211782217493 acc: 0.81\n",
      "loss: 1.8612259881738902 acc: 0.84\n",
      "loss: 1.8596806826559285 acc: 0.87\n",
      "loss: 1.862828191151289 acc: 0.77\n",
      "loss: 1.8657627952095481 acc: 0.76\n",
      "loss: 1.8135668729793315 acc: 0.83\n",
      "loss: 1.8007054991490852 acc: 0.8\n",
      "loss: 1.8216954355089487 acc: 0.87\n",
      "loss: 1.797079692238366 acc: 0.83\n",
      "loss: 1.8215734495635068 acc: 0.83\n",
      "loss: 1.8430993978311028 acc: 0.84\n",
      "loss: 1.8868419836823218 acc: 0.75\n",
      "loss: 1.8202680416542143 acc: 0.87\n",
      "loss: 1.8634431939339704 acc: 0.8\n",
      "loss: 1.8465525392830366 acc: 0.8\n",
      "loss: 1.7945277752810256 acc: 0.78\n",
      "loss: 1.821636261711117 acc: 0.84\n",
      "loss: 1.7991505180388787 acc: 0.83\n",
      "loss: 1.8280734633937066 acc: 0.86\n",
      "loss: 1.8448942042804866 acc: 0.8\n",
      "loss: 1.8922538886003302 acc: 0.78\n",
      "loss: 1.8571493008744655 acc: 0.8\n",
      "loss: 1.8265659961650784 acc: 0.82\n",
      "loss: 1.7740126239642044 acc: 0.88\n",
      "loss: 1.7895359759409621 acc: 0.88\n",
      "loss: 1.876673068049995 acc: 0.75\n",
      "loss: 1.8370104904816438 acc: 0.81\n",
      "loss: 1.8346886464012773 acc: 0.85\n",
      "loss: 1.8201552344734073 acc: 0.88\n",
      "loss: 1.8678925929128394 acc: 0.77\n",
      "loss: 1.8137388298798742 acc: 0.84\n",
      "loss: 1.83516833048963 acc: 0.85\n",
      "loss: 1.8615132358743522 acc: 0.81\n",
      "loss: 1.8708613156525329 acc: 0.8\n",
      "loss: 1.8724876513744615 acc: 0.78\n",
      "loss: 1.7947420454441696 acc: 0.92\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 1.7947\t Accuracy 0.9200\n",
      "loss: 1.8778030832321722 acc: 0.76\n",
      "loss: 1.8145305056457948 acc: 0.85\n",
      "loss: 1.8116456663946239 acc: 0.84\n",
      "loss: 1.8031051976390315 acc: 0.86\n",
      "loss: 1.8069858246180823 acc: 0.88\n",
      "loss: 1.7922889160602649 acc: 0.84\n",
      "loss: 1.874374322115096 acc: 0.81\n",
      "loss: 1.8376954061819206 acc: 0.86\n",
      "loss: 1.8431700758367284 acc: 0.84\n",
      "loss: 1.8828184244982313 acc: 0.78\n",
      "loss: 1.853753985119483 acc: 0.76\n",
      "loss: 1.8194446691343513 acc: 0.89\n",
      "loss: 1.8820186790219304 acc: 0.77\n",
      "loss: 1.8354732360479087 acc: 0.82\n",
      "loss: 1.8397369694827697 acc: 0.83\n",
      "loss: 1.8171061326972688 acc: 0.83\n",
      "loss: 1.8362242861357623 acc: 0.84\n",
      "loss: 1.8013691244653476 acc: 0.85\n",
      "loss: 1.8685399444152757 acc: 0.81\n",
      "loss: 1.8261913093460798 acc: 0.85\n",
      "loss: 1.8817104704598617 acc: 0.75\n",
      "loss: 1.8347361413121905 acc: 0.82\n",
      "loss: 1.8568656353610242 acc: 0.82\n",
      "loss: 1.8080142940659867 acc: 0.83\n",
      "loss: 1.8038956517656235 acc: 0.92\n",
      "loss: 1.8589067444752978 acc: 0.79\n",
      "loss: 1.8438108739428294 acc: 0.83\n",
      "loss: 1.8101477616177422 acc: 0.88\n",
      "loss: 1.844565831426186 acc: 0.84\n",
      "loss: 1.8381918723016688 acc: 0.82\n",
      "loss: 1.8500075243094187 acc: 0.78\n",
      "loss: 1.7763103013372932 acc: 0.85\n",
      "loss: 1.7908573693883658 acc: 0.87\n",
      "loss: 1.8420788122336085 acc: 0.81\n",
      "loss: 1.7999120685519017 acc: 0.84\n",
      "loss: 1.7916768329651203 acc: 0.86\n",
      "loss: 1.8203011149563955 acc: 0.85\n",
      "loss: 1.8267311452649415 acc: 0.82\n",
      "loss: 1.848321577891164 acc: 0.76\n",
      "loss: 1.8534249162663103 acc: 0.8\n",
      "loss: 1.8642634814111587 acc: 0.8\n",
      "loss: 1.7999248569233972 acc: 0.84\n",
      "loss: 1.8309838782399255 acc: 0.83\n",
      "loss: 1.8251913609388577 acc: 0.82\n",
      "loss: 1.806430925748285 acc: 0.81\n",
      "loss: 1.8524584584004986 acc: 0.83\n",
      "loss: 1.8584616064105381 acc: 0.88\n",
      "loss: 1.8229085661351165 acc: 0.81\n",
      "loss: 1.8436781075020687 acc: 0.84\n",
      "loss: 1.848777150009918 acc: 0.82\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 1.8488\t Accuracy 0.8200\n",
      "loss: 1.8603514400437362 acc: 0.82\n",
      "loss: 1.8267989616763496 acc: 0.76\n",
      "loss: 1.8080036875168037 acc: 0.89\n",
      "loss: 1.8371702702955925 acc: 0.85\n",
      "loss: 1.8509051171443571 acc: 0.82\n",
      "loss: 1.8231698114277979 acc: 0.84\n",
      "loss: 1.8806290563932806 acc: 0.78\n",
      "loss: 1.8324969846314665 acc: 0.84\n",
      "loss: 1.8204462468870366 acc: 0.79\n",
      "loss: 1.811436404925436 acc: 0.86\n",
      "loss: 1.8818933563506435 acc: 0.78\n",
      "loss: 1.8353944367636308 acc: 0.84\n",
      "loss: 1.8424063169241187 acc: 0.8\n",
      "loss: 1.7954923490960342 acc: 0.86\n",
      "loss: 1.8132481004742274 acc: 0.83\n",
      "loss: 1.8610338528448935 acc: 0.78\n",
      "loss: 1.8056520119800217 acc: 0.86\n",
      "loss: 1.8581627715053686 acc: 0.84\n",
      "loss: 1.8392681363040546 acc: 0.89\n",
      "loss: 1.819422814457318 acc: 0.81\n",
      "loss: 1.87094974748306 acc: 0.8\n",
      "loss: 1.8656518349678364 acc: 0.8\n",
      "loss: 1.7820290930511118 acc: 0.91\n",
      "loss: 1.8318373633592204 acc: 0.83\n",
      "loss: 1.8135833433482924 acc: 0.79\n",
      "loss: 1.8938111541459124 acc: 0.8\n",
      "loss: 1.8112405058279775 acc: 0.89\n",
      "loss: 1.8267754004495445 acc: 0.84\n",
      "loss: 1.7953430676187327 acc: 0.89\n",
      "loss: 1.8497237379789688 acc: 0.79\n",
      "loss: 1.904692469261298 acc: 0.65\n",
      "loss: 1.8144127584938392 acc: 0.82\n",
      "loss: 1.8220286929464438 acc: 0.81\n",
      "loss: 1.8442843497198067 acc: 0.79\n",
      "loss: 1.8492435630984363 acc: 0.8\n",
      "loss: 1.8585723437995614 acc: 0.85\n",
      "loss: 1.82195966532063 acc: 0.83\n",
      "loss: 1.8160520679076575 acc: 0.87\n",
      "loss: 1.8332442271406535 acc: 0.88\n",
      "loss: 1.8161365796227285 acc: 0.9\n",
      "loss: 1.8358444925109916 acc: 0.84\n",
      "loss: 1.8340531210631372 acc: 0.8\n",
      "loss: 1.8433878093202125 acc: 0.85\n",
      "loss: 1.8571950811429738 acc: 0.81\n",
      "loss: 1.833195044286792 acc: 0.8\n",
      "loss: 1.812654379979805 acc: 0.86\n",
      "loss: 1.8072928494375382 acc: 0.9\n",
      "loss: 1.812100703823594 acc: 0.85\n",
      "loss: 1.8076142108025763 acc: 0.88\n",
      "loss: 1.8798963466341307 acc: 0.79\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 1.8799\t Accuracy 0.7900\n",
      "loss: 1.8527625069292482 acc: 0.8\n",
      "loss: 1.8181988266447504 acc: 0.87\n",
      "loss: 1.832723182525864 acc: 0.82\n",
      "loss: 1.820416222444978 acc: 0.81\n",
      "loss: 1.813978979157251 acc: 0.81\n",
      "loss: 1.8281509490283867 acc: 0.79\n",
      "loss: 1.7880300214659706 acc: 0.92\n",
      "loss: 1.800794787731607 acc: 0.9\n",
      "loss: 1.792240394426518 acc: 0.91\n",
      "loss: 1.8326709679203368 acc: 0.82\n",
      "loss: 1.8103573181238093 acc: 0.89\n",
      "loss: 1.7825198105964548 acc: 0.92\n",
      "loss: 1.843146374638537 acc: 0.86\n",
      "loss: 1.8447710568349676 acc: 0.78\n",
      "loss: 1.8110518966497835 acc: 0.9\n",
      "loss: 1.8283378674235096 acc: 0.81\n",
      "loss: 1.8175195690542847 acc: 0.83\n",
      "loss: 1.8204529704113654 acc: 0.82\n",
      "loss: 1.8078623241164973 acc: 0.88\n",
      "loss: 1.814431340456607 acc: 0.87\n",
      "loss: 1.7855428015648371 acc: 0.87\n",
      "loss: 1.802703758081646 acc: 0.89\n",
      "loss: 1.8655470361325037 acc: 0.79\n",
      "loss: 1.8024127886064278 acc: 0.86\n",
      "loss: 1.799088256827101 acc: 0.85\n",
      "loss: 1.7781177444817615 acc: 0.89\n",
      "loss: 1.842652128447618 acc: 0.86\n",
      "loss: 1.8294451797175728 acc: 0.84\n",
      "loss: 1.8426854996642579 acc: 0.75\n",
      "loss: 1.8437223813353927 acc: 0.84\n",
      "loss: 1.834347693637345 acc: 0.76\n",
      "loss: 1.7974929723402153 acc: 0.86\n",
      "loss: 1.8372947115172011 acc: 0.85\n",
      "loss: 1.7982559077869977 acc: 0.87\n",
      "loss: 1.8094533372563135 acc: 0.9\n",
      "loss: 1.8192459805594239 acc: 0.87\n",
      "loss: 1.8479153824466987 acc: 0.78\n",
      "loss: 1.838190511468826 acc: 0.77\n",
      "loss: 1.854465663126473 acc: 0.79\n",
      "loss: 1.8167646295113635 acc: 0.85\n",
      "loss: 1.8510418826837005 acc: 0.81\n",
      "loss: 1.7912514331469458 acc: 0.87\n",
      "loss: 1.7861768194596421 acc: 0.9\n",
      "loss: 1.8251317564568843 acc: 0.84\n",
      "loss: 1.807582552321128 acc: 0.9\n",
      "loss: 1.842339549265744 acc: 0.84\n",
      "loss: 1.858176066466959 acc: 0.8\n",
      "loss: 1.8208913438626972 acc: 0.85\n",
      "loss: 1.8416923801700162 acc: 0.79\n",
      "loss: 1.8676440306191473 acc: 0.78\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 1.8676\t Accuracy 0.7800\n",
      "loss: 1.8314080339412755 acc: 0.85\n",
      "loss: 1.835961585730448 acc: 0.86\n",
      "loss: 1.816516676120709 acc: 0.92\n",
      "loss: 1.8571112701212034 acc: 0.89\n",
      "loss: 1.8010202926480487 acc: 0.84\n",
      "loss: 1.8039177449953294 acc: 0.88\n",
      "loss: 1.86692717282694 acc: 0.77\n",
      "loss: 1.8380903579271635 acc: 0.86\n",
      "loss: 1.814194062997894 acc: 0.82\n",
      "loss: 1.8061129836391516 acc: 0.84\n",
      "loss: 1.8017054411691706 acc: 0.87\n",
      "loss: 1.8189553599707102 acc: 0.86\n",
      "loss: 1.7971741640765289 acc: 0.88\n",
      "loss: 1.8853010692921517 acc: 0.77\n",
      "loss: 1.8618030046986038 acc: 0.81\n",
      "loss: 1.8492288192011563 acc: 0.8\n",
      "loss: 1.8277730054719563 acc: 0.82\n",
      "loss: 1.8285575190819627 acc: 0.84\n",
      "loss: 1.791189383176014 acc: 0.89\n",
      "loss: 1.8404276176809014 acc: 0.87\n",
      "loss: 1.8350693465763672 acc: 0.86\n",
      "loss: 1.8277006923975088 acc: 0.86\n",
      "loss: 1.835177293006675 acc: 0.82\n",
      "loss: 1.81939657373079 acc: 0.87\n",
      "loss: 1.8570227342457628 acc: 0.82\n",
      "loss: 1.8258490787874433 acc: 0.89\n",
      "loss: 1.8436328209436126 acc: 0.85\n",
      "loss: 1.8096607782116478 acc: 0.84\n",
      "loss: 1.8355774464109313 acc: 0.89\n",
      "loss: 1.842037690477192 acc: 0.81\n",
      "loss: 1.7469469024827038 acc: 0.91\n",
      "loss: 1.7782139209602479 acc: 0.91\n",
      "loss: 1.8424299874450054 acc: 0.76\n",
      "loss: 1.8327019950152483 acc: 0.8\n",
      "loss: 1.8210273521101104 acc: 0.83\n",
      "loss: 1.8169108180938642 acc: 0.85\n",
      "loss: 1.8229667824339737 acc: 0.84\n",
      "loss: 1.7887389724881817 acc: 0.89\n",
      "loss: 1.8106764611880166 acc: 0.83\n",
      "loss: 1.810839952060607 acc: 0.87\n",
      "loss: 1.8656201600455242 acc: 0.83\n",
      "loss: 1.8030526810679854 acc: 0.91\n",
      "loss: 1.7792397171517396 acc: 0.92\n",
      "loss: 1.794660189041816 acc: 0.88\n",
      "loss: 1.8441658800537726 acc: 0.82\n",
      "loss: 1.8141805654161476 acc: 0.84\n",
      "loss: 1.8086163651510534 acc: 0.86\n",
      "loss: 1.819804966286697 acc: 0.8\n",
      "loss: 1.8434607306471793 acc: 0.83\n",
      "loss: 1.7932204307611617 acc: 0.86\n",
      "loss: 1.7991641555606324 acc: 0.87\n",
      "loss: 1.8087921666455282 acc: 0.88\n",
      "loss: 1.8347844633792303 acc: 0.87\n",
      "loss: 1.8307259865590768 acc: 0.89\n",
      "loss: 1.80325849460969 acc: 0.85\n",
      "loss: 1.760892201795062 acc: 0.86\n",
      "loss: 1.7631575387536076 acc: 0.84\n",
      "loss: 1.8268498113384748 acc: 0.85\n",
      "loss: 1.7566205392764085 acc: 0.9\n",
      "loss: 1.8007333337215903 acc: 0.87\n",
      "loss: 1.7989917118990126 acc: 0.83\n",
      "loss: 1.8507498511505276 acc: 0.82\n",
      "loss: 1.864450661470517 acc: 0.83\n",
      "loss: 1.8719940352729194 acc: 0.86\n",
      "loss: 1.7938037644245604 acc: 0.91\n",
      "loss: 1.810868975749553 acc: 0.83\n",
      "loss: 1.763183116321912 acc: 0.91\n",
      "loss: 1.784215443933583 acc: 0.88\n",
      "loss: 1.7992816896287764 acc: 0.88\n",
      "loss: 1.877760997997497 acc: 0.85\n",
      "loss: 1.8317145768430114 acc: 0.88\n",
      "loss: 1.8200340144501395 acc: 0.75\n",
      "loss: 1.8634992520616924 acc: 0.86\n",
      "loss: 1.8314527481530205 acc: 0.88\n",
      "loss: 1.8521097911407511 acc: 0.79\n",
      "loss: 1.8689113584442472 acc: 0.77\n",
      "loss: 1.8829733748790993 acc: 0.78\n",
      "loss: 1.8293638986404988 acc: 0.9\n",
      "loss: 1.7987162973137807 acc: 0.88\n",
      "loss: 1.8221055515725961 acc: 0.84\n",
      "loss: 1.781211082989894 acc: 0.92\n",
      "loss: 1.7527752929355436 acc: 0.9\n",
      "loss: 1.7894509548561988 acc: 0.88\n",
      "loss: 1.7891052553824847 acc: 0.85\n",
      "loss: 1.8205888222095998 acc: 0.89\n",
      "loss: 1.8581617538896509 acc: 0.9\n",
      "loss: 1.846853341200335 acc: 0.84\n",
      "loss: 1.7317621612764384 acc: 0.89\n",
      "loss: 1.7152855649505054 acc: 0.97\n",
      "loss: 1.7468785833774394 acc: 0.96\n",
      "loss: 1.7510033223546824 acc: 0.96\n",
      "loss: 1.8022285195319363 acc: 0.89\n",
      "loss: 1.7905253574188733 acc: 0.84\n",
      "loss: 1.732581179716395 acc: 0.83\n",
      "loss: 1.8041769849811744 acc: 0.93\n",
      "loss: 1.8067596041709808 acc: 0.92\n",
      "loss: 1.9007564775871313 acc: 0.81\n",
      "loss: 1.709985675684828 acc: 0.95\n",
      "loss: 1.861717936144076 acc: 0.85\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8300\t Average training accuracy 0.8331\n",
      "Epoch [1]\t Average validation loss 1.8077\t Average validation accuracy 0.8690\n",
      "\n",
      "loss: 1.8862406409749475 acc: 0.75\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 1.8862\t Accuracy 0.7500\n",
      "loss: 1.802260623414609 acc: 0.89\n",
      "loss: 1.8079921290971155 acc: 0.84\n",
      "loss: 1.8047169253867943 acc: 0.91\n",
      "loss: 1.8408095197832761 acc: 0.83\n",
      "loss: 1.8213633460610579 acc: 0.82\n",
      "loss: 1.8439094342874023 acc: 0.84\n",
      "loss: 1.8283675948639675 acc: 0.83\n",
      "loss: 1.8080013984346903 acc: 0.86\n",
      "loss: 1.8884368730952317 acc: 0.77\n",
      "loss: 1.8107288419692187 acc: 0.84\n",
      "loss: 1.794448378822799 acc: 0.85\n",
      "loss: 1.8359635249273787 acc: 0.85\n",
      "loss: 1.8053169561185056 acc: 0.91\n",
      "loss: 1.7995355427277375 acc: 0.89\n",
      "loss: 1.808958004220749 acc: 0.85\n",
      "loss: 1.8206927758087925 acc: 0.87\n",
      "loss: 1.8430948619977932 acc: 0.83\n",
      "loss: 1.8218379116452184 acc: 0.85\n",
      "loss: 1.8241851659760062 acc: 0.83\n",
      "loss: 1.8062444412018726 acc: 0.91\n",
      "loss: 1.829068492833008 acc: 0.86\n",
      "loss: 1.8202509244607052 acc: 0.86\n",
      "loss: 1.8420082525050179 acc: 0.81\n",
      "loss: 1.835252905513926 acc: 0.81\n",
      "loss: 1.8253312227282406 acc: 0.86\n",
      "loss: 1.7969146715794353 acc: 0.87\n",
      "loss: 1.8002122452661267 acc: 0.82\n",
      "loss: 1.8029450369909634 acc: 0.89\n",
      "loss: 1.8322492887734478 acc: 0.79\n",
      "loss: 1.8413314133434044 acc: 0.88\n",
      "loss: 1.8407807110187604 acc: 0.82\n",
      "loss: 1.836289915146132 acc: 0.85\n",
      "loss: 1.8263335125168418 acc: 0.86\n",
      "loss: 1.8568632503994416 acc: 0.83\n",
      "loss: 1.8010047560693507 acc: 0.88\n",
      "loss: 1.8696355487811496 acc: 0.84\n",
      "loss: 1.842630789450788 acc: 0.8\n",
      "loss: 1.7744598572810226 acc: 0.85\n",
      "loss: 1.8482873652954583 acc: 0.83\n",
      "loss: 1.8220266713097037 acc: 0.82\n",
      "loss: 1.8293582967862205 acc: 0.87\n",
      "loss: 1.8070938765192597 acc: 0.9\n",
      "loss: 1.777591275796297 acc: 0.92\n",
      "loss: 1.8351702689449363 acc: 0.84\n",
      "loss: 1.8442455347055604 acc: 0.82\n",
      "loss: 1.8131268118019568 acc: 0.86\n",
      "loss: 1.8467203852562961 acc: 0.79\n",
      "loss: 1.8579456091863933 acc: 0.81\n",
      "loss: 1.8196214653537799 acc: 0.86\n",
      "loss: 1.8390729818750182 acc: 0.81\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 1.8391\t Accuracy 0.8100\n",
      "loss: 1.817862568277432 acc: 0.9\n",
      "loss: 1.7991242768169697 acc: 0.82\n",
      "loss: 1.8087713346295766 acc: 0.83\n",
      "loss: 1.7875884163464377 acc: 0.86\n",
      "loss: 1.8374934499622009 acc: 0.77\n",
      "loss: 1.831692452123233 acc: 0.83\n",
      "loss: 1.84436068267883 acc: 0.85\n",
      "loss: 1.8139258912845246 acc: 0.84\n",
      "loss: 1.8683161210617119 acc: 0.84\n",
      "loss: 1.8412084520601513 acc: 0.86\n",
      "loss: 1.8053198981149785 acc: 0.86\n",
      "loss: 1.8189091388561716 acc: 0.86\n",
      "loss: 1.8514821055588198 acc: 0.79\n",
      "loss: 1.8078402357189283 acc: 0.88\n",
      "loss: 1.7854035403828896 acc: 0.87\n",
      "loss: 1.8065757079728884 acc: 0.86\n",
      "loss: 1.8508849162598353 acc: 0.81\n",
      "loss: 1.8139463976050774 acc: 0.82\n",
      "loss: 1.813169695021895 acc: 0.83\n",
      "loss: 1.8569625246301382 acc: 0.8\n",
      "loss: 1.8249254700071917 acc: 0.83\n",
      "loss: 1.8231339775972217 acc: 0.88\n",
      "loss: 1.848127470499165 acc: 0.87\n",
      "loss: 1.7768584480846088 acc: 0.89\n",
      "loss: 1.8295181595935133 acc: 0.84\n",
      "loss: 1.8202755573087408 acc: 0.82\n",
      "loss: 1.8121023774493004 acc: 0.88\n",
      "loss: 1.7924299797842158 acc: 0.84\n",
      "loss: 1.8392673108953759 acc: 0.81\n",
      "loss: 1.7510752462869932 acc: 0.9\n",
      "loss: 1.7916846226636816 acc: 0.89\n",
      "loss: 1.8127807076493283 acc: 0.81\n",
      "loss: 1.8153536393195915 acc: 0.84\n",
      "loss: 1.8066678531971605 acc: 0.89\n",
      "loss: 1.81940148715584 acc: 0.81\n",
      "loss: 1.8523672800022672 acc: 0.81\n",
      "loss: 1.8456063002263958 acc: 0.82\n",
      "loss: 1.8151048516627855 acc: 0.9\n",
      "loss: 1.8391505118834914 acc: 0.82\n",
      "loss: 1.8373068542267057 acc: 0.81\n",
      "loss: 1.788622121218026 acc: 0.84\n",
      "loss: 1.8516915017561768 acc: 0.81\n",
      "loss: 1.8003684266561413 acc: 0.85\n",
      "loss: 1.799212474640006 acc: 0.86\n",
      "loss: 1.8122327407815524 acc: 0.85\n",
      "loss: 1.8473667261118607 acc: 0.8\n",
      "loss: 1.853239611508257 acc: 0.82\n",
      "loss: 1.8428402888749187 acc: 0.81\n",
      "loss: 1.8350865588238332 acc: 0.81\n",
      "loss: 1.8175536757421649 acc: 0.83\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 1.8176\t Accuracy 0.8300\n",
      "loss: 1.8196551967914478 acc: 0.84\n",
      "loss: 1.8660128735149966 acc: 0.77\n",
      "loss: 1.8257076098974392 acc: 0.84\n",
      "loss: 1.8185467209992263 acc: 0.82\n",
      "loss: 1.8485452348290474 acc: 0.81\n",
      "loss: 1.824094270262629 acc: 0.81\n",
      "loss: 1.8100742289678726 acc: 0.84\n",
      "loss: 1.7858752085497307 acc: 0.91\n",
      "loss: 1.80251934318639 acc: 0.87\n",
      "loss: 1.8804730806960481 acc: 0.78\n",
      "loss: 1.8345679820337528 acc: 0.85\n",
      "loss: 1.8318627545279031 acc: 0.85\n",
      "loss: 1.8026337484771673 acc: 0.83\n",
      "loss: 1.8148853108058205 acc: 0.86\n",
      "loss: 1.8419103086220767 acc: 0.88\n",
      "loss: 1.827859153957139 acc: 0.81\n",
      "loss: 1.794258452720755 acc: 0.85\n",
      "loss: 1.8372797811730046 acc: 0.81\n",
      "loss: 1.8357698225705312 acc: 0.85\n",
      "loss: 1.8045996601385932 acc: 0.92\n",
      "loss: 1.790249038528184 acc: 0.85\n",
      "loss: 1.837157079264902 acc: 0.83\n",
      "loss: 1.8368862413701894 acc: 0.82\n",
      "loss: 1.8537642018774272 acc: 0.79\n",
      "loss: 1.8220248232968117 acc: 0.84\n",
      "loss: 1.8125644872171716 acc: 0.84\n",
      "loss: 1.8178345606736008 acc: 0.88\n",
      "loss: 1.8184477104542964 acc: 0.82\n",
      "loss: 1.8175555094697455 acc: 0.82\n",
      "loss: 1.830476889648861 acc: 0.87\n",
      "loss: 1.8157796370830428 acc: 0.91\n",
      "loss: 1.8540783241111194 acc: 0.8\n",
      "loss: 1.826849067615517 acc: 0.84\n",
      "loss: 1.8229181403680759 acc: 0.85\n",
      "loss: 1.849581898347102 acc: 0.83\n",
      "loss: 1.834292807391516 acc: 0.77\n",
      "loss: 1.8465909400952443 acc: 0.86\n",
      "loss: 1.824813782864711 acc: 0.86\n",
      "loss: 1.801971089941569 acc: 0.88\n",
      "loss: 1.8431145574899332 acc: 0.84\n",
      "loss: 1.7880165050695231 acc: 0.87\n",
      "loss: 1.7732113139610408 acc: 0.88\n",
      "loss: 1.8146933889160142 acc: 0.8\n",
      "loss: 1.825710760615963 acc: 0.84\n",
      "loss: 1.8838094643448966 acc: 0.75\n",
      "loss: 1.834000434838037 acc: 0.86\n",
      "loss: 1.8260883603181102 acc: 0.85\n",
      "loss: 1.8378660692858468 acc: 0.79\n",
      "loss: 1.8158919071946764 acc: 0.79\n",
      "loss: 1.8213372775510772 acc: 0.8\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 1.8213\t Accuracy 0.8000\n",
      "loss: 1.8210084567537006 acc: 0.84\n",
      "loss: 1.844219675898143 acc: 0.83\n",
      "loss: 1.7831981695178447 acc: 0.88\n",
      "loss: 1.824471088833335 acc: 0.85\n",
      "loss: 1.8182997506398282 acc: 0.88\n",
      "loss: 1.8195878029698271 acc: 0.87\n",
      "loss: 1.8017679074683999 acc: 0.81\n",
      "loss: 1.815482842191791 acc: 0.86\n",
      "loss: 1.8314552684009973 acc: 0.85\n",
      "loss: 1.8236858779974368 acc: 0.85\n",
      "loss: 1.80827706130799 acc: 0.87\n",
      "loss: 1.8335021658805362 acc: 0.83\n",
      "loss: 1.85242337669158 acc: 0.77\n",
      "loss: 1.805690729821957 acc: 0.84\n",
      "loss: 1.8429898414182782 acc: 0.8\n",
      "loss: 1.849701015318939 acc: 0.75\n",
      "loss: 1.8302853815174691 acc: 0.83\n",
      "loss: 1.807525445127156 acc: 0.87\n",
      "loss: 1.8123611259718546 acc: 0.92\n",
      "loss: 1.791047606716115 acc: 0.88\n",
      "loss: 1.7972555066866815 acc: 0.89\n",
      "loss: 1.8417334979801447 acc: 0.81\n",
      "loss: 1.8464440204219956 acc: 0.77\n",
      "loss: 1.8179089501182386 acc: 0.9\n",
      "loss: 1.8153626871999915 acc: 0.89\n",
      "loss: 1.8168633540678534 acc: 0.88\n",
      "loss: 1.8305804632253921 acc: 0.78\n",
      "loss: 1.821522989925663 acc: 0.78\n",
      "loss: 1.760262577466162 acc: 0.87\n",
      "loss: 1.821139139731786 acc: 0.8\n",
      "loss: 1.7992258847038967 acc: 0.83\n",
      "loss: 1.797013371627036 acc: 0.81\n",
      "loss: 1.784615342767301 acc: 0.87\n",
      "loss: 1.8537703905319034 acc: 0.81\n",
      "loss: 1.8258492673519942 acc: 0.81\n",
      "loss: 1.8428022866488782 acc: 0.81\n",
      "loss: 1.7923190374990086 acc: 0.89\n",
      "loss: 1.8700457331050035 acc: 0.76\n",
      "loss: 1.866012227611632 acc: 0.84\n",
      "loss: 1.8406708592095082 acc: 0.8\n",
      "loss: 1.8381071608619686 acc: 0.83\n",
      "loss: 1.7816862009782313 acc: 0.87\n",
      "loss: 1.841584054440261 acc: 0.85\n",
      "loss: 1.841045439877454 acc: 0.83\n",
      "loss: 1.8648782956931402 acc: 0.77\n",
      "loss: 1.7858060530844477 acc: 0.87\n",
      "loss: 1.8453571539119915 acc: 0.85\n",
      "loss: 1.824107697176037 acc: 0.87\n",
      "loss: 1.802362354466155 acc: 0.89\n",
      "loss: 1.795513301727636 acc: 0.84\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 1.7955\t Accuracy 0.8400\n",
      "loss: 1.8043956506616243 acc: 0.88\n",
      "loss: 1.8200635614801133 acc: 0.88\n",
      "loss: 1.8735172704516896 acc: 0.76\n",
      "loss: 1.8332092000165148 acc: 0.85\n",
      "loss: 1.832392476194991 acc: 0.84\n",
      "loss: 1.8211190727786672 acc: 0.89\n",
      "loss: 1.7836167576635629 acc: 0.92\n",
      "loss: 1.8265061586890912 acc: 0.87\n",
      "loss: 1.798688599013262 acc: 0.91\n",
      "loss: 1.7873980831283771 acc: 0.87\n",
      "loss: 1.822134048954959 acc: 0.86\n",
      "loss: 1.837593187658215 acc: 0.87\n",
      "loss: 1.8236980666135634 acc: 0.82\n",
      "loss: 1.8318061032558048 acc: 0.84\n",
      "loss: 1.8030942110014263 acc: 0.84\n",
      "loss: 1.8187130714063005 acc: 0.88\n",
      "loss: 1.8038122237284608 acc: 0.85\n",
      "loss: 1.8487058664177178 acc: 0.83\n",
      "loss: 1.8228437183973296 acc: 0.82\n",
      "loss: 1.844153872883654 acc: 0.75\n",
      "loss: 1.7921907039886766 acc: 0.9\n",
      "loss: 1.8463982596283812 acc: 0.77\n",
      "loss: 1.7805714917490156 acc: 0.89\n",
      "loss: 1.8516512566842345 acc: 0.82\n",
      "loss: 1.841983409945237 acc: 0.84\n",
      "loss: 1.834825596116501 acc: 0.85\n",
      "loss: 1.8120039619675004 acc: 0.91\n",
      "loss: 1.828999872061673 acc: 0.86\n",
      "loss: 1.8445766037842943 acc: 0.84\n",
      "loss: 1.8765249900618632 acc: 0.76\n",
      "loss: 1.8330218025917346 acc: 0.81\n",
      "loss: 1.8271786593640151 acc: 0.86\n",
      "loss: 1.8535379217979355 acc: 0.8\n",
      "loss: 1.8222500196440052 acc: 0.84\n",
      "loss: 1.849539946362865 acc: 0.83\n",
      "loss: 1.8364629836410273 acc: 0.79\n",
      "loss: 1.8364051859075838 acc: 0.84\n",
      "loss: 1.8228469531593299 acc: 0.81\n",
      "loss: 1.8093214214674398 acc: 0.86\n",
      "loss: 1.8519304467588382 acc: 0.83\n",
      "loss: 1.8167950611508956 acc: 0.86\n",
      "loss: 1.8263017036628606 acc: 0.84\n",
      "loss: 1.835865083065243 acc: 0.89\n",
      "loss: 1.8375584161386018 acc: 0.81\n",
      "loss: 1.8332764385728084 acc: 0.82\n",
      "loss: 1.8146040266758552 acc: 0.83\n",
      "loss: 1.8056267204479648 acc: 0.92\n",
      "loss: 1.8668235367036272 acc: 0.81\n",
      "loss: 1.8482166859062823 acc: 0.76\n",
      "loss: 1.8710982565444598 acc: 0.81\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 1.8711\t Accuracy 0.8100\n",
      "loss: 1.792632839126859 acc: 0.94\n",
      "loss: 1.8252232559707273 acc: 0.87\n",
      "loss: 1.818062637729628 acc: 0.81\n",
      "loss: 1.8408472770543434 acc: 0.83\n",
      "loss: 1.8400106041624846 acc: 0.86\n",
      "loss: 1.842074603980202 acc: 0.86\n",
      "loss: 1.8618654902887115 acc: 0.82\n",
      "loss: 1.8092644453070603 acc: 0.81\n",
      "loss: 1.8646754607812408 acc: 0.85\n",
      "loss: 1.7797023337440177 acc: 0.91\n",
      "loss: 1.799406222246838 acc: 0.88\n",
      "loss: 1.838212306956462 acc: 0.87\n",
      "loss: 1.8029715296427709 acc: 0.89\n",
      "loss: 1.839061900640469 acc: 0.88\n",
      "loss: 1.7907272528807814 acc: 0.87\n",
      "loss: 1.8241160027662642 acc: 0.85\n",
      "loss: 1.7982956781487218 acc: 0.83\n",
      "loss: 1.822277847477434 acc: 0.8\n",
      "loss: 1.7789613964506745 acc: 0.87\n",
      "loss: 1.806450941483998 acc: 0.83\n",
      "loss: 1.8245599042983165 acc: 0.8\n",
      "loss: 1.8048486290587646 acc: 0.83\n",
      "loss: 1.8068830971234817 acc: 0.86\n",
      "loss: 1.8247674337983362 acc: 0.84\n",
      "loss: 1.877348470575358 acc: 0.84\n",
      "loss: 1.848086113716032 acc: 0.77\n",
      "loss: 1.7891299818918969 acc: 0.87\n",
      "loss: 1.8107970301776275 acc: 0.88\n",
      "loss: 1.817671764311026 acc: 0.84\n",
      "loss: 1.8108297311805743 acc: 0.87\n",
      "loss: 1.831565806482312 acc: 0.81\n",
      "loss: 1.8158700113291542 acc: 0.9\n",
      "loss: 1.824580640431233 acc: 0.85\n",
      "loss: 1.8120872061022106 acc: 0.84\n",
      "loss: 1.845036005199657 acc: 0.76\n",
      "loss: 1.8212588551202853 acc: 0.81\n",
      "loss: 1.8457875956364516 acc: 0.79\n",
      "loss: 1.8197532309701119 acc: 0.88\n",
      "loss: 1.7937118680457405 acc: 0.9\n",
      "loss: 1.8488725445973384 acc: 0.84\n",
      "loss: 1.8308699249352034 acc: 0.83\n",
      "loss: 1.7961835996691058 acc: 0.87\n",
      "loss: 1.8105264660180165 acc: 0.86\n",
      "loss: 1.8768297888104895 acc: 0.81\n",
      "loss: 1.8254498983241183 acc: 0.84\n",
      "loss: 1.8470120369109808 acc: 0.81\n",
      "loss: 1.7982940767412803 acc: 0.84\n",
      "loss: 1.8210821023230142 acc: 0.85\n",
      "loss: 1.8411643130063788 acc: 0.83\n",
      "loss: 1.7871789294063458 acc: 0.86\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 1.7872\t Accuracy 0.8600\n",
      "loss: 1.803797401696105 acc: 0.83\n",
      "loss: 1.871793780436166 acc: 0.81\n",
      "loss: 1.818703240371883 acc: 0.87\n",
      "loss: 1.8450398963782648 acc: 0.81\n",
      "loss: 1.8411700965714923 acc: 0.84\n",
      "loss: 1.8217189913700462 acc: 0.82\n",
      "loss: 1.8557679989931848 acc: 0.75\n",
      "loss: 1.871450542718708 acc: 0.75\n",
      "loss: 1.8519495385601659 acc: 0.78\n",
      "loss: 1.8111444085020887 acc: 0.89\n",
      "loss: 1.801323598731662 acc: 0.85\n",
      "loss: 1.8301335010955617 acc: 0.85\n",
      "loss: 1.793251450545215 acc: 0.8\n",
      "loss: 1.8340229320865473 acc: 0.83\n",
      "loss: 1.8096580495030268 acc: 0.84\n",
      "loss: 1.8704363626388496 acc: 0.79\n",
      "loss: 1.8028933799745692 acc: 0.88\n",
      "loss: 1.8260512477132713 acc: 0.88\n",
      "loss: 1.8534923287719807 acc: 0.81\n",
      "loss: 1.8042792000672945 acc: 0.87\n",
      "loss: 1.8079550785440708 acc: 0.83\n",
      "loss: 1.8352494985024512 acc: 0.83\n",
      "loss: 1.8127749212200401 acc: 0.89\n",
      "loss: 1.8263429932244692 acc: 0.79\n",
      "loss: 1.8265541100874574 acc: 0.82\n",
      "loss: 1.8368950342794188 acc: 0.78\n",
      "loss: 1.8732422702961136 acc: 0.8\n",
      "loss: 1.851550496066475 acc: 0.81\n",
      "loss: 1.8129074041045368 acc: 0.85\n",
      "loss: 1.8087194338257382 acc: 0.89\n",
      "loss: 1.8364450484279797 acc: 0.83\n",
      "loss: 1.7658801909165118 acc: 0.89\n",
      "loss: 1.846292826339225 acc: 0.83\n",
      "loss: 1.8330450599329127 acc: 0.82\n",
      "loss: 1.8418041281703013 acc: 0.82\n",
      "loss: 1.8124969617216862 acc: 0.83\n",
      "loss: 1.7959116150327694 acc: 0.89\n",
      "loss: 1.815860833120733 acc: 0.84\n",
      "loss: 1.809290094120841 acc: 0.87\n",
      "loss: 1.8335071897661044 acc: 0.81\n",
      "loss: 1.8469236799612019 acc: 0.82\n",
      "loss: 1.8095476280904246 acc: 0.86\n",
      "loss: 1.8535665742409975 acc: 0.81\n",
      "loss: 1.853422636718125 acc: 0.8\n",
      "loss: 1.850063130351379 acc: 0.85\n",
      "loss: 1.7820793232894616 acc: 0.87\n",
      "loss: 1.847509849606046 acc: 0.79\n",
      "loss: 1.834139215605934 acc: 0.82\n",
      "loss: 1.829042238790677 acc: 0.82\n",
      "loss: 1.8189019235211985 acc: 0.87\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 1.8189\t Accuracy 0.8700\n",
      "loss: 1.819948865228567 acc: 0.85\n",
      "loss: 1.8403055538289845 acc: 0.81\n",
      "loss: 1.8653102273990705 acc: 0.8\n",
      "loss: 1.824425769789456 acc: 0.8\n",
      "loss: 1.8287257405550748 acc: 0.84\n",
      "loss: 1.831306927746567 acc: 0.81\n",
      "loss: 1.8112544412174034 acc: 0.86\n",
      "loss: 1.8475666852377477 acc: 0.81\n",
      "loss: 1.8257085622581066 acc: 0.84\n",
      "loss: 1.8411088872088888 acc: 0.82\n",
      "loss: 1.789422760869164 acc: 0.88\n",
      "loss: 1.8106610497093873 acc: 0.84\n",
      "loss: 1.8113517413061988 acc: 0.88\n",
      "loss: 1.8318607379754765 acc: 0.83\n",
      "loss: 1.81196185166125 acc: 0.85\n",
      "loss: 1.8790346843791113 acc: 0.82\n",
      "loss: 1.8148763225070974 acc: 0.91\n",
      "loss: 1.8321937282337373 acc: 0.85\n",
      "loss: 1.825333973190373 acc: 0.89\n",
      "loss: 1.8532317049614493 acc: 0.78\n",
      "loss: 1.787693421891238 acc: 0.87\n",
      "loss: 1.8495249990977987 acc: 0.81\n",
      "loss: 1.8682222024479131 acc: 0.82\n",
      "loss: 1.8085982834120446 acc: 0.89\n",
      "loss: 1.8525333341299048 acc: 0.82\n",
      "loss: 1.8276690163113052 acc: 0.86\n",
      "loss: 1.7986448001597557 acc: 0.86\n",
      "loss: 1.8282366695742436 acc: 0.85\n",
      "loss: 1.8111711059753661 acc: 0.84\n",
      "loss: 1.805631501135433 acc: 0.84\n",
      "loss: 1.8090071606859968 acc: 0.84\n",
      "loss: 1.8390045526156678 acc: 0.81\n",
      "loss: 1.816276682425301 acc: 0.81\n",
      "loss: 1.8131348102236224 acc: 0.87\n",
      "loss: 1.8148010060207636 acc: 0.81\n",
      "loss: 1.8392475540760431 acc: 0.82\n",
      "loss: 1.793532949361737 acc: 0.85\n",
      "loss: 1.8397982787071272 acc: 0.87\n",
      "loss: 1.8116973359801138 acc: 0.9\n",
      "loss: 1.838144119825288 acc: 0.81\n",
      "loss: 1.861548660539695 acc: 0.8\n",
      "loss: 1.812140568365443 acc: 0.85\n",
      "loss: 1.795400117030402 acc: 0.85\n",
      "loss: 1.835744701545288 acc: 0.85\n",
      "loss: 1.8398733826427818 acc: 0.88\n",
      "loss: 1.7668667052093736 acc: 0.87\n",
      "loss: 1.777726627109019 acc: 0.86\n",
      "loss: 1.796045709573836 acc: 0.88\n",
      "loss: 1.7797489776272035 acc: 0.9\n",
      "loss: 1.8058659456452084 acc: 0.83\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 1.8059\t Accuracy 0.8300\n",
      "loss: 1.8620730455575336 acc: 0.77\n",
      "loss: 1.782757207922122 acc: 0.91\n",
      "loss: 1.8418350821069231 acc: 0.84\n",
      "loss: 1.8363910214953554 acc: 0.87\n",
      "loss: 1.7904083589133801 acc: 0.81\n",
      "loss: 1.8153055277499015 acc: 0.76\n",
      "loss: 1.8244888611920391 acc: 0.86\n",
      "loss: 1.8699125592144918 acc: 0.79\n",
      "loss: 1.8200981974939328 acc: 0.87\n",
      "loss: 1.842290960609094 acc: 0.82\n",
      "loss: 1.7943188338460276 acc: 0.88\n",
      "loss: 1.8142095026144096 acc: 0.81\n",
      "loss: 1.8224266171143182 acc: 0.83\n",
      "loss: 1.814712748669976 acc: 0.84\n",
      "loss: 1.834244008726109 acc: 0.87\n",
      "loss: 1.7864902809640437 acc: 0.88\n",
      "loss: 1.8388906227134214 acc: 0.81\n",
      "loss: 1.8541397145213199 acc: 0.79\n",
      "loss: 1.7822006287838996 acc: 0.88\n",
      "loss: 1.8037934842657364 acc: 0.87\n",
      "loss: 1.8796680734988 acc: 0.75\n",
      "loss: 1.8093591894370777 acc: 0.8\n",
      "loss: 1.828056768506429 acc: 0.85\n",
      "loss: 1.8408883765704247 acc: 0.8\n",
      "loss: 1.8327396215442593 acc: 0.8\n",
      "loss: 1.8413372101012673 acc: 0.85\n",
      "loss: 1.824612558505171 acc: 0.82\n",
      "loss: 1.809040459177202 acc: 0.85\n",
      "loss: 1.8666496077903005 acc: 0.79\n",
      "loss: 1.809298909787945 acc: 0.87\n",
      "loss: 1.7907966009318594 acc: 0.92\n",
      "loss: 1.851613557529356 acc: 0.86\n",
      "loss: 1.8820948072540395 acc: 0.79\n",
      "loss: 1.8055267078160973 acc: 0.84\n",
      "loss: 1.8274063674846197 acc: 0.84\n",
      "loss: 1.8181968088052716 acc: 0.89\n",
      "loss: 1.7882262571428509 acc: 0.88\n",
      "loss: 1.833421609699136 acc: 0.78\n",
      "loss: 1.8475901078031252 acc: 0.88\n",
      "loss: 1.801496607027405 acc: 0.83\n",
      "loss: 1.8220196283327283 acc: 0.86\n",
      "loss: 1.83535956440999 acc: 0.87\n",
      "loss: 1.80492223643008 acc: 0.86\n",
      "loss: 1.8068108797304723 acc: 0.87\n",
      "loss: 1.817364853630181 acc: 0.87\n",
      "loss: 1.8290252018445081 acc: 0.84\n",
      "loss: 1.8337163941942751 acc: 0.86\n",
      "loss: 1.8380225034431552 acc: 0.81\n",
      "loss: 1.8107973154937493 acc: 0.84\n",
      "loss: 1.8198009942144455 acc: 0.8\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 1.8198\t Accuracy 0.8000\n",
      "loss: 1.8226430908437643 acc: 0.84\n",
      "loss: 1.8687270210497433 acc: 0.78\n",
      "loss: 1.8390325006754973 acc: 0.82\n",
      "loss: 1.8603754076350938 acc: 0.82\n",
      "loss: 1.8494548325930094 acc: 0.82\n",
      "loss: 1.7896083717814832 acc: 0.82\n",
      "loss: 1.8884024809920417 acc: 0.78\n",
      "loss: 1.8577968875835251 acc: 0.78\n",
      "loss: 1.851486268936799 acc: 0.79\n",
      "loss: 1.8113680821688505 acc: 0.86\n",
      "loss: 1.8473775972419049 acc: 0.8\n",
      "loss: 1.853876880659091 acc: 0.76\n",
      "loss: 1.8565114841904011 acc: 0.79\n",
      "loss: 1.8179476581623362 acc: 0.9\n",
      "loss: 1.858951846105635 acc: 0.86\n",
      "loss: 1.8239365961522234 acc: 0.83\n",
      "loss: 1.8315155942658645 acc: 0.87\n",
      "loss: 1.8572852848905919 acc: 0.83\n",
      "loss: 1.7911174581168516 acc: 0.87\n",
      "loss: 1.823410136428418 acc: 0.83\n",
      "loss: 1.8437976954420932 acc: 0.83\n",
      "loss: 1.8569508817152696 acc: 0.74\n",
      "loss: 1.8167025324204005 acc: 0.83\n",
      "loss: 1.8191158552170663 acc: 0.86\n",
      "loss: 1.793644765573253 acc: 0.85\n",
      "loss: 1.8368835574313758 acc: 0.79\n",
      "loss: 1.8374928639945878 acc: 0.82\n",
      "loss: 1.8165532453296425 acc: 0.85\n",
      "loss: 1.813748907944256 acc: 0.84\n",
      "loss: 1.8179882233440035 acc: 0.89\n",
      "loss: 1.7756644407000926 acc: 0.82\n",
      "loss: 1.840747870043705 acc: 0.77\n",
      "loss: 1.8414544293604314 acc: 0.77\n",
      "loss: 1.835058723933348 acc: 0.75\n",
      "loss: 1.816408581726481 acc: 0.81\n",
      "loss: 1.8006327047392634 acc: 0.83\n",
      "loss: 1.8256351969799887 acc: 0.78\n",
      "loss: 1.819236786413616 acc: 0.86\n",
      "loss: 1.8170186311917127 acc: 0.85\n",
      "loss: 1.7901598497075815 acc: 0.86\n",
      "loss: 1.8373103413175127 acc: 0.89\n",
      "loss: 1.7998120412339689 acc: 0.84\n",
      "loss: 1.839253101630695 acc: 0.83\n",
      "loss: 1.8328943844480088 acc: 0.84\n",
      "loss: 1.795450947884984 acc: 0.88\n",
      "loss: 1.8958014173322948 acc: 0.77\n",
      "loss: 1.7716139367442936 acc: 0.91\n",
      "loss: 1.8388352802002061 acc: 0.87\n",
      "loss: 1.8316435699626132 acc: 0.84\n",
      "loss: 1.8323711362516384 acc: 0.85\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 1.8324\t Accuracy 0.8500\n",
      "loss: 1.8394884009052117 acc: 0.79\n",
      "loss: 1.882759054718825 acc: 0.81\n",
      "loss: 1.8327577012091114 acc: 0.78\n",
      "loss: 1.8673438939282057 acc: 0.79\n",
      "loss: 1.7985267914105574 acc: 0.87\n",
      "loss: 1.8586364745833581 acc: 0.8\n",
      "loss: 1.8007087763174523 acc: 0.88\n",
      "loss: 1.8210655081448746 acc: 0.83\n",
      "loss: 1.8350268014661193 acc: 0.85\n",
      "loss: 1.8092294305450691 acc: 0.85\n",
      "loss: 1.7872678776004454 acc: 0.87\n",
      "loss: 1.8664851655773316 acc: 0.71\n",
      "loss: 1.8558474297371672 acc: 0.8\n",
      "loss: 1.8296513759525057 acc: 0.84\n",
      "loss: 1.7796729567336675 acc: 0.86\n",
      "loss: 1.8164332188773005 acc: 0.85\n",
      "loss: 1.802595440532819 acc: 0.85\n",
      "loss: 1.8249872510760017 acc: 0.82\n",
      "loss: 1.8292982881373065 acc: 0.89\n",
      "loss: 1.8400549936748496 acc: 0.84\n",
      "loss: 1.8401770572709546 acc: 0.86\n",
      "loss: 1.8292179762965164 acc: 0.79\n",
      "loss: 1.7947261455209342 acc: 0.82\n",
      "loss: 1.7798808549189917 acc: 0.87\n",
      "loss: 1.7927670610253004 acc: 0.89\n",
      "loss: 1.7596824675974923 acc: 0.94\n",
      "loss: 1.8085194598657832 acc: 0.85\n",
      "loss: 1.8411594811242693 acc: 0.81\n",
      "loss: 1.8430848925504353 acc: 0.82\n",
      "loss: 1.8153646852063536 acc: 0.83\n",
      "loss: 1.8252595437220813 acc: 0.84\n",
      "loss: 1.866076877978071 acc: 0.77\n",
      "loss: 1.814121624906318 acc: 0.85\n",
      "loss: 1.8146618001758483 acc: 0.91\n",
      "loss: 1.856996542153401 acc: 0.85\n",
      "loss: 1.790066101766452 acc: 0.89\n",
      "loss: 1.8259578283590399 acc: 0.89\n",
      "loss: 1.799891594309502 acc: 0.84\n",
      "loss: 1.8431046844467676 acc: 0.8\n",
      "loss: 1.8467573071806647 acc: 0.81\n",
      "loss: 1.8108630342149474 acc: 0.88\n",
      "loss: 1.837654010997291 acc: 0.8\n",
      "loss: 1.824458768328795 acc: 0.81\n",
      "loss: 1.832571574798632 acc: 0.8\n",
      "loss: 1.8591859103606305 acc: 0.83\n",
      "loss: 1.831144076707117 acc: 0.84\n",
      "loss: 1.8582938002259473 acc: 0.81\n",
      "loss: 1.831110196098557 acc: 0.86\n",
      "loss: 1.844169496535288 acc: 0.85\n",
      "loss: 1.7935747352130955 acc: 0.86\n",
      "loss: 1.792974447994451 acc: 0.88\n",
      "loss: 1.8021805533992878 acc: 0.89\n",
      "loss: 1.8278058452849968 acc: 0.87\n",
      "loss: 1.81341974900898 acc: 0.91\n",
      "loss: 1.803455707531633 acc: 0.86\n",
      "loss: 1.7601396483785618 acc: 0.83\n",
      "loss: 1.7547015829967092 acc: 0.87\n",
      "loss: 1.817671150284739 acc: 0.86\n",
      "loss: 1.7488277431520618 acc: 0.93\n",
      "loss: 1.787944841444056 acc: 0.88\n",
      "loss: 1.793225746661989 acc: 0.85\n",
      "loss: 1.841024797961483 acc: 0.82\n",
      "loss: 1.8571831935775918 acc: 0.84\n",
      "loss: 1.8635414849782144 acc: 0.84\n",
      "loss: 1.7900284565331475 acc: 0.92\n",
      "loss: 1.8063884409831323 acc: 0.86\n",
      "loss: 1.7548582368822583 acc: 0.92\n",
      "loss: 1.784414154545912 acc: 0.88\n",
      "loss: 1.7963296672700795 acc: 0.89\n",
      "loss: 1.8718546412005654 acc: 0.85\n",
      "loss: 1.8245894593282543 acc: 0.89\n",
      "loss: 1.820229562825521 acc: 0.76\n",
      "loss: 1.85977053614744 acc: 0.86\n",
      "loss: 1.8297623692032048 acc: 0.84\n",
      "loss: 1.8460152367462073 acc: 0.82\n",
      "loss: 1.8609346211421949 acc: 0.78\n",
      "loss: 1.8827904237573014 acc: 0.8\n",
      "loss: 1.8248769951017918 acc: 0.92\n",
      "loss: 1.8001566148647365 acc: 0.89\n",
      "loss: 1.8162602690494132 acc: 0.84\n",
      "loss: 1.7714834955952035 acc: 0.92\n",
      "loss: 1.7452944929193306 acc: 0.94\n",
      "loss: 1.7849737845021085 acc: 0.89\n",
      "loss: 1.7789121372295318 acc: 0.86\n",
      "loss: 1.8117702480588078 acc: 0.88\n",
      "loss: 1.84733912339802 acc: 0.89\n",
      "loss: 1.8357636643836528 acc: 0.87\n",
      "loss: 1.7420521298352174 acc: 0.9\n",
      "loss: 1.709516600089725 acc: 0.96\n",
      "loss: 1.7364481193200634 acc: 0.96\n",
      "loss: 1.7373136800363238 acc: 0.98\n",
      "loss: 1.7884554536484156 acc: 0.91\n",
      "loss: 1.7713471998330723 acc: 0.87\n",
      "loss: 1.7279100913496968 acc: 0.86\n",
      "loss: 1.7992314406264638 acc: 0.93\n",
      "loss: 1.7923175852703728 acc: 0.93\n",
      "loss: 1.8834293282538834 acc: 0.83\n",
      "loss: 1.700951352374326 acc: 0.97\n",
      "loss: 1.8578686489197502 acc: 0.84\n",
      "\n",
      "Epoch [2]\t Average training loss 1.8252\t Average training accuracy 0.8389\n",
      "Epoch [2]\t Average validation loss 1.8010\t Average validation accuracy 0.8780\n",
      "\n",
      "loss: 1.8215285054481325 acc: 0.85\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 1.8215\t Accuracy 0.8500\n",
      "loss: 1.8386961494228677 acc: 0.84\n",
      "loss: 1.853988909938814 acc: 0.83\n",
      "loss: 1.7889762160122504 acc: 0.93\n",
      "loss: 1.8031211517513959 acc: 0.87\n",
      "loss: 1.777852407196844 acc: 0.88\n",
      "loss: 1.7891773433048215 acc: 0.86\n",
      "loss: 1.8216327815463742 acc: 0.9\n",
      "loss: 1.7831556994852233 acc: 0.88\n",
      "loss: 1.795626269332807 acc: 0.87\n",
      "loss: 1.8307564706823576 acc: 0.79\n",
      "loss: 1.8278563301485276 acc: 0.82\n",
      "loss: 1.877012497230053 acc: 0.78\n",
      "loss: 1.8030838305322667 acc: 0.82\n",
      "loss: 1.8294512407380465 acc: 0.8\n",
      "loss: 1.7948103687760817 acc: 0.85\n",
      "loss: 1.8536519860812672 acc: 0.79\n",
      "loss: 1.845306234479101 acc: 0.77\n",
      "loss: 1.8014551272376946 acc: 0.88\n",
      "loss: 1.8289548890856133 acc: 0.81\n",
      "loss: 1.8057252979864236 acc: 0.82\n",
      "loss: 1.8213604075099277 acc: 0.82\n",
      "loss: 1.8305367458941382 acc: 0.82\n",
      "loss: 1.7974224748311107 acc: 0.86\n",
      "loss: 1.8000179294992529 acc: 0.85\n",
      "loss: 1.8147069626501817 acc: 0.78\n",
      "loss: 1.822452882814284 acc: 0.82\n",
      "loss: 1.8305787054762859 acc: 0.84\n",
      "loss: 1.8317391608205111 acc: 0.83\n",
      "loss: 1.839585297191913 acc: 0.82\n",
      "loss: 1.8525995838011347 acc: 0.79\n",
      "loss: 1.813457066080456 acc: 0.85\n",
      "loss: 1.8336750507273445 acc: 0.83\n",
      "loss: 1.7814097784969436 acc: 0.87\n",
      "loss: 1.8112247040492264 acc: 0.87\n",
      "loss: 1.838046511318148 acc: 0.79\n",
      "loss: 1.8171612022827026 acc: 0.88\n",
      "loss: 1.8126594451699045 acc: 0.88\n",
      "loss: 1.8247130026926734 acc: 0.8\n",
      "loss: 1.8248313689581652 acc: 0.84\n",
      "loss: 1.7941713073450034 acc: 0.85\n",
      "loss: 1.8001735973475466 acc: 0.87\n",
      "loss: 1.8216382636007546 acc: 0.82\n",
      "loss: 1.8643636877385754 acc: 0.75\n",
      "loss: 1.8133878033017523 acc: 0.83\n",
      "loss: 1.861909799222665 acc: 0.8\n",
      "loss: 1.7740869561125772 acc: 0.9\n",
      "loss: 1.8523647733928648 acc: 0.84\n",
      "loss: 1.820698065986223 acc: 0.86\n",
      "loss: 1.837443625557258 acc: 0.85\n",
      "loss: 1.7963860281796755 acc: 0.86\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 1.7964\t Accuracy 0.8600\n",
      "loss: 1.7988837517476182 acc: 0.83\n",
      "loss: 1.785113630357938 acc: 0.89\n",
      "loss: 1.8009848007472307 acc: 0.85\n",
      "loss: 1.8237275834711866 acc: 0.84\n",
      "loss: 1.8639548689330043 acc: 0.79\n",
      "loss: 1.813008527532254 acc: 0.91\n",
      "loss: 1.8497729955312285 acc: 0.84\n",
      "loss: 1.818326955293568 acc: 0.87\n",
      "loss: 1.8194309211409223 acc: 0.87\n",
      "loss: 1.8164729797137342 acc: 0.87\n",
      "loss: 1.816136809582172 acc: 0.87\n",
      "loss: 1.796300822304189 acc: 0.88\n",
      "loss: 1.8171346038149414 acc: 0.82\n",
      "loss: 1.8336741208696483 acc: 0.82\n",
      "loss: 1.8243922068808647 acc: 0.83\n",
      "loss: 1.858091687010574 acc: 0.79\n",
      "loss: 1.8169685283803236 acc: 0.89\n",
      "loss: 1.813874589014157 acc: 0.85\n",
      "loss: 1.7918103577723128 acc: 0.88\n",
      "loss: 1.801182112854025 acc: 0.9\n",
      "loss: 1.8226276353148116 acc: 0.88\n",
      "loss: 1.8869690275242639 acc: 0.74\n",
      "loss: 1.8231579375715554 acc: 0.83\n",
      "loss: 1.8146738830156306 acc: 0.88\n",
      "loss: 1.8088835087941249 acc: 0.81\n",
      "loss: 1.8142860773688656 acc: 0.9\n",
      "loss: 1.8509647198020316 acc: 0.81\n",
      "loss: 1.7912384372059342 acc: 0.88\n",
      "loss: 1.804864619173002 acc: 0.8\n",
      "loss: 1.8066536248145268 acc: 0.82\n",
      "loss: 1.7903119461091979 acc: 0.85\n",
      "loss: 1.802423864290775 acc: 0.88\n",
      "loss: 1.8390578841093028 acc: 0.84\n",
      "loss: 1.803759223924546 acc: 0.86\n",
      "loss: 1.819530245402172 acc: 0.85\n",
      "loss: 1.8355734324947277 acc: 0.84\n",
      "loss: 1.8038133519146486 acc: 0.83\n",
      "loss: 1.8036852771102894 acc: 0.92\n",
      "loss: 1.8235988505680012 acc: 0.87\n",
      "loss: 1.8313644963265583 acc: 0.83\n",
      "loss: 1.8427648813474857 acc: 0.8\n",
      "loss: 1.8193426510848283 acc: 0.85\n",
      "loss: 1.8482152605306237 acc: 0.83\n",
      "loss: 1.7930009661628707 acc: 0.92\n",
      "loss: 1.7799527426624844 acc: 0.85\n",
      "loss: 1.7979798797651245 acc: 0.84\n",
      "loss: 1.8290092282761008 acc: 0.87\n",
      "loss: 1.8567539868886842 acc: 0.77\n",
      "loss: 1.8531037225108207 acc: 0.86\n",
      "loss: 1.8558342651255828 acc: 0.82\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 1.8558\t Accuracy 0.8200\n",
      "loss: 1.8054650680739044 acc: 0.84\n",
      "loss: 1.8321710915583214 acc: 0.82\n",
      "loss: 1.851924195758148 acc: 0.86\n",
      "loss: 1.787021503122897 acc: 0.84\n",
      "loss: 1.8210826997439984 acc: 0.88\n",
      "loss: 1.8032054150981753 acc: 0.88\n",
      "loss: 1.860140311060733 acc: 0.81\n",
      "loss: 1.863734660350973 acc: 0.8\n",
      "loss: 1.8621824624374925 acc: 0.83\n",
      "loss: 1.7980446401174484 acc: 0.84\n",
      "loss: 1.811963800825788 acc: 0.84\n",
      "loss: 1.7964898302540757 acc: 0.86\n",
      "loss: 1.7800533484410916 acc: 0.88\n",
      "loss: 1.830307659596252 acc: 0.81\n",
      "loss: 1.850670512503446 acc: 0.83\n",
      "loss: 1.8326187252726742 acc: 0.81\n",
      "loss: 1.8355421502167835 acc: 0.8\n",
      "loss: 1.831137886610774 acc: 0.83\n",
      "loss: 1.7825312922739744 acc: 0.84\n",
      "loss: 1.8258298734964071 acc: 0.85\n",
      "loss: 1.8022760650282001 acc: 0.85\n",
      "loss: 1.8217140602360162 acc: 0.82\n",
      "loss: 1.826987088433408 acc: 0.83\n",
      "loss: 1.8188378733421744 acc: 0.86\n",
      "loss: 1.830662939130272 acc: 0.84\n",
      "loss: 1.8090028968826335 acc: 0.87\n",
      "loss: 1.8239972833920048 acc: 0.86\n",
      "loss: 1.8550365781597438 acc: 0.84\n",
      "loss: 1.8697742886775324 acc: 0.8\n",
      "loss: 1.8462712875750154 acc: 0.83\n",
      "loss: 1.8576650286813767 acc: 0.83\n",
      "loss: 1.8499309791553313 acc: 0.82\n",
      "loss: 1.8374803288377146 acc: 0.83\n",
      "loss: 1.8308956952869047 acc: 0.85\n",
      "loss: 1.8408082462393367 acc: 0.82\n",
      "loss: 1.8171337445244566 acc: 0.8\n",
      "loss: 1.8048652673494772 acc: 0.89\n",
      "loss: 1.8599763872115778 acc: 0.81\n",
      "loss: 1.840148439300147 acc: 0.86\n",
      "loss: 1.852758101207031 acc: 0.8\n",
      "loss: 1.8489571701575553 acc: 0.84\n",
      "loss: 1.8229924359578762 acc: 0.79\n",
      "loss: 1.8097555083843941 acc: 0.86\n",
      "loss: 1.8555820913295915 acc: 0.79\n",
      "loss: 1.8176192764459618 acc: 0.86\n",
      "loss: 1.8662801654165173 acc: 0.81\n",
      "loss: 1.879559290195107 acc: 0.78\n",
      "loss: 1.7984535995355933 acc: 0.89\n",
      "loss: 1.8495139212947884 acc: 0.79\n",
      "loss: 1.7759370482423582 acc: 0.9\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 1.7759\t Accuracy 0.9000\n",
      "loss: 1.8200692599754345 acc: 0.82\n",
      "loss: 1.8523465714421394 acc: 0.83\n",
      "loss: 1.85722757001017 acc: 0.84\n",
      "loss: 1.805432804698937 acc: 0.83\n",
      "loss: 1.8475611226706738 acc: 0.83\n",
      "loss: 1.7997377174051652 acc: 0.88\n",
      "loss: 1.818429107039678 acc: 0.87\n",
      "loss: 1.8419105608122703 acc: 0.79\n",
      "loss: 1.8219545974139661 acc: 0.83\n",
      "loss: 1.8289761450665534 acc: 0.87\n",
      "loss: 1.8660260571560527 acc: 0.75\n",
      "loss: 1.8407163433647384 acc: 0.81\n",
      "loss: 1.7625161524491082 acc: 0.9\n",
      "loss: 1.823376213978735 acc: 0.83\n",
      "loss: 1.8589611451748163 acc: 0.81\n",
      "loss: 1.832217070146596 acc: 0.84\n",
      "loss: 1.7706842229625377 acc: 0.9\n",
      "loss: 1.8365462880891155 acc: 0.85\n",
      "loss: 1.8226267911720644 acc: 0.83\n",
      "loss: 1.8002510548088586 acc: 0.83\n",
      "loss: 1.8403840389954107 acc: 0.9\n",
      "loss: 1.802521744555808 acc: 0.87\n",
      "loss: 1.8194612279808136 acc: 0.83\n",
      "loss: 1.8243712073454459 acc: 0.79\n",
      "loss: 1.7847272127684624 acc: 0.88\n",
      "loss: 1.8818193549905287 acc: 0.78\n",
      "loss: 1.8318992660601203 acc: 0.76\n",
      "loss: 1.8104251610251547 acc: 0.83\n",
      "loss: 1.8237298085312332 acc: 0.85\n",
      "loss: 1.8150596641310544 acc: 0.82\n",
      "loss: 1.7898360327249088 acc: 0.85\n",
      "loss: 1.810535279205282 acc: 0.83\n",
      "loss: 1.8200142128036356 acc: 0.84\n",
      "loss: 1.7861496256667346 acc: 0.89\n",
      "loss: 1.831725274139592 acc: 0.88\n",
      "loss: 1.8318682115257192 acc: 0.88\n",
      "loss: 1.827183594349075 acc: 0.85\n",
      "loss: 1.8546417590105728 acc: 0.78\n",
      "loss: 1.8262543201097692 acc: 0.84\n",
      "loss: 1.8007985072609296 acc: 0.83\n",
      "loss: 1.8646282464006692 acc: 0.77\n",
      "loss: 1.8491295023977918 acc: 0.84\n",
      "loss: 1.8324489017743166 acc: 0.87\n",
      "loss: 1.7778149863110608 acc: 0.96\n",
      "loss: 1.8718888299445984 acc: 0.79\n",
      "loss: 1.830312418828303 acc: 0.85\n",
      "loss: 1.793065699912316 acc: 0.9\n",
      "loss: 1.8236738454437904 acc: 0.86\n",
      "loss: 1.8509490253731462 acc: 0.82\n",
      "loss: 1.837576107017124 acc: 0.84\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 1.8376\t Accuracy 0.8400\n",
      "loss: 1.8515532498353517 acc: 0.84\n",
      "loss: 1.8336873979293902 acc: 0.83\n",
      "loss: 1.8174288698408012 acc: 0.81\n",
      "loss: 1.8055483563691772 acc: 0.9\n",
      "loss: 1.8170386764882323 acc: 0.85\n",
      "loss: 1.8035987936494198 acc: 0.89\n",
      "loss: 1.8253067365801738 acc: 0.82\n",
      "loss: 1.8045096002059273 acc: 0.85\n",
      "loss: 1.8296795214844936 acc: 0.83\n",
      "loss: 1.8665802243732015 acc: 0.77\n",
      "loss: 1.8681058257625118 acc: 0.84\n",
      "loss: 1.8221204687820964 acc: 0.88\n",
      "loss: 1.8305941828256609 acc: 0.84\n",
      "loss: 1.8043522551346212 acc: 0.85\n",
      "loss: 1.815901323838587 acc: 0.87\n",
      "loss: 1.8445391455108402 acc: 0.85\n",
      "loss: 1.832256931717064 acc: 0.8\n",
      "loss: 1.8024396566151462 acc: 0.83\n",
      "loss: 1.806339871684206 acc: 0.89\n",
      "loss: 1.8412661325399169 acc: 0.82\n",
      "loss: 1.8255941849701316 acc: 0.87\n",
      "loss: 1.8022612349851643 acc: 0.84\n",
      "loss: 1.8228413149127118 acc: 0.83\n",
      "loss: 1.836039871964558 acc: 0.77\n",
      "loss: 1.8308145431227074 acc: 0.81\n",
      "loss: 1.8029763501757745 acc: 0.83\n",
      "loss: 1.8835888398170477 acc: 0.78\n",
      "loss: 1.8573994795954047 acc: 0.82\n",
      "loss: 1.8249085097270774 acc: 0.83\n",
      "loss: 1.8284617726711552 acc: 0.84\n",
      "loss: 1.8679442835239422 acc: 0.8\n",
      "loss: 1.8689246410841966 acc: 0.77\n",
      "loss: 1.8419110419498295 acc: 0.85\n",
      "loss: 1.8129395290524406 acc: 0.84\n",
      "loss: 1.884735636609073 acc: 0.76\n",
      "loss: 1.8501764789297244 acc: 0.78\n",
      "loss: 1.8453189548020887 acc: 0.81\n",
      "loss: 1.816164947111277 acc: 0.88\n",
      "loss: 1.8252807591682887 acc: 0.81\n",
      "loss: 1.8525110816331605 acc: 0.86\n",
      "loss: 1.7985303162699047 acc: 0.88\n",
      "loss: 1.8145291645308728 acc: 0.83\n",
      "loss: 1.7954951635524157 acc: 0.89\n",
      "loss: 1.833916860979998 acc: 0.86\n",
      "loss: 1.8589792540945667 acc: 0.82\n",
      "loss: 1.844723179813619 acc: 0.85\n",
      "loss: 1.7990510157933395 acc: 0.85\n",
      "loss: 1.8437557167196093 acc: 0.8\n",
      "loss: 1.788666723103676 acc: 0.85\n",
      "loss: 1.8220116648750686 acc: 0.81\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 1.8220\t Accuracy 0.8100\n",
      "loss: 1.8489294613337082 acc: 0.78\n",
      "loss: 1.8114425386025834 acc: 0.9\n",
      "loss: 1.8058102037721975 acc: 0.88\n",
      "loss: 1.8262784172026267 acc: 0.86\n",
      "loss: 1.820851196201533 acc: 0.84\n",
      "loss: 1.8276672723092553 acc: 0.88\n",
      "loss: 1.8231042404043467 acc: 0.84\n",
      "loss: 1.8292023795539718 acc: 0.82\n",
      "loss: 1.8243342798868698 acc: 0.87\n",
      "loss: 1.812411812393232 acc: 0.85\n",
      "loss: 1.8089767999042559 acc: 0.89\n",
      "loss: 1.814223677618408 acc: 0.85\n",
      "loss: 1.8118737252278359 acc: 0.83\n",
      "loss: 1.7867388362592098 acc: 0.88\n",
      "loss: 1.8140761443545832 acc: 0.83\n",
      "loss: 1.8302893099284199 acc: 0.82\n",
      "loss: 1.8174019797857595 acc: 0.87\n",
      "loss: 1.8584561761230864 acc: 0.82\n",
      "loss: 1.7878049912608294 acc: 0.88\n",
      "loss: 1.8176235004169827 acc: 0.9\n",
      "loss: 1.8598171064664757 acc: 0.82\n",
      "loss: 1.830021358832473 acc: 0.83\n",
      "loss: 1.8073163718357796 acc: 0.83\n",
      "loss: 1.8192239013817257 acc: 0.89\n",
      "loss: 1.8544344482733928 acc: 0.76\n",
      "loss: 1.779744449992759 acc: 0.91\n",
      "loss: 1.762053987051728 acc: 0.89\n",
      "loss: 1.8249431572667536 acc: 0.83\n",
      "loss: 1.7951641090790913 acc: 0.86\n",
      "loss: 1.8240145736045066 acc: 0.83\n",
      "loss: 1.7959497965290296 acc: 0.89\n",
      "loss: 1.8371286934734812 acc: 0.84\n",
      "loss: 1.7638454216405606 acc: 0.88\n",
      "loss: 1.8564635178608964 acc: 0.83\n",
      "loss: 1.8728425910735402 acc: 0.78\n",
      "loss: 1.857631223264967 acc: 0.8\n",
      "loss: 1.8600420985668316 acc: 0.8\n",
      "loss: 1.8521284079648876 acc: 0.8\n",
      "loss: 1.8279679319466333 acc: 0.86\n",
      "loss: 1.852513652698972 acc: 0.85\n",
      "loss: 1.8570523267232624 acc: 0.8\n",
      "loss: 1.8270050596765282 acc: 0.86\n",
      "loss: 1.8695529518347258 acc: 0.81\n",
      "loss: 1.8184272496370812 acc: 0.83\n",
      "loss: 1.8278698094266213 acc: 0.83\n",
      "loss: 1.83980840300276 acc: 0.83\n",
      "loss: 1.8242582838183425 acc: 0.89\n",
      "loss: 1.8398840370646312 acc: 0.81\n",
      "loss: 1.8564099282282647 acc: 0.77\n",
      "loss: 1.7913175013679776 acc: 0.86\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 1.7913\t Accuracy 0.8600\n",
      "loss: 1.820979243166004 acc: 0.8\n",
      "loss: 1.789571399072358 acc: 0.84\n",
      "loss: 1.7991611512876837 acc: 0.9\n",
      "loss: 1.8203154582846606 acc: 0.84\n",
      "loss: 1.805161567182697 acc: 0.85\n",
      "loss: 1.81138835907174 acc: 0.78\n",
      "loss: 1.8118444241647156 acc: 0.9\n",
      "loss: 1.818639122857188 acc: 0.82\n",
      "loss: 1.788000170406622 acc: 0.92\n",
      "loss: 1.7752842068490673 acc: 0.84\n",
      "loss: 1.7870791300398274 acc: 0.86\n",
      "loss: 1.8163020310323577 acc: 0.84\n",
      "loss: 1.8260231796780744 acc: 0.84\n",
      "loss: 1.8324039784290336 acc: 0.86\n",
      "loss: 1.8397750237284343 acc: 0.79\n",
      "loss: 1.814189264634507 acc: 0.88\n",
      "loss: 1.7899518348441168 acc: 0.87\n",
      "loss: 1.8107058627447408 acc: 0.86\n",
      "loss: 1.8260650389470252 acc: 0.85\n",
      "loss: 1.8083841982704774 acc: 0.9\n",
      "loss: 1.8403005392079677 acc: 0.8\n",
      "loss: 1.8010040112646875 acc: 0.88\n",
      "loss: 1.79998949288595 acc: 0.85\n",
      "loss: 1.87021098424394 acc: 0.84\n",
      "loss: 1.855318811341374 acc: 0.81\n",
      "loss: 1.8420069084252657 acc: 0.79\n",
      "loss: 1.8344302726515134 acc: 0.81\n",
      "loss: 1.822547647279786 acc: 0.89\n",
      "loss: 1.8608814519431105 acc: 0.82\n",
      "loss: 1.8269914866209407 acc: 0.84\n",
      "loss: 1.8343845882488463 acc: 0.86\n",
      "loss: 1.8259153490690412 acc: 0.82\n",
      "loss: 1.7518669205758288 acc: 0.9\n",
      "loss: 1.8198955568814839 acc: 0.83\n",
      "loss: 1.8144974240442706 acc: 0.87\n",
      "loss: 1.8239988098321882 acc: 0.78\n",
      "loss: 1.8173335879006176 acc: 0.85\n",
      "loss: 1.842687695787746 acc: 0.82\n",
      "loss: 1.816994363141103 acc: 0.88\n",
      "loss: 1.8280498639066456 acc: 0.84\n",
      "loss: 1.8226370949031445 acc: 0.86\n",
      "loss: 1.885488212523624 acc: 0.78\n",
      "loss: 1.792989162910965 acc: 0.82\n",
      "loss: 1.8034524920197448 acc: 0.85\n",
      "loss: 1.8540016552149186 acc: 0.84\n",
      "loss: 1.8095223398537363 acc: 0.88\n",
      "loss: 1.8455512109084875 acc: 0.76\n",
      "loss: 1.8110298146178463 acc: 0.86\n",
      "loss: 1.8339452243642018 acc: 0.8\n",
      "loss: 1.80987359602632 acc: 0.86\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 1.8099\t Accuracy 0.8600\n",
      "loss: 1.8550541574915733 acc: 0.77\n",
      "loss: 1.8572709924724087 acc: 0.84\n",
      "loss: 1.8087697041126223 acc: 0.86\n",
      "loss: 1.7991577419769331 acc: 0.89\n",
      "loss: 1.8589600194018105 acc: 0.86\n",
      "loss: 1.8473573618900232 acc: 0.8\n",
      "loss: 1.8022445716983966 acc: 0.84\n",
      "loss: 1.795564301013688 acc: 0.88\n",
      "loss: 1.8093224971743738 acc: 0.89\n",
      "loss: 1.8309077450580469 acc: 0.83\n",
      "loss: 1.8224043091517164 acc: 0.82\n",
      "loss: 1.8177979969243965 acc: 0.83\n",
      "loss: 1.7858019694500666 acc: 0.87\n",
      "loss: 1.827179409798473 acc: 0.84\n",
      "loss: 1.843733937353673 acc: 0.86\n",
      "loss: 1.8130233891428957 acc: 0.81\n",
      "loss: 1.842014037536769 acc: 0.8\n",
      "loss: 1.8242486656963295 acc: 0.83\n",
      "loss: 1.8341204891470853 acc: 0.84\n",
      "loss: 1.8060590571193984 acc: 0.85\n",
      "loss: 1.8411324700453406 acc: 0.8\n",
      "loss: 1.8136301696412807 acc: 0.84\n",
      "loss: 1.8295764813300273 acc: 0.86\n",
      "loss: 1.8017427921070341 acc: 0.89\n",
      "loss: 1.8289362626906926 acc: 0.8\n",
      "loss: 1.8442236249358885 acc: 0.88\n",
      "loss: 1.823799640702512 acc: 0.84\n",
      "loss: 1.8316193384285782 acc: 0.8\n",
      "loss: 1.8362492142786937 acc: 0.9\n",
      "loss: 1.8377219159047327 acc: 0.87\n",
      "loss: 1.7743552464760575 acc: 0.9\n",
      "loss: 1.8399970970985529 acc: 0.85\n",
      "loss: 1.836014682111045 acc: 0.81\n",
      "loss: 1.7936095457199672 acc: 0.84\n",
      "loss: 1.797401021765635 acc: 0.83\n",
      "loss: 1.801151959092655 acc: 0.89\n",
      "loss: 1.8106991854013073 acc: 0.83\n",
      "loss: 1.8239027962683116 acc: 0.83\n",
      "loss: 1.8141253279787128 acc: 0.82\n",
      "loss: 1.8483495862310126 acc: 0.76\n",
      "loss: 1.8262576997500752 acc: 0.87\n",
      "loss: 1.8644095239697211 acc: 0.78\n",
      "loss: 1.8139433110797694 acc: 0.88\n",
      "loss: 1.8412954053244703 acc: 0.85\n",
      "loss: 1.787345997542279 acc: 0.89\n",
      "loss: 1.832112552616264 acc: 0.84\n",
      "loss: 1.8414373560350332 acc: 0.8\n",
      "loss: 1.8203937748555068 acc: 0.85\n",
      "loss: 1.8383252246687887 acc: 0.84\n",
      "loss: 1.826257876697706 acc: 0.8\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 1.8263\t Accuracy 0.8000\n",
      "loss: 1.8264060039853263 acc: 0.84\n",
      "loss: 1.838977543635235 acc: 0.82\n",
      "loss: 1.8295983263341449 acc: 0.79\n",
      "loss: 1.8381184602451521 acc: 0.79\n",
      "loss: 1.813250175874021 acc: 0.85\n",
      "loss: 1.805601623168058 acc: 0.88\n",
      "loss: 1.8256680914102876 acc: 0.82\n",
      "loss: 1.8443889041861996 acc: 0.8\n",
      "loss: 1.8470690704070538 acc: 0.81\n",
      "loss: 1.7943066641772054 acc: 0.88\n",
      "loss: 1.7358639755836986 acc: 0.89\n",
      "loss: 1.8416539889899752 acc: 0.84\n",
      "loss: 1.871337492746893 acc: 0.78\n",
      "loss: 1.7837986001810082 acc: 0.9\n",
      "loss: 1.8663773367577523 acc: 0.8\n",
      "loss: 1.8240527803086772 acc: 0.8\n",
      "loss: 1.8340528804997072 acc: 0.83\n",
      "loss: 1.8460215785335934 acc: 0.82\n",
      "loss: 1.8243888145219316 acc: 0.88\n",
      "loss: 1.7510593581656757 acc: 0.92\n",
      "loss: 1.820060176032004 acc: 0.84\n",
      "loss: 1.8245719030352012 acc: 0.83\n",
      "loss: 1.8343869618954929 acc: 0.85\n",
      "loss: 1.8518085834537599 acc: 0.82\n",
      "loss: 1.8160754700994357 acc: 0.87\n",
      "loss: 1.7826369108174995 acc: 0.86\n",
      "loss: 1.7930243039760212 acc: 0.9\n",
      "loss: 1.82170715397236 acc: 0.86\n",
      "loss: 1.8279007356374735 acc: 0.87\n",
      "loss: 1.8254519795755515 acc: 0.81\n",
      "loss: 1.8128846662728728 acc: 0.88\n",
      "loss: 1.8308951819648291 acc: 0.77\n",
      "loss: 1.7837214681790703 acc: 0.86\n",
      "loss: 1.822905209428882 acc: 0.85\n",
      "loss: 1.8139755807031503 acc: 0.89\n",
      "loss: 1.8047600464794653 acc: 0.79\n",
      "loss: 1.8439335587097225 acc: 0.77\n",
      "loss: 1.7856261362881032 acc: 0.89\n",
      "loss: 1.8662417244983178 acc: 0.83\n",
      "loss: 1.820578537109476 acc: 0.83\n",
      "loss: 1.808893389400057 acc: 0.86\n",
      "loss: 1.839331547306748 acc: 0.86\n",
      "loss: 1.8057243503938962 acc: 0.88\n",
      "loss: 1.855099886474361 acc: 0.85\n",
      "loss: 1.8437176521260468 acc: 0.8\n",
      "loss: 1.8359053403059542 acc: 0.81\n",
      "loss: 1.8340364890679988 acc: 0.9\n",
      "loss: 1.8107724946600183 acc: 0.85\n",
      "loss: 1.7960616360462414 acc: 0.82\n",
      "loss: 1.8108041364175589 acc: 0.86\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 1.8108\t Accuracy 0.8600\n",
      "loss: 1.8378512567547816 acc: 0.84\n",
      "loss: 1.8320143726889069 acc: 0.86\n",
      "loss: 1.8117229980634013 acc: 0.81\n",
      "loss: 1.7928975570338455 acc: 0.86\n",
      "loss: 1.8262503982227682 acc: 0.83\n",
      "loss: 1.807987421555419 acc: 0.86\n",
      "loss: 1.8031691694789194 acc: 0.84\n",
      "loss: 1.8336012501152734 acc: 0.84\n",
      "loss: 1.852312619063912 acc: 0.81\n",
      "loss: 1.8255531758720716 acc: 0.83\n",
      "loss: 1.804175175803025 acc: 0.84\n",
      "loss: 1.8415573632753452 acc: 0.81\n",
      "loss: 1.8435660268315999 acc: 0.83\n",
      "loss: 1.7717132970707528 acc: 0.87\n",
      "loss: 1.8111009371805864 acc: 0.87\n",
      "loss: 1.831565907546363 acc: 0.86\n",
      "loss: 1.7916997846146772 acc: 0.88\n",
      "loss: 1.8175239197877067 acc: 0.82\n",
      "loss: 1.815244740925204 acc: 0.83\n",
      "loss: 1.7973450556889239 acc: 0.87\n",
      "loss: 1.8310309488479808 acc: 0.86\n",
      "loss: 1.8360782899228647 acc: 0.81\n",
      "loss: 1.7973376160951668 acc: 0.86\n",
      "loss: 1.8709762476150018 acc: 0.78\n",
      "loss: 1.8258274260275837 acc: 0.83\n",
      "loss: 1.840347544258546 acc: 0.8\n",
      "loss: 1.8469432315504262 acc: 0.84\n",
      "loss: 1.8301029753084803 acc: 0.84\n",
      "loss: 1.8468489374893104 acc: 0.82\n",
      "loss: 1.813375864615926 acc: 0.83\n",
      "loss: 1.8269817624849312 acc: 0.87\n",
      "loss: 1.862728673223379 acc: 0.81\n",
      "loss: 1.7910070056411218 acc: 0.89\n",
      "loss: 1.793892624304383 acc: 0.85\n",
      "loss: 1.7982214991564982 acc: 0.86\n",
      "loss: 1.799431836260431 acc: 0.87\n",
      "loss: 1.8098989736242175 acc: 0.85\n",
      "loss: 1.8697803110791984 acc: 0.76\n",
      "loss: 1.81368675525818 acc: 0.82\n",
      "loss: 1.8278275751734447 acc: 0.83\n",
      "loss: 1.805521873415861 acc: 0.9\n",
      "loss: 1.825599112020262 acc: 0.84\n",
      "loss: 1.8410871578905315 acc: 0.84\n",
      "loss: 1.7907083767717547 acc: 0.83\n",
      "loss: 1.8015438428150645 acc: 0.87\n",
      "loss: 1.8192254971972646 acc: 0.8\n",
      "loss: 1.8478900161839755 acc: 0.84\n",
      "loss: 1.8224620748307154 acc: 0.85\n",
      "loss: 1.8273471239131458 acc: 0.81\n",
      "loss: 1.82809073145465 acc: 0.88\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 1.8281\t Accuracy 0.8800\n",
      "loss: 1.8348124848954808 acc: 0.89\n",
      "loss: 1.848831062626248 acc: 0.82\n",
      "loss: 1.814924932591376 acc: 0.91\n",
      "loss: 1.7907071271790307 acc: 0.89\n",
      "loss: 1.8060038557976297 acc: 0.87\n",
      "loss: 1.8322641722533732 acc: 0.84\n",
      "loss: 1.8377266057179835 acc: 0.81\n",
      "loss: 1.819738787867682 acc: 0.85\n",
      "loss: 1.828680661239412 acc: 0.84\n",
      "loss: 1.8542853221410778 acc: 0.79\n",
      "loss: 1.8383162468024998 acc: 0.78\n",
      "loss: 1.8181449884734422 acc: 0.84\n",
      "loss: 1.7753706312615285 acc: 0.88\n",
      "loss: 1.8035080899066944 acc: 0.84\n",
      "loss: 1.7940973720374558 acc: 0.89\n",
      "loss: 1.854527989814496 acc: 0.77\n",
      "loss: 1.8231920312115235 acc: 0.85\n",
      "loss: 1.7902014504423966 acc: 0.87\n",
      "loss: 1.8158380735325195 acc: 0.85\n",
      "loss: 1.8177005029522348 acc: 0.87\n",
      "loss: 1.8640003646121113 acc: 0.77\n",
      "loss: 1.8650829498168493 acc: 0.75\n",
      "loss: 1.8355138758176313 acc: 0.8\n",
      "loss: 1.842231332690783 acc: 0.86\n",
      "loss: 1.821972698796425 acc: 0.85\n",
      "loss: 1.8279537540255941 acc: 0.83\n",
      "loss: 1.7852628145906961 acc: 0.89\n",
      "loss: 1.8103474802568116 acc: 0.88\n",
      "loss: 1.7988553664826812 acc: 0.88\n",
      "loss: 1.8057182623855783 acc: 0.91\n",
      "loss: 1.859279637844657 acc: 0.8\n",
      "loss: 1.800671012614616 acc: 0.89\n",
      "loss: 1.846686718024195 acc: 0.87\n",
      "loss: 1.8010473956319077 acc: 0.89\n",
      "loss: 1.8210461086579535 acc: 0.87\n",
      "loss: 1.8649320723875074 acc: 0.78\n",
      "loss: 1.8151829511193756 acc: 0.88\n",
      "loss: 1.8280447029644407 acc: 0.8\n",
      "loss: 1.7908534999351808 acc: 0.91\n",
      "loss: 1.8031349077101486 acc: 0.87\n",
      "loss: 1.8205340081929406 acc: 0.84\n",
      "loss: 1.8489737907304127 acc: 0.79\n",
      "loss: 1.820900730742113 acc: 0.83\n",
      "loss: 1.767482851811568 acc: 0.94\n",
      "loss: 1.8403326552474635 acc: 0.79\n",
      "loss: 1.8564831118270524 acc: 0.82\n",
      "loss: 1.8427403768497812 acc: 0.81\n",
      "loss: 1.83913978992761 acc: 0.82\n",
      "loss: 1.8414815075599642 acc: 0.78\n",
      "loss: 1.7976927598002703 acc: 0.86\n",
      "loss: 1.799475759241053 acc: 0.88\n",
      "loss: 1.8146656694470362 acc: 0.88\n",
      "loss: 1.8353644961434723 acc: 0.87\n",
      "loss: 1.8242510315950218 acc: 0.94\n",
      "loss: 1.8090406142906068 acc: 0.85\n",
      "loss: 1.7554711684988815 acc: 0.86\n",
      "loss: 1.7663080122403014 acc: 0.86\n",
      "loss: 1.8314228404479158 acc: 0.86\n",
      "loss: 1.7548630657380946 acc: 0.94\n",
      "loss: 1.80578643485574 acc: 0.87\n",
      "loss: 1.8001871758817714 acc: 0.84\n",
      "loss: 1.8484016206760763 acc: 0.84\n",
      "loss: 1.8540436524121546 acc: 0.88\n",
      "loss: 1.8714508730332222 acc: 0.85\n",
      "loss: 1.7866781142128323 acc: 0.96\n",
      "loss: 1.8032642798079572 acc: 0.85\n",
      "loss: 1.7574997094123301 acc: 0.93\n",
      "loss: 1.7915267016568834 acc: 0.87\n",
      "loss: 1.7961456992502485 acc: 0.87\n",
      "loss: 1.8736475175105898 acc: 0.87\n",
      "loss: 1.823712050538323 acc: 0.88\n",
      "loss: 1.8292486283937044 acc: 0.76\n",
      "loss: 1.8613446273731897 acc: 0.87\n",
      "loss: 1.8305304619897482 acc: 0.9\n",
      "loss: 1.8519841826782106 acc: 0.79\n",
      "loss: 1.8551048839903999 acc: 0.81\n",
      "loss: 1.884496967890953 acc: 0.79\n",
      "loss: 1.8202506276942756 acc: 0.91\n",
      "loss: 1.7953195850787131 acc: 0.89\n",
      "loss: 1.8226377901829591 acc: 0.82\n",
      "loss: 1.7767601218755862 acc: 0.94\n",
      "loss: 1.744561065017151 acc: 0.94\n",
      "loss: 1.7941032645731414 acc: 0.92\n",
      "loss: 1.7893082093429027 acc: 0.88\n",
      "loss: 1.8153767946244905 acc: 0.9\n",
      "loss: 1.8515768284088672 acc: 0.91\n",
      "loss: 1.8434308314234817 acc: 0.89\n",
      "loss: 1.7429724401576268 acc: 0.91\n",
      "loss: 1.7186695968794397 acc: 0.97\n",
      "loss: 1.7406834340707869 acc: 0.96\n",
      "loss: 1.7471801217209206 acc: 0.96\n",
      "loss: 1.7992342986979586 acc: 0.87\n",
      "loss: 1.7814026835749195 acc: 0.84\n",
      "loss: 1.7293887472732743 acc: 0.86\n",
      "loss: 1.8061948224113404 acc: 0.9\n",
      "loss: 1.8144237271419255 acc: 0.92\n",
      "loss: 1.8826112474734993 acc: 0.79\n",
      "loss: 1.7181013864494523 acc: 0.96\n",
      "loss: 1.8665179572000201 acc: 0.85\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8237\t Average training accuracy 0.8407\n",
      "Epoch [3]\t Average validation loss 1.8063\t Average validation accuracy 0.8804\n",
      "\n",
      "loss: 1.8554651140433365 acc: 0.79\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 1.8555\t Accuracy 0.7900\n",
      "loss: 1.788979311708175 acc: 0.87\n",
      "loss: 1.7776842281807452 acc: 0.87\n",
      "loss: 1.8514357999353848 acc: 0.75\n",
      "loss: 1.843522049031756 acc: 0.84\n",
      "loss: 1.8440626323099634 acc: 0.83\n",
      "loss: 1.8399246175271233 acc: 0.88\n",
      "loss: 1.8365100320643082 acc: 0.83\n",
      "loss: 1.8426306791741973 acc: 0.85\n",
      "loss: 1.8066666544935928 acc: 0.87\n",
      "loss: 1.812423505001464 acc: 0.88\n",
      "loss: 1.861895567088962 acc: 0.84\n",
      "loss: 1.8037192042918164 acc: 0.94\n",
      "loss: 1.8412296441249227 acc: 0.83\n",
      "loss: 1.8694028051010347 acc: 0.83\n",
      "loss: 1.8069205785021152 acc: 0.86\n",
      "loss: 1.8448369014665564 acc: 0.84\n",
      "loss: 1.8616516111423633 acc: 0.81\n",
      "loss: 1.8523467584991735 acc: 0.82\n",
      "loss: 1.79889637442396 acc: 0.87\n",
      "loss: 1.7991463242223713 acc: 0.85\n",
      "loss: 1.8317066822835417 acc: 0.8\n",
      "loss: 1.8440496649622333 acc: 0.82\n",
      "loss: 1.8426358019086018 acc: 0.83\n",
      "loss: 1.7876070860140356 acc: 0.89\n",
      "loss: 1.8337132851010804 acc: 0.83\n",
      "loss: 1.8195100456282023 acc: 0.85\n",
      "loss: 1.8340713540927966 acc: 0.86\n",
      "loss: 1.861689858250013 acc: 0.8\n",
      "loss: 1.804606833276254 acc: 0.84\n",
      "loss: 1.816600899079418 acc: 0.85\n",
      "loss: 1.8156881241603384 acc: 0.83\n",
      "loss: 1.820165692862663 acc: 0.79\n",
      "loss: 1.821565654517862 acc: 0.85\n",
      "loss: 1.82446748633036 acc: 0.81\n",
      "loss: 1.840522972102149 acc: 0.79\n",
      "loss: 1.8148362027243288 acc: 0.86\n",
      "loss: 1.8158579270178217 acc: 0.86\n",
      "loss: 1.859104740460561 acc: 0.74\n",
      "loss: 1.8315795915386095 acc: 0.8\n",
      "loss: 1.8240617954609823 acc: 0.84\n",
      "loss: 1.8373366421482205 acc: 0.89\n",
      "loss: 1.825838839924511 acc: 0.86\n",
      "loss: 1.8413088211122446 acc: 0.8\n",
      "loss: 1.857985986575479 acc: 0.78\n",
      "loss: 1.8446398610659582 acc: 0.88\n",
      "loss: 1.8266443861805122 acc: 0.82\n",
      "loss: 1.8378325091484162 acc: 0.87\n",
      "loss: 1.8448448376870856 acc: 0.79\n",
      "loss: 1.8106656415133449 acc: 0.86\n",
      "loss: 1.8508667322991192 acc: 0.84\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 1.8509\t Accuracy 0.8400\n",
      "loss: 1.8591549083317702 acc: 0.78\n",
      "loss: 1.8486451209415113 acc: 0.82\n",
      "loss: 1.8150073718566382 acc: 0.81\n",
      "loss: 1.8146365388881176 acc: 0.89\n",
      "loss: 1.8235509004157742 acc: 0.86\n",
      "loss: 1.8332299610293357 acc: 0.84\n",
      "loss: 1.7966604938533868 acc: 0.85\n",
      "loss: 1.8136411549848088 acc: 0.85\n",
      "loss: 1.8045546891353865 acc: 0.92\n",
      "loss: 1.8423943641783018 acc: 0.85\n",
      "loss: 1.756273590601599 acc: 0.91\n",
      "loss: 1.8265535075479693 acc: 0.84\n",
      "loss: 1.800904584124802 acc: 0.85\n",
      "loss: 1.8386246706129368 acc: 0.81\n",
      "loss: 1.7891542616126932 acc: 0.89\n",
      "loss: 1.8204607191178672 acc: 0.83\n",
      "loss: 1.8328028195121513 acc: 0.81\n",
      "loss: 1.8153492096370503 acc: 0.84\n",
      "loss: 1.8169473374253713 acc: 0.87\n",
      "loss: 1.7884108654283912 acc: 0.88\n",
      "loss: 1.8211683372134575 acc: 0.83\n",
      "loss: 1.759662801460347 acc: 0.89\n",
      "loss: 1.8051495577310055 acc: 0.82\n",
      "loss: 1.8448847788789298 acc: 0.8\n",
      "loss: 1.8182778591665087 acc: 0.8\n",
      "loss: 1.8073157587633781 acc: 0.87\n",
      "loss: 1.8229969779590265 acc: 0.9\n",
      "loss: 1.8463858919594693 acc: 0.81\n",
      "loss: 1.7930085452318196 acc: 0.85\n",
      "loss: 1.7751698814176735 acc: 0.87\n",
      "loss: 1.8353101464885422 acc: 0.82\n",
      "loss: 1.8301391263008546 acc: 0.88\n",
      "loss: 1.8452359693607354 acc: 0.78\n",
      "loss: 1.7919298612983954 acc: 0.85\n",
      "loss: 1.8479376433941865 acc: 0.84\n",
      "loss: 1.8226885549938134 acc: 0.84\n",
      "loss: 1.8247637648575041 acc: 0.88\n",
      "loss: 1.7678862093296166 acc: 0.91\n",
      "loss: 1.8168458311915183 acc: 0.88\n",
      "loss: 1.8166012738069 acc: 0.88\n",
      "loss: 1.8248030521735998 acc: 0.77\n",
      "loss: 1.8135731458053161 acc: 0.83\n",
      "loss: 1.8489354537844969 acc: 0.81\n",
      "loss: 1.8282425344763473 acc: 0.78\n",
      "loss: 1.806643944088806 acc: 0.87\n",
      "loss: 1.8324019530155249 acc: 0.86\n",
      "loss: 1.822932804755649 acc: 0.89\n",
      "loss: 1.811868684252504 acc: 0.84\n",
      "loss: 1.8002773835724395 acc: 0.87\n",
      "loss: 1.8287350584722162 acc: 0.85\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 1.8287\t Accuracy 0.8500\n",
      "loss: 1.8894749139296112 acc: 0.81\n",
      "loss: 1.8292356397872058 acc: 0.88\n",
      "loss: 1.8235214561300033 acc: 0.84\n",
      "loss: 1.8401023486622803 acc: 0.79\n",
      "loss: 1.804594463570545 acc: 0.91\n",
      "loss: 1.8180555974354828 acc: 0.8\n",
      "loss: 1.7555847881732272 acc: 0.91\n",
      "loss: 1.8180196386434844 acc: 0.88\n",
      "loss: 1.8139433838021406 acc: 0.81\n",
      "loss: 1.8171662076694677 acc: 0.83\n",
      "loss: 1.8520593307865345 acc: 0.81\n",
      "loss: 1.807250201174923 acc: 0.81\n",
      "loss: 1.8435339959818828 acc: 0.87\n",
      "loss: 1.8219619266482934 acc: 0.81\n",
      "loss: 1.830049593394921 acc: 0.88\n",
      "loss: 1.8187938847284801 acc: 0.84\n",
      "loss: 1.827833316673305 acc: 0.88\n",
      "loss: 1.837255230186223 acc: 0.78\n",
      "loss: 1.8286893584816182 acc: 0.84\n",
      "loss: 1.7915270751407197 acc: 0.87\n",
      "loss: 1.8239509514935583 acc: 0.84\n",
      "loss: 1.84436132702126 acc: 0.8\n",
      "loss: 1.8149017618512995 acc: 0.83\n",
      "loss: 1.8286363307686546 acc: 0.85\n",
      "loss: 1.83367522550733 acc: 0.82\n",
      "loss: 1.8106477392960514 acc: 0.83\n",
      "loss: 1.789448270209079 acc: 0.84\n",
      "loss: 1.7952769557371773 acc: 0.85\n",
      "loss: 1.8546528677565641 acc: 0.74\n",
      "loss: 1.8453405454025873 acc: 0.83\n",
      "loss: 1.783216687784745 acc: 0.86\n",
      "loss: 1.852563517289863 acc: 0.8\n",
      "loss: 1.8437771147314945 acc: 0.79\n",
      "loss: 1.7923862729403248 acc: 0.89\n",
      "loss: 1.8211237435442431 acc: 0.86\n",
      "loss: 1.8166399581283048 acc: 0.9\n",
      "loss: 1.8109566303184637 acc: 0.87\n",
      "loss: 1.848087729516969 acc: 0.81\n",
      "loss: 1.8386225221425536 acc: 0.79\n",
      "loss: 1.8354893489065947 acc: 0.84\n",
      "loss: 1.7633895147840988 acc: 0.9\n",
      "loss: 1.7997546701792553 acc: 0.89\n",
      "loss: 1.8419178858421537 acc: 0.85\n",
      "loss: 1.8159127163747775 acc: 0.8\n",
      "loss: 1.8415950363299896 acc: 0.81\n",
      "loss: 1.8436288355665296 acc: 0.84\n",
      "loss: 1.8272546506022207 acc: 0.77\n",
      "loss: 1.8106892349704842 acc: 0.85\n",
      "loss: 1.8022237885885515 acc: 0.8\n",
      "loss: 1.7878571808678896 acc: 0.9\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 1.7879\t Accuracy 0.9000\n",
      "loss: 1.8249899521593707 acc: 0.87\n",
      "loss: 1.8067639685920318 acc: 0.9\n",
      "loss: 1.8559844603399878 acc: 0.82\n",
      "loss: 1.8302144551258572 acc: 0.87\n",
      "loss: 1.8210746869991636 acc: 0.85\n",
      "loss: 1.8299281257467568 acc: 0.83\n",
      "loss: 1.8524844238125533 acc: 0.82\n",
      "loss: 1.833422791646835 acc: 0.78\n",
      "loss: 1.842156110690024 acc: 0.84\n",
      "loss: 1.8286092244675698 acc: 0.84\n",
      "loss: 1.8065894587770608 acc: 0.86\n",
      "loss: 1.7999709683478364 acc: 0.83\n",
      "loss: 1.8439228557993816 acc: 0.85\n",
      "loss: 1.8199778192214169 acc: 0.89\n",
      "loss: 1.7905482105988098 acc: 0.91\n",
      "loss: 1.806864091911888 acc: 0.89\n",
      "loss: 1.834002180937775 acc: 0.82\n",
      "loss: 1.828985786400725 acc: 0.85\n",
      "loss: 1.794102643588487 acc: 0.83\n",
      "loss: 1.8283991377112938 acc: 0.8\n",
      "loss: 1.795163429962782 acc: 0.9\n",
      "loss: 1.822652900339602 acc: 0.85\n",
      "loss: 1.831854305832406 acc: 0.84\n",
      "loss: 1.8145600577285494 acc: 0.85\n",
      "loss: 1.8160577502055812 acc: 0.87\n",
      "loss: 1.827073707108052 acc: 0.83\n",
      "loss: 1.8777751270424219 acc: 0.76\n",
      "loss: 1.8043887295207153 acc: 0.85\n",
      "loss: 1.8190283317774418 acc: 0.76\n",
      "loss: 1.8018679132934472 acc: 0.89\n",
      "loss: 1.8384034126702307 acc: 0.8\n",
      "loss: 1.772728136302363 acc: 0.93\n",
      "loss: 1.822438146210142 acc: 0.82\n",
      "loss: 1.8028734781085876 acc: 0.85\n",
      "loss: 1.8287694811370534 acc: 0.81\n",
      "loss: 1.8495084572270721 acc: 0.84\n",
      "loss: 1.837578873916308 acc: 0.84\n",
      "loss: 1.8331482317985945 acc: 0.84\n",
      "loss: 1.7815995828458175 acc: 0.85\n",
      "loss: 1.8232563766227496 acc: 0.84\n",
      "loss: 1.8587393842646245 acc: 0.83\n",
      "loss: 1.8255269827900973 acc: 0.83\n",
      "loss: 1.7945564721586584 acc: 0.91\n",
      "loss: 1.8420281377412702 acc: 0.8\n",
      "loss: 1.8510313375593441 acc: 0.81\n",
      "loss: 1.8523774172507677 acc: 0.82\n",
      "loss: 1.819747331643846 acc: 0.88\n",
      "loss: 1.8529345319450155 acc: 0.84\n",
      "loss: 1.8209990597995516 acc: 0.86\n",
      "loss: 1.795917191314183 acc: 0.89\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 1.7959\t Accuracy 0.8900\n",
      "loss: 1.8332242893935446 acc: 0.86\n",
      "loss: 1.7915118895396256 acc: 0.86\n",
      "loss: 1.829350066479344 acc: 0.87\n",
      "loss: 1.8606329408237634 acc: 0.76\n",
      "loss: 1.8042672807432256 acc: 0.86\n",
      "loss: 1.7821680227773173 acc: 0.89\n",
      "loss: 1.8278719159610128 acc: 0.83\n",
      "loss: 1.7989300961640184 acc: 0.87\n",
      "loss: 1.8221606000465085 acc: 0.84\n",
      "loss: 1.8241041348628977 acc: 0.81\n",
      "loss: 1.7937945669101603 acc: 0.89\n",
      "loss: 1.8210524485521653 acc: 0.84\n",
      "loss: 1.8120767911478088 acc: 0.86\n",
      "loss: 1.832714603497486 acc: 0.87\n",
      "loss: 1.8073931354811257 acc: 0.86\n",
      "loss: 1.7670183794012158 acc: 0.9\n",
      "loss: 1.781426196085937 acc: 0.91\n",
      "loss: 1.7868518014400987 acc: 0.87\n",
      "loss: 1.783653249197574 acc: 0.88\n",
      "loss: 1.8284125590905038 acc: 0.84\n",
      "loss: 1.8221122931473857 acc: 0.85\n",
      "loss: 1.8011797312697109 acc: 0.83\n",
      "loss: 1.7882815865042847 acc: 0.89\n",
      "loss: 1.7954133824834164 acc: 0.86\n",
      "loss: 1.8304494642909013 acc: 0.84\n",
      "loss: 1.8307688985107702 acc: 0.84\n",
      "loss: 1.8302642179588346 acc: 0.8\n",
      "loss: 1.835674736772968 acc: 0.85\n",
      "loss: 1.8454951278809886 acc: 0.82\n",
      "loss: 1.8296712182986141 acc: 0.84\n",
      "loss: 1.803900743507328 acc: 0.86\n",
      "loss: 1.843643309993505 acc: 0.81\n",
      "loss: 1.7821533409916848 acc: 0.89\n",
      "loss: 1.803374726062446 acc: 0.86\n",
      "loss: 1.7877167553283695 acc: 0.88\n",
      "loss: 1.8245180180194893 acc: 0.76\n",
      "loss: 1.8187028604208177 acc: 0.82\n",
      "loss: 1.8769319516350935 acc: 0.8\n",
      "loss: 1.8147995290449348 acc: 0.84\n",
      "loss: 1.8072492207395876 acc: 0.89\n",
      "loss: 1.838179546306908 acc: 0.82\n",
      "loss: 1.8177901171757664 acc: 0.83\n",
      "loss: 1.817133511310105 acc: 0.88\n",
      "loss: 1.793331311028301 acc: 0.9\n",
      "loss: 1.8539483909022199 acc: 0.77\n",
      "loss: 1.836028531305714 acc: 0.89\n",
      "loss: 1.8352924207390282 acc: 0.91\n",
      "loss: 1.7890376343580763 acc: 0.87\n",
      "loss: 1.818091018236985 acc: 0.88\n",
      "loss: 1.8241706221598895 acc: 0.8\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 1.8242\t Accuracy 0.8000\n",
      "loss: 1.8229108775056444 acc: 0.84\n",
      "loss: 1.7762289937032214 acc: 0.9\n",
      "loss: 1.8261675707931448 acc: 0.83\n",
      "loss: 1.833351345898596 acc: 0.84\n",
      "loss: 1.8199662148948739 acc: 0.87\n",
      "loss: 1.8277323409905912 acc: 0.85\n",
      "loss: 1.7982327010348582 acc: 0.9\n",
      "loss: 1.837517728399566 acc: 0.84\n",
      "loss: 1.815881777284921 acc: 0.86\n",
      "loss: 1.8203561427023565 acc: 0.81\n",
      "loss: 1.8054393082529507 acc: 0.87\n",
      "loss: 1.8083539278310787 acc: 0.84\n",
      "loss: 1.8434123294116431 acc: 0.82\n",
      "loss: 1.8286481288327516 acc: 0.78\n",
      "loss: 1.844382026583252 acc: 0.84\n",
      "loss: 1.8368481669540728 acc: 0.81\n",
      "loss: 1.8187735932545857 acc: 0.88\n",
      "loss: 1.8105004453682787 acc: 0.88\n",
      "loss: 1.8222488337864804 acc: 0.88\n",
      "loss: 1.8866799338583198 acc: 0.74\n",
      "loss: 1.8493096577584804 acc: 0.82\n",
      "loss: 1.8207384524783572 acc: 0.82\n",
      "loss: 1.8181720083896886 acc: 0.92\n",
      "loss: 1.798631474681771 acc: 0.88\n",
      "loss: 1.8430937462050918 acc: 0.78\n",
      "loss: 1.7923372629998264 acc: 0.91\n",
      "loss: 1.8633404781859115 acc: 0.83\n",
      "loss: 1.8121588878905632 acc: 0.89\n",
      "loss: 1.8010740564055028 acc: 0.85\n",
      "loss: 1.8741535392412936 acc: 0.83\n",
      "loss: 1.8054189741406907 acc: 0.85\n",
      "loss: 1.838374451565053 acc: 0.82\n",
      "loss: 1.8407390466351172 acc: 0.83\n",
      "loss: 1.8490107987633462 acc: 0.86\n",
      "loss: 1.8005401838109438 acc: 0.83\n",
      "loss: 1.8168654482630304 acc: 0.84\n",
      "loss: 1.7989066948418884 acc: 0.87\n",
      "loss: 1.798358887653865 acc: 0.85\n",
      "loss: 1.8376859415450417 acc: 0.83\n",
      "loss: 1.7845475234048696 acc: 0.89\n",
      "loss: 1.9211371578104748 acc: 0.74\n",
      "loss: 1.8434691613284628 acc: 0.81\n",
      "loss: 1.7795717980396575 acc: 0.84\n",
      "loss: 1.7893298564770577 acc: 0.83\n",
      "loss: 1.8295706153661115 acc: 0.77\n",
      "loss: 1.8079101432774147 acc: 0.83\n",
      "loss: 1.8083217592066236 acc: 0.85\n",
      "loss: 1.830445070884671 acc: 0.76\n",
      "loss: 1.7979301402219454 acc: 0.86\n",
      "loss: 1.8672476750085358 acc: 0.77\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 1.8672\t Accuracy 0.7700\n",
      "loss: 1.791078711939886 acc: 0.87\n",
      "loss: 1.836745892804048 acc: 0.84\n",
      "loss: 1.8558263635531782 acc: 0.86\n",
      "loss: 1.8030318951484567 acc: 0.9\n",
      "loss: 1.8319249266764257 acc: 0.86\n",
      "loss: 1.864889671471612 acc: 0.83\n",
      "loss: 1.8356060795945843 acc: 0.8\n",
      "loss: 1.850947197847411 acc: 0.83\n",
      "loss: 1.8584737578946684 acc: 0.79\n",
      "loss: 1.7989411338638355 acc: 0.86\n",
      "loss: 1.8315711354072015 acc: 0.81\n",
      "loss: 1.8245384579190702 acc: 0.8\n",
      "loss: 1.834929323242121 acc: 0.86\n",
      "loss: 1.7979067555314003 acc: 0.78\n",
      "loss: 1.8324472312652198 acc: 0.85\n",
      "loss: 1.8354933905476443 acc: 0.83\n",
      "loss: 1.845980818203395 acc: 0.82\n",
      "loss: 1.8274043254612478 acc: 0.85\n",
      "loss: 1.805828848063334 acc: 0.89\n",
      "loss: 1.8285764371703672 acc: 0.86\n",
      "loss: 1.826640703286653 acc: 0.92\n",
      "loss: 1.8591411428181914 acc: 0.83\n",
      "loss: 1.8055492557372856 acc: 0.87\n",
      "loss: 1.839696843887271 acc: 0.8\n",
      "loss: 1.8481765833849142 acc: 0.84\n",
      "loss: 1.8104937763254838 acc: 0.91\n",
      "loss: 1.7963655400014926 acc: 0.93\n",
      "loss: 1.8229928645294442 acc: 0.86\n",
      "loss: 1.8302893249899055 acc: 0.87\n",
      "loss: 1.8596288807009091 acc: 0.85\n",
      "loss: 1.8238273851345383 acc: 0.87\n",
      "loss: 1.832055883875535 acc: 0.87\n",
      "loss: 1.8128810855954083 acc: 0.86\n",
      "loss: 1.792975709703438 acc: 0.91\n",
      "loss: 1.8555078231129274 acc: 0.84\n",
      "loss: 1.831345725930872 acc: 0.81\n",
      "loss: 1.805056478095971 acc: 0.85\n",
      "loss: 1.7811929131382511 acc: 0.91\n",
      "loss: 1.8156793733908594 acc: 0.93\n",
      "loss: 1.806545039272698 acc: 0.85\n",
      "loss: 1.8463378556238639 acc: 0.85\n",
      "loss: 1.8505570149525037 acc: 0.83\n",
      "loss: 1.7928281982395142 acc: 0.84\n",
      "loss: 1.832242075824235 acc: 0.82\n",
      "loss: 1.8255017730696794 acc: 0.78\n",
      "loss: 1.823710704235987 acc: 0.83\n",
      "loss: 1.78931715458395 acc: 0.93\n",
      "loss: 1.777466397212502 acc: 0.86\n",
      "loss: 1.8321031869868971 acc: 0.77\n",
      "loss: 1.8243560625604403 acc: 0.8\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 1.8244\t Accuracy 0.8000\n",
      "loss: 1.8436964784810064 acc: 0.82\n",
      "loss: 1.7951560065164285 acc: 0.88\n",
      "loss: 1.8030668893013828 acc: 0.9\n",
      "loss: 1.8275198252754268 acc: 0.83\n",
      "loss: 1.819665200395283 acc: 0.83\n",
      "loss: 1.8313811462409024 acc: 0.83\n",
      "loss: 1.8138565758577732 acc: 0.84\n",
      "loss: 1.7896208300934924 acc: 0.85\n",
      "loss: 1.825470919693829 acc: 0.84\n",
      "loss: 1.8064182534858184 acc: 0.91\n",
      "loss: 1.812364224173781 acc: 0.84\n",
      "loss: 1.812919723515858 acc: 0.92\n",
      "loss: 1.8276919439739225 acc: 0.83\n",
      "loss: 1.8708401111753696 acc: 0.71\n",
      "loss: 1.8197740989736753 acc: 0.84\n",
      "loss: 1.840758976120543 acc: 0.86\n",
      "loss: 1.829134567962898 acc: 0.91\n",
      "loss: 1.7953635318348304 acc: 0.89\n",
      "loss: 1.830541185149138 acc: 0.81\n",
      "loss: 1.808463064633537 acc: 0.87\n",
      "loss: 1.8151612841834128 acc: 0.83\n",
      "loss: 1.8153611107434733 acc: 0.87\n",
      "loss: 1.8195809160435112 acc: 0.84\n",
      "loss: 1.810089130995203 acc: 0.87\n",
      "loss: 1.8227151696141204 acc: 0.79\n",
      "loss: 1.8774320714436723 acc: 0.75\n",
      "loss: 1.8385046829264464 acc: 0.87\n",
      "loss: 1.8166870165607043 acc: 0.88\n",
      "loss: 1.768885498242541 acc: 0.9\n",
      "loss: 1.805741651884456 acc: 0.84\n",
      "loss: 1.7909511146328756 acc: 0.87\n",
      "loss: 1.8210737314316818 acc: 0.85\n",
      "loss: 1.8385658707412267 acc: 0.82\n",
      "loss: 1.802378092811877 acc: 0.88\n",
      "loss: 1.8245090307678469 acc: 0.88\n",
      "loss: 1.8218586277937876 acc: 0.81\n",
      "loss: 1.7847067511939307 acc: 0.86\n",
      "loss: 1.8112006574177082 acc: 0.82\n",
      "loss: 1.8112123076106839 acc: 0.84\n",
      "loss: 1.8497955883194448 acc: 0.79\n",
      "loss: 1.8145220383964393 acc: 0.84\n",
      "loss: 1.7730590198828906 acc: 0.89\n",
      "loss: 1.8424135040509118 acc: 0.79\n",
      "loss: 1.816195180241611 acc: 0.87\n",
      "loss: 1.8278464493480076 acc: 0.83\n",
      "loss: 1.8225004578524864 acc: 0.82\n",
      "loss: 1.7873093560689106 acc: 0.88\n",
      "loss: 1.832902544016734 acc: 0.86\n",
      "loss: 1.8262279033942845 acc: 0.82\n",
      "loss: 1.8134536524550635 acc: 0.87\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 1.8135\t Accuracy 0.8700\n",
      "loss: 1.8256945533362667 acc: 0.76\n",
      "loss: 1.8041972884543742 acc: 0.85\n",
      "loss: 1.8594742299884468 acc: 0.78\n",
      "loss: 1.8666617001906811 acc: 0.81\n",
      "loss: 1.7993933376179085 acc: 0.91\n",
      "loss: 1.8476132652259034 acc: 0.76\n",
      "loss: 1.8280921928397205 acc: 0.8\n",
      "loss: 1.8067234744491278 acc: 0.85\n",
      "loss: 1.8367181828814938 acc: 0.84\n",
      "loss: 1.794737226753356 acc: 0.87\n",
      "loss: 1.820646077410892 acc: 0.86\n",
      "loss: 1.819358473560112 acc: 0.85\n",
      "loss: 1.8262481695938235 acc: 0.83\n",
      "loss: 1.8481108479141721 acc: 0.81\n",
      "loss: 1.842595290311559 acc: 0.81\n",
      "loss: 1.8044902428594027 acc: 0.91\n",
      "loss: 1.8037748101610307 acc: 0.85\n",
      "loss: 1.820194869408943 acc: 0.86\n",
      "loss: 1.8129600235634384 acc: 0.82\n",
      "loss: 1.8627227534682376 acc: 0.83\n",
      "loss: 1.8254476433439402 acc: 0.83\n",
      "loss: 1.8152564343985989 acc: 0.87\n",
      "loss: 1.824397671857627 acc: 0.84\n",
      "loss: 1.794065064920808 acc: 0.85\n",
      "loss: 1.7658595208302876 acc: 0.87\n",
      "loss: 1.785056308228417 acc: 0.9\n",
      "loss: 1.8128412376137166 acc: 0.84\n",
      "loss: 1.8061167776215654 acc: 0.83\n",
      "loss: 1.8285672046528776 acc: 0.81\n",
      "loss: 1.8649285454389857 acc: 0.8\n",
      "loss: 1.789296362185175 acc: 0.87\n",
      "loss: 1.7796021239132573 acc: 0.91\n",
      "loss: 1.8113504136492482 acc: 0.86\n",
      "loss: 1.8190831866156056 acc: 0.87\n",
      "loss: 1.8471449387716756 acc: 0.75\n",
      "loss: 1.8166589619502305 acc: 0.87\n",
      "loss: 1.8004197820582286 acc: 0.86\n",
      "loss: 1.8571549720627283 acc: 0.84\n",
      "loss: 1.841684185115746 acc: 0.85\n",
      "loss: 1.819471178374228 acc: 0.87\n",
      "loss: 1.849960736493731 acc: 0.82\n",
      "loss: 1.8716295328767996 acc: 0.8\n",
      "loss: 1.7695783578534618 acc: 0.85\n",
      "loss: 1.8185990462792265 acc: 0.87\n",
      "loss: 1.807497455412416 acc: 0.88\n",
      "loss: 1.8072294617904314 acc: 0.89\n",
      "loss: 1.8290175299259386 acc: 0.87\n",
      "loss: 1.7898858192384604 acc: 0.87\n",
      "loss: 1.8571173082840966 acc: 0.77\n",
      "loss: 1.844250437409289 acc: 0.8\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 1.8443\t Accuracy 0.8000\n",
      "loss: 1.8335176544342935 acc: 0.81\n",
      "loss: 1.800086754731814 acc: 0.87\n",
      "loss: 1.8336668053351366 acc: 0.83\n",
      "loss: 1.8136997068403082 acc: 0.84\n",
      "loss: 1.7778389681248428 acc: 0.83\n",
      "loss: 1.869077314616776 acc: 0.79\n",
      "loss: 1.8430407328960214 acc: 0.93\n",
      "loss: 1.8154373456074016 acc: 0.84\n",
      "loss: 1.8016928357740565 acc: 0.88\n",
      "loss: 1.8266224006521774 acc: 0.83\n",
      "loss: 1.8048250208184524 acc: 0.81\n",
      "loss: 1.8423066280575437 acc: 0.79\n",
      "loss: 1.801151161082822 acc: 0.89\n",
      "loss: 1.8120209612728626 acc: 0.78\n",
      "loss: 1.8252569318924734 acc: 0.86\n",
      "loss: 1.8142122720146485 acc: 0.83\n",
      "loss: 1.8877545394956838 acc: 0.8\n",
      "loss: 1.8711828338385101 acc: 0.84\n",
      "loss: 1.8414463402009595 acc: 0.86\n",
      "loss: 1.8366062678526693 acc: 0.78\n",
      "loss: 1.8789178607361294 acc: 0.82\n",
      "loss: 1.833596470142858 acc: 0.81\n",
      "loss: 1.8360474521671943 acc: 0.76\n",
      "loss: 1.8696900716565588 acc: 0.77\n",
      "loss: 1.8219064266083476 acc: 0.9\n",
      "loss: 1.793922533844745 acc: 0.86\n",
      "loss: 1.8565589739726933 acc: 0.81\n",
      "loss: 1.8337403651942674 acc: 0.81\n",
      "loss: 1.8141100882198575 acc: 0.88\n",
      "loss: 1.8163317162580837 acc: 0.86\n",
      "loss: 1.8148713917018109 acc: 0.85\n",
      "loss: 1.8409218610929106 acc: 0.88\n",
      "loss: 1.8332695036780746 acc: 0.84\n",
      "loss: 1.7799264990755097 acc: 0.9\n",
      "loss: 1.8150445067238947 acc: 0.83\n",
      "loss: 1.8865300441704376 acc: 0.76\n",
      "loss: 1.7930856569082687 acc: 0.86\n",
      "loss: 1.8320045664876727 acc: 0.83\n",
      "loss: 1.8117320232044245 acc: 0.86\n",
      "loss: 1.823119153062167 acc: 0.79\n",
      "loss: 1.7856749919809456 acc: 0.9\n",
      "loss: 1.8182950546774566 acc: 0.88\n",
      "loss: 1.8578875209253172 acc: 0.8\n",
      "loss: 1.8057532506753984 acc: 0.86\n",
      "loss: 1.8771445640371807 acc: 0.78\n",
      "loss: 1.804359689826348 acc: 0.86\n",
      "loss: 1.8606637815397213 acc: 0.81\n",
      "loss: 1.8345590370347253 acc: 0.79\n",
      "loss: 1.8355326432130945 acc: 0.82\n",
      "loss: 1.7991094517076756 acc: 0.84\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 1.7991\t Accuracy 0.8400\n",
      "loss: 1.849584639862543 acc: 0.83\n",
      "loss: 1.8077870689964914 acc: 0.9\n",
      "loss: 1.832511603531576 acc: 0.85\n",
      "loss: 1.7755511625754479 acc: 0.88\n",
      "loss: 1.788806211785061 acc: 0.87\n",
      "loss: 1.8253626169991228 acc: 0.81\n",
      "loss: 1.7890318153103595 acc: 0.89\n",
      "loss: 1.8297388102016054 acc: 0.77\n",
      "loss: 1.8154535078709204 acc: 0.91\n",
      "loss: 1.8239246802641103 acc: 0.87\n",
      "loss: 1.8446526859203147 acc: 0.87\n",
      "loss: 1.855704071395938 acc: 0.79\n",
      "loss: 1.8097168389134857 acc: 0.91\n",
      "loss: 1.8054471044975589 acc: 0.85\n",
      "loss: 1.81992049277862 acc: 0.8\n",
      "loss: 1.8156147127161515 acc: 0.85\n",
      "loss: 1.807430192091295 acc: 0.83\n",
      "loss: 1.845687987053252 acc: 0.79\n",
      "loss: 1.8415297643789414 acc: 0.79\n",
      "loss: 1.8494910657172385 acc: 0.83\n",
      "loss: 1.8548108470063283 acc: 0.83\n",
      "loss: 1.820252496668846 acc: 0.8\n",
      "loss: 1.8226788424112434 acc: 0.83\n",
      "loss: 1.8104327574721717 acc: 0.88\n",
      "loss: 1.8417183369944357 acc: 0.85\n",
      "loss: 1.784277358392836 acc: 0.89\n",
      "loss: 1.8442308435748487 acc: 0.83\n",
      "loss: 1.8457800691151354 acc: 0.84\n",
      "loss: 1.841522317447389 acc: 0.84\n",
      "loss: 1.8303064935198945 acc: 0.81\n",
      "loss: 1.8025998129568734 acc: 0.9\n",
      "loss: 1.8272039827293012 acc: 0.83\n",
      "loss: 1.7979467897872423 acc: 0.86\n",
      "loss: 1.8249681196445757 acc: 0.9\n",
      "loss: 1.824364914681926 acc: 0.84\n",
      "loss: 1.8021208788947496 acc: 0.85\n",
      "loss: 1.7995345624405183 acc: 0.9\n",
      "loss: 1.8148090035012114 acc: 0.84\n",
      "loss: 1.8366764577041013 acc: 0.86\n",
      "loss: 1.8163368358034697 acc: 0.82\n",
      "loss: 1.796185429059683 acc: 0.9\n",
      "loss: 1.8547650247101435 acc: 0.81\n",
      "loss: 1.7970205212602954 acc: 0.83\n",
      "loss: 1.8119618344834985 acc: 0.85\n",
      "loss: 1.8141708567901693 acc: 0.87\n",
      "loss: 1.829756754816606 acc: 0.86\n",
      "loss: 1.8706354873146227 acc: 0.79\n",
      "loss: 1.8422346443344961 acc: 0.83\n",
      "loss: 1.8221826934556424 acc: 0.89\n",
      "loss: 1.7904659567153671 acc: 0.87\n",
      "loss: 1.787935559558192 acc: 0.91\n",
      "loss: 1.8035448063021644 acc: 0.89\n",
      "loss: 1.8204559429164389 acc: 0.88\n",
      "loss: 1.8203104683801907 acc: 0.94\n",
      "loss: 1.8001651174283888 acc: 0.88\n",
      "loss: 1.7487782700252907 acc: 0.85\n",
      "loss: 1.7529853321278384 acc: 0.85\n",
      "loss: 1.8243826892800952 acc: 0.82\n",
      "loss: 1.7450779172749507 acc: 0.93\n",
      "loss: 1.79626931349809 acc: 0.9\n",
      "loss: 1.7909070216946685 acc: 0.84\n",
      "loss: 1.8429810453112896 acc: 0.83\n",
      "loss: 1.8530480897827106 acc: 0.83\n",
      "loss: 1.867767740821356 acc: 0.85\n",
      "loss: 1.7774413841705095 acc: 0.93\n",
      "loss: 1.7901238720472492 acc: 0.84\n",
      "loss: 1.7578163515221044 acc: 0.9\n",
      "loss: 1.7808086813531236 acc: 0.88\n",
      "loss: 1.7832580780620932 acc: 0.87\n",
      "loss: 1.8635743090511576 acc: 0.87\n",
      "loss: 1.8142029949512066 acc: 0.89\n",
      "loss: 1.816608779627993 acc: 0.75\n",
      "loss: 1.8457718908456826 acc: 0.85\n",
      "loss: 1.8279206450127803 acc: 0.89\n",
      "loss: 1.8450709363851194 acc: 0.77\n",
      "loss: 1.8477471252217448 acc: 0.8\n",
      "loss: 1.875706554467509 acc: 0.79\n",
      "loss: 1.8137393197792346 acc: 0.9\n",
      "loss: 1.7927511102047353 acc: 0.89\n",
      "loss: 1.8120068790858412 acc: 0.85\n",
      "loss: 1.7775927768538424 acc: 0.95\n",
      "loss: 1.7386768691818912 acc: 0.93\n",
      "loss: 1.783756484871074 acc: 0.89\n",
      "loss: 1.7674216103507252 acc: 0.86\n",
      "loss: 1.8023043306107391 acc: 0.89\n",
      "loss: 1.855001365053616 acc: 0.9\n",
      "loss: 1.8337681224079663 acc: 0.87\n",
      "loss: 1.7273879335138753 acc: 0.93\n",
      "loss: 1.709492876180002 acc: 0.97\n",
      "loss: 1.7364657390777398 acc: 0.93\n",
      "loss: 1.7416977799701863 acc: 0.97\n",
      "loss: 1.7896692207592464 acc: 0.87\n",
      "loss: 1.773233521051732 acc: 0.85\n",
      "loss: 1.7183532313298315 acc: 0.84\n",
      "loss: 1.8016486458170289 acc: 0.89\n",
      "loss: 1.8100351719909538 acc: 0.92\n",
      "loss: 1.8857040264732492 acc: 0.8\n",
      "loss: 1.70363747860072 acc: 0.95\n",
      "loss: 1.858318741364835 acc: 0.87\n",
      "\n",
      "Epoch [4]\t Average training loss 1.8226\t Average training accuracy 0.8433\n",
      "Epoch [4]\t Average validation loss 1.7981\t Average validation accuracy 0.8764\n",
      "\n",
      "loss: 1.8315978848110714 acc: 0.79\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 1.8316\t Accuracy 0.7900\n",
      "loss: 1.8133039753696352 acc: 0.82\n",
      "loss: 1.817024748657901 acc: 0.87\n",
      "loss: 1.8194974787847593 acc: 0.88\n",
      "loss: 1.8423486515607235 acc: 0.8\n",
      "loss: 1.8526064395159814 acc: 0.8\n",
      "loss: 1.8611033215187291 acc: 0.78\n",
      "loss: 1.8139297381466186 acc: 0.86\n",
      "loss: 1.8001772523258208 acc: 0.88\n",
      "loss: 1.7892485030844647 acc: 0.88\n",
      "loss: 1.825660659226905 acc: 0.88\n",
      "loss: 1.7963110329426 acc: 0.83\n",
      "loss: 1.8074057010441786 acc: 0.85\n",
      "loss: 1.8458428191511902 acc: 0.87\n",
      "loss: 1.8004156825019475 acc: 0.9\n",
      "loss: 1.8284103398579041 acc: 0.82\n",
      "loss: 1.8152609001546578 acc: 0.85\n",
      "loss: 1.8428872685197542 acc: 0.87\n",
      "loss: 1.8396563599038647 acc: 0.82\n",
      "loss: 1.836673817695673 acc: 0.82\n",
      "loss: 1.903691553151221 acc: 0.74\n",
      "loss: 1.8332639149676497 acc: 0.77\n",
      "loss: 1.807376709725149 acc: 0.89\n",
      "loss: 1.843839213611453 acc: 0.75\n",
      "loss: 1.8624405317179782 acc: 0.8\n",
      "loss: 1.815843811950723 acc: 0.82\n",
      "loss: 1.8439816071282016 acc: 0.86\n",
      "loss: 1.8194847737673046 acc: 0.86\n",
      "loss: 1.820978641462664 acc: 0.81\n",
      "loss: 1.8471157920393189 acc: 0.82\n",
      "loss: 1.8326765537867862 acc: 0.83\n",
      "loss: 1.813576393249295 acc: 0.84\n",
      "loss: 1.8049358219485843 acc: 0.9\n",
      "loss: 1.8292590458507039 acc: 0.83\n",
      "loss: 1.8350170674052593 acc: 0.85\n",
      "loss: 1.8189694965003407 acc: 0.85\n",
      "loss: 1.8323836002883318 acc: 0.8\n",
      "loss: 1.8526204993497881 acc: 0.83\n",
      "loss: 1.8491196925177042 acc: 0.82\n",
      "loss: 1.8344525236281815 acc: 0.84\n",
      "loss: 1.806755767735197 acc: 0.87\n",
      "loss: 1.7951485550259232 acc: 0.89\n",
      "loss: 1.823098258499119 acc: 0.88\n",
      "loss: 1.838722887761451 acc: 0.87\n",
      "loss: 1.808461836860709 acc: 0.89\n",
      "loss: 1.7899166652263545 acc: 0.87\n",
      "loss: 1.839212013131744 acc: 0.81\n",
      "loss: 1.7937208576317902 acc: 0.9\n",
      "loss: 1.8281381603986606 acc: 0.83\n",
      "loss: 1.834187817967285 acc: 0.76\n",
      "loss: 1.819707663286604 acc: 0.89\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 1.8197\t Accuracy 0.8900\n",
      "loss: 1.812081738839513 acc: 0.84\n",
      "loss: 1.8060580567739473 acc: 0.8\n",
      "loss: 1.8593006850844482 acc: 0.83\n",
      "loss: 1.8035810136075656 acc: 0.84\n",
      "loss: 1.830592087356438 acc: 0.85\n",
      "loss: 1.8152947836026578 acc: 0.84\n",
      "loss: 1.8337849207395123 acc: 0.79\n",
      "loss: 1.8087198998868874 acc: 0.83\n",
      "loss: 1.8211673417857301 acc: 0.82\n",
      "loss: 1.803722680462624 acc: 0.86\n",
      "loss: 1.8268330002996191 acc: 0.82\n",
      "loss: 1.7793642972490193 acc: 0.86\n",
      "loss: 1.8065155936421362 acc: 0.83\n",
      "loss: 1.851063121361734 acc: 0.84\n",
      "loss: 1.8264216358623255 acc: 0.83\n",
      "loss: 1.8133924492643965 acc: 0.84\n",
      "loss: 1.8104956322347405 acc: 0.86\n",
      "loss: 1.8547382080784356 acc: 0.83\n",
      "loss: 1.8287337079507677 acc: 0.83\n",
      "loss: 1.8558578726865966 acc: 0.82\n",
      "loss: 1.7954021479737239 acc: 0.88\n",
      "loss: 1.835511743939036 acc: 0.83\n",
      "loss: 1.8592060195831326 acc: 0.8\n",
      "loss: 1.8446384747383633 acc: 0.84\n",
      "loss: 1.822266499471883 acc: 0.86\n",
      "loss: 1.8497311959444613 acc: 0.83\n",
      "loss: 1.82672018846017 acc: 0.87\n",
      "loss: 1.855416986703822 acc: 0.85\n",
      "loss: 1.8373923514168178 acc: 0.81\n",
      "loss: 1.8117445448418428 acc: 0.85\n",
      "loss: 1.8213170669654206 acc: 0.81\n",
      "loss: 1.8383830841203783 acc: 0.8\n",
      "loss: 1.83082627582461 acc: 0.87\n",
      "loss: 1.841240611137142 acc: 0.8\n",
      "loss: 1.8417397219888787 acc: 0.83\n",
      "loss: 1.8102158789346294 acc: 0.85\n",
      "loss: 1.8223627918139405 acc: 0.85\n",
      "loss: 1.8040669325187202 acc: 0.9\n",
      "loss: 1.8298771029189282 acc: 0.87\n",
      "loss: 1.8114639306864098 acc: 0.9\n",
      "loss: 1.7931146867254038 acc: 0.87\n",
      "loss: 1.82084446430287 acc: 0.88\n",
      "loss: 1.8352170791690599 acc: 0.81\n",
      "loss: 1.8166454381963997 acc: 0.84\n",
      "loss: 1.8159568161038373 acc: 0.78\n",
      "loss: 1.8306155574010565 acc: 0.83\n",
      "loss: 1.7813932220464799 acc: 0.89\n",
      "loss: 1.8405791180137694 acc: 0.76\n",
      "loss: 1.8509785312864258 acc: 0.86\n",
      "loss: 1.784780961002551 acc: 0.86\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 1.7848\t Accuracy 0.8600\n",
      "loss: 1.8135015325387294 acc: 0.88\n",
      "loss: 1.8120883020432417 acc: 0.84\n",
      "loss: 1.8214694556938016 acc: 0.86\n",
      "loss: 1.7917168083373682 acc: 0.84\n",
      "loss: 1.8407208805239095 acc: 0.8\n",
      "loss: 1.802976418967703 acc: 0.81\n",
      "loss: 1.8060015037607948 acc: 0.83\n",
      "loss: 1.8430747959388498 acc: 0.84\n",
      "loss: 1.836858887693774 acc: 0.83\n",
      "loss: 1.8010077236778534 acc: 0.82\n",
      "loss: 1.8134750059483966 acc: 0.85\n",
      "loss: 1.8777834943711422 acc: 0.79\n",
      "loss: 1.7879279080769495 acc: 0.89\n",
      "loss: 1.7824780663181858 acc: 0.89\n",
      "loss: 1.8223109159337374 acc: 0.82\n",
      "loss: 1.7951806440968907 acc: 0.84\n",
      "loss: 1.8450636507068392 acc: 0.86\n",
      "loss: 1.8487803214563652 acc: 0.78\n",
      "loss: 1.8497346486120185 acc: 0.83\n",
      "loss: 1.8253315902430451 acc: 0.85\n",
      "loss: 1.8091875486910036 acc: 0.85\n",
      "loss: 1.7909500858366174 acc: 0.88\n",
      "loss: 1.8534679874194029 acc: 0.85\n",
      "loss: 1.7981662693217912 acc: 0.88\n",
      "loss: 1.80360610076832 acc: 0.9\n",
      "loss: 1.8085964230827536 acc: 0.87\n",
      "loss: 1.8114944214285973 acc: 0.86\n",
      "loss: 1.8163189192536382 acc: 0.84\n",
      "loss: 1.8121392619486487 acc: 0.82\n",
      "loss: 1.8157286991857229 acc: 0.85\n",
      "loss: 1.8063191365831042 acc: 0.89\n",
      "loss: 1.8688435002099026 acc: 0.8\n",
      "loss: 1.8279802303018524 acc: 0.85\n",
      "loss: 1.8468165961906513 acc: 0.87\n",
      "loss: 1.8381584303912748 acc: 0.84\n",
      "loss: 1.8458834676857876 acc: 0.84\n",
      "loss: 1.8187046789518049 acc: 0.86\n",
      "loss: 1.7895413970852059 acc: 0.85\n",
      "loss: 1.859365524536752 acc: 0.76\n",
      "loss: 1.832644466058088 acc: 0.78\n",
      "loss: 1.84709215801984 acc: 0.8\n",
      "loss: 1.8389954136342193 acc: 0.8\n",
      "loss: 1.8449967968600325 acc: 0.8\n",
      "loss: 1.822911285047316 acc: 0.87\n",
      "loss: 1.8332615122921814 acc: 0.82\n",
      "loss: 1.7924449869568795 acc: 0.86\n",
      "loss: 1.8045115962520653 acc: 0.86\n",
      "loss: 1.83396782765965 acc: 0.85\n",
      "loss: 1.8193778959159355 acc: 0.84\n",
      "loss: 1.8152753167528777 acc: 0.89\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 1.8153\t Accuracy 0.8900\n",
      "loss: 1.8513271375717983 acc: 0.81\n",
      "loss: 1.8438199235033377 acc: 0.82\n",
      "loss: 1.8215201611112972 acc: 0.82\n",
      "loss: 1.835014806613281 acc: 0.81\n",
      "loss: 1.7876620831877386 acc: 0.83\n",
      "loss: 1.784430444379069 acc: 0.87\n",
      "loss: 1.8253959484392341 acc: 0.91\n",
      "loss: 1.78503665665939 acc: 0.87\n",
      "loss: 1.8232730830663986 acc: 0.9\n",
      "loss: 1.7869068202962515 acc: 0.86\n",
      "loss: 1.832814894090197 acc: 0.84\n",
      "loss: 1.8297124742256745 acc: 0.8\n",
      "loss: 1.865104020875979 acc: 0.85\n",
      "loss: 1.841560856293296 acc: 0.88\n",
      "loss: 1.8255029232942268 acc: 0.85\n",
      "loss: 1.8446612395009878 acc: 0.84\n",
      "loss: 1.8239733232075266 acc: 0.83\n",
      "loss: 1.847853445821315 acc: 0.85\n",
      "loss: 1.7908891690627682 acc: 0.82\n",
      "loss: 1.7762406406674425 acc: 0.87\n",
      "loss: 1.8308358010860166 acc: 0.88\n",
      "loss: 1.7801927632894623 acc: 0.9\n",
      "loss: 1.8232160327123241 acc: 0.85\n",
      "loss: 1.8308938167408355 acc: 0.82\n",
      "loss: 1.8357730373102257 acc: 0.85\n",
      "loss: 1.8353964766781197 acc: 0.83\n",
      "loss: 1.843521117258135 acc: 0.84\n",
      "loss: 1.861294284658954 acc: 0.82\n",
      "loss: 1.8100985241203262 acc: 0.86\n",
      "loss: 1.798453752551839 acc: 0.86\n",
      "loss: 1.8522028536786015 acc: 0.81\n",
      "loss: 1.8205451724326867 acc: 0.82\n",
      "loss: 1.8474091374491932 acc: 0.83\n",
      "loss: 1.8089013869169883 acc: 0.85\n",
      "loss: 1.8348806197759904 acc: 0.84\n",
      "loss: 1.814252288896804 acc: 0.92\n",
      "loss: 1.8283529680949033 acc: 0.81\n",
      "loss: 1.832944053505841 acc: 0.8\n",
      "loss: 1.814843568550311 acc: 0.84\n",
      "loss: 1.8626860535857597 acc: 0.8\n",
      "loss: 1.8656240496337892 acc: 0.8\n",
      "loss: 1.813035071590579 acc: 0.87\n",
      "loss: 1.8192180139583491 acc: 0.8\n",
      "loss: 1.8150726629803589 acc: 0.87\n",
      "loss: 1.8273177553576525 acc: 0.84\n",
      "loss: 1.8246348464078925 acc: 0.83\n",
      "loss: 1.8360040202241443 acc: 0.83\n",
      "loss: 1.8425166879869976 acc: 0.81\n",
      "loss: 1.7933225093522247 acc: 0.84\n",
      "loss: 1.829005952696903 acc: 0.82\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 1.8290\t Accuracy 0.8200\n",
      "loss: 1.8222416761912101 acc: 0.82\n",
      "loss: 1.8244706478958053 acc: 0.89\n",
      "loss: 1.7868779080005093 acc: 0.9\n",
      "loss: 1.8420972554967525 acc: 0.82\n",
      "loss: 1.8212173812889583 acc: 0.78\n",
      "loss: 1.8125458154193168 acc: 0.85\n",
      "loss: 1.8496547451354888 acc: 0.81\n",
      "loss: 1.7733232418608802 acc: 0.88\n",
      "loss: 1.8384014086361924 acc: 0.84\n",
      "loss: 1.8131251360635512 acc: 0.88\n",
      "loss: 1.8797848554511032 acc: 0.74\n",
      "loss: 1.7994460655378481 acc: 0.85\n",
      "loss: 1.8359192357459562 acc: 0.83\n",
      "loss: 1.8534246809386323 acc: 0.79\n",
      "loss: 1.7880084740964362 acc: 0.86\n",
      "loss: 1.7768687821519469 acc: 0.93\n",
      "loss: 1.8077316957249543 acc: 0.84\n",
      "loss: 1.8771017280269304 acc: 0.8\n",
      "loss: 1.8472259797957176 acc: 0.78\n",
      "loss: 1.8547243038604906 acc: 0.83\n",
      "loss: 1.8246821701450076 acc: 0.87\n",
      "loss: 1.8034570213812595 acc: 0.87\n",
      "loss: 1.7877714753048708 acc: 0.86\n",
      "loss: 1.8100375769246966 acc: 0.84\n",
      "loss: 1.8490616498691383 acc: 0.81\n",
      "loss: 1.7942284786093985 acc: 0.83\n",
      "loss: 1.8321203199064806 acc: 0.84\n",
      "loss: 1.7879992078990172 acc: 0.95\n",
      "loss: 1.8304122642372707 acc: 0.81\n",
      "loss: 1.8549326621879043 acc: 0.81\n",
      "loss: 1.8047057635351993 acc: 0.86\n",
      "loss: 1.8230390856215237 acc: 0.82\n",
      "loss: 1.8268716167745893 acc: 0.88\n",
      "loss: 1.8021321151027154 acc: 0.88\n",
      "loss: 1.834450374772405 acc: 0.86\n",
      "loss: 1.7686648808885013 acc: 0.89\n",
      "loss: 1.8279168361878948 acc: 0.79\n",
      "loss: 1.8299405406595755 acc: 0.85\n",
      "loss: 1.797667791141747 acc: 0.9\n",
      "loss: 1.8174428319199256 acc: 0.87\n",
      "loss: 1.8317351892062184 acc: 0.83\n",
      "loss: 1.8098908122444357 acc: 0.85\n",
      "loss: 1.8370648037799564 acc: 0.82\n",
      "loss: 1.8352567452465565 acc: 0.81\n",
      "loss: 1.8784006297614213 acc: 0.71\n",
      "loss: 1.7976539432129883 acc: 0.9\n",
      "loss: 1.8287706618918391 acc: 0.84\n",
      "loss: 1.8102058878572864 acc: 0.89\n",
      "loss: 1.8058182731563042 acc: 0.86\n",
      "loss: 1.797074574609926 acc: 0.92\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 1.7971\t Accuracy 0.9200\n",
      "loss: 1.7830462696876137 acc: 0.86\n",
      "loss: 1.7709590815318452 acc: 0.86\n",
      "loss: 1.8392098459969346 acc: 0.81\n",
      "loss: 1.774764068768164 acc: 0.85\n",
      "loss: 1.8421532375239602 acc: 0.79\n",
      "loss: 1.8255477853711013 acc: 0.81\n",
      "loss: 1.8255344402057878 acc: 0.85\n",
      "loss: 1.7965275479488991 acc: 0.88\n",
      "loss: 1.8362129740299096 acc: 0.9\n",
      "loss: 1.8049983793535174 acc: 0.88\n",
      "loss: 1.8180565512517455 acc: 0.84\n",
      "loss: 1.857712852449211 acc: 0.77\n",
      "loss: 1.7653986237824855 acc: 0.88\n",
      "loss: 1.8247889249925462 acc: 0.8\n",
      "loss: 1.8308710790654996 acc: 0.86\n",
      "loss: 1.8192828523516744 acc: 0.84\n",
      "loss: 1.8186146386515147 acc: 0.86\n",
      "loss: 1.790777131596505 acc: 0.88\n",
      "loss: 1.8106956286244573 acc: 0.84\n",
      "loss: 1.775980447640401 acc: 0.93\n",
      "loss: 1.7977993419212694 acc: 0.88\n",
      "loss: 1.790127434255301 acc: 0.89\n",
      "loss: 1.8191640235335718 acc: 0.87\n",
      "loss: 1.8395631224453723 acc: 0.81\n",
      "loss: 1.7910957104929408 acc: 0.86\n",
      "loss: 1.773775694405372 acc: 0.9\n",
      "loss: 1.8098827240877382 acc: 0.87\n",
      "loss: 1.776645073425498 acc: 0.85\n",
      "loss: 1.8682150729885376 acc: 0.78\n",
      "loss: 1.878142020221874 acc: 0.79\n",
      "loss: 1.8044172992559169 acc: 0.88\n",
      "loss: 1.8325234461887987 acc: 0.82\n",
      "loss: 1.8178580444096126 acc: 0.83\n",
      "loss: 1.8064250717841397 acc: 0.85\n",
      "loss: 1.8466207805847399 acc: 0.79\n",
      "loss: 1.8316239260671063 acc: 0.81\n",
      "loss: 1.8533153745714055 acc: 0.84\n",
      "loss: 1.7945084432688143 acc: 0.85\n",
      "loss: 1.8658594520184206 acc: 0.79\n",
      "loss: 1.8367828393649928 acc: 0.82\n",
      "loss: 1.8198598038764353 acc: 0.83\n",
      "loss: 1.8851681303841672 acc: 0.79\n",
      "loss: 1.8410935060932097 acc: 0.79\n",
      "loss: 1.8317326086513994 acc: 0.84\n",
      "loss: 1.8001981817742385 acc: 0.85\n",
      "loss: 1.856534519689917 acc: 0.85\n",
      "loss: 1.8401039750697685 acc: 0.87\n",
      "loss: 1.817522269034101 acc: 0.84\n",
      "loss: 1.8202428987972603 acc: 0.84\n",
      "loss: 1.8328441504544872 acc: 0.87\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 1.8328\t Accuracy 0.8700\n",
      "loss: 1.8586684510020939 acc: 0.83\n",
      "loss: 1.7923485839595008 acc: 0.87\n",
      "loss: 1.7998097495007355 acc: 0.85\n",
      "loss: 1.8019693732167006 acc: 0.87\n",
      "loss: 1.8387223575042007 acc: 0.84\n",
      "loss: 1.8314019283518388 acc: 0.82\n",
      "loss: 1.8143308236332485 acc: 0.85\n",
      "loss: 1.8481343191345148 acc: 0.83\n",
      "loss: 1.8101274568698034 acc: 0.87\n",
      "loss: 1.796079368295952 acc: 0.84\n",
      "loss: 1.8289757976199608 acc: 0.86\n",
      "loss: 1.853143605108242 acc: 0.83\n",
      "loss: 1.8148269149963565 acc: 0.83\n",
      "loss: 1.7946372808673565 acc: 0.89\n",
      "loss: 1.7821883482265555 acc: 0.91\n",
      "loss: 1.8281685845636182 acc: 0.8\n",
      "loss: 1.8493517868754288 acc: 0.8\n",
      "loss: 1.7955849000443387 acc: 0.89\n",
      "loss: 1.8039418624105903 acc: 0.87\n",
      "loss: 1.7598298664509477 acc: 0.83\n",
      "loss: 1.8107652809306187 acc: 0.85\n",
      "loss: 1.8558581631632811 acc: 0.81\n",
      "loss: 1.8233442722183244 acc: 0.86\n",
      "loss: 1.8020183940345251 acc: 0.87\n",
      "loss: 1.8318871410169486 acc: 0.8\n",
      "loss: 1.806838162091507 acc: 0.85\n",
      "loss: 1.8001181170548404 acc: 0.88\n",
      "loss: 1.8330912837025284 acc: 0.79\n",
      "loss: 1.7975370133405932 acc: 0.89\n",
      "loss: 1.8013037455040963 acc: 0.86\n",
      "loss: 1.8366487823699138 acc: 0.81\n",
      "loss: 1.8471015102712705 acc: 0.85\n",
      "loss: 1.8382600134089822 acc: 0.82\n",
      "loss: 1.8180638239501605 acc: 0.89\n",
      "loss: 1.8189034115353726 acc: 0.82\n",
      "loss: 1.8410798945081346 acc: 0.87\n",
      "loss: 1.8653719943288953 acc: 0.81\n",
      "loss: 1.7969347057433285 acc: 0.86\n",
      "loss: 1.8181743959619845 acc: 0.84\n",
      "loss: 1.864142535583887 acc: 0.81\n",
      "loss: 1.839585017654774 acc: 0.83\n",
      "loss: 1.871454059657355 acc: 0.77\n",
      "loss: 1.8111564409854934 acc: 0.88\n",
      "loss: 1.8281257349627478 acc: 0.85\n",
      "loss: 1.8614698205568132 acc: 0.79\n",
      "loss: 1.8403882052916343 acc: 0.83\n",
      "loss: 1.7922664497129766 acc: 0.88\n",
      "loss: 1.8209917034515382 acc: 0.87\n",
      "loss: 1.83949590705816 acc: 0.86\n",
      "loss: 1.8188877653220823 acc: 0.88\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 1.8189\t Accuracy 0.8800\n",
      "loss: 1.8173893489578117 acc: 0.84\n",
      "loss: 1.8544709403821036 acc: 0.84\n",
      "loss: 1.8075663425846105 acc: 0.85\n",
      "loss: 1.8145417498117535 acc: 0.84\n",
      "loss: 1.8207584220953108 acc: 0.85\n",
      "loss: 1.8246634427576152 acc: 0.88\n",
      "loss: 1.8060965374076365 acc: 0.82\n",
      "loss: 1.8455610082086438 acc: 0.82\n",
      "loss: 1.8018944348272856 acc: 0.85\n",
      "loss: 1.8599109660655078 acc: 0.82\n",
      "loss: 1.8395574413007265 acc: 0.85\n",
      "loss: 1.8229236510321116 acc: 0.8\n",
      "loss: 1.7855507679350007 acc: 0.85\n",
      "loss: 1.8045275234972908 acc: 0.85\n",
      "loss: 1.8141024001346213 acc: 0.86\n",
      "loss: 1.816011461681225 acc: 0.83\n",
      "loss: 1.8104782086474085 acc: 0.87\n",
      "loss: 1.8121316313148421 acc: 0.8\n",
      "loss: 1.805527512694199 acc: 0.87\n",
      "loss: 1.803674692487125 acc: 0.84\n",
      "loss: 1.8178669289533083 acc: 0.86\n",
      "loss: 1.814779590836199 acc: 0.85\n",
      "loss: 1.8452321227958186 acc: 0.82\n",
      "loss: 1.80510158702223 acc: 0.88\n",
      "loss: 1.828297905207701 acc: 0.82\n",
      "loss: 1.8089490551596668 acc: 0.83\n",
      "loss: 1.8354550400610692 acc: 0.81\n",
      "loss: 1.8746871280074981 acc: 0.79\n",
      "loss: 1.8468690138750645 acc: 0.79\n",
      "loss: 1.824153081972131 acc: 0.82\n",
      "loss: 1.784441385671397 acc: 0.79\n",
      "loss: 1.7862526713596785 acc: 0.86\n",
      "loss: 1.809660603799938 acc: 0.87\n",
      "loss: 1.8181682191974522 acc: 0.8\n",
      "loss: 1.8265178700475693 acc: 0.8\n",
      "loss: 1.7796044919006626 acc: 0.84\n",
      "loss: 1.8491302177663562 acc: 0.79\n",
      "loss: 1.823003997091882 acc: 0.86\n",
      "loss: 1.8250074031625 acc: 0.87\n",
      "loss: 1.8429793424677985 acc: 0.77\n",
      "loss: 1.7935978375312296 acc: 0.91\n",
      "loss: 1.801023774310247 acc: 0.86\n",
      "loss: 1.8217355668196538 acc: 0.85\n",
      "loss: 1.8079344993515272 acc: 0.87\n",
      "loss: 1.8342365415288922 acc: 0.87\n",
      "loss: 1.7743996072435835 acc: 0.88\n",
      "loss: 1.810794531840306 acc: 0.84\n",
      "loss: 1.7975495661294054 acc: 0.86\n",
      "loss: 1.753028919394773 acc: 0.88\n",
      "loss: 1.8496705317470694 acc: 0.78\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 1.8497\t Accuracy 0.7800\n",
      "loss: 1.7918963779792443 acc: 0.88\n",
      "loss: 1.8309766568974337 acc: 0.83\n",
      "loss: 1.8194319933245597 acc: 0.86\n",
      "loss: 1.8236670627603115 acc: 0.83\n",
      "loss: 1.7973703586186678 acc: 0.87\n",
      "loss: 1.8245047603896858 acc: 0.82\n",
      "loss: 1.8269827490842767 acc: 0.87\n",
      "loss: 1.8042795004023608 acc: 0.89\n",
      "loss: 1.820514812313036 acc: 0.84\n",
      "loss: 1.8393137194414337 acc: 0.83\n",
      "loss: 1.806098249627838 acc: 0.87\n",
      "loss: 1.801853705986213 acc: 0.89\n",
      "loss: 1.786629721292815 acc: 0.88\n",
      "loss: 1.8224381900044062 acc: 0.85\n",
      "loss: 1.7817534737482241 acc: 0.94\n",
      "loss: 1.8334552293182045 acc: 0.81\n",
      "loss: 1.833314565594032 acc: 0.83\n",
      "loss: 1.8421299918455158 acc: 0.83\n",
      "loss: 1.8084330360224723 acc: 0.84\n",
      "loss: 1.8515658337578154 acc: 0.78\n",
      "loss: 1.8320919518094547 acc: 0.8\n",
      "loss: 1.829335006401163 acc: 0.83\n",
      "loss: 1.8307823740009226 acc: 0.82\n",
      "loss: 1.8454568242837035 acc: 0.83\n",
      "loss: 1.8256153401136508 acc: 0.87\n",
      "loss: 1.8106281231364059 acc: 0.85\n",
      "loss: 1.8625373222279966 acc: 0.82\n",
      "loss: 1.8089916500341483 acc: 0.81\n",
      "loss: 1.7800567347653145 acc: 0.89\n",
      "loss: 1.8259083344544254 acc: 0.86\n",
      "loss: 1.802489188877201 acc: 0.92\n",
      "loss: 1.8414160980936571 acc: 0.85\n",
      "loss: 1.8279992918933339 acc: 0.85\n",
      "loss: 1.8322274717619147 acc: 0.82\n",
      "loss: 1.8279490859975231 acc: 0.82\n",
      "loss: 1.8543338881512768 acc: 0.8\n",
      "loss: 1.788789915757304 acc: 0.89\n",
      "loss: 1.8450929888319108 acc: 0.84\n",
      "loss: 1.8014009996812488 acc: 0.9\n",
      "loss: 1.8385561710320055 acc: 0.84\n",
      "loss: 1.811581804560838 acc: 0.85\n",
      "loss: 1.828559849328839 acc: 0.79\n",
      "loss: 1.8247015106169289 acc: 0.82\n",
      "loss: 1.8076824623824268 acc: 0.88\n",
      "loss: 1.7752353152503133 acc: 0.87\n",
      "loss: 1.8222705640583658 acc: 0.82\n",
      "loss: 1.8641399671071257 acc: 0.78\n",
      "loss: 1.8133238612869997 acc: 0.86\n",
      "loss: 1.8394013625262513 acc: 0.8\n",
      "loss: 1.8028769758072156 acc: 0.85\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 1.8029\t Accuracy 0.8500\n",
      "loss: 1.832939463830917 acc: 0.84\n",
      "loss: 1.8188752920349247 acc: 0.82\n",
      "loss: 1.8269466559586618 acc: 0.8\n",
      "loss: 1.803093951234389 acc: 0.88\n",
      "loss: 1.8031051982816448 acc: 0.87\n",
      "loss: 1.830674429639426 acc: 0.84\n",
      "loss: 1.7903665387710723 acc: 0.89\n",
      "loss: 1.8165227128283348 acc: 0.83\n",
      "loss: 1.8231802311304415 acc: 0.86\n",
      "loss: 1.81868087531457 acc: 0.84\n",
      "loss: 1.8432726877729886 acc: 0.84\n",
      "loss: 1.8249511615776348 acc: 0.85\n",
      "loss: 1.8460905789005495 acc: 0.81\n",
      "loss: 1.8688725502579329 acc: 0.82\n",
      "loss: 1.8484563523227635 acc: 0.83\n",
      "loss: 1.787135769452453 acc: 0.92\n",
      "loss: 1.7906628493993615 acc: 0.9\n",
      "loss: 1.8256935666154606 acc: 0.85\n",
      "loss: 1.8191374190326621 acc: 0.84\n",
      "loss: 1.8378987834117613 acc: 0.86\n",
      "loss: 1.8342180914984374 acc: 0.82\n",
      "loss: 1.8549612194382994 acc: 0.83\n",
      "loss: 1.8238722490608044 acc: 0.86\n",
      "loss: 1.8586535463266127 acc: 0.82\n",
      "loss: 1.8049451632988291 acc: 0.87\n",
      "loss: 1.817561713125197 acc: 0.84\n",
      "loss: 1.853363749408305 acc: 0.8\n",
      "loss: 1.836736001543403 acc: 0.85\n",
      "loss: 1.7918543615549247 acc: 0.83\n",
      "loss: 1.802944606907139 acc: 0.82\n",
      "loss: 1.8246097465553164 acc: 0.87\n",
      "loss: 1.8443784096896432 acc: 0.83\n",
      "loss: 1.855427014872962 acc: 0.86\n",
      "loss: 1.821774586042903 acc: 0.87\n",
      "loss: 1.857910755368105 acc: 0.76\n",
      "loss: 1.8437251124563139 acc: 0.84\n",
      "loss: 1.828105701482809 acc: 0.81\n",
      "loss: 1.889008066371545 acc: 0.81\n",
      "loss: 1.827857236482638 acc: 0.86\n",
      "loss: 1.811977617451706 acc: 0.89\n",
      "loss: 1.8489534243957138 acc: 0.82\n",
      "loss: 1.821982449031741 acc: 0.86\n",
      "loss: 1.8139801421471942 acc: 0.85\n",
      "loss: 1.8204503777784138 acc: 0.84\n",
      "loss: 1.821840212440858 acc: 0.82\n",
      "loss: 1.7999616489813242 acc: 0.9\n",
      "loss: 1.8136904705608163 acc: 0.82\n",
      "loss: 1.808251990219757 acc: 0.9\n",
      "loss: 1.833014691395918 acc: 0.84\n",
      "loss: 1.8131071844774629 acc: 0.88\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 1.8131\t Accuracy 0.8800\n",
      "loss: 1.770420999946406 acc: 0.92\n",
      "loss: 1.8088749166104003 acc: 0.84\n",
      "loss: 1.8392164306075873 acc: 0.82\n",
      "loss: 1.8177904877461262 acc: 0.86\n",
      "loss: 1.8325887132976642 acc: 0.8\n",
      "loss: 1.8181435551735274 acc: 0.82\n",
      "loss: 1.7477632331393318 acc: 0.92\n",
      "loss: 1.8023236086879648 acc: 0.83\n",
      "loss: 1.8075532508967138 acc: 0.87\n",
      "loss: 1.8861946235714486 acc: 0.81\n",
      "loss: 1.8367885400751682 acc: 0.81\n",
      "loss: 1.8118679797321156 acc: 0.89\n",
      "loss: 1.814132044787528 acc: 0.87\n",
      "loss: 1.7821784263980809 acc: 0.84\n",
      "loss: 1.8199243074510512 acc: 0.87\n",
      "loss: 1.8139642182912867 acc: 0.84\n",
      "loss: 1.7917096077821808 acc: 0.88\n",
      "loss: 1.8226010384897788 acc: 0.8\n",
      "loss: 1.8409656572880644 acc: 0.88\n",
      "loss: 1.7888390805062324 acc: 0.89\n",
      "loss: 1.7932377613255128 acc: 0.89\n",
      "loss: 1.8565109817326242 acc: 0.77\n",
      "loss: 1.8077312836427937 acc: 0.87\n",
      "loss: 1.81070842958612 acc: 0.89\n",
      "loss: 1.8069277696350732 acc: 0.78\n",
      "loss: 1.812010642739582 acc: 0.85\n",
      "loss: 1.8147863359406295 acc: 0.86\n",
      "loss: 1.8134896597827386 acc: 0.83\n",
      "loss: 1.8227771873366598 acc: 0.83\n",
      "loss: 1.823054287808641 acc: 0.83\n",
      "loss: 1.8121555547427377 acc: 0.86\n",
      "loss: 1.7777704450823273 acc: 0.83\n",
      "loss: 1.8571205866609577 acc: 0.88\n",
      "loss: 1.823993994505544 acc: 0.86\n",
      "loss: 1.8207011633194443 acc: 0.84\n",
      "loss: 1.8044705983291036 acc: 0.93\n",
      "loss: 1.888651293433731 acc: 0.74\n",
      "loss: 1.8085803564884608 acc: 0.94\n",
      "loss: 1.8211225384383132 acc: 0.82\n",
      "loss: 1.7972012644805246 acc: 0.84\n",
      "loss: 1.8415634033188868 acc: 0.8\n",
      "loss: 1.7943416893404391 acc: 0.87\n",
      "loss: 1.8498910795494503 acc: 0.79\n",
      "loss: 1.806004050698065 acc: 0.87\n",
      "loss: 1.7935793736993304 acc: 0.87\n",
      "loss: 1.7903180800300995 acc: 0.89\n",
      "loss: 1.8129592929133815 acc: 0.84\n",
      "loss: 1.7763300050173958 acc: 0.91\n",
      "loss: 1.8038627809339838 acc: 0.86\n",
      "loss: 1.7884225943667698 acc: 0.87\n",
      "loss: 1.7871192480604265 acc: 0.88\n",
      "loss: 1.79135198518408 acc: 0.88\n",
      "loss: 1.828910729009794 acc: 0.87\n",
      "loss: 1.8097363774340067 acc: 0.92\n",
      "loss: 1.7969544875000694 acc: 0.85\n",
      "loss: 1.750672777705898 acc: 0.86\n",
      "loss: 1.7518566641460835 acc: 0.85\n",
      "loss: 1.8197308963773071 acc: 0.86\n",
      "loss: 1.7501090186705033 acc: 0.92\n",
      "loss: 1.79366902677205 acc: 0.85\n",
      "loss: 1.7885515242009256 acc: 0.84\n",
      "loss: 1.844252252720989 acc: 0.82\n",
      "loss: 1.8562310780323443 acc: 0.8\n",
      "loss: 1.8581441926451792 acc: 0.84\n",
      "loss: 1.773420683357205 acc: 0.92\n",
      "loss: 1.7891611433424304 acc: 0.85\n",
      "loss: 1.743414361347578 acc: 0.89\n",
      "loss: 1.7747733995417432 acc: 0.84\n",
      "loss: 1.7817227699596396 acc: 0.88\n",
      "loss: 1.8592205645457525 acc: 0.86\n",
      "loss: 1.8141564747697163 acc: 0.87\n",
      "loss: 1.8123276495749483 acc: 0.76\n",
      "loss: 1.852041859985972 acc: 0.83\n",
      "loss: 1.8318095238273235 acc: 0.85\n",
      "loss: 1.8482114670445458 acc: 0.8\n",
      "loss: 1.8395228220713409 acc: 0.79\n",
      "loss: 1.8634525852438175 acc: 0.78\n",
      "loss: 1.8165002306238571 acc: 0.9\n",
      "loss: 1.797468012028152 acc: 0.88\n",
      "loss: 1.8107344052613874 acc: 0.85\n",
      "loss: 1.7692863597762583 acc: 0.93\n",
      "loss: 1.7286791262049157 acc: 0.93\n",
      "loss: 1.779634359263146 acc: 0.87\n",
      "loss: 1.7642079577224479 acc: 0.87\n",
      "loss: 1.8038688102859266 acc: 0.88\n",
      "loss: 1.847457308522085 acc: 0.87\n",
      "loss: 1.8255063035273376 acc: 0.88\n",
      "loss: 1.737168573999834 acc: 0.91\n",
      "loss: 1.6966927258368938 acc: 0.97\n",
      "loss: 1.7236373824951432 acc: 0.94\n",
      "loss: 1.7280061083137337 acc: 0.97\n",
      "loss: 1.7947094210424277 acc: 0.9\n",
      "loss: 1.7822102745305088 acc: 0.84\n",
      "loss: 1.7240160168723833 acc: 0.83\n",
      "loss: 1.7922323531611881 acc: 0.89\n",
      "loss: 1.7977735070952106 acc: 0.92\n",
      "loss: 1.8857042058359659 acc: 0.77\n",
      "loss: 1.70029938692781 acc: 0.96\n",
      "loss: 1.8486706214098567 acc: 0.87\n",
      "\n",
      "Epoch [5]\t Average training loss 1.8219\t Average training accuracy 0.8432\n",
      "Epoch [5]\t Average validation loss 1.7951\t Average validation accuracy 0.8692\n",
      "\n",
      "loss: 1.8101843733741365 acc: 0.86\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 1.8102\t Accuracy 0.8600\n",
      "loss: 1.828231141965347 acc: 0.84\n",
      "loss: 1.8183133944421197 acc: 0.88\n",
      "loss: 1.7803390020894398 acc: 0.89\n",
      "loss: 1.824921273276645 acc: 0.89\n",
      "loss: 1.811105983696433 acc: 0.82\n",
      "loss: 1.8042734685613928 acc: 0.87\n",
      "loss: 1.8617511755715892 acc: 0.82\n",
      "loss: 1.8670518142277734 acc: 0.83\n",
      "loss: 1.8255976503790612 acc: 0.9\n",
      "loss: 1.7884507485200323 acc: 0.9\n",
      "loss: 1.8804100203147573 acc: 0.79\n",
      "loss: 1.8328877869095792 acc: 0.85\n",
      "loss: 1.7997580361368366 acc: 0.84\n",
      "loss: 1.8129120157197314 acc: 0.86\n",
      "loss: 1.8321251103837577 acc: 0.84\n",
      "loss: 1.8376429721170777 acc: 0.83\n",
      "loss: 1.7921766954617928 acc: 0.85\n",
      "loss: 1.7866233355692245 acc: 0.89\n",
      "loss: 1.7819129766654087 acc: 0.84\n",
      "loss: 1.7813432174453678 acc: 0.92\n",
      "loss: 1.833726810175635 acc: 0.79\n",
      "loss: 1.8304892297807458 acc: 0.83\n",
      "loss: 1.8623741155635483 acc: 0.83\n",
      "loss: 1.8495256755868215 acc: 0.85\n",
      "loss: 1.783072008811993 acc: 0.92\n",
      "loss: 1.8450726739318046 acc: 0.73\n",
      "loss: 1.799108699914612 acc: 0.89\n",
      "loss: 1.8149660763414022 acc: 0.84\n",
      "loss: 1.8030715807434572 acc: 0.93\n",
      "loss: 1.803046974630547 acc: 0.85\n",
      "loss: 1.8199259296551418 acc: 0.88\n",
      "loss: 1.826293221910324 acc: 0.87\n",
      "loss: 1.7732161421374784 acc: 0.85\n",
      "loss: 1.8269981921250358 acc: 0.85\n",
      "loss: 1.7873517146758078 acc: 0.83\n",
      "loss: 1.871387694605332 acc: 0.78\n",
      "loss: 1.8313540811413689 acc: 0.79\n",
      "loss: 1.839191765176089 acc: 0.81\n",
      "loss: 1.855451281670631 acc: 0.77\n",
      "loss: 1.8479225331980529 acc: 0.8\n",
      "loss: 1.8010493423033815 acc: 0.85\n",
      "loss: 1.8117180496218723 acc: 0.86\n",
      "loss: 1.800904185950965 acc: 0.86\n",
      "loss: 1.8340549195843514 acc: 0.89\n",
      "loss: 1.8547471474925485 acc: 0.83\n",
      "loss: 1.8071165452844127 acc: 0.79\n",
      "loss: 1.825621740521764 acc: 0.83\n",
      "loss: 1.7836607376019324 acc: 0.87\n",
      "loss: 1.8415197885833121 acc: 0.81\n",
      "loss: 1.8017596619797982 acc: 0.9\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 1.8018\t Accuracy 0.9000\n",
      "loss: 1.783623066809592 acc: 0.87\n",
      "loss: 1.8592615282920777 acc: 0.82\n",
      "loss: 1.8222277637553737 acc: 0.83\n",
      "loss: 1.8676983755255978 acc: 0.81\n",
      "loss: 1.8249598494391979 acc: 0.83\n",
      "loss: 1.8090871013102368 acc: 0.89\n",
      "loss: 1.806705771123993 acc: 0.85\n",
      "loss: 1.8285997344176008 acc: 0.86\n",
      "loss: 1.8107188243458583 acc: 0.86\n",
      "loss: 1.798236594187125 acc: 0.88\n",
      "loss: 1.807412189237037 acc: 0.84\n",
      "loss: 1.8288274898634969 acc: 0.82\n",
      "loss: 1.8224509033688379 acc: 0.76\n",
      "loss: 1.7850938174502946 acc: 0.88\n",
      "loss: 1.8096399116417405 acc: 0.84\n",
      "loss: 1.851102182026621 acc: 0.76\n",
      "loss: 1.812411700682214 acc: 0.84\n",
      "loss: 1.794755350090432 acc: 0.9\n",
      "loss: 1.8473370297783402 acc: 0.83\n",
      "loss: 1.8449009312647933 acc: 0.84\n",
      "loss: 1.8026310920834754 acc: 0.88\n",
      "loss: 1.8580974340633543 acc: 0.8\n",
      "loss: 1.866425080670581 acc: 0.78\n",
      "loss: 1.8432269411549231 acc: 0.84\n",
      "loss: 1.866318502848682 acc: 0.83\n",
      "loss: 1.8358828556927855 acc: 0.83\n",
      "loss: 1.79915568778923 acc: 0.91\n",
      "loss: 1.80943393763256 acc: 0.86\n",
      "loss: 1.8590014875022154 acc: 0.78\n",
      "loss: 1.8117502557420033 acc: 0.87\n",
      "loss: 1.766359664060654 acc: 0.88\n",
      "loss: 1.7999361952894644 acc: 0.84\n",
      "loss: 1.8417303289945326 acc: 0.82\n",
      "loss: 1.8148245112560373 acc: 0.87\n",
      "loss: 1.84129043408267 acc: 0.85\n",
      "loss: 1.8435837009442946 acc: 0.84\n",
      "loss: 1.8039202960798422 acc: 0.88\n",
      "loss: 1.864016066646603 acc: 0.75\n",
      "loss: 1.8392538526347177 acc: 0.8\n",
      "loss: 1.841002618265524 acc: 0.82\n",
      "loss: 1.8187500734002233 acc: 0.88\n",
      "loss: 1.8365921034590613 acc: 0.85\n",
      "loss: 1.8271843152297467 acc: 0.84\n",
      "loss: 1.826817887333207 acc: 0.85\n",
      "loss: 1.8793751238114782 acc: 0.77\n",
      "loss: 1.817567481623062 acc: 0.87\n",
      "loss: 1.8696933120202368 acc: 0.8\n",
      "loss: 1.8115851278275692 acc: 0.78\n",
      "loss: 1.8377631844719964 acc: 0.86\n",
      "loss: 1.8169974479099829 acc: 0.86\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 1.8170\t Accuracy 0.8600\n",
      "loss: 1.831232968028333 acc: 0.82\n",
      "loss: 1.8089541181777522 acc: 0.83\n",
      "loss: 1.831054383259731 acc: 0.86\n",
      "loss: 1.8292872195664294 acc: 0.77\n",
      "loss: 1.822219187036544 acc: 0.83\n",
      "loss: 1.800435093160057 acc: 0.85\n",
      "loss: 1.7892558685895847 acc: 0.9\n",
      "loss: 1.8228514294851033 acc: 0.81\n",
      "loss: 1.8260088956528526 acc: 0.84\n",
      "loss: 1.8458091175708065 acc: 0.84\n",
      "loss: 1.85811764534467 acc: 0.85\n",
      "loss: 1.8234104170658074 acc: 0.86\n",
      "loss: 1.822879189357418 acc: 0.82\n",
      "loss: 1.801304472771054 acc: 0.85\n",
      "loss: 1.8195699700204448 acc: 0.83\n",
      "loss: 1.812400080991391 acc: 0.93\n",
      "loss: 1.816020438344481 acc: 0.82\n",
      "loss: 1.8206439038326343 acc: 0.87\n",
      "loss: 1.867282573124129 acc: 0.81\n",
      "loss: 1.8274169531557956 acc: 0.88\n",
      "loss: 1.7916251829983265 acc: 0.87\n",
      "loss: 1.8371217548894316 acc: 0.87\n",
      "loss: 1.8089889170448081 acc: 0.81\n",
      "loss: 1.8742086705916428 acc: 0.75\n",
      "loss: 1.81336674704551 acc: 0.87\n",
      "loss: 1.8404449142350257 acc: 0.84\n",
      "loss: 1.839845584847888 acc: 0.87\n",
      "loss: 1.8155737154487945 acc: 0.87\n",
      "loss: 1.7804839511243546 acc: 0.89\n",
      "loss: 1.8157134461424198 acc: 0.86\n",
      "loss: 1.849604952459107 acc: 0.76\n",
      "loss: 1.801292071750146 acc: 0.87\n",
      "loss: 1.8209473308752786 acc: 0.85\n",
      "loss: 1.8275568932016584 acc: 0.82\n",
      "loss: 1.8031448882571064 acc: 0.88\n",
      "loss: 1.7929563066891592 acc: 0.87\n",
      "loss: 1.8480871426285819 acc: 0.85\n",
      "loss: 1.8272856578474557 acc: 0.86\n",
      "loss: 1.8067042509253406 acc: 0.85\n",
      "loss: 1.8137355393857706 acc: 0.8\n",
      "loss: 1.8404451212517452 acc: 0.83\n",
      "loss: 1.7937941931758443 acc: 0.86\n",
      "loss: 1.8110824238420205 acc: 0.82\n",
      "loss: 1.8098102696379046 acc: 0.86\n",
      "loss: 1.8327996089932135 acc: 0.85\n",
      "loss: 1.8034285345501633 acc: 0.82\n",
      "loss: 1.808529431352211 acc: 0.85\n",
      "loss: 1.8132804521222772 acc: 0.88\n",
      "loss: 1.7983772679328007 acc: 0.87\n",
      "loss: 1.8237108182752926 acc: 0.78\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 1.8237\t Accuracy 0.7800\n",
      "loss: 1.8463867573780528 acc: 0.8\n",
      "loss: 1.8290749594948794 acc: 0.81\n",
      "loss: 1.7973599139668088 acc: 0.9\n",
      "loss: 1.8000346359106467 acc: 0.86\n",
      "loss: 1.8226616669796938 acc: 0.8\n",
      "loss: 1.8227099546776993 acc: 0.85\n",
      "loss: 1.7921450060764241 acc: 0.83\n",
      "loss: 1.8433052294568517 acc: 0.81\n",
      "loss: 1.8225183658781532 acc: 0.85\n",
      "loss: 1.8211063267385785 acc: 0.83\n",
      "loss: 1.844894076826363 acc: 0.83\n",
      "loss: 1.8315629108975864 acc: 0.88\n",
      "loss: 1.8573607903391167 acc: 0.79\n",
      "loss: 1.8184952654133488 acc: 0.83\n",
      "loss: 1.852546836830199 acc: 0.88\n",
      "loss: 1.8352014258750358 acc: 0.82\n",
      "loss: 1.8280359385019338 acc: 0.87\n",
      "loss: 1.8092619420632707 acc: 0.85\n",
      "loss: 1.7960560099287142 acc: 0.84\n",
      "loss: 1.7961660262105437 acc: 0.87\n",
      "loss: 1.855461993646754 acc: 0.8\n",
      "loss: 1.8134051411351053 acc: 0.89\n",
      "loss: 1.7700039843748814 acc: 0.89\n",
      "loss: 1.8310010187664145 acc: 0.84\n",
      "loss: 1.8772854668836363 acc: 0.8\n",
      "loss: 1.785804870636044 acc: 0.9\n",
      "loss: 1.8364174830017475 acc: 0.79\n",
      "loss: 1.856410068221441 acc: 0.74\n",
      "loss: 1.8368329249545914 acc: 0.83\n",
      "loss: 1.7985566061613114 acc: 0.88\n",
      "loss: 1.8192502931152603 acc: 0.84\n",
      "loss: 1.8103245532656476 acc: 0.83\n",
      "loss: 1.837973333133281 acc: 0.87\n",
      "loss: 1.8055076162682184 acc: 0.84\n",
      "loss: 1.8045336839926716 acc: 0.82\n",
      "loss: 1.831340144184616 acc: 0.86\n",
      "loss: 1.7719556927162348 acc: 0.9\n",
      "loss: 1.8406236951036579 acc: 0.89\n",
      "loss: 1.8125179658130532 acc: 0.88\n",
      "loss: 1.7801168635962004 acc: 0.89\n",
      "loss: 1.8259547610116564 acc: 0.86\n",
      "loss: 1.8258599814554526 acc: 0.82\n",
      "loss: 1.8090575898998742 acc: 0.86\n",
      "loss: 1.7744366878886546 acc: 0.84\n",
      "loss: 1.836197288023609 acc: 0.81\n",
      "loss: 1.8567852449990827 acc: 0.79\n",
      "loss: 1.8388345706541886 acc: 0.81\n",
      "loss: 1.8461496987364836 acc: 0.79\n",
      "loss: 1.8032600875102747 acc: 0.85\n",
      "loss: 1.7986153612093774 acc: 0.89\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 1.7986\t Accuracy 0.8900\n",
      "loss: 1.8073112645523193 acc: 0.84\n",
      "loss: 1.8110554376718344 acc: 0.87\n",
      "loss: 1.8101801976703433 acc: 0.87\n",
      "loss: 1.7991035875420622 acc: 0.85\n",
      "loss: 1.8012488942049387 acc: 0.87\n",
      "loss: 1.8293150751457097 acc: 0.8\n",
      "loss: 1.8170181632381748 acc: 0.81\n",
      "loss: 1.8012327715571816 acc: 0.85\n",
      "loss: 1.8294004253649951 acc: 0.84\n",
      "loss: 1.801988076775437 acc: 0.83\n",
      "loss: 1.8473265061911885 acc: 0.84\n",
      "loss: 1.8123283718813272 acc: 0.82\n",
      "loss: 1.7831857059062517 acc: 0.86\n",
      "loss: 1.8314628896589815 acc: 0.82\n",
      "loss: 1.7856978970687398 acc: 0.91\n",
      "loss: 1.8404424407802935 acc: 0.82\n",
      "loss: 1.8193956160813616 acc: 0.88\n",
      "loss: 1.8049029261367704 acc: 0.86\n",
      "loss: 1.8029370936680007 acc: 0.85\n",
      "loss: 1.825284399749527 acc: 0.79\n",
      "loss: 1.7987929226331227 acc: 0.85\n",
      "loss: 1.8644572185419295 acc: 0.81\n",
      "loss: 1.835321286541391 acc: 0.81\n",
      "loss: 1.7186372870081046 acc: 0.93\n",
      "loss: 1.8885289299358308 acc: 0.79\n",
      "loss: 1.815397146481889 acc: 0.83\n",
      "loss: 1.8054815678060299 acc: 0.84\n",
      "loss: 1.857427026922146 acc: 0.82\n",
      "loss: 1.8230864832248688 acc: 0.84\n",
      "loss: 1.8169590322391702 acc: 0.84\n",
      "loss: 1.8043037658116483 acc: 0.89\n",
      "loss: 1.8189281245865556 acc: 0.83\n",
      "loss: 1.795472320813147 acc: 0.87\n",
      "loss: 1.8294511363879922 acc: 0.84\n",
      "loss: 1.7784181607566674 acc: 0.84\n",
      "loss: 1.817234646551567 acc: 0.91\n",
      "loss: 1.8099923861955125 acc: 0.85\n",
      "loss: 1.8141098503957196 acc: 0.84\n",
      "loss: 1.8541621025896708 acc: 0.81\n",
      "loss: 1.8203721139691313 acc: 0.84\n",
      "loss: 1.8309960821639977 acc: 0.82\n",
      "loss: 1.8348923875233583 acc: 0.82\n",
      "loss: 1.7965695004958022 acc: 0.94\n",
      "loss: 1.8383177537046504 acc: 0.83\n",
      "loss: 1.8233372396765106 acc: 0.8\n",
      "loss: 1.828286495304353 acc: 0.84\n",
      "loss: 1.8382703006035372 acc: 0.82\n",
      "loss: 1.8584875312612041 acc: 0.79\n",
      "loss: 1.843257890052472 acc: 0.88\n",
      "loss: 1.8218053160610574 acc: 0.85\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 1.8218\t Accuracy 0.8500\n",
      "loss: 1.8237221140768338 acc: 0.84\n",
      "loss: 1.8015523161546991 acc: 0.87\n",
      "loss: 1.8518780972062034 acc: 0.85\n",
      "loss: 1.8625088868242048 acc: 0.8\n",
      "loss: 1.850474078673279 acc: 0.83\n",
      "loss: 1.8523046635925748 acc: 0.85\n",
      "loss: 1.833981565791779 acc: 0.79\n",
      "loss: 1.8236248234275927 acc: 0.82\n",
      "loss: 1.8466179246949281 acc: 0.8\n",
      "loss: 1.8276127849724262 acc: 0.86\n",
      "loss: 1.8470081682375332 acc: 0.79\n",
      "loss: 1.821033898761027 acc: 0.79\n",
      "loss: 1.8475015950549944 acc: 0.78\n",
      "loss: 1.8178868316571128 acc: 0.86\n",
      "loss: 1.8088646434805893 acc: 0.84\n",
      "loss: 1.827759093345104 acc: 0.87\n",
      "loss: 1.8273547966102621 acc: 0.86\n",
      "loss: 1.8357366520838128 acc: 0.81\n",
      "loss: 1.8002490111102756 acc: 0.91\n",
      "loss: 1.8174490049062106 acc: 0.84\n",
      "loss: 1.8134103835696722 acc: 0.84\n",
      "loss: 1.810182369466319 acc: 0.88\n",
      "loss: 1.8020264287971253 acc: 0.84\n",
      "loss: 1.7802227578438936 acc: 0.87\n",
      "loss: 1.8189094000669197 acc: 0.87\n",
      "loss: 1.8325886559099138 acc: 0.83\n",
      "loss: 1.788053286093921 acc: 0.85\n",
      "loss: 1.8179635676018124 acc: 0.85\n",
      "loss: 1.826767654761303 acc: 0.86\n",
      "loss: 1.8371987087509694 acc: 0.87\n",
      "loss: 1.8373934248049943 acc: 0.84\n",
      "loss: 1.808436948396985 acc: 0.84\n",
      "loss: 1.8139345891782164 acc: 0.85\n",
      "loss: 1.8095192613288014 acc: 0.84\n",
      "loss: 1.830667350653868 acc: 0.83\n",
      "loss: 1.8408501853846448 acc: 0.87\n",
      "loss: 1.8086004487782088 acc: 0.82\n",
      "loss: 1.8335879297333895 acc: 0.86\n",
      "loss: 1.8135621439479184 acc: 0.84\n",
      "loss: 1.8187838877241103 acc: 0.83\n",
      "loss: 1.7720900082439486 acc: 0.87\n",
      "loss: 1.8237564303874878 acc: 0.85\n",
      "loss: 1.822029915626832 acc: 0.81\n",
      "loss: 1.8189940442089694 acc: 0.85\n",
      "loss: 1.8181619419371002 acc: 0.89\n",
      "loss: 1.8099904856944715 acc: 0.87\n",
      "loss: 1.848984326249098 acc: 0.78\n",
      "loss: 1.8134924755571291 acc: 0.84\n",
      "loss: 1.8437812238039888 acc: 0.85\n",
      "loss: 1.8445640978414115 acc: 0.84\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 1.8446\t Accuracy 0.8400\n",
      "loss: 1.7887849940543594 acc: 0.89\n",
      "loss: 1.8140038789668134 acc: 0.81\n",
      "loss: 1.8224063980718832 acc: 0.79\n",
      "loss: 1.7971325889205059 acc: 0.87\n",
      "loss: 1.7971972850678983 acc: 0.83\n",
      "loss: 1.8308006751133796 acc: 0.83\n",
      "loss: 1.8568795164104983 acc: 0.78\n",
      "loss: 1.8466367882426158 acc: 0.81\n",
      "loss: 1.8241650059600978 acc: 0.85\n",
      "loss: 1.7913447531224675 acc: 0.87\n",
      "loss: 1.8050967096257515 acc: 0.85\n",
      "loss: 1.80531287160366 acc: 0.85\n",
      "loss: 1.7677481085438977 acc: 0.9\n",
      "loss: 1.8196711965673524 acc: 0.87\n",
      "loss: 1.8057057031475923 acc: 0.82\n",
      "loss: 1.8205827986159724 acc: 0.87\n",
      "loss: 1.8357911863089833 acc: 0.79\n",
      "loss: 1.8536943322524855 acc: 0.76\n",
      "loss: 1.8158931483465388 acc: 0.87\n",
      "loss: 1.7897692244875187 acc: 0.87\n",
      "loss: 1.8263547058513976 acc: 0.83\n",
      "loss: 1.8307934184969141 acc: 0.82\n",
      "loss: 1.8367425995932514 acc: 0.84\n",
      "loss: 1.8281356170878678 acc: 0.77\n",
      "loss: 1.8257269661168707 acc: 0.82\n",
      "loss: 1.8022266486960223 acc: 0.88\n",
      "loss: 1.862256612851284 acc: 0.77\n",
      "loss: 1.809720933808361 acc: 0.94\n",
      "loss: 1.8464224144174555 acc: 0.85\n",
      "loss: 1.7968901728562725 acc: 0.89\n",
      "loss: 1.8264809597590357 acc: 0.83\n",
      "loss: 1.8118635098731566 acc: 0.88\n",
      "loss: 1.8051935217202573 acc: 0.9\n",
      "loss: 1.8011434961263917 acc: 0.9\n",
      "loss: 1.829240481013037 acc: 0.84\n",
      "loss: 1.823098072314646 acc: 0.8\n",
      "loss: 1.8236137325826107 acc: 0.85\n",
      "loss: 1.8256903968596703 acc: 0.81\n",
      "loss: 1.8441544692536795 acc: 0.82\n",
      "loss: 1.8363072782290546 acc: 0.78\n",
      "loss: 1.802696732825254 acc: 0.86\n",
      "loss: 1.8254624414676692 acc: 0.86\n",
      "loss: 1.8279068052489378 acc: 0.81\n",
      "loss: 1.7913044804391491 acc: 0.92\n",
      "loss: 1.812546229569515 acc: 0.84\n",
      "loss: 1.8203914555422378 acc: 0.9\n",
      "loss: 1.7947914061976709 acc: 0.9\n",
      "loss: 1.844041160379404 acc: 0.8\n",
      "loss: 1.8441598529220602 acc: 0.82\n",
      "loss: 1.8195713910267686 acc: 0.83\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 1.8196\t Accuracy 0.8300\n",
      "loss: 1.8133348206474909 acc: 0.86\n",
      "loss: 1.805469380875426 acc: 0.84\n",
      "loss: 1.7981977702869498 acc: 0.91\n",
      "loss: 1.8271762691848155 acc: 0.91\n",
      "loss: 1.8173311674037205 acc: 0.88\n",
      "loss: 1.8244913201218247 acc: 0.81\n",
      "loss: 1.807963057950485 acc: 0.85\n",
      "loss: 1.8203045665755246 acc: 0.82\n",
      "loss: 1.847340386628798 acc: 0.8\n",
      "loss: 1.833532246729643 acc: 0.8\n",
      "loss: 1.8363075536458942 acc: 0.86\n",
      "loss: 1.8400961706370316 acc: 0.8\n",
      "loss: 1.7928009264586993 acc: 0.9\n",
      "loss: 1.8289581303000286 acc: 0.82\n",
      "loss: 1.843466601325271 acc: 0.84\n",
      "loss: 1.803945714283021 acc: 0.9\n",
      "loss: 1.815939344617599 acc: 0.8\n",
      "loss: 1.7882488247781723 acc: 0.89\n",
      "loss: 1.8591876741735018 acc: 0.82\n",
      "loss: 1.8163292820587962 acc: 0.83\n",
      "loss: 1.8273400821191692 acc: 0.84\n",
      "loss: 1.7568987241645875 acc: 0.9\n",
      "loss: 1.8044987028259947 acc: 0.87\n",
      "loss: 1.8340415149383107 acc: 0.82\n",
      "loss: 1.871785722159673 acc: 0.83\n",
      "loss: 1.813561547163275 acc: 0.9\n",
      "loss: 1.821733933073508 acc: 0.89\n",
      "loss: 1.8026715921991487 acc: 0.87\n",
      "loss: 1.8324089852965093 acc: 0.83\n",
      "loss: 1.8456764233110439 acc: 0.83\n",
      "loss: 1.7849132660271667 acc: 0.89\n",
      "loss: 1.8143085076112717 acc: 0.84\n",
      "loss: 1.8183200841039735 acc: 0.82\n",
      "loss: 1.8551677353145757 acc: 0.79\n",
      "loss: 1.7881678059463788 acc: 0.85\n",
      "loss: 1.7894271456125805 acc: 0.86\n",
      "loss: 1.7959664022000497 acc: 0.89\n",
      "loss: 1.8376670672288231 acc: 0.81\n",
      "loss: 1.7765907909822207 acc: 0.82\n",
      "loss: 1.7873842789819392 acc: 0.88\n",
      "loss: 1.8169274206270773 acc: 0.89\n",
      "loss: 1.7713280317815867 acc: 0.88\n",
      "loss: 1.7923630759437905 acc: 0.87\n",
      "loss: 1.805618461762706 acc: 0.88\n",
      "loss: 1.826494119392988 acc: 0.8\n",
      "loss: 1.807042742016754 acc: 0.88\n",
      "loss: 1.8068378576956294 acc: 0.88\n",
      "loss: 1.8281144741469872 acc: 0.83\n",
      "loss: 1.8018620027034984 acc: 0.85\n",
      "loss: 1.8189171630178818 acc: 0.83\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 1.8189\t Accuracy 0.8300\n",
      "loss: 1.805136495596373 acc: 0.84\n",
      "loss: 1.806086684301738 acc: 0.87\n",
      "loss: 1.853347921169722 acc: 0.82\n",
      "loss: 1.8539021758272463 acc: 0.86\n",
      "loss: 1.8495786267741747 acc: 0.82\n",
      "loss: 1.8352516170171038 acc: 0.83\n",
      "loss: 1.8395268936507625 acc: 0.85\n",
      "loss: 1.8365835699958646 acc: 0.85\n",
      "loss: 1.80824680479512 acc: 0.88\n",
      "loss: 1.8165795599245613 acc: 0.89\n",
      "loss: 1.809305430362533 acc: 0.89\n",
      "loss: 1.825918501008137 acc: 0.78\n",
      "loss: 1.8489998451575187 acc: 0.83\n",
      "loss: 1.8189201969856708 acc: 0.83\n",
      "loss: 1.7918491629413875 acc: 0.87\n",
      "loss: 1.8028327186943427 acc: 0.83\n",
      "loss: 1.819079359952112 acc: 0.86\n",
      "loss: 1.8169294976041943 acc: 0.87\n",
      "loss: 1.8335920665310357 acc: 0.81\n",
      "loss: 1.8024603563917057 acc: 0.87\n",
      "loss: 1.8293355133909237 acc: 0.88\n",
      "loss: 1.8344902706503445 acc: 0.81\n",
      "loss: 1.802520800785939 acc: 0.86\n",
      "loss: 1.8501641111403533 acc: 0.85\n",
      "loss: 1.8601507235259922 acc: 0.79\n",
      "loss: 1.8268946237435977 acc: 0.88\n",
      "loss: 1.8418596281371364 acc: 0.87\n",
      "loss: 1.8385615700532836 acc: 0.84\n",
      "loss: 1.7967934647200108 acc: 0.88\n",
      "loss: 1.794365772489696 acc: 0.87\n",
      "loss: 1.8308724327879908 acc: 0.87\n",
      "loss: 1.797293058939561 acc: 0.85\n",
      "loss: 1.8545531420243038 acc: 0.82\n",
      "loss: 1.833247489817439 acc: 0.81\n",
      "loss: 1.7977127058379614 acc: 0.91\n",
      "loss: 1.8640897352295134 acc: 0.83\n",
      "loss: 1.8127451955456562 acc: 0.84\n",
      "loss: 1.8319359557396389 acc: 0.89\n",
      "loss: 1.8357375107323262 acc: 0.87\n",
      "loss: 1.8682907497142198 acc: 0.77\n",
      "loss: 1.8649469473420566 acc: 0.82\n",
      "loss: 1.8233638204427285 acc: 0.85\n",
      "loss: 1.8150424767649067 acc: 0.82\n",
      "loss: 1.8240663989283379 acc: 0.86\n",
      "loss: 1.8178734767298252 acc: 0.84\n",
      "loss: 1.8472785216501253 acc: 0.81\n",
      "loss: 1.8015915952148942 acc: 0.81\n",
      "loss: 1.7996440193199823 acc: 0.83\n",
      "loss: 1.8201583461946784 acc: 0.85\n",
      "loss: 1.8680886953863785 acc: 0.81\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 1.8681\t Accuracy 0.8100\n",
      "loss: 1.8091829444309204 acc: 0.89\n",
      "loss: 1.8004477959704193 acc: 0.85\n",
      "loss: 1.8311469439812402 acc: 0.85\n",
      "loss: 1.760433230296898 acc: 0.87\n",
      "loss: 1.8135497761579058 acc: 0.88\n",
      "loss: 1.8280930679284757 acc: 0.83\n",
      "loss: 1.813035388756604 acc: 0.85\n",
      "loss: 1.8087063787469706 acc: 0.85\n",
      "loss: 1.7967587671413288 acc: 0.89\n",
      "loss: 1.8198981805354437 acc: 0.83\n",
      "loss: 1.7864135636049028 acc: 0.87\n",
      "loss: 1.8167274933384872 acc: 0.86\n",
      "loss: 1.8341846021111592 acc: 0.82\n",
      "loss: 1.8726763621516358 acc: 0.77\n",
      "loss: 1.8440517741725575 acc: 0.88\n",
      "loss: 1.822502393607605 acc: 0.8\n",
      "loss: 1.8023477862681327 acc: 0.81\n",
      "loss: 1.7770816807916512 acc: 0.9\n",
      "loss: 1.8460294024304778 acc: 0.82\n",
      "loss: 1.8169166723811279 acc: 0.86\n",
      "loss: 1.8082746032348884 acc: 0.85\n",
      "loss: 1.8387429370849582 acc: 0.79\n",
      "loss: 1.816188760210435 acc: 0.8\n",
      "loss: 1.8008593624336786 acc: 0.87\n",
      "loss: 1.7979209115598036 acc: 0.82\n",
      "loss: 1.800040612988854 acc: 0.86\n",
      "loss: 1.8404666640456504 acc: 0.83\n",
      "loss: 1.8147432149752092 acc: 0.87\n",
      "loss: 1.810922419361403 acc: 0.87\n",
      "loss: 1.819554333885455 acc: 0.82\n",
      "loss: 1.7977557041148475 acc: 0.86\n",
      "loss: 1.8231204653698663 acc: 0.84\n",
      "loss: 1.7915298193070361 acc: 0.9\n",
      "loss: 1.8142657120735886 acc: 0.87\n",
      "loss: 1.8275393287484745 acc: 0.82\n",
      "loss: 1.8523157457986306 acc: 0.84\n",
      "loss: 1.7810290291579196 acc: 0.87\n",
      "loss: 1.829655943738827 acc: 0.89\n",
      "loss: 1.8427998145806581 acc: 0.79\n",
      "loss: 1.853490484554151 acc: 0.82\n",
      "loss: 1.821459843922337 acc: 0.84\n",
      "loss: 1.8118524642518161 acc: 0.86\n",
      "loss: 1.825016169269224 acc: 0.85\n",
      "loss: 1.7789248335632755 acc: 0.88\n",
      "loss: 1.826705264563015 acc: 0.83\n",
      "loss: 1.8731569543734503 acc: 0.76\n",
      "loss: 1.810919151583891 acc: 0.83\n",
      "loss: 1.8863163709098787 acc: 0.79\n",
      "loss: 1.835169247202916 acc: 0.83\n",
      "loss: 1.809435332227758 acc: 0.9\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 1.8094\t Accuracy 0.9000\n",
      "loss: 1.8029215347058474 acc: 0.9\n",
      "loss: 1.7979503829571564 acc: 0.88\n",
      "loss: 1.7982270948897223 acc: 0.85\n",
      "loss: 1.846756655783351 acc: 0.87\n",
      "loss: 1.8210583791127288 acc: 0.91\n",
      "loss: 1.8076901489713135 acc: 0.88\n",
      "loss: 1.828683653550711 acc: 0.83\n",
      "loss: 1.829243774244094 acc: 0.8\n",
      "loss: 1.8545201570832608 acc: 0.85\n",
      "loss: 1.8578759787429617 acc: 0.79\n",
      "loss: 1.8235890400082315 acc: 0.86\n",
      "loss: 1.8174666389986833 acc: 0.84\n",
      "loss: 1.7812642050837642 acc: 0.88\n",
      "loss: 1.8351765857694804 acc: 0.81\n",
      "loss: 1.8378081766009067 acc: 0.87\n",
      "loss: 1.8137448488658863 acc: 0.87\n",
      "loss: 1.8156830915948412 acc: 0.85\n",
      "loss: 1.8358409872731616 acc: 0.88\n",
      "loss: 1.799103109546082 acc: 0.87\n",
      "loss: 1.8132511764224413 acc: 0.86\n",
      "loss: 1.8267611982314924 acc: 0.89\n",
      "loss: 1.832412432312921 acc: 0.82\n",
      "loss: 1.8038892952020336 acc: 0.93\n",
      "loss: 1.854571019938557 acc: 0.83\n",
      "loss: 1.8163441019607638 acc: 0.79\n",
      "loss: 1.8051394737571322 acc: 0.86\n",
      "loss: 1.831998385221806 acc: 0.84\n",
      "loss: 1.8359939069571993 acc: 0.81\n",
      "loss: 1.8018313455423014 acc: 0.86\n",
      "loss: 1.8230656358006985 acc: 0.84\n",
      "loss: 1.7946970042704762 acc: 0.88\n",
      "loss: 1.8348998623422204 acc: 0.84\n",
      "loss: 1.791990812035897 acc: 0.84\n",
      "loss: 1.8408020416163646 acc: 0.81\n",
      "loss: 1.8450416900782178 acc: 0.77\n",
      "loss: 1.7952931677110462 acc: 0.92\n",
      "loss: 1.8724275704054447 acc: 0.79\n",
      "loss: 1.832795428612566 acc: 0.87\n",
      "loss: 1.8127044221907662 acc: 0.85\n",
      "loss: 1.7864137532505147 acc: 0.84\n",
      "loss: 1.850157214412922 acc: 0.81\n",
      "loss: 1.855090322895954 acc: 0.82\n",
      "loss: 1.7998666466191673 acc: 0.86\n",
      "loss: 1.809566689118883 acc: 0.91\n",
      "loss: 1.8159913669297816 acc: 0.87\n",
      "loss: 1.8384037567665408 acc: 0.76\n",
      "loss: 1.830410599031255 acc: 0.81\n",
      "loss: 1.8433341266032204 acc: 0.83\n",
      "loss: 1.838786184389085 acc: 0.83\n",
      "loss: 1.7901252498538553 acc: 0.88\n",
      "loss: 1.7861642270871672 acc: 0.9\n",
      "loss: 1.7924499537648426 acc: 0.88\n",
      "loss: 1.8202345547232763 acc: 0.87\n",
      "loss: 1.8147402062993083 acc: 0.93\n",
      "loss: 1.7957046493580422 acc: 0.87\n",
      "loss: 1.7590926880652558 acc: 0.85\n",
      "loss: 1.7534483406287793 acc: 0.85\n",
      "loss: 1.8137981036100457 acc: 0.87\n",
      "loss: 1.7440357847537273 acc: 0.92\n",
      "loss: 1.788538265692699 acc: 0.88\n",
      "loss: 1.7895011375298049 acc: 0.84\n",
      "loss: 1.8411293275008136 acc: 0.83\n",
      "loss: 1.8503474179636823 acc: 0.82\n",
      "loss: 1.856415765980685 acc: 0.87\n",
      "loss: 1.7765589973155078 acc: 0.96\n",
      "loss: 1.787505366095715 acc: 0.84\n",
      "loss: 1.7507380645166126 acc: 0.88\n",
      "loss: 1.774763325548887 acc: 0.86\n",
      "loss: 1.7811525565150694 acc: 0.88\n",
      "loss: 1.86015253595754 acc: 0.86\n",
      "loss: 1.81094365233342 acc: 0.89\n",
      "loss: 1.8105777207158504 acc: 0.74\n",
      "loss: 1.8520976984141944 acc: 0.86\n",
      "loss: 1.8210034125195664 acc: 0.88\n",
      "loss: 1.8408939563102376 acc: 0.8\n",
      "loss: 1.8501255168348216 acc: 0.78\n",
      "loss: 1.8779011305556788 acc: 0.8\n",
      "loss: 1.8211529632576997 acc: 0.89\n",
      "loss: 1.7960857900720493 acc: 0.89\n",
      "loss: 1.8137664904916468 acc: 0.84\n",
      "loss: 1.7769983711875557 acc: 0.94\n",
      "loss: 1.7406953087522574 acc: 0.92\n",
      "loss: 1.778699534598962 acc: 0.9\n",
      "loss: 1.7743975031284271 acc: 0.88\n",
      "loss: 1.8000050193905504 acc: 0.89\n",
      "loss: 1.8489959118271657 acc: 0.9\n",
      "loss: 1.8322034698305378 acc: 0.9\n",
      "loss: 1.7261186762605545 acc: 0.92\n",
      "loss: 1.702210413730016 acc: 0.97\n",
      "loss: 1.735260281057888 acc: 0.94\n",
      "loss: 1.7333026490044672 acc: 0.98\n",
      "loss: 1.7849652194300285 acc: 0.92\n",
      "loss: 1.7731534214960218 acc: 0.88\n",
      "loss: 1.7174107239889076 acc: 0.84\n",
      "loss: 1.786946624966031 acc: 0.95\n",
      "loss: 1.8024700167062973 acc: 0.91\n",
      "loss: 1.8707050672370085 acc: 0.82\n",
      "loss: 1.69522509530329 acc: 0.97\n",
      "loss: 1.853929983965171 acc: 0.88\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8214\t Average training accuracy 0.8442\n",
      "Epoch [6]\t Average validation loss 1.7951\t Average validation accuracy 0.8804\n",
      "\n",
      "loss: 1.8502652700960847 acc: 0.85\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 1.8503\t Accuracy 0.8500\n",
      "loss: 1.8112318895452015 acc: 0.89\n",
      "loss: 1.812559377755476 acc: 0.84\n",
      "loss: 1.785433740171569 acc: 0.87\n",
      "loss: 1.8188262060360663 acc: 0.83\n",
      "loss: 1.8006703220320837 acc: 0.92\n",
      "loss: 1.7856318200350612 acc: 0.87\n",
      "loss: 1.836001673495387 acc: 0.82\n",
      "loss: 1.8133224393072696 acc: 0.85\n",
      "loss: 1.808754247908218 acc: 0.86\n",
      "loss: 1.8363854645341144 acc: 0.83\n",
      "loss: 1.7594714659688586 acc: 0.92\n",
      "loss: 1.8492150207485283 acc: 0.82\n",
      "loss: 1.8479683892959675 acc: 0.78\n",
      "loss: 1.847639928384001 acc: 0.78\n",
      "loss: 1.8239753695965177 acc: 0.87\n",
      "loss: 1.8215024737084031 acc: 0.82\n",
      "loss: 1.8739142248752034 acc: 0.79\n",
      "loss: 1.806167639417996 acc: 0.83\n",
      "loss: 1.840627174663534 acc: 0.84\n",
      "loss: 1.7994145846243461 acc: 0.84\n",
      "loss: 1.8047407397388513 acc: 0.87\n",
      "loss: 1.8042250122150028 acc: 0.88\n",
      "loss: 1.828424582226803 acc: 0.82\n",
      "loss: 1.850038809876929 acc: 0.8\n",
      "loss: 1.8085121813531626 acc: 0.83\n",
      "loss: 1.82683606299268 acc: 0.82\n",
      "loss: 1.8106523884976746 acc: 0.88\n",
      "loss: 1.8658185109686456 acc: 0.81\n",
      "loss: 1.8045956529758231 acc: 0.83\n",
      "loss: 1.8509805509452022 acc: 0.81\n",
      "loss: 1.8145556046334017 acc: 0.83\n",
      "loss: 1.8197991983280106 acc: 0.84\n",
      "loss: 1.781044759125643 acc: 0.87\n",
      "loss: 1.7951472233437291 acc: 0.84\n",
      "loss: 1.8095926976979952 acc: 0.82\n",
      "loss: 1.8362306590685396 acc: 0.82\n",
      "loss: 1.8397586141121567 acc: 0.79\n",
      "loss: 1.84039786466409 acc: 0.87\n",
      "loss: 1.83631735672492 acc: 0.89\n",
      "loss: 1.855618329804093 acc: 0.79\n",
      "loss: 1.8190116877915858 acc: 0.87\n",
      "loss: 1.838520605807674 acc: 0.83\n",
      "loss: 1.8094187219867004 acc: 0.88\n",
      "loss: 1.7872928971910855 acc: 0.84\n",
      "loss: 1.8193255598515048 acc: 0.84\n",
      "loss: 1.811020405158203 acc: 0.84\n",
      "loss: 1.8097604714486193 acc: 0.86\n",
      "loss: 1.8150857233109616 acc: 0.86\n",
      "loss: 1.8348055795117688 acc: 0.82\n",
      "loss: 1.8236871007608662 acc: 0.85\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 1.8237\t Accuracy 0.8500\n",
      "loss: 1.8468245831789227 acc: 0.8\n",
      "loss: 1.84094712680584 acc: 0.78\n",
      "loss: 1.825869445599944 acc: 0.84\n",
      "loss: 1.8138887365098189 acc: 0.88\n",
      "loss: 1.808495571193177 acc: 0.84\n",
      "loss: 1.8505569853013173 acc: 0.8\n",
      "loss: 1.8315693624436502 acc: 0.85\n",
      "loss: 1.842209116222194 acc: 0.89\n",
      "loss: 1.824189987070075 acc: 0.79\n",
      "loss: 1.8043995499012977 acc: 0.9\n",
      "loss: 1.8407700437698904 acc: 0.83\n",
      "loss: 1.8242716010382185 acc: 0.85\n",
      "loss: 1.8452254803654589 acc: 0.84\n",
      "loss: 1.8267473653364925 acc: 0.89\n",
      "loss: 1.8100059841365448 acc: 0.85\n",
      "loss: 1.8099749230184259 acc: 0.85\n",
      "loss: 1.8263677292889824 acc: 0.83\n",
      "loss: 1.8145875736605779 acc: 0.87\n",
      "loss: 1.7873588599625803 acc: 0.89\n",
      "loss: 1.8183903152642114 acc: 0.82\n",
      "loss: 1.8070499995515699 acc: 0.82\n",
      "loss: 1.8328139862370216 acc: 0.8\n",
      "loss: 1.8291651061773622 acc: 0.85\n",
      "loss: 1.8273695612543273 acc: 0.81\n",
      "loss: 1.8513389361018888 acc: 0.84\n",
      "loss: 1.8011085382874827 acc: 0.89\n",
      "loss: 1.772559149360155 acc: 0.94\n",
      "loss: 1.779386467193887 acc: 0.9\n",
      "loss: 1.8441658243125945 acc: 0.88\n",
      "loss: 1.857844500430133 acc: 0.78\n",
      "loss: 1.7989923602844098 acc: 0.87\n",
      "loss: 1.8548733112679858 acc: 0.85\n",
      "loss: 1.8421834801106474 acc: 0.82\n",
      "loss: 1.8142167847819735 acc: 0.87\n",
      "loss: 1.8471190507594275 acc: 0.78\n",
      "loss: 1.8220135961878035 acc: 0.83\n",
      "loss: 1.8088978525910167 acc: 0.86\n",
      "loss: 1.848749873618166 acc: 0.81\n",
      "loss: 1.8367149389411817 acc: 0.84\n",
      "loss: 1.8135968865173009 acc: 0.83\n",
      "loss: 1.7980010415912282 acc: 0.85\n",
      "loss: 1.8043881772339854 acc: 0.88\n",
      "loss: 1.8085477751564938 acc: 0.84\n",
      "loss: 1.8396912482565408 acc: 0.84\n",
      "loss: 1.8195987676373 acc: 0.77\n",
      "loss: 1.8095531142967096 acc: 0.84\n",
      "loss: 1.7970282989772821 acc: 0.84\n",
      "loss: 1.8105698487608661 acc: 0.89\n",
      "loss: 1.8324054630796518 acc: 0.82\n",
      "loss: 1.8241059860186644 acc: 0.83\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 1.8241\t Accuracy 0.8300\n",
      "loss: 1.850921185899749 acc: 0.87\n",
      "loss: 1.9022169240624849 acc: 0.77\n",
      "loss: 1.840578124181266 acc: 0.84\n",
      "loss: 1.8622080793572746 acc: 0.83\n",
      "loss: 1.8669604626041931 acc: 0.79\n",
      "loss: 1.8155725458951946 acc: 0.87\n",
      "loss: 1.837582858689376 acc: 0.84\n",
      "loss: 1.8150232906290937 acc: 0.81\n",
      "loss: 1.823769358057816 acc: 0.84\n",
      "loss: 1.8163992731603107 acc: 0.84\n",
      "loss: 1.8182695557378132 acc: 0.83\n",
      "loss: 1.834187196838624 acc: 0.84\n",
      "loss: 1.8022533214205978 acc: 0.88\n",
      "loss: 1.8091777096364254 acc: 0.83\n",
      "loss: 1.8003281235836834 acc: 0.87\n",
      "loss: 1.8130479526312682 acc: 0.84\n",
      "loss: 1.8130997993574078 acc: 0.87\n",
      "loss: 1.7801159004183558 acc: 0.9\n",
      "loss: 1.8268578306506853 acc: 0.87\n",
      "loss: 1.8374787669617296 acc: 0.79\n",
      "loss: 1.8526229463065034 acc: 0.8\n",
      "loss: 1.816270767439017 acc: 0.84\n",
      "loss: 1.8085388329119971 acc: 0.84\n",
      "loss: 1.7713911846106623 acc: 0.87\n",
      "loss: 1.8580329719683493 acc: 0.84\n",
      "loss: 1.7920674438815665 acc: 0.84\n",
      "loss: 1.7872941905425364 acc: 0.92\n",
      "loss: 1.8543941860285318 acc: 0.86\n",
      "loss: 1.833735752880345 acc: 0.89\n",
      "loss: 1.8305502172934882 acc: 0.83\n",
      "loss: 1.7814914709246086 acc: 0.88\n",
      "loss: 1.7741363402318284 acc: 0.86\n",
      "loss: 1.8606999511321967 acc: 0.81\n",
      "loss: 1.8062515316855825 acc: 0.81\n",
      "loss: 1.8174528943812882 acc: 0.81\n",
      "loss: 1.8289515223001045 acc: 0.81\n",
      "loss: 1.7843995695875499 acc: 0.86\n",
      "loss: 1.8387621677633985 acc: 0.81\n",
      "loss: 1.850704920922498 acc: 0.82\n",
      "loss: 1.8660290529253953 acc: 0.78\n",
      "loss: 1.8009518209018325 acc: 0.89\n",
      "loss: 1.8581623008252925 acc: 0.77\n",
      "loss: 1.8539192808940106 acc: 0.8\n",
      "loss: 1.8607599098326373 acc: 0.81\n",
      "loss: 1.823228554368149 acc: 0.83\n",
      "loss: 1.8198415822081313 acc: 0.83\n",
      "loss: 1.8049044895714172 acc: 0.88\n",
      "loss: 1.8477167520715212 acc: 0.8\n",
      "loss: 1.8553206515045313 acc: 0.84\n",
      "loss: 1.802711713812142 acc: 0.85\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 1.8027\t Accuracy 0.8500\n",
      "loss: 1.8261355394722265 acc: 0.83\n",
      "loss: 1.823975026360949 acc: 0.84\n",
      "loss: 1.8148105223328692 acc: 0.85\n",
      "loss: 1.870512224367406 acc: 0.78\n",
      "loss: 1.8056494901386932 acc: 0.84\n",
      "loss: 1.8065078375996668 acc: 0.88\n",
      "loss: 1.7986034967603906 acc: 0.83\n",
      "loss: 1.8310362937922589 acc: 0.85\n",
      "loss: 1.8352168885568776 acc: 0.84\n",
      "loss: 1.8319140514518244 acc: 0.83\n",
      "loss: 1.8333113987844487 acc: 0.87\n",
      "loss: 1.8398895041974144 acc: 0.79\n",
      "loss: 1.8040179226148576 acc: 0.9\n",
      "loss: 1.8063631869010441 acc: 0.85\n",
      "loss: 1.8195264606381212 acc: 0.8\n",
      "loss: 1.8055687071755124 acc: 0.87\n",
      "loss: 1.833927347898501 acc: 0.83\n",
      "loss: 1.8080843480778799 acc: 0.86\n",
      "loss: 1.7805635239200621 acc: 0.89\n",
      "loss: 1.805414674344133 acc: 0.92\n",
      "loss: 1.835770592972618 acc: 0.86\n",
      "loss: 1.7881410983276798 acc: 0.88\n",
      "loss: 1.841271618139739 acc: 0.88\n",
      "loss: 1.843163217605866 acc: 0.82\n",
      "loss: 1.8009278160139732 acc: 0.86\n",
      "loss: 1.77232095750095 acc: 0.86\n",
      "loss: 1.809731636846032 acc: 0.84\n",
      "loss: 1.8206135049416425 acc: 0.84\n",
      "loss: 1.7952066211069666 acc: 0.91\n",
      "loss: 1.8122353464656384 acc: 0.9\n",
      "loss: 1.8240704186415464 acc: 0.88\n",
      "loss: 1.8310300626116334 acc: 0.83\n",
      "loss: 1.828497844947368 acc: 0.84\n",
      "loss: 1.8012389264739537 acc: 0.81\n",
      "loss: 1.8306680012515932 acc: 0.89\n",
      "loss: 1.8453561028528953 acc: 0.83\n",
      "loss: 1.7876307405402536 acc: 0.89\n",
      "loss: 1.830508264812714 acc: 0.8\n",
      "loss: 1.8083482004331786 acc: 0.87\n",
      "loss: 1.8005220925377896 acc: 0.87\n",
      "loss: 1.821673102121633 acc: 0.82\n",
      "loss: 1.8130636285189865 acc: 0.86\n",
      "loss: 1.8350291045542502 acc: 0.8\n",
      "loss: 1.7960799553530706 acc: 0.84\n",
      "loss: 1.814294837571058 acc: 0.87\n",
      "loss: 1.859349336969693 acc: 0.8\n",
      "loss: 1.809631518049473 acc: 0.84\n",
      "loss: 1.7878379458082265 acc: 0.86\n",
      "loss: 1.7992602808336284 acc: 0.86\n",
      "loss: 1.8258627149334656 acc: 0.87\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 1.8259\t Accuracy 0.8700\n",
      "loss: 1.7692588796492796 acc: 0.9\n",
      "loss: 1.8014717036304446 acc: 0.9\n",
      "loss: 1.8366339658177255 acc: 0.8\n",
      "loss: 1.7853090189482925 acc: 0.78\n",
      "loss: 1.8217263365862433 acc: 0.83\n",
      "loss: 1.8136048630768 acc: 0.82\n",
      "loss: 1.813640723104922 acc: 0.81\n",
      "loss: 1.789144938718471 acc: 0.89\n",
      "loss: 1.7956557738341778 acc: 0.87\n",
      "loss: 1.8098816094694288 acc: 0.9\n",
      "loss: 1.8109879223136158 acc: 0.88\n",
      "loss: 1.8512694558319815 acc: 0.83\n",
      "loss: 1.8154921374608681 acc: 0.84\n",
      "loss: 1.8339168778082624 acc: 0.79\n",
      "loss: 1.8453696976649665 acc: 0.81\n",
      "loss: 1.7924804085072763 acc: 0.89\n",
      "loss: 1.8341803379974508 acc: 0.87\n",
      "loss: 1.831240483993085 acc: 0.88\n",
      "loss: 1.837623448410435 acc: 0.84\n",
      "loss: 1.8085984818413963 acc: 0.91\n",
      "loss: 1.8307860877717541 acc: 0.87\n",
      "loss: 1.8418099456743762 acc: 0.84\n",
      "loss: 1.8099712880643855 acc: 0.87\n",
      "loss: 1.8326912937351003 acc: 0.78\n",
      "loss: 1.8463820342513222 acc: 0.78\n",
      "loss: 1.864449614767079 acc: 0.78\n",
      "loss: 1.822663951932878 acc: 0.84\n",
      "loss: 1.850245928846441 acc: 0.79\n",
      "loss: 1.8030796142276229 acc: 0.87\n",
      "loss: 1.8155025083041412 acc: 0.88\n",
      "loss: 1.7752676521414756 acc: 0.86\n",
      "loss: 1.8503611514782619 acc: 0.87\n",
      "loss: 1.8440845081761026 acc: 0.83\n",
      "loss: 1.819957688359419 acc: 0.83\n",
      "loss: 1.8436398420261404 acc: 0.82\n",
      "loss: 1.7985847549832823 acc: 0.84\n",
      "loss: 1.8098274372188996 acc: 0.85\n",
      "loss: 1.8049352620950545 acc: 0.91\n",
      "loss: 1.823429728601454 acc: 0.83\n",
      "loss: 1.8234579049142843 acc: 0.82\n",
      "loss: 1.8232032266436278 acc: 0.87\n",
      "loss: 1.8266059513724593 acc: 0.81\n",
      "loss: 1.7876008932703138 acc: 0.88\n",
      "loss: 1.8328586368534463 acc: 0.78\n",
      "loss: 1.820354859415319 acc: 0.89\n",
      "loss: 1.8070102089831108 acc: 0.86\n",
      "loss: 1.8081515955448837 acc: 0.85\n",
      "loss: 1.806778443906956 acc: 0.89\n",
      "loss: 1.809068115456799 acc: 0.83\n",
      "loss: 1.8368946362440395 acc: 0.84\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 1.8369\t Accuracy 0.8400\n",
      "loss: 1.8315990293687696 acc: 0.84\n",
      "loss: 1.7982016214427612 acc: 0.85\n",
      "loss: 1.7962576683507518 acc: 0.89\n",
      "loss: 1.8386708746933527 acc: 0.84\n",
      "loss: 1.8416022700311974 acc: 0.84\n",
      "loss: 1.7987222329594754 acc: 0.88\n",
      "loss: 1.8442155242938358 acc: 0.86\n",
      "loss: 1.7721075610850456 acc: 0.89\n",
      "loss: 1.8444126965865808 acc: 0.8\n",
      "loss: 1.7572691375081604 acc: 0.85\n",
      "loss: 1.8381880579130445 acc: 0.86\n",
      "loss: 1.8212187789714671 acc: 0.85\n",
      "loss: 1.83199040471655 acc: 0.83\n",
      "loss: 1.8649944894586403 acc: 0.76\n",
      "loss: 1.8191370129144908 acc: 0.82\n",
      "loss: 1.7975252854518877 acc: 0.88\n",
      "loss: 1.7933874954356608 acc: 0.9\n",
      "loss: 1.81301196667356 acc: 0.88\n",
      "loss: 1.780323324954295 acc: 0.94\n",
      "loss: 1.789033908648673 acc: 0.91\n",
      "loss: 1.7747593940049093 acc: 0.85\n",
      "loss: 1.808316195422239 acc: 0.83\n",
      "loss: 1.788905057696617 acc: 0.82\n",
      "loss: 1.82399142127902 acc: 0.83\n",
      "loss: 1.8035610441635566 acc: 0.85\n",
      "loss: 1.8159594957892948 acc: 0.87\n",
      "loss: 1.850111828526053 acc: 0.86\n",
      "loss: 1.8066154013058209 acc: 0.9\n",
      "loss: 1.80438373537541 acc: 0.89\n",
      "loss: 1.8259568223591578 acc: 0.83\n",
      "loss: 1.810560102268521 acc: 0.84\n",
      "loss: 1.7882623376554099 acc: 0.85\n",
      "loss: 1.849748297262475 acc: 0.88\n",
      "loss: 1.8276015138625885 acc: 0.79\n",
      "loss: 1.8232062564364748 acc: 0.83\n",
      "loss: 1.8128658208757706 acc: 0.83\n",
      "loss: 1.8254531609654117 acc: 0.83\n",
      "loss: 1.8186563521439263 acc: 0.85\n",
      "loss: 1.8889058270438688 acc: 0.82\n",
      "loss: 1.8820036298913747 acc: 0.8\n",
      "loss: 1.8373387333291606 acc: 0.87\n",
      "loss: 1.7904618040836733 acc: 0.92\n",
      "loss: 1.7893148260571723 acc: 0.86\n",
      "loss: 1.840569298695125 acc: 0.83\n",
      "loss: 1.8468610477713097 acc: 0.78\n",
      "loss: 1.8110165599023333 acc: 0.89\n",
      "loss: 1.8089366043974673 acc: 0.85\n",
      "loss: 1.8061964794114374 acc: 0.86\n",
      "loss: 1.835529785984511 acc: 0.81\n",
      "loss: 1.7818756265647735 acc: 0.87\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 1.7819\t Accuracy 0.8700\n",
      "loss: 1.8439324366612266 acc: 0.81\n",
      "loss: 1.7849120478314062 acc: 0.86\n",
      "loss: 1.7850549287196438 acc: 0.85\n",
      "loss: 1.8191095472183778 acc: 0.83\n",
      "loss: 1.8277743354496343 acc: 0.84\n",
      "loss: 1.83820570770361 acc: 0.81\n",
      "loss: 1.8170057627947995 acc: 0.84\n",
      "loss: 1.85476173283263 acc: 0.83\n",
      "loss: 1.8211163817539193 acc: 0.82\n",
      "loss: 1.7970893820530103 acc: 0.84\n",
      "loss: 1.8234801580617508 acc: 0.82\n",
      "loss: 1.8269779330948939 acc: 0.82\n",
      "loss: 1.8110380194787123 acc: 0.83\n",
      "loss: 1.8388276602900482 acc: 0.85\n",
      "loss: 1.8220844879580187 acc: 0.86\n",
      "loss: 1.7974894283119314 acc: 0.88\n",
      "loss: 1.813277613744269 acc: 0.89\n",
      "loss: 1.8427548437436334 acc: 0.84\n",
      "loss: 1.8013969131237937 acc: 0.88\n",
      "loss: 1.811907942367898 acc: 0.86\n",
      "loss: 1.8507813494461984 acc: 0.81\n",
      "loss: 1.8076927965113603 acc: 0.83\n",
      "loss: 1.8238043185908686 acc: 0.85\n",
      "loss: 1.7762386078399033 acc: 0.96\n",
      "loss: 1.8023274503845281 acc: 0.85\n",
      "loss: 1.8248910721512184 acc: 0.82\n",
      "loss: 1.7800520596136367 acc: 0.88\n",
      "loss: 1.8224660810445048 acc: 0.83\n",
      "loss: 1.8216964887147975 acc: 0.79\n",
      "loss: 1.8024501148029477 acc: 0.82\n",
      "loss: 1.8281034224661554 acc: 0.8\n",
      "loss: 1.7700848597083285 acc: 0.86\n",
      "loss: 1.8260866511891982 acc: 0.83\n",
      "loss: 1.775432750838911 acc: 0.89\n",
      "loss: 1.8091644365734203 acc: 0.83\n",
      "loss: 1.8147031564000824 acc: 0.76\n",
      "loss: 1.8322885985807893 acc: 0.8\n",
      "loss: 1.8533073740560493 acc: 0.77\n",
      "loss: 1.8072135383058678 acc: 0.88\n",
      "loss: 1.7892465055954723 acc: 0.85\n",
      "loss: 1.8333333525575728 acc: 0.82\n",
      "loss: 1.8159336782065365 acc: 0.84\n",
      "loss: 1.8135256849415933 acc: 0.84\n",
      "loss: 1.8187231510076902 acc: 0.84\n",
      "loss: 1.7872060346083367 acc: 0.91\n",
      "loss: 1.8332592291702279 acc: 0.89\n",
      "loss: 1.8170344319271339 acc: 0.87\n",
      "loss: 1.8505035752385826 acc: 0.85\n",
      "loss: 1.845353082273922 acc: 0.82\n",
      "loss: 1.8324768649084253 acc: 0.84\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 1.8325\t Accuracy 0.8400\n",
      "loss: 1.836843808479948 acc: 0.84\n",
      "loss: 1.8480428199071437 acc: 0.84\n",
      "loss: 1.8242447398663668 acc: 0.85\n",
      "loss: 1.8250893676338489 acc: 0.83\n",
      "loss: 1.7909235486929556 acc: 0.9\n",
      "loss: 1.8053656232017858 acc: 0.87\n",
      "loss: 1.8782564305265206 acc: 0.81\n",
      "loss: 1.811867744292222 acc: 0.86\n",
      "loss: 1.8399830072184853 acc: 0.83\n",
      "loss: 1.8244660871854328 acc: 0.85\n",
      "loss: 1.7825502533919733 acc: 0.89\n",
      "loss: 1.7810766668839904 acc: 0.9\n",
      "loss: 1.8351843975157407 acc: 0.85\n",
      "loss: 1.8107058162339111 acc: 0.8\n",
      "loss: 1.877872660148992 acc: 0.82\n",
      "loss: 1.8215184147829648 acc: 0.87\n",
      "loss: 1.830896949441298 acc: 0.89\n",
      "loss: 1.7903086635204608 acc: 0.86\n",
      "loss: 1.7798334508440634 acc: 0.9\n",
      "loss: 1.8300345319004594 acc: 0.87\n",
      "loss: 1.822923493779323 acc: 0.84\n",
      "loss: 1.7977012743417293 acc: 0.86\n",
      "loss: 1.846138799225855 acc: 0.81\n",
      "loss: 1.813238266944743 acc: 0.82\n",
      "loss: 1.8055149616489155 acc: 0.84\n",
      "loss: 1.7898304307607382 acc: 0.83\n",
      "loss: 1.8184696828248597 acc: 0.8\n",
      "loss: 1.8064501991448196 acc: 0.89\n",
      "loss: 1.7936404417387686 acc: 0.89\n",
      "loss: 1.8320530649580087 acc: 0.84\n",
      "loss: 1.8190513986259342 acc: 0.84\n",
      "loss: 1.8158704645717296 acc: 0.83\n",
      "loss: 1.8593974521451122 acc: 0.81\n",
      "loss: 1.817854675532121 acc: 0.84\n",
      "loss: 1.8544547067897914 acc: 0.78\n",
      "loss: 1.8112871965838462 acc: 0.88\n",
      "loss: 1.8062820858020425 acc: 0.87\n",
      "loss: 1.8225421227286538 acc: 0.81\n",
      "loss: 1.8228817539573121 acc: 0.83\n",
      "loss: 1.8082428251687594 acc: 0.86\n",
      "loss: 1.8407602084407673 acc: 0.87\n",
      "loss: 1.835003889017319 acc: 0.89\n",
      "loss: 1.8019833598377237 acc: 0.89\n",
      "loss: 1.8055828972736552 acc: 0.85\n",
      "loss: 1.8122509200264452 acc: 0.86\n",
      "loss: 1.8207704995188356 acc: 0.86\n",
      "loss: 1.790184811695606 acc: 0.88\n",
      "loss: 1.8229954012596576 acc: 0.83\n",
      "loss: 1.7901382324807624 acc: 0.9\n",
      "loss: 1.7765817006684128 acc: 0.87\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 1.7766\t Accuracy 0.8700\n",
      "loss: 1.822275256161049 acc: 0.89\n",
      "loss: 1.8192428518734747 acc: 0.87\n",
      "loss: 1.814586008066282 acc: 0.88\n",
      "loss: 1.792887273065769 acc: 0.88\n",
      "loss: 1.8118514407213973 acc: 0.86\n",
      "loss: 1.853823097081905 acc: 0.75\n",
      "loss: 1.8079132756465497 acc: 0.81\n",
      "loss: 1.817152243886831 acc: 0.83\n",
      "loss: 1.8228330564610402 acc: 0.85\n",
      "loss: 1.7941906724763355 acc: 0.86\n",
      "loss: 1.78760174943084 acc: 0.88\n",
      "loss: 1.8539581287873639 acc: 0.76\n",
      "loss: 1.808785508704287 acc: 0.84\n",
      "loss: 1.8341239946306276 acc: 0.83\n",
      "loss: 1.8257027811104636 acc: 0.84\n",
      "loss: 1.8104816881243635 acc: 0.86\n",
      "loss: 1.7911823444217796 acc: 0.85\n",
      "loss: 1.8081261311793504 acc: 0.87\n",
      "loss: 1.8482486677141057 acc: 0.8\n",
      "loss: 1.8129319803794746 acc: 0.88\n",
      "loss: 1.8745271282994767 acc: 0.78\n",
      "loss: 1.802542452438926 acc: 0.83\n",
      "loss: 1.788638488975788 acc: 0.91\n",
      "loss: 1.7953644654712722 acc: 0.86\n",
      "loss: 1.8580184077270134 acc: 0.85\n",
      "loss: 1.79776144992259 acc: 0.87\n",
      "loss: 1.8139112013249765 acc: 0.84\n",
      "loss: 1.8331732357551687 acc: 0.81\n",
      "loss: 1.8070755900598447 acc: 0.91\n",
      "loss: 1.7782026740497878 acc: 0.87\n",
      "loss: 1.8502762916329354 acc: 0.74\n",
      "loss: 1.8408313805761916 acc: 0.82\n",
      "loss: 1.8182372084104588 acc: 0.85\n",
      "loss: 1.8354148186180428 acc: 0.79\n",
      "loss: 1.8573316854017348 acc: 0.81\n",
      "loss: 1.7960389567790571 acc: 0.86\n",
      "loss: 1.8418160543583284 acc: 0.82\n",
      "loss: 1.8389376750529711 acc: 0.81\n",
      "loss: 1.7699384429584584 acc: 0.89\n",
      "loss: 1.8182099275916854 acc: 0.89\n",
      "loss: 1.8063661004642473 acc: 0.87\n",
      "loss: 1.8425876645929122 acc: 0.86\n",
      "loss: 1.828583891176811 acc: 0.8\n",
      "loss: 1.820236461347251 acc: 0.83\n",
      "loss: 1.83886773794666 acc: 0.77\n",
      "loss: 1.8557276931512063 acc: 0.77\n",
      "loss: 1.8320194067419384 acc: 0.83\n",
      "loss: 1.8400226690248311 acc: 0.84\n",
      "loss: 1.8604661051360123 acc: 0.8\n",
      "loss: 1.8298833835464339 acc: 0.83\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 1.8299\t Accuracy 0.8300\n",
      "loss: 1.7951388954241583 acc: 0.87\n",
      "loss: 1.801675781137329 acc: 0.93\n",
      "loss: 1.7784028811545554 acc: 0.87\n",
      "loss: 1.7967901585643955 acc: 0.87\n",
      "loss: 1.8149985485473612 acc: 0.88\n",
      "loss: 1.8234317581362078 acc: 0.8\n",
      "loss: 1.8122803499701632 acc: 0.82\n",
      "loss: 1.8184467315876338 acc: 0.87\n",
      "loss: 1.8238757942362216 acc: 0.86\n",
      "loss: 1.8391947213270663 acc: 0.8\n",
      "loss: 1.877019801019198 acc: 0.8\n",
      "loss: 1.8643425074281708 acc: 0.76\n",
      "loss: 1.7926747644520469 acc: 0.89\n",
      "loss: 1.8250788255052512 acc: 0.83\n",
      "loss: 1.8189827410579105 acc: 0.8\n",
      "loss: 1.7841904059856515 acc: 0.91\n",
      "loss: 1.8211931210781822 acc: 0.84\n",
      "loss: 1.832526930744241 acc: 0.84\n",
      "loss: 1.853663963365925 acc: 0.83\n",
      "loss: 1.8061653872443284 acc: 0.87\n",
      "loss: 1.864347787242103 acc: 0.88\n",
      "loss: 1.848251868054675 acc: 0.78\n",
      "loss: 1.869959527672803 acc: 0.8\n",
      "loss: 1.8462652435210791 acc: 0.84\n",
      "loss: 1.8235148606691993 acc: 0.86\n",
      "loss: 1.8648770956921263 acc: 0.84\n",
      "loss: 1.8464356636563335 acc: 0.82\n",
      "loss: 1.7909924069653114 acc: 0.9\n",
      "loss: 1.8487966097161124 acc: 0.82\n",
      "loss: 1.8675924016750687 acc: 0.75\n",
      "loss: 1.831478952750768 acc: 0.84\n",
      "loss: 1.801235046384638 acc: 0.84\n",
      "loss: 1.8606589439741168 acc: 0.81\n",
      "loss: 1.8501680280984814 acc: 0.83\n",
      "loss: 1.87209201809005 acc: 0.79\n",
      "loss: 1.836560922280911 acc: 0.81\n",
      "loss: 1.8357639451059515 acc: 0.83\n",
      "loss: 1.8007902170085917 acc: 0.86\n",
      "loss: 1.8508613734743726 acc: 0.87\n",
      "loss: 1.805516674319703 acc: 0.89\n",
      "loss: 1.83544096250102 acc: 0.83\n",
      "loss: 1.8181933720593162 acc: 0.88\n",
      "loss: 1.8303391109051355 acc: 0.86\n",
      "loss: 1.7875835013881776 acc: 0.88\n",
      "loss: 1.8062794119172483 acc: 0.89\n",
      "loss: 1.7891634963831828 acc: 0.91\n",
      "loss: 1.81019027219175 acc: 0.81\n",
      "loss: 1.8383652964086574 acc: 0.84\n",
      "loss: 1.8103647084410621 acc: 0.84\n",
      "loss: 1.8541462301977734 acc: 0.83\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 1.8541\t Accuracy 0.8300\n",
      "loss: 1.8424422737878055 acc: 0.81\n",
      "loss: 1.7924848500412967 acc: 0.88\n",
      "loss: 1.7884783226932006 acc: 0.87\n",
      "loss: 1.8105842922660378 acc: 0.86\n",
      "loss: 1.8704435312550713 acc: 0.76\n",
      "loss: 1.8453560849360622 acc: 0.81\n",
      "loss: 1.8581756415932162 acc: 0.75\n",
      "loss: 1.8316781366911445 acc: 0.86\n",
      "loss: 1.836880357175808 acc: 0.87\n",
      "loss: 1.8227238482562456 acc: 0.82\n",
      "loss: 1.8440338004686527 acc: 0.82\n",
      "loss: 1.8233008618992264 acc: 0.84\n",
      "loss: 1.815196699977339 acc: 0.9\n",
      "loss: 1.8140931871900938 acc: 0.86\n",
      "loss: 1.7943940245805343 acc: 0.84\n",
      "loss: 1.8119856765301712 acc: 0.79\n",
      "loss: 1.8132702179329951 acc: 0.8\n",
      "loss: 1.8462708901312408 acc: 0.83\n",
      "loss: 1.772218750013971 acc: 0.86\n",
      "loss: 1.8107870722755184 acc: 0.83\n",
      "loss: 1.8303881336087733 acc: 0.85\n",
      "loss: 1.7814460848152083 acc: 0.85\n",
      "loss: 1.770944184070064 acc: 0.89\n",
      "loss: 1.8417728039738634 acc: 0.74\n",
      "loss: 1.8160897536912335 acc: 0.83\n",
      "loss: 1.825811952209104 acc: 0.83\n",
      "loss: 1.802356703602946 acc: 0.91\n",
      "loss: 1.8245362086551586 acc: 0.81\n",
      "loss: 1.8382576446152894 acc: 0.8\n",
      "loss: 1.8306520033538967 acc: 0.81\n",
      "loss: 1.8135845744742898 acc: 0.85\n",
      "loss: 1.8250648574066148 acc: 0.81\n",
      "loss: 1.8260464159035308 acc: 0.83\n",
      "loss: 1.821471729959969 acc: 0.85\n",
      "loss: 1.8240585636545756 acc: 0.86\n",
      "loss: 1.8399908374053076 acc: 0.86\n",
      "loss: 1.7845114025210007 acc: 0.87\n",
      "loss: 1.7884710321702142 acc: 0.9\n",
      "loss: 1.7987112262365577 acc: 0.85\n",
      "loss: 1.8109553515440233 acc: 0.85\n",
      "loss: 1.8143912881803044 acc: 0.92\n",
      "loss: 1.8152316800018233 acc: 0.88\n",
      "loss: 1.8083064161538442 acc: 0.85\n",
      "loss: 1.8274666193499598 acc: 0.87\n",
      "loss: 1.8611249269042545 acc: 0.79\n",
      "loss: 1.8299831038242713 acc: 0.82\n",
      "loss: 1.8311850297056411 acc: 0.81\n",
      "loss: 1.814343837202163 acc: 0.9\n",
      "loss: 1.8336168430355086 acc: 0.86\n",
      "loss: 1.7875588683322727 acc: 0.86\n",
      "loss: 1.793661483260039 acc: 0.89\n",
      "loss: 1.808700310243981 acc: 0.87\n",
      "loss: 1.829559187724453 acc: 0.87\n",
      "loss: 1.8267780076115496 acc: 0.92\n",
      "loss: 1.799366422421267 acc: 0.89\n",
      "loss: 1.7570361477154262 acc: 0.85\n",
      "loss: 1.7585848516892697 acc: 0.84\n",
      "loss: 1.8232476704502307 acc: 0.87\n",
      "loss: 1.745571376386097 acc: 0.93\n",
      "loss: 1.7995804769568138 acc: 0.87\n",
      "loss: 1.7981480255805942 acc: 0.83\n",
      "loss: 1.8434207624417018 acc: 0.83\n",
      "loss: 1.8577225014694299 acc: 0.83\n",
      "loss: 1.8635380007818327 acc: 0.86\n",
      "loss: 1.7737324678809891 acc: 0.95\n",
      "loss: 1.7963978449601399 acc: 0.82\n",
      "loss: 1.7564641105514527 acc: 0.91\n",
      "loss: 1.789529717715996 acc: 0.85\n",
      "loss: 1.7853300572767654 acc: 0.87\n",
      "loss: 1.8665749554358144 acc: 0.87\n",
      "loss: 1.8239757001391683 acc: 0.86\n",
      "loss: 1.8190594605839747 acc: 0.79\n",
      "loss: 1.8521752991729983 acc: 0.88\n",
      "loss: 1.8293577354134771 acc: 0.9\n",
      "loss: 1.8501778772572215 acc: 0.81\n",
      "loss: 1.852382305488402 acc: 0.78\n",
      "loss: 1.879411705602948 acc: 0.78\n",
      "loss: 1.8222833838495751 acc: 0.9\n",
      "loss: 1.7934496056747253 acc: 0.91\n",
      "loss: 1.8147415309128163 acc: 0.87\n",
      "loss: 1.7792722839750315 acc: 0.93\n",
      "loss: 1.7436419735943773 acc: 0.94\n",
      "loss: 1.7818042869942732 acc: 0.91\n",
      "loss: 1.77342458011964 acc: 0.87\n",
      "loss: 1.8130672741266871 acc: 0.9\n",
      "loss: 1.8480257976840182 acc: 0.89\n",
      "loss: 1.8463922747753423 acc: 0.89\n",
      "loss: 1.7247301187462114 acc: 0.92\n",
      "loss: 1.7061217993239262 acc: 0.98\n",
      "loss: 1.7388050733929459 acc: 0.94\n",
      "loss: 1.745106808796717 acc: 0.97\n",
      "loss: 1.8033146765070915 acc: 0.88\n",
      "loss: 1.7713137026831733 acc: 0.85\n",
      "loss: 1.7173618578360796 acc: 0.85\n",
      "loss: 1.8051240507271296 acc: 0.94\n",
      "loss: 1.8046965234381538 acc: 0.94\n",
      "loss: 1.8759454099269743 acc: 0.83\n",
      "loss: 1.7094161232522713 acc: 0.97\n",
      "loss: 1.8570494447618524 acc: 0.87\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8207\t Average training accuracy 0.8443\n",
      "Epoch [7]\t Average validation loss 1.8008\t Average validation accuracy 0.8806\n",
      "\n",
      "loss: 1.8202062569668076 acc: 0.83\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 1.8202\t Accuracy 0.8300\n",
      "loss: 1.8335341576813644 acc: 0.86\n",
      "loss: 1.8214239977464346 acc: 0.86\n",
      "loss: 1.825575977208485 acc: 0.79\n",
      "loss: 1.8363927764600338 acc: 0.81\n",
      "loss: 1.8097513422428206 acc: 0.91\n",
      "loss: 1.822935132218761 acc: 0.84\n",
      "loss: 1.8097562724065637 acc: 0.89\n",
      "loss: 1.825277526469907 acc: 0.86\n",
      "loss: 1.825589721845161 acc: 0.77\n",
      "loss: 1.7982541333561135 acc: 0.86\n",
      "loss: 1.7945008886413099 acc: 0.85\n",
      "loss: 1.808664607232385 acc: 0.9\n",
      "loss: 1.809456577647965 acc: 0.9\n",
      "loss: 1.8059630138323508 acc: 0.86\n",
      "loss: 1.864489164299246 acc: 0.81\n",
      "loss: 1.842754417679642 acc: 0.81\n",
      "loss: 1.800409414426518 acc: 0.9\n",
      "loss: 1.8263446363555378 acc: 0.76\n",
      "loss: 1.7900173738993927 acc: 0.89\n",
      "loss: 1.8107335554255226 acc: 0.89\n",
      "loss: 1.845924109857687 acc: 0.79\n",
      "loss: 1.8535201917760835 acc: 0.8\n",
      "loss: 1.8197572224169354 acc: 0.81\n",
      "loss: 1.7882418355011396 acc: 0.88\n",
      "loss: 1.8479502968120085 acc: 0.85\n",
      "loss: 1.835716727626547 acc: 0.84\n",
      "loss: 1.8428832627249867 acc: 0.77\n",
      "loss: 1.8028728083425143 acc: 0.84\n",
      "loss: 1.801236110138201 acc: 0.9\n",
      "loss: 1.8231192059258632 acc: 0.84\n",
      "loss: 1.795507569229037 acc: 0.85\n",
      "loss: 1.825413369717355 acc: 0.83\n",
      "loss: 1.8480383337252257 acc: 0.81\n",
      "loss: 1.8300880065533398 acc: 0.83\n",
      "loss: 1.869007736287808 acc: 0.85\n",
      "loss: 1.7946482715588599 acc: 0.9\n",
      "loss: 1.8302889138744984 acc: 0.82\n",
      "loss: 1.818805691255873 acc: 0.89\n",
      "loss: 1.7950474005614587 acc: 0.89\n",
      "loss: 1.8216191589149857 acc: 0.85\n",
      "loss: 1.8028804589751917 acc: 0.86\n",
      "loss: 1.7869406013592735 acc: 0.87\n",
      "loss: 1.800466630909506 acc: 0.9\n",
      "loss: 1.8468558661633834 acc: 0.83\n",
      "loss: 1.7877371170152134 acc: 0.81\n",
      "loss: 1.8088583841911146 acc: 0.89\n",
      "loss: 1.8286137076921078 acc: 0.83\n",
      "loss: 1.8238772660043554 acc: 0.88\n",
      "loss: 1.7949561578330826 acc: 0.9\n",
      "loss: 1.8725851154352264 acc: 0.8\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 1.8726\t Accuracy 0.8000\n",
      "loss: 1.8663827613290502 acc: 0.82\n",
      "loss: 1.8068792402783973 acc: 0.87\n",
      "loss: 1.7990741071391412 acc: 0.89\n",
      "loss: 1.8114831569012253 acc: 0.85\n",
      "loss: 1.7862038788591252 acc: 0.9\n",
      "loss: 1.7954503108618474 acc: 0.87\n",
      "loss: 1.8125078694138212 acc: 0.83\n",
      "loss: 1.8530221458771075 acc: 0.83\n",
      "loss: 1.8076950537819065 acc: 0.88\n",
      "loss: 1.8261961704511143 acc: 0.82\n",
      "loss: 1.8394259099488486 acc: 0.84\n",
      "loss: 1.839289357549073 acc: 0.86\n",
      "loss: 1.8275182764223792 acc: 0.88\n",
      "loss: 1.8356278093738267 acc: 0.79\n",
      "loss: 1.8361978724032024 acc: 0.84\n",
      "loss: 1.8075112984223767 acc: 0.83\n",
      "loss: 1.840866783517325 acc: 0.81\n",
      "loss: 1.8310165414000472 acc: 0.81\n",
      "loss: 1.8069647329984622 acc: 0.84\n",
      "loss: 1.8334163765183724 acc: 0.84\n",
      "loss: 1.823208729480392 acc: 0.83\n",
      "loss: 1.8047207892164008 acc: 0.82\n",
      "loss: 1.852962517431685 acc: 0.81\n",
      "loss: 1.8361736195362268 acc: 0.8\n",
      "loss: 1.8258993904080283 acc: 0.83\n",
      "loss: 1.8397694303862628 acc: 0.82\n",
      "loss: 1.735094214651905 acc: 0.9\n",
      "loss: 1.834217479133029 acc: 0.81\n",
      "loss: 1.8071678427864826 acc: 0.84\n",
      "loss: 1.7932686602631243 acc: 0.82\n",
      "loss: 1.8168812155081762 acc: 0.81\n",
      "loss: 1.820700904833909 acc: 0.86\n",
      "loss: 1.8314870831551908 acc: 0.85\n",
      "loss: 1.8010986702947382 acc: 0.89\n",
      "loss: 1.839979900050217 acc: 0.77\n",
      "loss: 1.8290022365390712 acc: 0.85\n",
      "loss: 1.781901012821939 acc: 0.91\n",
      "loss: 1.821211985784618 acc: 0.89\n",
      "loss: 1.830091793254393 acc: 0.8\n",
      "loss: 1.7643773248505001 acc: 0.92\n",
      "loss: 1.808867360641924 acc: 0.88\n",
      "loss: 1.8434920892081756 acc: 0.84\n",
      "loss: 1.8099958091132808 acc: 0.85\n",
      "loss: 1.8009691138520614 acc: 0.85\n",
      "loss: 1.81206158841206 acc: 0.88\n",
      "loss: 1.799261101131507 acc: 0.85\n",
      "loss: 1.8177636721965393 acc: 0.82\n",
      "loss: 1.8569046529438145 acc: 0.81\n",
      "loss: 1.8205557908583716 acc: 0.89\n",
      "loss: 1.7901496695728811 acc: 0.81\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 1.7901\t Accuracy 0.8100\n",
      "loss: 1.8022205036714536 acc: 0.86\n",
      "loss: 1.8367417163752149 acc: 0.8\n",
      "loss: 1.8113589001242891 acc: 0.91\n",
      "loss: 1.8473198969143585 acc: 0.8\n",
      "loss: 1.8197888297458749 acc: 0.84\n",
      "loss: 1.8505875598151649 acc: 0.78\n",
      "loss: 1.7925734484257116 acc: 0.86\n",
      "loss: 1.8429128439490523 acc: 0.84\n",
      "loss: 1.8035410425255296 acc: 0.83\n",
      "loss: 1.8252021660441706 acc: 0.79\n",
      "loss: 1.8108621394943827 acc: 0.87\n",
      "loss: 1.8418093630654715 acc: 0.83\n",
      "loss: 1.8336047441903878 acc: 0.86\n",
      "loss: 1.8524409518871903 acc: 0.84\n",
      "loss: 1.811927725892222 acc: 0.82\n",
      "loss: 1.8156448769318092 acc: 0.84\n",
      "loss: 1.8268452848104588 acc: 0.8\n",
      "loss: 1.7975655462271758 acc: 0.82\n",
      "loss: 1.7643260574726776 acc: 0.9\n",
      "loss: 1.8386149238451734 acc: 0.85\n",
      "loss: 1.8022127674758113 acc: 0.82\n",
      "loss: 1.8077357152937545 acc: 0.84\n",
      "loss: 1.7505842530543623 acc: 0.89\n",
      "loss: 1.8473881241683336 acc: 0.81\n",
      "loss: 1.821338918883678 acc: 0.76\n",
      "loss: 1.8261760636887905 acc: 0.84\n",
      "loss: 1.8597473835878142 acc: 0.82\n",
      "loss: 1.8057384194384856 acc: 0.81\n",
      "loss: 1.8150646023035093 acc: 0.86\n",
      "loss: 1.7990958371566481 acc: 0.87\n",
      "loss: 1.798673541971161 acc: 0.84\n",
      "loss: 1.849573563539073 acc: 0.87\n",
      "loss: 1.853709411380183 acc: 0.83\n",
      "loss: 1.802665125609061 acc: 0.94\n",
      "loss: 1.7993092118989795 acc: 0.87\n",
      "loss: 1.8076070054480227 acc: 0.88\n",
      "loss: 1.869054747673785 acc: 0.79\n",
      "loss: 1.8438991730732568 acc: 0.86\n",
      "loss: 1.829483676255296 acc: 0.82\n",
      "loss: 1.80159668447976 acc: 0.88\n",
      "loss: 1.8080662572421065 acc: 0.93\n",
      "loss: 1.8088599071014468 acc: 0.9\n",
      "loss: 1.8106510104450342 acc: 0.87\n",
      "loss: 1.8143892769709895 acc: 0.78\n",
      "loss: 1.781694623093703 acc: 0.9\n",
      "loss: 1.833873721437788 acc: 0.83\n",
      "loss: 1.8180086956741288 acc: 0.86\n",
      "loss: 1.7897695066907446 acc: 0.9\n",
      "loss: 1.8180537852553023 acc: 0.81\n",
      "loss: 1.8338701956450942 acc: 0.82\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 1.8339\t Accuracy 0.8200\n",
      "loss: 1.7584008690912254 acc: 0.9\n",
      "loss: 1.829874285420463 acc: 0.82\n",
      "loss: 1.7890466695131437 acc: 0.92\n",
      "loss: 1.8465246525416772 acc: 0.8\n",
      "loss: 1.8726655669571444 acc: 0.78\n",
      "loss: 1.7832087635872735 acc: 0.84\n",
      "loss: 1.790422164481347 acc: 0.86\n",
      "loss: 1.8051202085647517 acc: 0.84\n",
      "loss: 1.855568928237542 acc: 0.85\n",
      "loss: 1.7846654291999875 acc: 0.87\n",
      "loss: 1.803775315587079 acc: 0.91\n",
      "loss: 1.8139702508265818 acc: 0.85\n",
      "loss: 1.822663098434142 acc: 0.86\n",
      "loss: 1.8393487294281732 acc: 0.79\n",
      "loss: 1.815680769936054 acc: 0.83\n",
      "loss: 1.824882693426676 acc: 0.89\n",
      "loss: 1.859056537188956 acc: 0.75\n",
      "loss: 1.8291216312309262 acc: 0.84\n",
      "loss: 1.85436994900329 acc: 0.86\n",
      "loss: 1.809643993023201 acc: 0.84\n",
      "loss: 1.8294085492621943 acc: 0.86\n",
      "loss: 1.7952556925877454 acc: 0.82\n",
      "loss: 1.8484959448461453 acc: 0.85\n",
      "loss: 1.8217091018567708 acc: 0.86\n",
      "loss: 1.843925753781065 acc: 0.81\n",
      "loss: 1.800052938868119 acc: 0.87\n",
      "loss: 1.8180762863007183 acc: 0.84\n",
      "loss: 1.7961572759784739 acc: 0.89\n",
      "loss: 1.817970271436463 acc: 0.83\n",
      "loss: 1.8614657027246075 acc: 0.88\n",
      "loss: 1.7945846510872983 acc: 0.9\n",
      "loss: 1.8209031081224174 acc: 0.85\n",
      "loss: 1.7945032319081984 acc: 0.86\n",
      "loss: 1.7460359818813003 acc: 0.94\n",
      "loss: 1.858131530317627 acc: 0.84\n",
      "loss: 1.8101968865646532 acc: 0.81\n",
      "loss: 1.8125262875667414 acc: 0.84\n",
      "loss: 1.8146318005357 acc: 0.86\n",
      "loss: 1.8076833647561625 acc: 0.82\n",
      "loss: 1.8494489599076291 acc: 0.78\n",
      "loss: 1.8429169704274708 acc: 0.79\n",
      "loss: 1.8278088407633746 acc: 0.83\n",
      "loss: 1.780265587166975 acc: 0.86\n",
      "loss: 1.8162419209313276 acc: 0.83\n",
      "loss: 1.8584058856515535 acc: 0.81\n",
      "loss: 1.863924365360962 acc: 0.82\n",
      "loss: 1.8165160011983614 acc: 0.87\n",
      "loss: 1.830786637015569 acc: 0.82\n",
      "loss: 1.802951545209039 acc: 0.89\n",
      "loss: 1.8309379893328177 acc: 0.81\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 1.8309\t Accuracy 0.8100\n",
      "loss: 1.8123109104809298 acc: 0.85\n",
      "loss: 1.8156719148189635 acc: 0.86\n",
      "loss: 1.8658926445911659 acc: 0.77\n",
      "loss: 1.8103738455563902 acc: 0.87\n",
      "loss: 1.8315018933181961 acc: 0.88\n",
      "loss: 1.8260910151893146 acc: 0.86\n",
      "loss: 1.7893705383697078 acc: 0.9\n",
      "loss: 1.830959302845577 acc: 0.82\n",
      "loss: 1.7893424644573273 acc: 0.84\n",
      "loss: 1.791939512591081 acc: 0.9\n",
      "loss: 1.8058181834969675 acc: 0.84\n",
      "loss: 1.8265182825136048 acc: 0.8\n",
      "loss: 1.8506421294973325 acc: 0.82\n",
      "loss: 1.8269873253120748 acc: 0.81\n",
      "loss: 1.8346766047892884 acc: 0.86\n",
      "loss: 1.8470750084120875 acc: 0.79\n",
      "loss: 1.772283103884079 acc: 0.85\n",
      "loss: 1.7911934032073322 acc: 0.87\n",
      "loss: 1.852478751538318 acc: 0.81\n",
      "loss: 1.7860011211174165 acc: 0.88\n",
      "loss: 1.8703865952504881 acc: 0.78\n",
      "loss: 1.8268477291312157 acc: 0.84\n",
      "loss: 1.8348543815140446 acc: 0.83\n",
      "loss: 1.816023674601146 acc: 0.87\n",
      "loss: 1.7539571221319374 acc: 0.94\n",
      "loss: 1.8439307456597223 acc: 0.78\n",
      "loss: 1.8405567430159857 acc: 0.8\n",
      "loss: 1.8233407548862155 acc: 0.87\n",
      "loss: 1.8376459226820945 acc: 0.83\n",
      "loss: 1.8265650196900813 acc: 0.82\n",
      "loss: 1.7948074031057004 acc: 0.85\n",
      "loss: 1.8134771463100912 acc: 0.86\n",
      "loss: 1.8460049527034013 acc: 0.79\n",
      "loss: 1.8332649900690732 acc: 0.88\n",
      "loss: 1.8399925088732454 acc: 0.78\n",
      "loss: 1.827218584781065 acc: 0.8\n",
      "loss: 1.8153728188892069 acc: 0.82\n",
      "loss: 1.7873400207297234 acc: 0.88\n",
      "loss: 1.7948737010005973 acc: 0.92\n",
      "loss: 1.8279912854233802 acc: 0.82\n",
      "loss: 1.7978744416108197 acc: 0.81\n",
      "loss: 1.8293151234081588 acc: 0.83\n",
      "loss: 1.8232921888856382 acc: 0.81\n",
      "loss: 1.8097526571025184 acc: 0.89\n",
      "loss: 1.8151581512647579 acc: 0.8\n",
      "loss: 1.8446713649259345 acc: 0.87\n",
      "loss: 1.835994191630109 acc: 0.84\n",
      "loss: 1.8168430649030354 acc: 0.86\n",
      "loss: 1.7730488222350462 acc: 0.9\n",
      "loss: 1.8497686433896428 acc: 0.81\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 1.8498\t Accuracy 0.8100\n",
      "loss: 1.8090432441402706 acc: 0.86\n",
      "loss: 1.7900159962671254 acc: 0.87\n",
      "loss: 1.8072460571683646 acc: 0.88\n",
      "loss: 1.8216774214081546 acc: 0.91\n",
      "loss: 1.8257762432021687 acc: 0.82\n",
      "loss: 1.822630229386846 acc: 0.86\n",
      "loss: 1.8441001311463905 acc: 0.8\n",
      "loss: 1.7732085540835305 acc: 0.93\n",
      "loss: 1.8664594402681995 acc: 0.82\n",
      "loss: 1.7958842221950704 acc: 0.88\n",
      "loss: 1.8291941870825779 acc: 0.84\n",
      "loss: 1.8355454194173806 acc: 0.83\n",
      "loss: 1.8167685606826636 acc: 0.86\n",
      "loss: 1.8402024635886585 acc: 0.81\n",
      "loss: 1.8052863911670332 acc: 0.81\n",
      "loss: 1.8294370876791686 acc: 0.85\n",
      "loss: 1.7877513009190928 acc: 0.87\n",
      "loss: 1.8348688871189365 acc: 0.81\n",
      "loss: 1.821361577582177 acc: 0.88\n",
      "loss: 1.8267866482730801 acc: 0.83\n",
      "loss: 1.80015446259821 acc: 0.87\n",
      "loss: 1.7863228481823157 acc: 0.87\n",
      "loss: 1.7857136390820767 acc: 0.83\n",
      "loss: 1.8328607637485004 acc: 0.79\n",
      "loss: 1.819670421888539 acc: 0.8\n",
      "loss: 1.8487247552177757 acc: 0.8\n",
      "loss: 1.8064514875019848 acc: 0.87\n",
      "loss: 1.8453223500283997 acc: 0.8\n",
      "loss: 1.8642595778874864 acc: 0.77\n",
      "loss: 1.8429655783555048 acc: 0.8\n",
      "loss: 1.8127941476772023 acc: 0.87\n",
      "loss: 1.8222404973137463 acc: 0.82\n",
      "loss: 1.7867484633898496 acc: 0.86\n",
      "loss: 1.835018317868732 acc: 0.79\n",
      "loss: 1.8473128720283831 acc: 0.79\n",
      "loss: 1.8227333128562448 acc: 0.86\n",
      "loss: 1.7913255265898584 acc: 0.91\n",
      "loss: 1.837132042012534 acc: 0.85\n",
      "loss: 1.8065948266976173 acc: 0.8\n",
      "loss: 1.8158407307267808 acc: 0.83\n",
      "loss: 1.8008854424277942 acc: 0.86\n",
      "loss: 1.8184405027439836 acc: 0.83\n",
      "loss: 1.842747696813493 acc: 0.8\n",
      "loss: 1.825241673955461 acc: 0.83\n",
      "loss: 1.8509224009148133 acc: 0.79\n",
      "loss: 1.8658427002522684 acc: 0.77\n",
      "loss: 1.8482169040058865 acc: 0.81\n",
      "loss: 1.8109698539842012 acc: 0.8\n",
      "loss: 1.8510398974225402 acc: 0.8\n",
      "loss: 1.789152238859102 acc: 0.9\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 1.7892\t Accuracy 0.9000\n",
      "loss: 1.8405434849920146 acc: 0.84\n",
      "loss: 1.8363351702174504 acc: 0.85\n",
      "loss: 1.8127417520244518 acc: 0.81\n",
      "loss: 1.8110651555702326 acc: 0.87\n",
      "loss: 1.8163484480345005 acc: 0.86\n",
      "loss: 1.8440998156926398 acc: 0.79\n",
      "loss: 1.8301926568790814 acc: 0.81\n",
      "loss: 1.8349050167611436 acc: 0.84\n",
      "loss: 1.828622807089847 acc: 0.89\n",
      "loss: 1.853292635362254 acc: 0.82\n",
      "loss: 1.8172912666040884 acc: 0.88\n",
      "loss: 1.8323418344505071 acc: 0.88\n",
      "loss: 1.7875187619794235 acc: 0.85\n",
      "loss: 1.8367801216916484 acc: 0.85\n",
      "loss: 1.8183004948680093 acc: 0.83\n",
      "loss: 1.8276927845816022 acc: 0.84\n",
      "loss: 1.8219633501861103 acc: 0.83\n",
      "loss: 1.8048560309793342 acc: 0.83\n",
      "loss: 1.8500486918510322 acc: 0.79\n",
      "loss: 1.8060983890364304 acc: 0.84\n",
      "loss: 1.8217733775533345 acc: 0.87\n",
      "loss: 1.815761644872735 acc: 0.88\n",
      "loss: 1.8491161786728134 acc: 0.81\n",
      "loss: 1.8014368642088687 acc: 0.87\n",
      "loss: 1.8050025137236176 acc: 0.81\n",
      "loss: 1.8013504558487512 acc: 0.85\n",
      "loss: 1.8000729319589974 acc: 0.84\n",
      "loss: 1.8147607206622687 acc: 0.86\n",
      "loss: 1.8370471554359005 acc: 0.85\n",
      "loss: 1.8036256210349655 acc: 0.84\n",
      "loss: 1.7819531902686438 acc: 0.88\n",
      "loss: 1.8397939796491487 acc: 0.85\n",
      "loss: 1.8979596408212476 acc: 0.78\n",
      "loss: 1.8455991884059475 acc: 0.77\n",
      "loss: 1.8058464025177179 acc: 0.87\n",
      "loss: 1.8291345501878948 acc: 0.85\n",
      "loss: 1.8355985241638797 acc: 0.84\n",
      "loss: 1.8479680906234353 acc: 0.81\n",
      "loss: 1.8092337330458605 acc: 0.84\n",
      "loss: 1.7637764963607276 acc: 0.9\n",
      "loss: 1.7964642486492146 acc: 0.81\n",
      "loss: 1.8123836505424533 acc: 0.87\n",
      "loss: 1.831394981572568 acc: 0.88\n",
      "loss: 1.7919838137118551 acc: 0.86\n",
      "loss: 1.81617275523445 acc: 0.84\n",
      "loss: 1.7949132790385178 acc: 0.87\n",
      "loss: 1.7753526834103448 acc: 0.86\n",
      "loss: 1.8891216823533563 acc: 0.77\n",
      "loss: 1.7958599317409696 acc: 0.88\n",
      "loss: 1.8162898608994027 acc: 0.86\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 1.8163\t Accuracy 0.8600\n",
      "loss: 1.8061895625994355 acc: 0.85\n",
      "loss: 1.8178400773614798 acc: 0.85\n",
      "loss: 1.8263238296121542 acc: 0.86\n",
      "loss: 1.7744514513534295 acc: 0.87\n",
      "loss: 1.8606992808447977 acc: 0.79\n",
      "loss: 1.8363196160598236 acc: 0.79\n",
      "loss: 1.8165702008149185 acc: 0.86\n",
      "loss: 1.8181405282347456 acc: 0.86\n",
      "loss: 1.8159069415749862 acc: 0.86\n",
      "loss: 1.8536377587085977 acc: 0.8\n",
      "loss: 1.8459997164113218 acc: 0.83\n",
      "loss: 1.8396373117181661 acc: 0.78\n",
      "loss: 1.793366595106501 acc: 0.89\n",
      "loss: 1.805049125085838 acc: 0.87\n",
      "loss: 1.8618592767079358 acc: 0.76\n",
      "loss: 1.8452551680719478 acc: 0.85\n",
      "loss: 1.8324824107017628 acc: 0.82\n",
      "loss: 1.8169411102757105 acc: 0.81\n",
      "loss: 1.83052692877277 acc: 0.85\n",
      "loss: 1.826454143377475 acc: 0.83\n",
      "loss: 1.8297207704999945 acc: 0.84\n",
      "loss: 1.8222123117780515 acc: 0.79\n",
      "loss: 1.8186383580287262 acc: 0.85\n",
      "loss: 1.826018640119007 acc: 0.87\n",
      "loss: 1.815576429949182 acc: 0.88\n",
      "loss: 1.8221739048950858 acc: 0.84\n",
      "loss: 1.8494376891321473 acc: 0.82\n",
      "loss: 1.8318345305369548 acc: 0.85\n",
      "loss: 1.7989751239713951 acc: 0.87\n",
      "loss: 1.823274473195455 acc: 0.81\n",
      "loss: 1.8319351409199751 acc: 0.8\n",
      "loss: 1.8078041166171954 acc: 0.87\n",
      "loss: 1.805407500590708 acc: 0.86\n",
      "loss: 1.8302330406186196 acc: 0.81\n",
      "loss: 1.8417290899455419 acc: 0.78\n",
      "loss: 1.8345910043208096 acc: 0.81\n",
      "loss: 1.7874875569749598 acc: 0.86\n",
      "loss: 1.8232853966457656 acc: 0.85\n",
      "loss: 1.8287562023503199 acc: 0.82\n",
      "loss: 1.8182736744616639 acc: 0.84\n",
      "loss: 1.8206885380678361 acc: 0.83\n",
      "loss: 1.8489641566251465 acc: 0.81\n",
      "loss: 1.8184237286696214 acc: 0.82\n",
      "loss: 1.8225252767026126 acc: 0.85\n",
      "loss: 1.8335644313352415 acc: 0.88\n",
      "loss: 1.834797488917301 acc: 0.82\n",
      "loss: 1.8438629267979523 acc: 0.85\n",
      "loss: 1.835899782767427 acc: 0.81\n",
      "loss: 1.7984364222223683 acc: 0.82\n",
      "loss: 1.8218715118986564 acc: 0.87\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 1.8219\t Accuracy 0.8700\n",
      "loss: 1.8206279186649559 acc: 0.83\n",
      "loss: 1.8053030543433206 acc: 0.9\n",
      "loss: 1.809532080535195 acc: 0.84\n",
      "loss: 1.8532959183692486 acc: 0.81\n",
      "loss: 1.8491796684884807 acc: 0.86\n",
      "loss: 1.7986384351093136 acc: 0.92\n",
      "loss: 1.822098324933368 acc: 0.85\n",
      "loss: 1.8341552485087826 acc: 0.84\n",
      "loss: 1.82297236816799 acc: 0.86\n",
      "loss: 1.8586815637252472 acc: 0.84\n",
      "loss: 1.8457524137010006 acc: 0.8\n",
      "loss: 1.8280991029403284 acc: 0.83\n",
      "loss: 1.8270004157223538 acc: 0.82\n",
      "loss: 1.8147022175248793 acc: 0.83\n",
      "loss: 1.822014394317112 acc: 0.83\n",
      "loss: 1.8017695094908535 acc: 0.9\n",
      "loss: 1.7946632476398179 acc: 0.85\n",
      "loss: 1.8554951613840494 acc: 0.84\n",
      "loss: 1.8095356343201394 acc: 0.89\n",
      "loss: 1.8227101987728505 acc: 0.85\n",
      "loss: 1.8439265000156706 acc: 0.79\n",
      "loss: 1.8139939039166249 acc: 0.86\n",
      "loss: 1.8090916197864089 acc: 0.89\n",
      "loss: 1.8097954269853576 acc: 0.86\n",
      "loss: 1.8030840132855417 acc: 0.84\n",
      "loss: 1.776461439698836 acc: 0.92\n",
      "loss: 1.823058609778604 acc: 0.87\n",
      "loss: 1.801073335160539 acc: 0.83\n",
      "loss: 1.8211200571094839 acc: 0.86\n",
      "loss: 1.8492945301856247 acc: 0.83\n",
      "loss: 1.8215680099170712 acc: 0.84\n",
      "loss: 1.8000361503987319 acc: 0.9\n",
      "loss: 1.8295198183120434 acc: 0.82\n",
      "loss: 1.8387362440751298 acc: 0.82\n",
      "loss: 1.8393302941682776 acc: 0.79\n",
      "loss: 1.7937663033399898 acc: 0.87\n",
      "loss: 1.8201617871909743 acc: 0.87\n",
      "loss: 1.7929875295249382 acc: 0.84\n",
      "loss: 1.8703754427269592 acc: 0.77\n",
      "loss: 1.7964241468858022 acc: 0.85\n",
      "loss: 1.8257173210865643 acc: 0.91\n",
      "loss: 1.8330703035926823 acc: 0.79\n",
      "loss: 1.8107873078621362 acc: 0.85\n",
      "loss: 1.817273041760169 acc: 0.84\n",
      "loss: 1.8403242147547982 acc: 0.83\n",
      "loss: 1.8076546350869225 acc: 0.85\n",
      "loss: 1.8618810339497047 acc: 0.85\n",
      "loss: 1.8149757023748299 acc: 0.83\n",
      "loss: 1.8159337131106688 acc: 0.88\n",
      "loss: 1.8007633500539562 acc: 0.89\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 1.8008\t Accuracy 0.8900\n",
      "loss: 1.79614849591516 acc: 0.88\n",
      "loss: 1.8162694335726997 acc: 0.82\n",
      "loss: 1.8166693921381611 acc: 0.85\n",
      "loss: 1.838391471599619 acc: 0.82\n",
      "loss: 1.819594321365457 acc: 0.83\n",
      "loss: 1.8677431948861312 acc: 0.89\n",
      "loss: 1.8213926723484268 acc: 0.86\n",
      "loss: 1.8189497833562722 acc: 0.85\n",
      "loss: 1.8186685401170488 acc: 0.86\n",
      "loss: 1.8246977932139197 acc: 0.85\n",
      "loss: 1.780624829397133 acc: 0.88\n",
      "loss: 1.7966912894330003 acc: 0.94\n",
      "loss: 1.8255473909704074 acc: 0.85\n",
      "loss: 1.8706567919497485 acc: 0.82\n",
      "loss: 1.8168225453695266 acc: 0.85\n",
      "loss: 1.8143301827717904 acc: 0.81\n",
      "loss: 1.817102549250337 acc: 0.84\n",
      "loss: 1.793415441346009 acc: 0.85\n",
      "loss: 1.8086086019005159 acc: 0.85\n",
      "loss: 1.8249974272276974 acc: 0.77\n",
      "loss: 1.8300583519924318 acc: 0.78\n",
      "loss: 1.7703067367200815 acc: 0.87\n",
      "loss: 1.7636472103312764 acc: 0.89\n",
      "loss: 1.8057633949778205 acc: 0.85\n",
      "loss: 1.832121903083494 acc: 0.86\n",
      "loss: 1.84714080560945 acc: 0.82\n",
      "loss: 1.82215227544526 acc: 0.84\n",
      "loss: 1.8143824495375134 acc: 0.86\n",
      "loss: 1.8054036836828136 acc: 0.79\n",
      "loss: 1.817683210253829 acc: 0.84\n",
      "loss: 1.831662803522469 acc: 0.89\n",
      "loss: 1.857075585205405 acc: 0.78\n",
      "loss: 1.8526494820416497 acc: 0.84\n",
      "loss: 1.8209531057417139 acc: 0.89\n",
      "loss: 1.8332523305451034 acc: 0.85\n",
      "loss: 1.8413610302521837 acc: 0.82\n",
      "loss: 1.8413162086056127 acc: 0.8\n",
      "loss: 1.8794721677873614 acc: 0.86\n",
      "loss: 1.8036620888965857 acc: 0.88\n",
      "loss: 1.8583358074876601 acc: 0.83\n",
      "loss: 1.8180179920771418 acc: 0.82\n",
      "loss: 1.8457020309677992 acc: 0.83\n",
      "loss: 1.848879181885809 acc: 0.81\n",
      "loss: 1.8180314849996781 acc: 0.85\n",
      "loss: 1.8235170988578755 acc: 0.83\n",
      "loss: 1.8326641120295764 acc: 0.82\n",
      "loss: 1.8483655065928888 acc: 0.82\n",
      "loss: 1.8298954285213833 acc: 0.8\n",
      "loss: 1.8197860837527733 acc: 0.86\n",
      "loss: 1.8453871641615212 acc: 0.82\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 1.8454\t Accuracy 0.8200\n",
      "loss: 1.807093359178381 acc: 0.89\n",
      "loss: 1.8283894887727794 acc: 0.83\n",
      "loss: 1.830381393061572 acc: 0.83\n",
      "loss: 1.8118514662510492 acc: 0.88\n",
      "loss: 1.8424304192423642 acc: 0.84\n",
      "loss: 1.8155042932376009 acc: 0.85\n",
      "loss: 1.8323913912822878 acc: 0.85\n",
      "loss: 1.7994380674180235 acc: 0.88\n",
      "loss: 1.8398634608385587 acc: 0.83\n",
      "loss: 1.8008889564755848 acc: 0.9\n",
      "loss: 1.7892585905724434 acc: 0.85\n",
      "loss: 1.826396067045208 acc: 0.82\n",
      "loss: 1.830969593772047 acc: 0.82\n",
      "loss: 1.7983814886498575 acc: 0.83\n",
      "loss: 1.7913868808133082 acc: 0.85\n",
      "loss: 1.8416595344597884 acc: 0.88\n",
      "loss: 1.8572632930870743 acc: 0.77\n",
      "loss: 1.774965844554774 acc: 0.87\n",
      "loss: 1.8183991305394196 acc: 0.84\n",
      "loss: 1.8218769600668645 acc: 0.89\n",
      "loss: 1.8036949526576467 acc: 0.88\n",
      "loss: 1.8067423741750732 acc: 0.91\n",
      "loss: 1.852803485427068 acc: 0.77\n",
      "loss: 1.808896880219742 acc: 0.86\n",
      "loss: 1.80343851788644 acc: 0.85\n",
      "loss: 1.8305627277168295 acc: 0.84\n",
      "loss: 1.846518635966754 acc: 0.78\n",
      "loss: 1.835139081338028 acc: 0.84\n",
      "loss: 1.8114886277603688 acc: 0.87\n",
      "loss: 1.8063155486431375 acc: 0.87\n",
      "loss: 1.8086250034108764 acc: 0.87\n",
      "loss: 1.8372759893983923 acc: 0.83\n",
      "loss: 1.803405703437802 acc: 0.89\n",
      "loss: 1.8131942537570915 acc: 0.83\n",
      "loss: 1.8154747412020753 acc: 0.86\n",
      "loss: 1.7836038514907988 acc: 0.88\n",
      "loss: 1.8255819133748803 acc: 0.82\n",
      "loss: 1.8007407200406471 acc: 0.88\n",
      "loss: 1.82587987330034 acc: 0.81\n",
      "loss: 1.8083224634934356 acc: 0.81\n",
      "loss: 1.7737184264392587 acc: 0.87\n",
      "loss: 1.8377130901361607 acc: 0.83\n",
      "loss: 1.8399154792589132 acc: 0.82\n",
      "loss: 1.8008943129729753 acc: 0.83\n",
      "loss: 1.79760100039192 acc: 0.87\n",
      "loss: 1.7958769645261825 acc: 0.9\n",
      "loss: 1.8314312485067925 acc: 0.81\n",
      "loss: 1.8204866347923232 acc: 0.85\n",
      "loss: 1.8592541888252168 acc: 0.8\n",
      "loss: 1.794074725330119 acc: 0.87\n",
      "loss: 1.7894302818601262 acc: 0.88\n",
      "loss: 1.8009201479254306 acc: 0.88\n",
      "loss: 1.8335251324700081 acc: 0.86\n",
      "loss: 1.8236332344134996 acc: 0.89\n",
      "loss: 1.8006458648330295 acc: 0.8\n",
      "loss: 1.7520195452580263 acc: 0.83\n",
      "loss: 1.76382750262274 acc: 0.84\n",
      "loss: 1.8341847466757195 acc: 0.81\n",
      "loss: 1.7526896512924393 acc: 0.89\n",
      "loss: 1.7947242965013694 acc: 0.87\n",
      "loss: 1.792283645509982 acc: 0.85\n",
      "loss: 1.8504677769101228 acc: 0.82\n",
      "loss: 1.856818257074695 acc: 0.82\n",
      "loss: 1.8661428582755126 acc: 0.86\n",
      "loss: 1.7717610955163963 acc: 0.94\n",
      "loss: 1.7904477158576213 acc: 0.83\n",
      "loss: 1.7473925407096795 acc: 0.9\n",
      "loss: 1.7774371338455857 acc: 0.87\n",
      "loss: 1.7820214820585323 acc: 0.88\n",
      "loss: 1.8528940197036479 acc: 0.88\n",
      "loss: 1.8198222695791404 acc: 0.87\n",
      "loss: 1.82412149900786 acc: 0.74\n",
      "loss: 1.8479630917009642 acc: 0.89\n",
      "loss: 1.8301125518997288 acc: 0.86\n",
      "loss: 1.849493951363929 acc: 0.81\n",
      "loss: 1.849683220515602 acc: 0.77\n",
      "loss: 1.8700015120933051 acc: 0.81\n",
      "loss: 1.819982577228113 acc: 0.89\n",
      "loss: 1.7956938132801423 acc: 0.88\n",
      "loss: 1.8157538680517005 acc: 0.85\n",
      "loss: 1.775126750100004 acc: 0.94\n",
      "loss: 1.7440083149052603 acc: 0.91\n",
      "loss: 1.783277978251123 acc: 0.89\n",
      "loss: 1.7782467889179332 acc: 0.86\n",
      "loss: 1.8109927412272488 acc: 0.87\n",
      "loss: 1.8526119074965701 acc: 0.89\n",
      "loss: 1.840867269255441 acc: 0.89\n",
      "loss: 1.7348547276602917 acc: 0.91\n",
      "loss: 1.7044422871342704 acc: 0.98\n",
      "loss: 1.7369339264212196 acc: 0.96\n",
      "loss: 1.742644703941903 acc: 0.97\n",
      "loss: 1.7968364573288138 acc: 0.88\n",
      "loss: 1.7845263058345202 acc: 0.85\n",
      "loss: 1.7242506139656695 acc: 0.86\n",
      "loss: 1.805042064249183 acc: 0.89\n",
      "loss: 1.8009991024312 acc: 0.91\n",
      "loss: 1.8776660233042441 acc: 0.79\n",
      "loss: 1.7114146720770285 acc: 0.94\n",
      "loss: 1.847331065153147 acc: 0.88\n",
      "\n",
      "Epoch [8]\t Average training loss 1.8208\t Average training accuracy 0.8432\n",
      "Epoch [8]\t Average validation loss 1.8000\t Average validation accuracy 0.8702\n",
      "\n",
      "loss: 1.8406666409646797 acc: 0.85\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 1.8407\t Accuracy 0.8500\n",
      "loss: 1.8078498444838809 acc: 0.82\n",
      "loss: 1.8168313137120933 acc: 0.84\n",
      "loss: 1.7775291420293586 acc: 0.87\n",
      "loss: 1.8338441612417673 acc: 0.8\n",
      "loss: 1.853174341982814 acc: 0.8\n",
      "loss: 1.8134323052259176 acc: 0.87\n",
      "loss: 1.797097822378972 acc: 0.89\n",
      "loss: 1.823435289700432 acc: 0.89\n",
      "loss: 1.8342188284161045 acc: 0.88\n",
      "loss: 1.8533302367302 acc: 0.78\n",
      "loss: 1.8184568636575005 acc: 0.85\n",
      "loss: 1.7911406482788153 acc: 0.84\n",
      "loss: 1.8140298508954686 acc: 0.84\n",
      "loss: 1.8457655853462904 acc: 0.82\n",
      "loss: 1.7797582967783736 acc: 0.91\n",
      "loss: 1.8362488118240095 acc: 0.86\n",
      "loss: 1.8111577780744952 acc: 0.83\n",
      "loss: 1.7886031554222759 acc: 0.9\n",
      "loss: 1.8322680855560207 acc: 0.78\n",
      "loss: 1.8385430618207017 acc: 0.82\n",
      "loss: 1.8136218947844742 acc: 0.81\n",
      "loss: 1.8255987765239814 acc: 0.87\n",
      "loss: 1.8145257944245676 acc: 0.84\n",
      "loss: 1.8160677426413978 acc: 0.86\n",
      "loss: 1.8489294847953948 acc: 0.82\n",
      "loss: 1.860982207364584 acc: 0.81\n",
      "loss: 1.83513695688138 acc: 0.85\n",
      "loss: 1.791371618251301 acc: 0.86\n",
      "loss: 1.7898624526250173 acc: 0.88\n",
      "loss: 1.8605684041132242 acc: 0.77\n",
      "loss: 1.7923783135522589 acc: 0.89\n",
      "loss: 1.7696418736289354 acc: 0.91\n",
      "loss: 1.843380107975139 acc: 0.85\n",
      "loss: 1.8413622534079663 acc: 0.81\n",
      "loss: 1.7977639596845814 acc: 0.89\n",
      "loss: 1.7831898527093437 acc: 0.85\n",
      "loss: 1.813418727300025 acc: 0.88\n",
      "loss: 1.7808079708892977 acc: 0.89\n",
      "loss: 1.8067133838925833 acc: 0.85\n",
      "loss: 1.7900798980413684 acc: 0.88\n",
      "loss: 1.8225533589362775 acc: 0.9\n",
      "loss: 1.8438606489635518 acc: 0.77\n",
      "loss: 1.808930453896233 acc: 0.8\n",
      "loss: 1.8119286148144371 acc: 0.88\n",
      "loss: 1.755748313201351 acc: 0.89\n",
      "loss: 1.8706521942736365 acc: 0.82\n",
      "loss: 1.814978844755652 acc: 0.87\n",
      "loss: 1.7987589112279785 acc: 0.85\n",
      "loss: 1.816275341112912 acc: 0.9\n",
      "loss: 1.796838828264948 acc: 0.88\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 1.7968\t Accuracy 0.8800\n",
      "loss: 1.8251119601523462 acc: 0.83\n",
      "loss: 1.8347720894102602 acc: 0.92\n",
      "loss: 1.8098120848095507 acc: 0.85\n",
      "loss: 1.8322075195865097 acc: 0.82\n",
      "loss: 1.8722221147391205 acc: 0.78\n",
      "loss: 1.802410690986319 acc: 0.89\n",
      "loss: 1.794908053360839 acc: 0.91\n",
      "loss: 1.8473879717222454 acc: 0.83\n",
      "loss: 1.815404275156904 acc: 0.87\n",
      "loss: 1.8191228541043751 acc: 0.84\n",
      "loss: 1.8183969067555852 acc: 0.86\n",
      "loss: 1.7810944427385402 acc: 0.85\n",
      "loss: 1.8504019343105957 acc: 0.77\n",
      "loss: 1.836806209776249 acc: 0.82\n",
      "loss: 1.7797978700297308 acc: 0.89\n",
      "loss: 1.8415690763276213 acc: 0.78\n",
      "loss: 1.8324798983098947 acc: 0.84\n",
      "loss: 1.8254694383569108 acc: 0.8\n",
      "loss: 1.8088765282486536 acc: 0.86\n",
      "loss: 1.852166643016248 acc: 0.78\n",
      "loss: 1.8220053703329415 acc: 0.82\n",
      "loss: 1.7393152734546293 acc: 0.9\n",
      "loss: 1.8542157965746413 acc: 0.83\n",
      "loss: 1.8155235764307023 acc: 0.89\n",
      "loss: 1.8442672537288205 acc: 0.81\n",
      "loss: 1.83364419404636 acc: 0.82\n",
      "loss: 1.8176450183875192 acc: 0.86\n",
      "loss: 1.7930261475574187 acc: 0.84\n",
      "loss: 1.8388126797338489 acc: 0.82\n",
      "loss: 1.8385635434326975 acc: 0.74\n",
      "loss: 1.814139488498765 acc: 0.86\n",
      "loss: 1.7882808903342404 acc: 0.88\n",
      "loss: 1.8297995309906674 acc: 0.83\n",
      "loss: 1.8020842754622137 acc: 0.84\n",
      "loss: 1.8213345389329847 acc: 0.88\n",
      "loss: 1.82006672325979 acc: 0.88\n",
      "loss: 1.8472047063308148 acc: 0.81\n",
      "loss: 1.824090489787765 acc: 0.83\n",
      "loss: 1.8275137669143038 acc: 0.82\n",
      "loss: 1.8318000696688284 acc: 0.85\n",
      "loss: 1.8415630052548337 acc: 0.79\n",
      "loss: 1.7996095389621811 acc: 0.88\n",
      "loss: 1.8112979556144095 acc: 0.88\n",
      "loss: 1.7999122283818578 acc: 0.86\n",
      "loss: 1.775995797424233 acc: 0.91\n",
      "loss: 1.8218650729044041 acc: 0.84\n",
      "loss: 1.8115944459447493 acc: 0.83\n",
      "loss: 1.805850490451097 acc: 0.85\n",
      "loss: 1.8406685310210522 acc: 0.8\n",
      "loss: 1.8798658600166085 acc: 0.83\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 1.8799\t Accuracy 0.8300\n",
      "loss: 1.8320317168370837 acc: 0.86\n",
      "loss: 1.796505067365598 acc: 0.84\n",
      "loss: 1.7722087561650257 acc: 0.91\n",
      "loss: 1.8026102760687466 acc: 0.86\n",
      "loss: 1.8256682814987326 acc: 0.83\n",
      "loss: 1.7867959426218016 acc: 0.88\n",
      "loss: 1.7848889335305569 acc: 0.88\n",
      "loss: 1.7820688205019184 acc: 0.89\n",
      "loss: 1.8580464531189975 acc: 0.78\n",
      "loss: 1.7914999262428128 acc: 0.91\n",
      "loss: 1.836729609954447 acc: 0.78\n",
      "loss: 1.8611818826776654 acc: 0.79\n",
      "loss: 1.792799610463373 acc: 0.87\n",
      "loss: 1.835115004735892 acc: 0.84\n",
      "loss: 1.8089087632527197 acc: 0.8\n",
      "loss: 1.7864148003702598 acc: 0.86\n",
      "loss: 1.8654720981596356 acc: 0.81\n",
      "loss: 1.8294853062813317 acc: 0.82\n",
      "loss: 1.848180204180479 acc: 0.84\n",
      "loss: 1.8401684106122556 acc: 0.85\n",
      "loss: 1.8216327639981273 acc: 0.82\n",
      "loss: 1.821835635506528 acc: 0.86\n",
      "loss: 1.774889929269194 acc: 0.91\n",
      "loss: 1.8645218421461889 acc: 0.85\n",
      "loss: 1.8121757357263413 acc: 0.87\n",
      "loss: 1.814835301633438 acc: 0.81\n",
      "loss: 1.8510056896225666 acc: 0.82\n",
      "loss: 1.7696955398945033 acc: 0.89\n",
      "loss: 1.7863670167870196 acc: 0.88\n",
      "loss: 1.840186469365911 acc: 0.81\n",
      "loss: 1.8317582571563475 acc: 0.86\n",
      "loss: 1.8264032443750438 acc: 0.85\n",
      "loss: 1.8372943284849041 acc: 0.82\n",
      "loss: 1.8362861700625503 acc: 0.84\n",
      "loss: 1.8515581662983964 acc: 0.82\n",
      "loss: 1.7644743787164767 acc: 0.92\n",
      "loss: 1.760567832124143 acc: 0.88\n",
      "loss: 1.8404658631394115 acc: 0.84\n",
      "loss: 1.7913855041905506 acc: 0.85\n",
      "loss: 1.8545833019978613 acc: 0.8\n",
      "loss: 1.7839522913318278 acc: 0.87\n",
      "loss: 1.79338704238767 acc: 0.88\n",
      "loss: 1.8605441580771955 acc: 0.78\n",
      "loss: 1.8383299198407657 acc: 0.76\n",
      "loss: 1.8398708243439736 acc: 0.82\n",
      "loss: 1.790779719133486 acc: 0.88\n",
      "loss: 1.8442561612292585 acc: 0.81\n",
      "loss: 1.7837139475371324 acc: 0.86\n",
      "loss: 1.8065690815183266 acc: 0.86\n",
      "loss: 1.8183409733843447 acc: 0.87\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 1.8183\t Accuracy 0.8700\n",
      "loss: 1.8183296979978811 acc: 0.83\n",
      "loss: 1.784302766138463 acc: 0.89\n",
      "loss: 1.8040381484644912 acc: 0.86\n",
      "loss: 1.8444310629546026 acc: 0.81\n",
      "loss: 1.834721004525488 acc: 0.85\n",
      "loss: 1.8647584457550475 acc: 0.8\n",
      "loss: 1.8161929289198175 acc: 0.9\n",
      "loss: 1.7922592204293941 acc: 0.84\n",
      "loss: 1.8291892259494742 acc: 0.84\n",
      "loss: 1.83379158651049 acc: 0.82\n",
      "loss: 1.8220959810559874 acc: 0.8\n",
      "loss: 1.8240626554107278 acc: 0.86\n",
      "loss: 1.8522048906142086 acc: 0.86\n",
      "loss: 1.7539669652710181 acc: 0.91\n",
      "loss: 1.783147655969174 acc: 0.85\n",
      "loss: 1.820583533051154 acc: 0.87\n",
      "loss: 1.8222880900892826 acc: 0.83\n",
      "loss: 1.847184446604268 acc: 0.82\n",
      "loss: 1.8484744591000464 acc: 0.73\n",
      "loss: 1.805741324574843 acc: 0.8\n",
      "loss: 1.7871869501223403 acc: 0.92\n",
      "loss: 1.8055137536098655 acc: 0.88\n",
      "loss: 1.8014603509678444 acc: 0.85\n",
      "loss: 1.8539725822814024 acc: 0.8\n",
      "loss: 1.8344128374970061 acc: 0.85\n",
      "loss: 1.8432501855105394 acc: 0.87\n",
      "loss: 1.8125276080324084 acc: 0.85\n",
      "loss: 1.9093525962672842 acc: 0.74\n",
      "loss: 1.803173593495395 acc: 0.86\n",
      "loss: 1.810481146982007 acc: 0.86\n",
      "loss: 1.80443903736376 acc: 0.83\n",
      "loss: 1.8327900173264564 acc: 0.85\n",
      "loss: 1.850910876765721 acc: 0.76\n",
      "loss: 1.825141077572681 acc: 0.82\n",
      "loss: 1.7837395498621504 acc: 0.94\n",
      "loss: 1.8261066075094943 acc: 0.82\n",
      "loss: 1.8273569148933066 acc: 0.85\n",
      "loss: 1.8263523964735713 acc: 0.87\n",
      "loss: 1.818914164970844 acc: 0.86\n",
      "loss: 1.851925894036153 acc: 0.86\n",
      "loss: 1.7987746516545269 acc: 0.87\n",
      "loss: 1.825819648729216 acc: 0.88\n",
      "loss: 1.8099772114250374 acc: 0.88\n",
      "loss: 1.826536615050305 acc: 0.82\n",
      "loss: 1.8215600159707415 acc: 0.89\n",
      "loss: 1.8091875064398024 acc: 0.84\n",
      "loss: 1.7949113328173647 acc: 0.87\n",
      "loss: 1.8278491580380072 acc: 0.77\n",
      "loss: 1.8075337911916063 acc: 0.82\n",
      "loss: 1.823332936733063 acc: 0.84\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 1.8233\t Accuracy 0.8400\n",
      "loss: 1.8099111496163156 acc: 0.89\n",
      "loss: 1.8262606001280575 acc: 0.77\n",
      "loss: 1.7864112906315475 acc: 0.86\n",
      "loss: 1.7785811023703146 acc: 0.88\n",
      "loss: 1.86676815800013 acc: 0.82\n",
      "loss: 1.7823738877513637 acc: 0.89\n",
      "loss: 1.8212556396348234 acc: 0.84\n",
      "loss: 1.8138294017167544 acc: 0.85\n",
      "loss: 1.7899104732111615 acc: 0.83\n",
      "loss: 1.8066948486179135 acc: 0.84\n",
      "loss: 1.8196983236774 acc: 0.85\n",
      "loss: 1.7816205366681115 acc: 0.85\n",
      "loss: 1.8068133556450914 acc: 0.86\n",
      "loss: 1.859303575824377 acc: 0.8\n",
      "loss: 1.811899640598792 acc: 0.82\n",
      "loss: 1.8147371447941116 acc: 0.84\n",
      "loss: 1.8387641404082202 acc: 0.81\n",
      "loss: 1.8327351772110578 acc: 0.85\n",
      "loss: 1.8370853691243374 acc: 0.88\n",
      "loss: 1.7988996604281147 acc: 0.86\n",
      "loss: 1.8017146231231451 acc: 0.82\n",
      "loss: 1.8195096714686394 acc: 0.84\n",
      "loss: 1.8125588636111976 acc: 0.82\n",
      "loss: 1.8583811737126608 acc: 0.86\n",
      "loss: 1.816554032005204 acc: 0.86\n",
      "loss: 1.809085572321638 acc: 0.88\n",
      "loss: 1.8197535605997064 acc: 0.88\n",
      "loss: 1.8559343640280728 acc: 0.84\n",
      "loss: 1.7891511327385121 acc: 0.87\n",
      "loss: 1.7944091063054395 acc: 0.85\n",
      "loss: 1.8251108719598994 acc: 0.8\n",
      "loss: 1.8331825178861119 acc: 0.82\n",
      "loss: 1.79313919703356 acc: 0.87\n",
      "loss: 1.837125915637271 acc: 0.82\n",
      "loss: 1.8046877098787075 acc: 0.86\n",
      "loss: 1.7950098470534561 acc: 0.88\n",
      "loss: 1.8373184105766271 acc: 0.84\n",
      "loss: 1.8027421363317169 acc: 0.84\n",
      "loss: 1.7998862445806671 acc: 0.81\n",
      "loss: 1.826387455183549 acc: 0.84\n",
      "loss: 1.8316370610963046 acc: 0.83\n",
      "loss: 1.8252173122526119 acc: 0.86\n",
      "loss: 1.8336214414918675 acc: 0.9\n",
      "loss: 1.8356050504051424 acc: 0.83\n",
      "loss: 1.8544215575992582 acc: 0.75\n",
      "loss: 1.8326357343570532 acc: 0.86\n",
      "loss: 1.7911909191224473 acc: 0.84\n",
      "loss: 1.839036286462794 acc: 0.79\n",
      "loss: 1.8226105451326715 acc: 0.85\n",
      "loss: 1.8327618577274092 acc: 0.83\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 1.8328\t Accuracy 0.8300\n",
      "loss: 1.8567232258598836 acc: 0.79\n",
      "loss: 1.7958503820567537 acc: 0.89\n",
      "loss: 1.8129411500497767 acc: 0.83\n",
      "loss: 1.8376986806399207 acc: 0.84\n",
      "loss: 1.8337265421591888 acc: 0.83\n",
      "loss: 1.8123899266726136 acc: 0.82\n",
      "loss: 1.7991129415109883 acc: 0.87\n",
      "loss: 1.8142909424068403 acc: 0.91\n",
      "loss: 1.842869264735361 acc: 0.86\n",
      "loss: 1.8447685239470066 acc: 0.84\n",
      "loss: 1.7834913262345138 acc: 0.86\n",
      "loss: 1.8407375990212735 acc: 0.83\n",
      "loss: 1.8302545034067645 acc: 0.83\n",
      "loss: 1.7953397227959649 acc: 0.88\n",
      "loss: 1.809755827081419 acc: 0.85\n",
      "loss: 1.7791610417137267 acc: 0.9\n",
      "loss: 1.7951055979347401 acc: 0.89\n",
      "loss: 1.8125011296970188 acc: 0.83\n",
      "loss: 1.8110191077095086 acc: 0.77\n",
      "loss: 1.7919659811121307 acc: 0.9\n",
      "loss: 1.7991860811402858 acc: 0.8\n",
      "loss: 1.8528426064505794 acc: 0.82\n",
      "loss: 1.8166263450352957 acc: 0.85\n",
      "loss: 1.7818756278684742 acc: 0.89\n",
      "loss: 1.817581404669787 acc: 0.82\n",
      "loss: 1.8219625300420221 acc: 0.81\n",
      "loss: 1.80639204057645 acc: 0.9\n",
      "loss: 1.861023981059415 acc: 0.83\n",
      "loss: 1.8164918673162251 acc: 0.84\n",
      "loss: 1.8425458253675995 acc: 0.79\n",
      "loss: 1.8935162595133184 acc: 0.73\n",
      "loss: 1.7994611107272882 acc: 0.94\n",
      "loss: 1.849275684428868 acc: 0.8\n",
      "loss: 1.8308912672512516 acc: 0.82\n",
      "loss: 1.812067386189488 acc: 0.88\n",
      "loss: 1.8429902799113222 acc: 0.89\n",
      "loss: 1.881575782531894 acc: 0.8\n",
      "loss: 1.8467169512709125 acc: 0.81\n",
      "loss: 1.817005451967411 acc: 0.84\n",
      "loss: 1.8038475890932708 acc: 0.78\n",
      "loss: 1.8303827073475352 acc: 0.81\n",
      "loss: 1.8771249493309659 acc: 0.75\n",
      "loss: 1.771420669533013 acc: 0.87\n",
      "loss: 1.8451983805366337 acc: 0.84\n",
      "loss: 1.843140769505093 acc: 0.85\n",
      "loss: 1.8055946295761478 acc: 0.89\n",
      "loss: 1.8576485142649326 acc: 0.81\n",
      "loss: 1.8023423767381614 acc: 0.89\n",
      "loss: 1.8409781373723382 acc: 0.83\n",
      "loss: 1.78667823946169 acc: 0.88\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 1.7867\t Accuracy 0.8800\n",
      "loss: 1.8030917307167817 acc: 0.85\n",
      "loss: 1.8429619783673448 acc: 0.82\n",
      "loss: 1.8062273586458966 acc: 0.84\n",
      "loss: 1.8313702047247415 acc: 0.84\n",
      "loss: 1.8196849786286093 acc: 0.88\n",
      "loss: 1.828061331752594 acc: 0.87\n",
      "loss: 1.857117404382603 acc: 0.84\n",
      "loss: 1.813500600569114 acc: 0.9\n",
      "loss: 1.8345420955794884 acc: 0.8\n",
      "loss: 1.855182683643589 acc: 0.76\n",
      "loss: 1.8076758373825441 acc: 0.86\n",
      "loss: 1.857197807845989 acc: 0.83\n",
      "loss: 1.7930798719007603 acc: 0.89\n",
      "loss: 1.8593943938126403 acc: 0.82\n",
      "loss: 1.778266520212944 acc: 0.86\n",
      "loss: 1.8199777798831343 acc: 0.85\n",
      "loss: 1.8326230854342986 acc: 0.81\n",
      "loss: 1.8313769659203623 acc: 0.81\n",
      "loss: 1.8586694991256525 acc: 0.85\n",
      "loss: 1.8475324181008204 acc: 0.83\n",
      "loss: 1.7638384687783057 acc: 0.88\n",
      "loss: 1.827702123839833 acc: 0.8\n",
      "loss: 1.8290073387134356 acc: 0.85\n",
      "loss: 1.8515970524488652 acc: 0.83\n",
      "loss: 1.8055770899114085 acc: 0.82\n",
      "loss: 1.835605386801694 acc: 0.81\n",
      "loss: 1.8441372735835333 acc: 0.75\n",
      "loss: 1.8248889152401195 acc: 0.89\n",
      "loss: 1.8284452827894384 acc: 0.87\n",
      "loss: 1.7985766170665698 acc: 0.85\n",
      "loss: 1.803032797173416 acc: 0.88\n",
      "loss: 1.8183464322296803 acc: 0.89\n",
      "loss: 1.7990891188431926 acc: 0.86\n",
      "loss: 1.8490109217168247 acc: 0.78\n",
      "loss: 1.8029579068316308 acc: 0.82\n",
      "loss: 1.8072007663689746 acc: 0.82\n",
      "loss: 1.810673131094035 acc: 0.81\n",
      "loss: 1.7953151826189937 acc: 0.81\n",
      "loss: 1.7525140577021434 acc: 0.95\n",
      "loss: 1.8483112714322345 acc: 0.75\n",
      "loss: 1.8036476048716619 acc: 0.84\n",
      "loss: 1.8102380201846615 acc: 0.83\n",
      "loss: 1.8128378052668719 acc: 0.8\n",
      "loss: 1.806459432501305 acc: 0.86\n",
      "loss: 1.8027687822933314 acc: 0.82\n",
      "loss: 1.8274236958616663 acc: 0.83\n",
      "loss: 1.8128680363942828 acc: 0.82\n",
      "loss: 1.848770051391899 acc: 0.85\n",
      "loss: 1.846040130859721 acc: 0.79\n",
      "loss: 1.777855502055507 acc: 0.84\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 1.7779\t Accuracy 0.8400\n",
      "loss: 1.8392978846923338 acc: 0.79\n",
      "loss: 1.834422892861763 acc: 0.83\n",
      "loss: 1.8608967190217316 acc: 0.84\n",
      "loss: 1.8230103066067773 acc: 0.85\n",
      "loss: 1.800800834665534 acc: 0.9\n",
      "loss: 1.8397143047109028 acc: 0.83\n",
      "loss: 1.8128466770213842 acc: 0.85\n",
      "loss: 1.8202774082344988 acc: 0.84\n",
      "loss: 1.7818355814427342 acc: 0.89\n",
      "loss: 1.8112820897112847 acc: 0.85\n",
      "loss: 1.8241084916090267 acc: 0.87\n",
      "loss: 1.8089518741800512 acc: 0.86\n",
      "loss: 1.8491921244983671 acc: 0.83\n",
      "loss: 1.8415716082402076 acc: 0.8\n",
      "loss: 1.8426779213119682 acc: 0.84\n",
      "loss: 1.802904963064445 acc: 0.87\n",
      "loss: 1.8069136686557368 acc: 0.83\n",
      "loss: 1.8531917708280536 acc: 0.8\n",
      "loss: 1.8501558100681217 acc: 0.83\n",
      "loss: 1.8040128387705194 acc: 0.84\n",
      "loss: 1.8381445676444665 acc: 0.81\n",
      "loss: 1.8243501915290004 acc: 0.85\n",
      "loss: 1.7898987867822242 acc: 0.89\n",
      "loss: 1.81143000898114 acc: 0.86\n",
      "loss: 1.8143068910131737 acc: 0.84\n",
      "loss: 1.7855929020269576 acc: 0.88\n",
      "loss: 1.7937483286584486 acc: 0.82\n",
      "loss: 1.8142601180174591 acc: 0.85\n",
      "loss: 1.806094002905753 acc: 0.82\n",
      "loss: 1.8236739974548632 acc: 0.86\n",
      "loss: 1.7955412652341078 acc: 0.87\n",
      "loss: 1.8003456784700231 acc: 0.89\n",
      "loss: 1.8335081146613297 acc: 0.85\n",
      "loss: 1.8107509256320742 acc: 0.8\n",
      "loss: 1.8317212327607737 acc: 0.83\n",
      "loss: 1.8206236317149171 acc: 0.88\n",
      "loss: 1.871735660048833 acc: 0.79\n",
      "loss: 1.8043596222043428 acc: 0.87\n",
      "loss: 1.8150357821482086 acc: 0.9\n",
      "loss: 1.8038215766879504 acc: 0.84\n",
      "loss: 1.8056260609784254 acc: 0.8\n",
      "loss: 1.831217187099538 acc: 0.81\n",
      "loss: 1.8178193348982494 acc: 0.91\n",
      "loss: 1.8436029970095322 acc: 0.83\n",
      "loss: 1.8210042762652048 acc: 0.84\n",
      "loss: 1.828215027139297 acc: 0.82\n",
      "loss: 1.8284165903595389 acc: 0.85\n",
      "loss: 1.8382310217352533 acc: 0.82\n",
      "loss: 1.867693128685928 acc: 0.81\n",
      "loss: 1.8739386013731612 acc: 0.8\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 1.8739\t Accuracy 0.8000\n",
      "loss: 1.794758134885309 acc: 0.88\n",
      "loss: 1.8232972280452053 acc: 0.85\n",
      "loss: 1.8246392907036184 acc: 0.84\n",
      "loss: 1.8300619282539712 acc: 0.85\n",
      "loss: 1.8590153293445288 acc: 0.83\n",
      "loss: 1.7724675898562368 acc: 0.9\n",
      "loss: 1.8053738866971516 acc: 0.81\n",
      "loss: 1.82568455309065 acc: 0.83\n",
      "loss: 1.8529328581102957 acc: 0.82\n",
      "loss: 1.8724796435693927 acc: 0.82\n",
      "loss: 1.8366575562837475 acc: 0.85\n",
      "loss: 1.829497655344632 acc: 0.86\n",
      "loss: 1.7646056941162893 acc: 0.85\n",
      "loss: 1.8593529753676898 acc: 0.78\n",
      "loss: 1.8180233648952862 acc: 0.81\n",
      "loss: 1.8538973790596245 acc: 0.79\n",
      "loss: 1.8011047615099198 acc: 0.89\n",
      "loss: 1.7905429372735977 acc: 0.79\n",
      "loss: 1.8057938482223337 acc: 0.83\n",
      "loss: 1.8214815972233325 acc: 0.83\n",
      "loss: 1.8009484177344235 acc: 0.87\n",
      "loss: 1.7952149427422401 acc: 0.9\n",
      "loss: 1.83876653742765 acc: 0.85\n",
      "loss: 1.8316155350247354 acc: 0.86\n",
      "loss: 1.8349021094009352 acc: 0.85\n",
      "loss: 1.8168086236600882 acc: 0.85\n",
      "loss: 1.7887614412859887 acc: 0.9\n",
      "loss: 1.849259401735963 acc: 0.84\n",
      "loss: 1.7834198285716736 acc: 0.86\n",
      "loss: 1.8336804947414245 acc: 0.85\n",
      "loss: 1.8284207830717634 acc: 0.79\n",
      "loss: 1.84915565250491 acc: 0.79\n",
      "loss: 1.8243454455909895 acc: 0.81\n",
      "loss: 1.817543560953541 acc: 0.86\n",
      "loss: 1.8205780077969649 acc: 0.85\n",
      "loss: 1.8262147050227453 acc: 0.85\n",
      "loss: 1.8023405034662858 acc: 0.86\n",
      "loss: 1.862536295165789 acc: 0.85\n",
      "loss: 1.8109023289097201 acc: 0.9\n",
      "loss: 1.810085005122167 acc: 0.84\n",
      "loss: 1.8012738067491534 acc: 0.85\n",
      "loss: 1.7639313729737691 acc: 0.91\n",
      "loss: 1.7663153881096272 acc: 0.9\n",
      "loss: 1.8107271914573653 acc: 0.86\n",
      "loss: 1.8361027925153415 acc: 0.84\n",
      "loss: 1.8225312772672473 acc: 0.85\n",
      "loss: 1.8273628411249871 acc: 0.88\n",
      "loss: 1.8183470422497772 acc: 0.87\n",
      "loss: 1.8257691027026612 acc: 0.84\n",
      "loss: 1.8555392961130417 acc: 0.8\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 1.8555\t Accuracy 0.8000\n",
      "loss: 1.8103399356810175 acc: 0.8\n",
      "loss: 1.8604539745260633 acc: 0.83\n",
      "loss: 1.7825961690054561 acc: 0.89\n",
      "loss: 1.845789104350298 acc: 0.8\n",
      "loss: 1.8026381811213728 acc: 0.83\n",
      "loss: 1.8230862871674216 acc: 0.87\n",
      "loss: 1.8381932149533566 acc: 0.83\n",
      "loss: 1.8620564733017508 acc: 0.8\n",
      "loss: 1.807108994344634 acc: 0.86\n",
      "loss: 1.8343260995146418 acc: 0.83\n",
      "loss: 1.8458664521141444 acc: 0.82\n",
      "loss: 1.8495463222064261 acc: 0.81\n",
      "loss: 1.8584199974149633 acc: 0.85\n",
      "loss: 1.7637621674907067 acc: 0.91\n",
      "loss: 1.8351556886055678 acc: 0.83\n",
      "loss: 1.7769634202065037 acc: 0.89\n",
      "loss: 1.8647037703238905 acc: 0.83\n",
      "loss: 1.7708780604210848 acc: 0.9\n",
      "loss: 1.7711568774217534 acc: 0.92\n",
      "loss: 1.8215777127913035 acc: 0.86\n",
      "loss: 1.7846864722439522 acc: 0.9\n",
      "loss: 1.843295767160648 acc: 0.77\n",
      "loss: 1.8265329498997844 acc: 0.82\n",
      "loss: 1.8473321110074026 acc: 0.87\n",
      "loss: 1.815221623988478 acc: 0.91\n",
      "loss: 1.8375193745605436 acc: 0.83\n",
      "loss: 1.8309344953223246 acc: 0.85\n",
      "loss: 1.8630751737635987 acc: 0.84\n",
      "loss: 1.7983240975947659 acc: 0.83\n",
      "loss: 1.8481738760154256 acc: 0.81\n",
      "loss: 1.82139727281799 acc: 0.85\n",
      "loss: 1.8369205323255655 acc: 0.85\n",
      "loss: 1.8341069224727677 acc: 0.83\n",
      "loss: 1.8166255919294403 acc: 0.87\n",
      "loss: 1.7972075625410953 acc: 0.78\n",
      "loss: 1.7960787542601766 acc: 0.88\n",
      "loss: 1.8042735763606823 acc: 0.88\n",
      "loss: 1.8127642528878487 acc: 0.81\n",
      "loss: 1.818469201133308 acc: 0.87\n",
      "loss: 1.814626273223594 acc: 0.86\n",
      "loss: 1.8250365503032437 acc: 0.84\n",
      "loss: 1.8406602717899505 acc: 0.84\n",
      "loss: 1.8410443860455654 acc: 0.87\n",
      "loss: 1.840450349693909 acc: 0.86\n",
      "loss: 1.8329776116122256 acc: 0.79\n",
      "loss: 1.8215170813418171 acc: 0.85\n",
      "loss: 1.8281692477806564 acc: 0.86\n",
      "loss: 1.8376369342834686 acc: 0.82\n",
      "loss: 1.783300450856634 acc: 0.85\n",
      "loss: 1.788274605401682 acc: 0.88\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 1.7883\t Accuracy 0.8800\n",
      "loss: 1.8198817219976484 acc: 0.82\n",
      "loss: 1.795444242650118 acc: 0.84\n",
      "loss: 1.8137996700647745 acc: 0.86\n",
      "loss: 1.8041125683550672 acc: 0.87\n",
      "loss: 1.7839832188555789 acc: 0.92\n",
      "loss: 1.8163009204938974 acc: 0.87\n",
      "loss: 1.7886751605491917 acc: 0.85\n",
      "loss: 1.7682711442544337 acc: 0.85\n",
      "loss: 1.8060042259922684 acc: 0.85\n",
      "loss: 1.794324929454655 acc: 0.85\n",
      "loss: 1.7927190723556548 acc: 0.9\n",
      "loss: 1.8311353012421177 acc: 0.81\n",
      "loss: 1.8349809649196025 acc: 0.86\n",
      "loss: 1.826551201054688 acc: 0.86\n",
      "loss: 1.8079844806993683 acc: 0.82\n",
      "loss: 1.8540601597878754 acc: 0.82\n",
      "loss: 1.8136401976367218 acc: 0.82\n",
      "loss: 1.8256552296025401 acc: 0.83\n",
      "loss: 1.8395955844431513 acc: 0.83\n",
      "loss: 1.819702667374263 acc: 0.81\n",
      "loss: 1.820801192846659 acc: 0.78\n",
      "loss: 1.7920146587522705 acc: 0.9\n",
      "loss: 1.820434985662718 acc: 0.82\n",
      "loss: 1.846537411298081 acc: 0.77\n",
      "loss: 1.811255616442146 acc: 0.87\n",
      "loss: 1.8455074836803107 acc: 0.82\n",
      "loss: 1.8438813943395151 acc: 0.83\n",
      "loss: 1.856561793981867 acc: 0.82\n",
      "loss: 1.81021491820283 acc: 0.79\n",
      "loss: 1.854172162840273 acc: 0.83\n",
      "loss: 1.8940838035664482 acc: 0.81\n",
      "loss: 1.8115001573855407 acc: 0.82\n",
      "loss: 1.8092928846816916 acc: 0.86\n",
      "loss: 1.793231971401072 acc: 0.82\n",
      "loss: 1.7838236082248387 acc: 0.87\n",
      "loss: 1.8410933513891516 acc: 0.82\n",
      "loss: 1.8361472278520725 acc: 0.82\n",
      "loss: 1.8457866235651494 acc: 0.85\n",
      "loss: 1.7951850431407066 acc: 0.84\n",
      "loss: 1.8268339624324474 acc: 0.84\n",
      "loss: 1.8164526611931584 acc: 0.89\n",
      "loss: 1.8106740666586132 acc: 0.84\n",
      "loss: 1.8238753740931009 acc: 0.85\n",
      "loss: 1.809350651668548 acc: 0.85\n",
      "loss: 1.8208526068537738 acc: 0.81\n",
      "loss: 1.8370136581274448 acc: 0.84\n",
      "loss: 1.818683263853034 acc: 0.89\n",
      "loss: 1.8339418006997534 acc: 0.88\n",
      "loss: 1.814796194006787 acc: 0.89\n",
      "loss: 1.7844195575745472 acc: 0.86\n",
      "loss: 1.7864670551426152 acc: 0.88\n",
      "loss: 1.7981295254733753 acc: 0.89\n",
      "loss: 1.830046030173481 acc: 0.87\n",
      "loss: 1.8142423755116273 acc: 0.95\n",
      "loss: 1.7901947257663813 acc: 0.89\n",
      "loss: 1.7598788007287425 acc: 0.83\n",
      "loss: 1.768607957861421 acc: 0.84\n",
      "loss: 1.8222510119759618 acc: 0.86\n",
      "loss: 1.7469459632661843 acc: 0.93\n",
      "loss: 1.7877106687605484 acc: 0.88\n",
      "loss: 1.7903405328692434 acc: 0.84\n",
      "loss: 1.8459533991817696 acc: 0.8\n",
      "loss: 1.851895453408929 acc: 0.84\n",
      "loss: 1.8659351343330601 acc: 0.87\n",
      "loss: 1.7745877402638448 acc: 0.93\n",
      "loss: 1.7919036693611314 acc: 0.86\n",
      "loss: 1.7489009703176694 acc: 0.9\n",
      "loss: 1.7834569998705758 acc: 0.85\n",
      "loss: 1.7747226328742074 acc: 0.9\n",
      "loss: 1.8624338340416964 acc: 0.88\n",
      "loss: 1.8085517262049342 acc: 0.9\n",
      "loss: 1.8196307636512747 acc: 0.76\n",
      "loss: 1.8478690788293246 acc: 0.88\n",
      "loss: 1.8315550377622498 acc: 0.86\n",
      "loss: 1.8460280983772637 acc: 0.81\n",
      "loss: 1.8446820263932289 acc: 0.78\n",
      "loss: 1.8726411936717668 acc: 0.81\n",
      "loss: 1.8090032001302654 acc: 0.93\n",
      "loss: 1.7853472685147316 acc: 0.89\n",
      "loss: 1.8133284796296545 acc: 0.83\n",
      "loss: 1.7723749619435776 acc: 0.93\n",
      "loss: 1.7379641823117857 acc: 0.92\n",
      "loss: 1.7771131635330475 acc: 0.88\n",
      "loss: 1.7737322941707214 acc: 0.85\n",
      "loss: 1.8038750487660917 acc: 0.89\n",
      "loss: 1.8495335378193136 acc: 0.9\n",
      "loss: 1.835301267394463 acc: 0.88\n",
      "loss: 1.7327294983687316 acc: 0.91\n",
      "loss: 1.7003503777386326 acc: 0.96\n",
      "loss: 1.7362047253459734 acc: 0.94\n",
      "loss: 1.7312508147392924 acc: 0.98\n",
      "loss: 1.7917148810527161 acc: 0.91\n",
      "loss: 1.7784200719219136 acc: 0.85\n",
      "loss: 1.7233817945636198 acc: 0.88\n",
      "loss: 1.8103875413712498 acc: 0.87\n",
      "loss: 1.7979488683974436 acc: 0.92\n",
      "loss: 1.8763062437825684 acc: 0.8\n",
      "loss: 1.7019542173060327 acc: 0.96\n",
      "loss: 1.8446723427069498 acc: 0.86\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8202\t Average training accuracy 0.8436\n",
      "Epoch [9]\t Average validation loss 1.7967\t Average validation accuracy 0.8778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train with momentum\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "loss2, acc2 = runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.8155867745567582 acc: 0.85\n",
      "loss: 1.837620336305423 acc: 0.89\n",
      "loss: 1.8411961540753672 acc: 0.83\n",
      "loss: 1.8689947289145963 acc: 0.8\n",
      "loss: 1.8823193771892812 acc: 0.81\n",
      "loss: 1.885617824595681 acc: 0.77\n",
      "loss: 1.8601998194586296 acc: 0.78\n",
      "loss: 1.8217576726391422 acc: 0.87\n",
      "loss: 1.8358411032336903 acc: 0.87\n",
      "loss: 1.8607495865032917 acc: 0.8\n",
      "loss: 1.8798843412778559 acc: 0.79\n",
      "loss: 1.8840342561640515 acc: 0.8\n",
      "loss: 1.9292142415956843 acc: 0.76\n",
      "loss: 1.8537818805412751 acc: 0.82\n",
      "loss: 1.8729220765092705 acc: 0.74\n",
      "loss: 1.8927451014543433 acc: 0.83\n",
      "loss: 1.8572141973482263 acc: 0.84\n",
      "loss: 1.8883884702103915 acc: 0.82\n",
      "loss: 1.8516235794249862 acc: 0.86\n",
      "loss: 1.8752958377922346 acc: 0.76\n",
      "loss: 1.89453375891051 acc: 0.8\n",
      "loss: 1.8968038173143154 acc: 0.74\n",
      "loss: 1.8751879611570827 acc: 0.82\n",
      "loss: 1.840807264305317 acc: 0.8\n",
      "loss: 1.8558418667843655 acc: 0.84\n",
      "loss: 1.8833637392651028 acc: 0.81\n",
      "loss: 1.8811931126348072 acc: 0.77\n",
      "loss: 1.845184095716588 acc: 0.82\n",
      "loss: 1.8320519613275303 acc: 0.83\n",
      "loss: 1.8756265984791545 acc: 0.84\n",
      "loss: 1.8293585545564937 acc: 0.9\n",
      "loss: 1.8780141705170506 acc: 0.8\n",
      "loss: 1.835255635715113 acc: 0.81\n",
      "loss: 1.8161511833833963 acc: 0.84\n",
      "loss: 1.844385714497025 acc: 0.87\n",
      "loss: 1.8771192569316761 acc: 0.8\n",
      "loss: 1.8055821550773 acc: 0.82\n",
      "loss: 1.8893074697858094 acc: 0.74\n",
      "loss: 1.8794065326405391 acc: 0.79\n",
      "loss: 1.8925170664922462 acc: 0.81\n",
      "loss: 1.8381776623846735 acc: 0.78\n",
      "loss: 1.865049527983301 acc: 0.79\n",
      "loss: 1.8639311300228758 acc: 0.78\n",
      "loss: 1.8997891005623695 acc: 0.78\n",
      "loss: 1.8939655820458343 acc: 0.75\n",
      "loss: 1.8593936772484778 acc: 0.8\n",
      "loss: 1.8373689302798233 acc: 0.84\n",
      "loss: 1.8414009071017556 acc: 0.85\n",
      "loss: 1.8922092753740016 acc: 0.77\n",
      "loss: 1.8562895194821258 acc: 0.78\n",
      "loss: 1.7683379018928438 acc: 0.9\n",
      "loss: 1.793412906509547 acc: 0.88\n",
      "loss: 1.7772710109171828 acc: 0.88\n",
      "loss: 1.7346629223620331 acc: 0.97\n",
      "loss: 1.7055025578215037 acc: 0.96\n",
      "loss: 1.7780973758854057 acc: 0.9\n",
      "loss: 1.7889385468150747 acc: 0.82\n",
      "loss: 1.7754476787562519 acc: 0.9\n",
      "loss: 1.8024814262992543 acc: 0.84\n",
      "loss: 1.8235954055909263 acc: 0.8\n",
      "loss: 1.7784201546476286 acc: 0.84\n",
      "loss: 1.6673404447917404 acc: 0.91\n",
      "loss: 1.6900210377449187 acc: 0.96\n",
      "loss: 1.6833857867512652 acc: 0.96\n",
      "loss: 1.7622774194359077 acc: 0.92\n",
      "loss: 1.8650399658362864 acc: 0.81\n",
      "loss: 1.858479402051434 acc: 0.81\n",
      "loss: 1.759809441920326 acc: 0.85\n",
      "loss: 1.7743818211051208 acc: 0.87\n",
      "loss: 1.7638495996317145 acc: 0.94\n",
      "loss: 1.7587045705386826 acc: 0.92\n",
      "loss: 1.7636389210803207 acc: 0.91\n",
      "loss: 1.8202809360811787 acc: 0.89\n",
      "loss: 1.7545289942505238 acc: 0.98\n",
      "loss: 1.8844498705323625 acc: 0.86\n",
      "loss: 1.805478899091441 acc: 0.89\n",
      "loss: 1.7926955293213198 acc: 0.93\n",
      "loss: 1.6669958490787014 acc: 0.97\n",
      "loss: 1.756705948637784 acc: 0.87\n",
      "loss: 1.7058176202449673 acc: 0.93\n",
      "loss: 1.7383924099703791 acc: 0.89\n",
      "loss: 1.7379243168441059 acc: 0.92\n",
      "loss: 1.7964833438900805 acc: 0.91\n",
      "loss: 1.7549143144067245 acc: 0.89\n",
      "loss: 1.7051681064324429 acc: 0.91\n",
      "loss: 1.728753312132284 acc: 0.95\n",
      "loss: 1.7517618857474202 acc: 0.98\n",
      "loss: 1.67976958057596 acc: 0.99\n",
      "loss: 1.671891255831556 acc: 0.98\n",
      "loss: 1.713630298767895 acc: 0.97\n",
      "loss: 1.773506065865959 acc: 0.82\n",
      "loss: 1.7103923615707828 acc: 0.91\n",
      "loss: 1.7866717234249418 acc: 0.9\n",
      "loss: 1.73422201964036 acc: 0.95\n",
      "loss: 1.7405502490259435 acc: 0.92\n",
      "loss: 1.7008857271796156 acc: 0.92\n",
      "loss: 1.8518139894595134 acc: 0.87\n",
      "loss: 1.8986754773493093 acc: 0.76\n",
      "loss: 1.9053220594808826 acc: 0.71\n",
      "loss: 1.8579189734249693 acc: 0.85\n",
      "Final test accuracy 0.8533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = runner.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAs/ElEQVR4nO3de3xV5Z3v8c8vN3Ih3AQCEhBa7nc04nWsiChWkWpnWmx79Djt8TijHduZVyvTq70cj1M7Z6YdbSl10E5Hi62CIiCCVqvWCwQLAQIoAkpMgAByJ+T2O3/sHdiEnWQHsvbaSb7v12u/9l7Pep61fjsbnt9ez9rrWebuiIiINJYWdgAiIpKalCBERCQuJQgREYlLCUJEROJSghARkbiUIEREJK7AEoSZzTOz3Wa2von1ZmY/N7MtZlZiZufHrJtuZpuj62YHFaOIiDQtyCOIx4Dpzay/DhgWfdwB/BLAzNKBh6PrRwO3mNnoAOMUEZE4AksQ7v4qsK+ZKjOB//KIt4AeZtYfmAxscfet7l4NzI/WFRGRJMoIcd8DgB0xy2XRsnjlFzW1ETO7g8gRCHl5eReMHDmy7SMVEemgVq9evcfd+8RbF2aCsDhl3kx5XO4+F5gLUFRU5MXFxW0TnYhIJ2BmHzS1LswEUQYMjFkuBMqBrCbKRUQkicL8mesi4Nbor5kuBg64ewWwChhmZkPMLAuYFa0rIiJJFNgRhJn9DrgS6G1mZcD3gUwAd58DLAU+DWwBjgK3R9fVmtndwAtAOjDP3TcEFaeIiMQXWIJw91taWO/AXU2sW0okgYhICGpqaigrK6OqqirsUKSNZGdnU1hYSGZmZsJtwjwHISIpqqysjPz8fAYPHoxZvN+NSHvi7uzdu5eysjKGDBmScDtNtSEip6mqquKcc85RcuggzIxzzjmn1UeEShAiEpeSQ8dyJp+nEoSIiMSlBCEikqA1a9awdGlyfz+zbNkyRowYwdChQ3nggQeSum+dpBaRs1L04xXsOVx9WnnvrlkUf2daCBEFZ82aNRQXF/PpT386Kfurq6vjrrvuYsWKFRQWFnLhhRdy4403Mnp0cuYv1RGEiJyVeMmhufJEbd++nZEjR/KVr3yFsWPH8sUvfpEXX3yRyy67jGHDhrFy5Ur27dvHZz7zGcaPH8/FF19MSUkJAPfddx+33XYb11xzDYMHD2bBggV885vfZNy4cUyfPp2amhoAVq9ezac+9SkuuOACrr32WioqKgC48soruffee5k8eTLDhw/ntddeo7q6mu9973s8+eSTTJw4kSeffJL77ruPn/70pydiHjt2LNu3b08o9kSsXLmSoUOH8olPfIKsrCxmzZrFs88+e1Z/19bQEYSINOsHz22gtPzgGbX9/K/ejFs++txufH/GmBbbb9myhT/84Q/MnTuXCy+8kCeeeILXX3+dRYsWcf/99zNw4EAmTZrEM888wx//+EduvfVW1qxZA8D777/Pyy+/TGlpKZdccglPP/00P/nJT7jppptYsmQJ119/PV/96ld59tln6dOnD08++STf/va3mTdvHgC1tbWsXLmSpUuX8oMf/IAXX3yRH/7whxQXF/PQQw8BkUR0prE/88wzvPzyy3z9618/rW1ubi5vvPEGH330EQMHnpx5qLCwkLfffrvFv1tbUYIQkZQ1ZMgQxo0bB8CYMWOYOnUqZsa4cePYvn07H3zwAU8//TQAV111FXv37uXAgQMAXHfddWRmZjJu3Djq6uqYPj1ye5qGtps3b2b9+vVMmxYZBqurq6N///4n9n3zzTcDcMEFF7B9+/Y2jx1gypQpJxJaPJHriU+VzF+XKUGISLNa+qY/ePaSJtc9+b8vOat9d+nS5cTrtLS0E8tpaWnU1taSkXF6F9bQgcbWzczMPFHe0NbdGTNmDG++Gf8op6F9eno6tbW1cetkZGRQX19/Yjn2OoOWYgdaPIIoLCxkx46Tdz8oKyvj3HPPjRtLEHQOQkTarSuuuILHH38cgFdeeYXevXvTrVu3hNqOGDGCysrKEwmipqaGDRuan/YtPz+fQ4cOnVgePHgw77zzDgDvvPMO27Zta1X8DUcQjR9vvPEGABdeeCHvvfce27Zto7q6mvnz53PjjTe2ah9nQwlCRM5K765ZrSpvS/fddx/FxcWMHz+e2bNn85vf/CbhtllZWTz11FPce++9TJgwgYkTJ57omJsyZcoUSktLT5yk/uxnP8u+ffuYOHEiv/zlLxk+fPjZvqVTZGRk8NBDD3HttdcyatQoPve5zzFmTMvnbtqKxRvjaq90wyCRtrFx40ZGjRoVdhjSxuJ9rma22t2L4tXXEYSIiMSlBCEiInEpQYiISFxKECIiElegCcLMppvZZjPbYmaz46zvaWYLzazEzFaa2diYddvNbJ2ZrTEznXkWEUmyIO9JnQ48DEwDyoBVZrbI3Utjqn0LWOPuN5nZyGj9qTHrp7j7nqBiFBGRpgV5BDEZ2OLuW929GpgPzGxUZzTwEoC7bwIGm1lBgDGJiJyxVJ3u++OPP+amm25i/PjxTJ48mfXr17fJvoOcamMAsCNmuQy4qFGdtcDNwOtmNhk4DygEdgEOLDczB37l7nPj7cTM7gDuABg0aFCbvgERScCDw+DI7tPL8/rCN95LfjwBStXpvu+//34mTpzIwoUL2bRpE3fddRcvvfTSWe8/yCOIeDNKNb4q7wGgp5mtAb4K/AVomPTkMnc/H7gOuMvMroi3E3ef6+5F7l7Up0+ftolcRBIXLzk0V54gTfed+HTfpaWlTJ0aGZ0fOXIk27dvZ9euXWf194dgjyDKgIExy4VAeWwFdz8I3A5gkZm0tkUfuHt59Hm3mS0kMmT1aoDxikg8z8+GnevOrO2j18cv7zcOrmv57mia7jux6b4nTJjAggULuPzyy1m5ciUffPABZWVlFBSc3Yh9kAliFTDMzIYAHwGzgC/EVjCzHsDR6DmKrwCvuvtBM8sD0tz9UPT1NcAPA4xVRFKQpvtObLrv2bNnc8899zBx4kTGjRvHpEmT4s5021qBJQh3rzWzu4EXgHRgnrtvMLM7o+vnAKOA/zKzOqAU+HK0eQGwMPqHyACecPdlQcUqIs1o6Zv+fd2bXnd701OBJ0LTfSc23Xe3bt149NFHgUhSGTJkCEOGDIkbc2sEej8Id18KLG1UNifm9ZvAsDjttgITgoxNRNq/hum+v/vd757VdN+XXHIJNTU1vPvuu83Olhpvuu/FixcDZzfdd1Nip/seMGAA8+fP54knnjit3v79+8nNzSUrK4tHHnmEK664IuG/Q3N0JbWInJ28vq0rb0OdebrvOXPmMGdO5Pv2xo0bGTNmDCNHjuT555/nZz/7WZvsX9N9i8hpNN13x6TpvkVEpE0oQYiISFxKECISV0cafpYz+zyVIETkNNnZ2ezdu1dJooNwd/bu3Ut2dnar2gX6M1cRaZ8KCwspKyujsrIy7FCkjWRnZ1NYWNiqNkoQInKazMzMNrnQSto3DTGJiEhcShAiIhKXEoSIiMSlBCEiInEpQYiISFxKECIiEpcShIiIxKUEISIicQWaIMxsupltNrMtZjY7zvqeZrbQzErMbKWZjU20rYiIBCuwBGFm6cDDwHXAaOAWMxvdqNq3gDXuPh64FfhZK9qKiEiAgjyCmAxscfet7l4NzAdmNqozGngJwN03AYPNrCDBtiIiEqAgE8QAYEfMclm0LNZa4GYAM5sMnAcUJtiWaLs7zKzYzIo1sZiISNsJMkFYnLLGcwc/APQ0szXAV4G/ALUJto0Uus919yJ3L+rTp89ZhCsiIrGCnM21DBgYs1wIlMdWcPeDwO0AZmbAtugjt6W2IiISrCCPIFYBw8xsiJllAbOARbEVzKxHdB3AV4BXo0mjxbYiIhKswI4g3L3WzO4GXgDSgXnuvsHM7oyunwOMAv7LzOqAUuDLzbUNKlYRETmddaRbChYVFXlxcXHYYYiItBtmttrdi+Kt05XUIiISlxKEiIjEpQQhIiJxKUGIiEhcShAiIhKXEoSIiMSlBCEiInEpQYiISFxKECIiEpcShIiIxKUEISIicSlBiIhIXEoQIiISlxKEiIjEpQQhIiJxKUGIiEhcgSYIM5tuZpvNbIuZzY6zvruZPWdma81sg5ndHrNuu5mtM7M1Zqa7AImIJFlgtxw1s3TgYWAaUAasMrNF7l4aU+0uoNTdZ5hZH2CzmT3u7tXR9VPcfU9QMYqISNOCPIKYDGxx963RDn8+MLNRHQfyzcyArsA+oDbAmEREJEGBHUEAA4AdMctlwEWN6jwELALKgXzg8+5eH13nwHIzc+BX7j433k7M7A7gDoBBgwa1XfQindWDw+DI7tPL8/rCN97rePuVJgWZICxOmTdavhZYA1wFfBJYYWavuftB4DJ3LzezvtHyTe7+6mkbjCSOuQBFRUWNty9ydjpjZxlvv82Vt/f9Quf8nBMQZIIoAwbGLBcSOVKIdTvwgLs7sMXMtgEjgZXuXg7g7rvNbCGRIavTEoR0AuosTy13h7oaqK+F+hqor2vb5ea8/m+Jxe9t/F1t9WOQlglpGZCWDukNrzNbuRzzSI/ZXms+Z3fw+jiPxuUxyzTRJsykmIAgE8QqYJiZDQE+AmYBX2hU50NgKvCamRUAI4CtZpYHpLn7oejra4AfBhirJCKsjjrR/0TuUFcN1Ueg5ihUH408n3h9pNHz0UZ145Q350d9wWIPlKOvEy07pdziro7rBz1aqBCgF+8LZ7/P3RPgxlv4g/+w96kdfScSWIJw91ozuxt4AUgH5rn7BjO7M7p+DvAj4DEzW0fkU7rX3feY2SeAhZFz12QAT7j7sqBilQS15ttOXS3UVkU67Nqq6ON4zCO6XNdoOd665vx80qmdute17j1lZENmLmTlRZ9zITMP8vpEXu95t+m2F9958vWJb8wep4wmyrzpsrfnNL3fK/858q234Rt1embbLv/r8Kb3/e2dTa87TUuZrpH/U9D0uq+XRo9wYh510aOf+ppWLtfGlEW39epPmt73pV8FS4skc0uLecQs03hdbJ2m1qXBwv/dur9RkgV5BIG7LwWWNiqbE/O6nMjRQeN2W4EJQcYmLTh+GA7vijwO7Yw8N+dfR0Y79mhCaG1HHU96l0gH3pxzzz/ZqWflNursYzv93FMTQENZWnrz27+ve9PrpgV4UNtsgjjtkqLkycwJZ7/dBwS7/eYSxNXfD26/nTlBSADOZpinvh6O7oXD0Q7/0K7o693RJLA7snxoV2S4pTWGTYt05ulZkeeMbMjoEvOILqfHLmdDRlZM3Zj26VmQFv0VdnOd9F//Z+vilJbl9W3631hH3K80SQmivWlumGfHqmgH36izbzgSOLw7/jf7Lt2ga1/o2g/OnQRdCyKP/H4ny/P7wU+GNB3Xjf/RNu8v1XTGzjKkX88UHf8Fe6qqTyvvnZFF0FMpVHp3+tiBuOVbt+7leG09VTV1HK+tjz7qOF5TT1X0uaGsqia6rrY+Wl538rnxNmrqeMWa3m+fgN9zIpQgOpL/vDpmwSLj6PnRzr5g7MnXp3T+BZGhmFTWCTvLVPiJY7LtOXx6cohXXl/vHK2p4+jxWo5U13HkeC1Hq+s4Ul3L0eMNz5F1R6trOXI8+lxdd0r5ybp1HDr+y6YDm/tWi7FnZaTRJSON7Mx0ukRfd8lIp0tmGtkZ6fTMyzpZFlPvwteb3u/2FvcaPCWIjuQLvz/Z+ef2hvQ2/njD6qg7YWcZpqIfr4jbWffumkXxd6ad8Xbdnaqaeg4cq+FgVQ0HjtVw4OjJ18254icvn+jsj9Ukfn4rPc3Iy0onr0sGuTHPBfnZ5PbOIC8rndysDOb9eVuT23jiKxfRJTPSuWdnnuzkGxJAVnoaaWmtPCEf9cjrTe83FShBdCTDrw12++qoO4XmvsnX1TsHYzr4g8dqT+nwDx6LPlfVnliOrV9Td2Y/Ez1/UA9yu5zs0PO6RJ67Nur4TzxnZZDbJZ2s9DTMWu68m0sQlw7tfUYxdwRKEO3Jkb1hRyBJEtS3eOBEJ78/2pkfOFbD/qPVJzr35nzyW0ubXZ+RZnTLyaR7TibdsjPolpPJgJ450eVoeU5Go+XI8/k/WtHkdv991qQzeq+prnfXrCY/51SgBNFeVB+BJz7X9Hr90qNDaWk83t05dLyWA0dPdvKRjj52uTpu+aGqM58P82tXDzutY4/t8HOz0hP6xp5qwuqozzbZB63FBGFmNwBLYybRk2Srq4Hf3wrl78DnH4dRN4QdkQSkpq6e8v3Hmq0z6YfLOVhVS11908M1WelpdMvJpEdupBMv6JbNiIL8E516Q3nDc+SRRfecTIZ/5/kmt/u1q5u5iO4shfltOtU76rAkcgQxC/iZmT0NPOruGwOOSWLV18Mzfw9bXoQZP1dy6AAOHKthx76jfLjvKB/sjTx/uO8IH+47Svn+qmY7foAbxp97slOP6eBPdPo5WWRnJjb2nkrUSaeeFhOEu3/JzLoBtwCPRqfffhT4nbsfCjrATs0dln8H1v0ervouXHBb2BF1OmdyLqCu3tl5sIoP9h5hxylJIPLYf/TUcf5eeVkM6pXLpIE9+czEXAb2yuWbT5U0GdOPPjP27N5UC1J9XFySJ6FzEO5+MHoEkQN8DbgJ+IaZ/dzdO+gVUingz/8Obz0MF90Jf/VPYUfTKTV3LmDTzoN8sPfoKUlgx76jlH18jOq6kyOyGWnGgJ45DOqVy/Xj+nPeObkM6hVJBIN65ZKfnXna9ptLEEHTN3lpkMg5iBnA3xK5X8NvgcnRKbhzgY2AEkQQ3vltZObMsX8N1/7fRrOBSjLU1jV/2m36v7924nV+dgbnnZPLyP75XDOmH4N65Z5IBP27Z5OR3rqbN+pbvKSCRI4g/gb4t8Y363H3o2b2t8GE1cltWgrP/QN88ir4zC9Pzkkkgaivd8o+Psa7uw6xedehyPPOQ2ytbH4+qv+4ZdKJRNA9J7NNx/z1LV5SQSIJ4vtARcOCmeUABe6+3d1fCiyyzuqDN+Cp26H/RPjcbyOT2UmbcHd2HTweSQI7I8ngvV2HeHfX4VOuzh3QI4dhBV351PA+/OrVrU1ub8aEc5MRtkhoEkkQfwAujVmui5ZdGEhEndmuDfDELOg+EL74B+jSNeyIUsKZnCjee/hkInh39+ETCSH2GoDeXbswol9XZk0eyPCCfIYX5DOsoCvdYs4JNJcgRDq6RBJEhruf+N/p7tVmpq+1be3jD+C3N0cmzvsfCyCv817e31hzJ4oPHKs5cRTQMDT03u5Dp7TpnpPJiIJ8bpxwLiP65Z9IBr3yWv5nrHMB0pklkiAqzexGd18EYGYzgT3BhtXJHK6E394Etcfg9mXQY1DYEbUbE36w/MTr3Kx0hhXkc9XIvgwvyD+RDPrmdznj8wM6FyCdWSIJ4k7gcTN7iMg9BHcAtyaycTObDvyMyC1HH3H3Bxqt7w78NzAoGstP3f3RRNp2GMcPweN/DQc/glufhYLRYUeUMtydjRXNX2rzzekjGBE9IhjQI+eMZ9UUkdMlcqHc+8DFZtYVsEQvjjOzdOBhYBpQBqwys0XuXhpT7S6g1N1nmFkfYLOZPU7kPEdLbdu/2uPw5Jdg5zqY9QQMujjsiEJXW1fPqu0fs7x0JytKd1H2cfPTTvz9lUOTFJlI55PQhXJmdj0wBshuOFR395ZuyDsZ2BK9vzRmNh+YCcR28g7kW2SjXYF9QC1wUQJt27f6elh4J2x9BWb+AkZMDzui0BytruXVdytZXrqLP27azf6jNWRlpHH50N7cPWUosxesCztEkU4pkQvl5gC5wBTgEeCvgZUJbHsAkeGoBmVEOv5YDwGLgHIgH/i8u9ebWSJtG+K7A7gDYNCgdjJ27w7L7oUNCyI3vp/0xbAjSro9h4/z0sZdLN+wi9e37OF4bT3dczKZOrIv00YXcMXwPuR1ifzz/OnyzTpRLBKCRI4gLnX38WZW4u4/MLN/BRYk0C7eYHDjWciuBdYAVxG5UnuFmb2WYNtIoftcYC5AUVHRmd2NJNle/SmsnAuX3A2X3RN2NEmzbc8Rlm+IDB2t/vBj3CPXHNwyeRDXjCngwsG9yIxzxbFOFIuEI5EEURV9Pmpm5wJ7gWbuXn9CGTAwZrmQyJFCrNuBB9zdgS1mtg0YmWDb9qn4UXj5xzB+Fkz7UdjRBKq+3llbtp8VpbtYXrqLLbsPAzDm3G7cM3UY00YXMLp/t3Y366hIZ5FIgnjOzHoADwLvEPkm/+sE2q0ChpnZEOAjItOGf6FRnQ+BqcBrZlYAjAC2AvsTaNv+lC6CJf8Iw66BmQ91yCk0jtfW8eb7e1leuosXS3ex+9Bx0tOMi4b04ksXDeLq0QUU9swNO0wRSUCzCcLM0oCX3H0/8LSZLQay3f1ASxt291ozuxt4gchPVee5+wYzuzO6fg7wI+AxM1tHZFjpXnffE933aW3P9E2mhG2vwdNfhgEXwN88Bumnz+CZ6pq6ovmcvCy+N2M0y0t38afNlRw+XktuVjpXjujDtNEFTBnRlx65Ol8g0t5YZHSnmQpmb7r7JUmK56wUFRV5cXFx2GGcrqIEHrseup0Ltz8Pub3CjuiMDJ69pNn1vbt2Ydrovlwzuh+XfPIcsjPTkxSZiJwpM1vt7kXx1iUyxLTczD4LLPCWsomcbt9W+O/PQpdu8KUF7TY5tOTpv7uUSQN76EI1kQ4kkQTxj0AeUGtmVUSGgtzduwUaWUdweHdkfqX6Gvifi6H7gLAjOmNVMbOdxnPBeT2TFImIJEsiV1LnJyOQDqfqYOTI4fAuuO056DMi7IjO2Ovv7eHbz+hiNZHOJpEL5a6IV974BkISo6YK5n8BdpfCLU9CYdzhvZS39/BxfrxkIwv/8hFDeueFHY6IJFkiQ0zfiHmdTWQKjdVELm6TxurrYMFXYPtrcPOvYdjVYUfUau7OH1aXcf/SjRw5Xss/XDWUv58ylMv/5Y+6olmkE0lkiGlG7LKZDQR+ElhE7Zk7LPkn2Phc5D7S4z8XdkSt9n7lYb61YB1vb9vHhYN7cv9N4xhWEBll1BXNIp1LQpP1NVIGjG3rQDqEV/4vrH4ULv86XPL3YUfTKsdr6/jFy+/zy1feJzszjQduHsfnigbqV0kinVgi5yD+g5PzIKUBE4G1AcbUPjw4DI7sPr08Ixumfj/58ZyFt7bu5VsL17G18ggzJ57Ld64fTZ/8LmGHJSIhS+QIIvbKs1rgd+7+54DiaT/iJQeA2ipoJ3MLfXykmvuXbuQPq8sY2CuH3/ztZD41vE/YYYlIikgkQTwFVLl7HURuBGRmue5+NNjQJCjuzjNrPuLHizey/1gNd37qk9wzdRg5WbryWUROSiRBvARcDRyOLucAy4FLgwpKgrN9zxG+88x6Xt+yh4kDe/DfN49jVH9d8ygip0skQWS7e0NywN0Pm5mm42xnqmvr+fVrW/n5S++RlZ7Gj2aO4QsXnUe6TkKLSBMSSRBHzOx8d38HwMwuAJq/UbCklOLt+/jWwnW8u+swnx7Xj+/PGENBt+ywwxKRFJdIgvga8Acza7hhT3/g84FF1F7k9Y1/ojqvb/JjacKBYzX8y7JNPPH2h5zbPZv/vK2IqaMKwg5LRNqJRC6UW2VmI4nczMeATe5eE3hkqe4b74UdQZPcnSXrKvjBc6XsPXycL18+hH+cNvzEPZ5FRBKRyHUQdwGPu/v66HJPM7vF3X8ReHTSajv2HeV7z67n5c2VjBvQnUf/54WMHdA97LBEpB1K5Cvl/3L3hxsW3P1jM/tfQIsJwsymAz8jcle4R9z9gUbrvwF8MSaWUUAfd99nZtuBQ0AdUNvUDS0koraunnl/3sa/rXgPM/juDaO57ZLzyEjveLc1FZHkSCRBpJmZNdwsyMzSgRZnZ4vWexiYRmR6jlVmtsjdSxvquPuDRO51jZnNAL7u7vtiNjOl4RakEtHUbT/T04y6eufqUQX8cOYYzu2RE0J0ItKRJJIgXgB+b2ZziEy5cSfwfALtJgNb3H0rgJnNB2YCpU3UvwX4XQLb7dTiJQeAunpnzpfO59ox/bB2ciW3iKS2RMYf7iVysdzfAXcBJUQulmvJAGBHzHJZtOw00esqpgNPxxQ7kdudrjazO5raiZndYWbFZlZcWVmZQFgd1/Sx/ZUcRKTNtJgg3L0eeAvYChQBU4GNCWw7Xk/V1D2tZwB/bjS8dJm7nw9cB9zVzI2L5rp7kbsX9emjeYRERNpKk0NMZjYcmEVk6Gcv8CSAu09JcNtlwMCY5UKgvIm6s2g0vOTu5dHn3Wa2kMiQle5iJyKSJM0dQWwicrQww90vd/f/IPKLokStAoaZ2RAzyyKSBBY1rmRm3YFPAc/GlOWZWX7Da+AaYH0r9i0iImepuQTxWWAn8LKZ/drMphJ/2Cgud68F7iZyknsj8Ht332Bmd5rZnTFVbwKWu/uRmLIC4HUzWwusBJa4+7JE992RNXV7T932U0TamkV/vdp0hcg3+M8QGWq6CvgNsNDdlwceXSsVFRV5cXFxyxXbMXfn8n95meEFXXn09slhhyMi7ZyZrW7qOrNETlIfcffH3f0GIucR1gCz2zZESdRfduzno/3HuH78uWGHIiIdXKsus3X3fe7+K3e/KqiApHlLSirISk/jmjGadE9EgqV5GNqR+npnSUkFVwzvQ7fszLDDEZEOTgmiHVn94cfsPFjFjAn9ww5FRDoBJYh2ZPHacrpkpOmeDiKSFEoQ7URdvbN0/U6mjOhLV93XQUSSQAminVi5bR+Vh45zg4aXRCRJlCDaicUl5eRkpnPVyNS5pamIdGxKEO1AbV09y9bv5KpRfcnN0vCSiCSHEkQ78NbWfew9Us2M8RpeEpHkUYJoBxaXlJOXlc6VIzS8JCLJowSR4mrq6lm2YSfTRheQnZkedjgi0okoQaS417fsYf/RGs29JCJJpwSR4paUVJCfncEVw3uHHYqIdDJKECnseG0dL2zYyTWj+9ElQ8NLIpJcShAp7LV393CoqlYXx4lIKAJNEGY23cw2m9kWMzvtHhJm9g0zWxN9rDezOjPrlUjbzmBxSTndczK57JMaXhKR5AssQZhZOvAwcB0wGrjFzEbH1nH3B919ortPBP4Z+JO770ukbUdXVVPHitJdTB/Tj6wMHeiJSPIF2fNMBra4+1Z3rwbmAzObqX8L8LszbNvhvLK5kiPVdRpeEpHQBJkgBgA7YpbLomWnMbNcYDrw9Bm0vcPMis2suLKy8qyDThWLS8rplZfFJZ84J+xQRKSTCjJBWJwyb6LuDODP7r6vtW3dfa67F7l7UZ8+fc4gzNRztLqWlzbuZvrYfmSka3hJRMIRZO9TBgyMWS4EypuoO4uTw0utbdvhvLypkmM1ddyguZdEJERBJohVwDAzG2JmWUSSwKLGlcysO/Ap4NnWtu2oFpeU0ye/CxcN0fCSiIQnsLmj3b3WzO4GXgDSgXnuvsHM7oyunxOtehOw3N2PtNQ2qFhTyeHjtfxx025mXTiQ9LR4I20iIskR6M0F3H0psLRR2ZxGy48BjyXStjN4aeMujtfWa+4lEQmdzoCmmMUlFfTrlk3ReT3DDkVEOjkliBRysKqGP22u5NPj+pOm4SURCZkSRApZsWEX1XX1ujhORFKCEkQKWVxSzoAeOUwa2CPsUERElCBSxYGjNbz23h6uH98fMw0viUj4lCBSxAsbdlJb77o4TkRShhJEiniupJxBvXIZN6B72KGIiABKEClh7+HjvPH+Xg0viUhKUYJIAS9s2EWdhpdEJMUoQaSAxSXlfKJ3HqP7dws7FBGRE5QgQlZ56Dhvbd3LDRpeEpEUowQRsufXV1DvaO4lEUk5ShAhW1xSwbC+XRnRLz/sUERETqEEEaJdB6tYtX0fN+joQURSkBJEiJaUVOAO1+vXSyKSgpQgQrS4pJyR/fIZ2rdr2KGIiJxGCSIkH+0/xjsf7mfGBA0viUhqCjRBmNl0M9tsZlvMbHYTda40szVmtsHM/hRTvt3M1kXXFQcZZxiWllQA6OI4EUlZgd1y1MzSgYeBaUAZsMrMFrl7aUydHsAvgOnu/qGZ9W20mSnuvieoGMO0uKSccQO6c945eWGHIiISV5BHEJOBLe6+1d2rgfnAzEZ1vgAscPcPAdx9d4DxpIwP9x5lbdkBHT2ISEoLMkEMAHbELJdFy2INB3qa2StmttrMbo1Z58DyaPkdTe3EzO4ws2IzK66srGyz4IO0ZF1keOnT45QgRCR1BTbEBMSbN8Lj7P8CYCqQA7xpZm+5+7vAZe5eHh12WmFmm9z91dM26D4XmAtQVFTUePspaXFJORMH9mBgr9ywQxERaVKQRxBlwMCY5UKgPE6dZe5+JHqu4VVgAoC7l0efdwMLiQxZtXvb9hxhQ/lBDS+JSMoLMkGsAoaZ2RAzywJmAYsa1XkW+CszyzCzXOAiYKOZ5ZlZPoCZ5QHXAOsDjDVpFq+N5EhdHCciqS6wISZ3rzWzu4EXgHRgnrtvMLM7o+vnuPtGM1sGlAD1wCPuvt7MPgEsjM5umgE84e7Lgoo1mZasq6DovJ70754TdigiIs0K8hwE7r4UWNqobE6j5QeBBxuVbSU61NSRbNl9iE07D3HfjNFhhyIi0iJdSZ1Ez62twEy/XhKR9kEJIkncncUl5Vw0pBd9u2WHHY6ISIuUIJJk865DvF95RDcGEpF2QwkiSRavrSDN4Lqx/cIORUQkIUoQSdAwvHTpJ3vTu2uXsMMREUmIEkQSbCg/yPa9R3VxnIi0K0oQSbC4pIKMNOPaMRpeEpH2QwkiYA3DS5cN7U3PvKywwxERSZgSRMDWlh2g7ONjGl4SkXZHCSJgi9eWk5luXKPhJRFpZ5QgAlRf7yxdV8EVw/rQPScz7HBERFpFCSJAf9nxMeUHqrhhgoaXRKT9UYII0HNrK8jKSOPqUQVhhyIi0mpKEAGpiw4vTRnRh/xsDS+JSPujBBGQ4u372H3ouOZeEpF2SwkiIItLKsjOTGPqyL5hhyIickYCTRBmNt3MNpvZFjOb3USdK81sjZltMLM/taZtqqqtq+f59RVMHVlAXpdA78kkIhKYwHovM0sHHgamAWXAKjNb5O6lMXV6AL8Aprv7h2bWN9G2qeztbfvYc7haF8eJSLsW5BHEZGCLu29192pgPjCzUZ0vAAvc/UMAd9/dirYpa3FJBblZ6Vw5QsNLItJ+BZkgBgA7YpbLomWxhgM9zewVM1ttZre2oi0AZnaHmRWbWXFlZWUbhX7maurqWba+gqtHFZCTlR52OCIiZyzIAXKLU+Zx9n8BMBXIAd40s7cSbBspdJ8LzAUoKiqKWyeZ3nh/Lx8frdHwkoi0e0EmiDJgYMxyIVAep84edz8CHDGzV4EJCbZNSYvXlpPfJYMrhvcJOxQRkbMS5BDTKmCYmQ0xsyxgFrCoUZ1ngb8yswwzywUuAjYm2DblVNfW88KGnUwbXUB2poaXRKR9C+wIwt1rzexu4AUgHZjn7hvM7M7o+jnuvtHMlgElQD3wiLuvB4jXNqhY28rrWyo5WFWruZdEpEMI9Ef67r4UWNqobE6j5QeBBxNpm+oWr62ge04mlw/V8JKItH+6krqNVNXUsbx0F9eOKSArQ39WEWn/1JO1kVffreTw8VrNvSQiHYYSRBtZXFJBz9xMLv3kOWGHIiLSJpQg2sCx6jpe3LiL6WP7k5muP6mIdAzqzdrAy5t3c7S6jhm6OE5EOhAliDawpKSC3l2zmDykV9ihiIi0Gc1FfYaKfryCPYerTykb+u3n6d01i+LvTAspKhGRtqMjiDPUODm0VC4i0t4oQYiISFzmHvoEqG3GzA4Bm5Oxr6x+Qy9oal31zi2rkxED0BvYk6R9pQq9585B7zl5znP3uNM/dLRzEJvdvSjsIJLFzIo70/sFvefOQu85NWiISURE4lKCEBGRuDpagpgbdgBJ1tneL+g9dxZ6zymgQ52kFhGRttPRjiBERKSNKEGIiEhcHSJBmNl0M9tsZlvMbHbY8QTNzAaa2ctmttHMNpjZPWHHlCxmlm5mfzGzxWHHkgxm1sPMnjKzTdHP+5KwYwqamX09+u96vZn9zsyyw46prZnZPDPbbWbrY8p6mdkKM3sv+twzzBihAyQIM0sHHgauA0YDt5jZ6HCjClwt8E/uPgq4GLirE7znBvcAG8MOIol+Bixz95HABDr4ezezAcA/AEXuPpbIPelnhRtVIB4Dpjcqmw285O7DgJeiy6Fq9wkCmAxscfet7l4NzAdmhhxToNy9wt3fib4+RKTTGBBuVMEzs0LgeuCRsGNJBjPrBlwB/CeAu1e7+/5Qg0qODCDHzDKAXKA85HjanLu/CuxrVDwT+E309W+AzyQzpng6QoIYAOyIWS6jE3SWDcxsMDAJeDvkUJLh34FvAvUhx5EsnwAqgUejw2qPmFle2EEFyd0/An4KfAhUAAfcfXm4USVNgbtXQORLINA35Hg6RIKwOGWd4re7ZtYVeBr4mrsfDDueIJnZDcBud0/WPFepIAM4H/ilu08CjpACww5Bio67zwSGAOcCeWb2pXCj6rw6QoIoAwbGLBfSAQ9JGzOzTCLJ4XF3XxB2PElwGXCjmW0nMox4lZn9d7ghBa4MKHP3hqPDp4gkjI7samCbu1e6ew2wALg05JiSZZeZ9QeIPu8OOZ4OkSBWAcPMbIiZZRE5obUo5JgCZWZGZFx6o7v/v7DjSQZ3/2d3L3T3wUQ+4z+6e4f+ZunuO4EdZjYiWjQVKA0xpGT4ELjYzHKj/86n0sFPzMdYBNwWfX0b8GyIsQAdYDZXd681s7uBF4j84mGeu28IOaygXQb8D2Cdma2Jln3L3ZeGF5IE5KvA49EvP1uB20OOJ1Du/raZPQW8Q+TXen8hBaegOFtm9jvgSqC3mZUB3wceAH5vZl8mkij/JrwIIzTVhoiIxNURhphERCQAShAiIhKXEoSIiMSlBCEiInEpQYiISFxKECKtYGZ1ZrYm5tFmVzab2eDY2T1Fwtbur4MQSbJj7j4x7CBEkkFHECJtwMy2m9m/mNnK6GNotPw8M3vJzEqiz4Oi5QVmttDM1kYfDdNJpJvZr6P3Q1huZjmhvSnp9JQgRFonp9EQ0+dj1h1098nAQ0RmniX6+r/cfTzwOPDzaPnPgT+5+wQi8ys1XP0/DHjY3ccA+4HPBvpuRJqhK6lFWsHMDrt71zjl24Gr3H1rdCLFne5+jpntAfq7e020vMLde5tZJVDo7sdjtjEYWBG9YQxmdi+Q6e4/TsJbEzmNjiBE2o438bqpOvEcj3ldh84TSoiUIETazudjnt+Mvn6Dk7fM/CLwevT1S8DfwYn7bHdLVpAiidK3E5HWyYmZQRci94tu+KlrFzN7m8gXr1uiZf8AzDOzbxC5O1zDbKz3AHOjM3fWEUkWFUEHL9IaOgch0gai5yCK3H1P2LGItBUNMYmISFw6ghARkbh0BCEiInEpQYiISFxKECIiEpcShIiIxKUEISIicf1/ShMxjmBZizsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({\n",
    "    \"momentum=0\": [loss1, acc1],\n",
    "    \"momentum=0.9\": [loss2, acc2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3)\n",
      "(1, 3)\n",
      "[[-0.30299694]\n",
      " [ 0.13746476]\n",
      " [-0.54639307]]\n",
      "[[ 0.53133751  1.04090438 -0.37768692 -0.8025295   0.54392463  0.19368977\n",
      "   0.96712941 -0.6222014   0.31706225  0.25087925]\n",
      " [-0.23243148  0.69716104  0.02804984 -1.32929883  0.08165972 -0.31759476\n",
      "   0.04190669 -0.98785723  0.24905791 -0.07940176]\n",
      " [-1.31209006 -0.49733679  0.61494164  0.46524103  0.17914572 -0.5483098\n",
      "  -1.55780342 -0.27046255 -0.33526063  0.33773651]]\n",
      "[[ 0.22834057  0.73790744 -0.68068385 -1.10552644  0.24092769 -0.10930717\n",
      "   0.66413247 -0.92519834  0.01406531 -0.05211768]\n",
      " [-0.09496672  0.83462581  0.1655146  -1.19183407  0.21912449 -0.18013\n",
      "   0.17937145 -0.85039246  0.38652268  0.05806301]\n",
      " [-1.85848312 -1.04372985  0.06854858 -0.08115203 -0.36724735 -1.09470286\n",
      "  -2.10419649 -0.81685561 -0.88165369 -0.20865655]]\n",
      "prob_M shape: (3, 10)\n",
      "t_M [[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "labels [1 1 0 1 0 1 0 0 1 2]\n",
      "t_M shape: [[0 0 1 0 1 0 1 1 0 0]\n",
      " [1 1 0 1 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/j1y8x0s17rggg6mtzyntr9500000gn/T/ipykernel_41381/1251500387.py:24: DeprecationWarning: This function is deprecated. Please call randint(0, 2 + 1) instead\n",
      "  labels = np.random.random_integers(0, num_output-1, (N, ))\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "import numpy as np\n",
    "num_input = 6 # m\n",
    "num_output = 3 # K\n",
    "N = 10\n",
    "raw_std = (2 / (num_input + num_output))**0.5\n",
    "init_std = raw_std * (2**0.5)\n",
    "W = np.random.normal(0, init_std, (num_input, num_output))\n",
    "b = np.random.normal(0, init_std, (1, num_output))\n",
    "Input = np.random.normal(0, init_std, (N, num_input)) # shape (N, m)\n",
    "print(W.shape)\n",
    "print(b.shape)\n",
    "\n",
    "X = Input.transpose() # shape(m, N)\n",
    "W_T = W.transpose() # shape(K,m)\n",
    "b_T = b.transpose() # shape(K,1)\n",
    "print(b_T)\n",
    "prob_M = np.dot(W_T,X)  # shape(K,N)\n",
    "print(prob_M)\n",
    "prob_M = prob_M + b_T\n",
    "print(prob_M)\n",
    "\n",
    "print('prob_M shape:', prob_M.shape)\n",
    "labels = np.random.random_integers(0, num_output-1, (N, )) \n",
    "t_M = np.zeros([num_output, N], dtype=int) # shape(K,N)\n",
    "print ('t_M',t_M)\n",
    "print ('labels',labels)\n",
    "\n",
    "for i in range(N):\n",
    "    t_M[labels[i]][i] = 1 # 将第i个元素的k类行设置为1\n",
    "print('t_M shape:', t_M) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "[18 22 26]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(12).reshape(4,3)\n",
    "print(a)\n",
    "s = np.sum(a, axis=0)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "132e6111d8ffa10be32dcc2d80c102638828bf77ba52c5acca41a3a150f1c1d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
